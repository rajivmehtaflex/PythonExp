{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Gajraj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base FAISS Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install faiss-cpu sentence-transformers\n",
    "# !pip install pandas numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = [['Where are your headquarters located?', 'location'],\n",
    "['Throw my cellphone in the water', 'random'],\n",
    "['Network Access Control?', 'networking'],\n",
    "['Address', 'location']]\n",
    "df = pd.DataFrame(data, columns = ['text', 'category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "text = df['text']\n",
    "encoder = SentenceTransformer(\"paraphrase-mpnet-base-v2\")\n",
    "vectors = encoder.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "vector_dimension = vectors.shape[1]\n",
    "index = faiss.IndexFlatL2(vector_dimension)\n",
    "faiss.normalize_L2(vectors)\n",
    "index.add(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "search_text = 'where is your office?'\n",
    "search_vector = encoder.encode(search_text)\n",
    "_vector = np.array([search_vector])\n",
    "faiss.normalize_L2(_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = index.ntotal\n",
    "distances, ann = index.search(_vector, k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({'distances': distances[0], 'ann': ann[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join by: df1.ann == data.index \n",
    "merge = pd.merge(results, df, left_on='ann', right_index=True)\n",
    "merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels  = df['category']\n",
    "category = labels[ann[0][0]]\n",
    "category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### txtai Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/neuml/txtai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from txtai.embeddings import Embeddings\n",
    "# Create embeddings model, backed by sentence-transformers & transformers\n",
    "embeddings = Embeddings({\"path\": \"sentence-transformers/nli-mpnet-base-v2\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"US tops 5 million confirmed virus cases\",\n",
    "        \"Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg\",\n",
    "        \"Beijing mobilises invasion craft along coast as Taiwan tensions escalate\",\n",
    "        \"The National Park Service warns against sacrificing slower friends in a bear attack\",\n",
    "        \"Maine man wins $1M from $25 lottery ticket\",\n",
    "        \"Make huge profits without work, earn up to $100,000 a day\"]\n",
    "\n",
    "print(\"%-20s %s\" % (\"Query\", \"Best Match\"))\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for query in (\"feel good story\", \"climate change\", \"public health story\", \"war\", \"wildlife\", \"asia\", \"lucky\", \"dishonest junk\"):\n",
    "    # Get index of best section that best matches query\n",
    "    uid = embeddings.similarity(query, data)[0][0]\n",
    "\n",
    "    print(\"%-20s %s\" % (query, data[uid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.index([(uid, text, None) for uid, text in enumerate(data)])\n",
    "\n",
    "print(\"%-20s %s\" % (\"Query\", \"Best Match\"))\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for query in (\"feel good story\", \"climate change\", \"public health story\", \"war\", \"wildlife\", \"asia\", \"lucky\", \"dishonest junk\"):\n",
    "    uid = embeddings.search(query, 1)[0][0]\n",
    "    print(\"%-20s %s\" % (query, data[uid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.save(\"index.bin\")\n",
    "\n",
    "embeddings = Embeddings()\n",
    "embeddings.load(\"index.bin\")\n",
    "\n",
    "uid = embeddings.search(\"climate change\", 1)[0][0]\n",
    "print(data[uid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run initial query\n",
    "uid = embeddings.search(\"feel good story\", 1)[0][0]\n",
    "print(\"Initial: \", data[uid])\n",
    "# Create a copy of data to modify\n",
    "udata = data.copy()\n",
    "udata[0] = \"See it: baby panda born\"\n",
    "embeddings.upsert([(0, udata[0], None)])\n",
    "uid = embeddings.search(\"feel good story\", 1)[0][0]\n",
    "print(\"After update: \", udata[uid])\n",
    "\n",
    "# Remove record just added from index\n",
    "embeddings.delete([0])\n",
    "\n",
    "# Ensure value matches previous value\n",
    "uid = embeddings.search(\"feel good story\", 1)[0][0]\n",
    "print(\"After delete: \", udata[uid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings index with content enabled. The default behavior is to only store indexed vectors.\n",
    "embeddings = Embeddings({\"path\": \"sentence-transformers/nli-mpnet-base-v2\", \"content\": True, \"objects\": True})\n",
    "\n",
    "# Create an index for the list of text\n",
    "embeddings.index([(uid, text, None) for uid, text in enumerate(data)])\n",
    "\n",
    "print(embeddings.search(\"dishonest junk\", 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an index for the list of text\n",
    "embeddings.index([(uid, {\"text\": text, \"length\": len(text)}, None) for uid, text in enumerate(data)])\n",
    "\n",
    "# Filter by score\n",
    "print(embeddings.search(\"select text, score from txtai where similar('hiking danger') and score >= 0.15\"))\n",
    "\n",
    "# Filter by metadata field 'length'\n",
    "print(embeddings.search(\"select text, length, score from txtai where similar('feel good story') and score >= 0.05 and length >= 40\"))\n",
    "\n",
    "# Run aggregate queries\n",
    "print(embeddings.search(\"select count(*), min(length), max(length), sum(length) from txtai\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "# Get an image\n",
    "request = urllib.request.urlopen(\"https://raw.githubusercontent.com/neuml/txtai/master/demo.gif\")\n",
    "\n",
    "# Upsert new record having both text and an object\n",
    "embeddings.upsert([(\"txtai\", {\"text\": \"txtai executes machine-learning workflows to transform data and build AI-powered semantic search applications.\", \"object\": request.read()}, None)])\n",
    "\n",
    "# Query txtai for the most similar result to \"machine learning\" and get associated object\n",
    "result = embeddings.search(\"select object from txtai where similar('machine learning') limit 1\")[0][\"object\"]\n",
    "\n",
    "# Display image\n",
    "Image(result.getvalue(), width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('shims')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9ac03a0a6051494cc606d484d27d20fce22fb7b4d169f583271e11d5ba46a56e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
