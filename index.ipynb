{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gajraj\n"
     ]
    }
   ],
   "source": [
    "print('Gajraj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base FAISS Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install faiss-cpu sentence-transformers\n",
    "# !pip install pandas numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = [['Where are your headquarters located?', 'location'],\n",
    "['Throw my cellphone in the water', 'random'],\n",
    "['Network Access Control?', 'networking'],\n",
    "['Address', 'location']]\n",
    "df = pd.DataFrame(data, columns = ['text', 'category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "text = df['text']\n",
    "encoder = SentenceTransformer(\"paraphrase-mpnet-base-v2\")\n",
    "vectors = encoder.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "vector_dimension = vectors.shape[1]\n",
    "index = faiss.IndexFlatL2(vector_dimension)\n",
    "faiss.normalize_L2(vectors)\n",
    "index.add(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "search_text = 'where is your office?'\n",
    "search_vector = encoder.encode(search_text)\n",
    "_vector = np.array([search_vector])\n",
    "faiss.normalize_L2(_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = index.ntotal\n",
    "distances, ann = index.search(_vector, k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({'distances': distances[0], 'ann': ann[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join by: df1.ann == data.index \n",
    "merge = pd.merge(results, df, left_on='ann', right_index=True)\n",
    "merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels  = df['category']\n",
    "category = labels[ann[0][0]]\n",
    "category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### txtai Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/neuml/txtai\n",
    "!pip install git+https://github.com/neuml/codequestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from txtai.embeddings import Embeddings\n",
    "# Create embeddings model, backed by sentence-transformers & transformers\n",
    "embeddings = Embeddings({\"path\": \"sentence-transformers/nli-mpnet-base-v2\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"US tops 5 million confirmed virus cases\",\n",
    "        \"Canada's last fully intact ice shelf has suddenly collapsed, forming a Manhattan-sized iceberg\",\n",
    "        \"Beijing mobilises invasion craft along coast as Taiwan tensions escalate\",\n",
    "        \"The National Park Service warns against sacrificing slower friends in a bear attack\",\n",
    "        \"Maine man wins $1M from $25 lottery ticket\",\n",
    "        \"Make huge profits without work, earn up to $100,000 a day\"]\n",
    "\n",
    "print(\"%-20s %s\" % (\"Query\", \"Best Match\"))\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for query in (\"feel good story\", \"climate change\", \"public health story\", \"war\", \"wildlife\", \"asia\", \"lucky\", \"dishonest junk\"):\n",
    "    # Get index of best section that best matches query\n",
    "    uid = embeddings.similarity(query, data)[0][0]\n",
    "\n",
    "    print(\"%-20s %s\" % (query, data[uid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.index([(uid, text, None) for uid, text in enumerate(data)])\n",
    "\n",
    "print(\"%-20s %s\" % (\"Query\", \"Best Match\"))\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for query in (\"feel good story\", \"climate change\", \"public health story\", \"war\", \"wildlife\", \"asia\", \"lucky\", \"dishonest junk\"):\n",
    "    uid = embeddings.search(query, 1)[0][0]\n",
    "    print(\"%-20s %s\" % (query, data[uid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.save(\"index.bin\")\n",
    "\n",
    "embeddings = Embeddings()\n",
    "embeddings.load(\"index.bin\")\n",
    "\n",
    "uid = embeddings.search(\"climate change\", 1)[0][0]\n",
    "print(data[uid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run initial query\n",
    "uid = embeddings.search(\"feel good story\", 1)[0][0]\n",
    "print(\"Initial: \", data[uid])\n",
    "# Create a copy of data to modify\n",
    "udata = data.copy()\n",
    "udata[0] = \"See it: baby panda born\"\n",
    "embeddings.upsert([(0, udata[0], None)])\n",
    "uid = embeddings.search(\"feel good story\", 1)[0][0]\n",
    "print(\"After update: \", udata[uid])\n",
    "\n",
    "# Remove record just added from index\n",
    "embeddings.delete([0])\n",
    "\n",
    "# Ensure value matches previous value\n",
    "uid = embeddings.search(\"feel good story\", 1)[0][0]\n",
    "print(\"After delete: \", udata[uid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings index with content enabled. The default behavior is to only store indexed vectors.\n",
    "embeddings = Embeddings({\"path\": \"sentence-transformers/nli-mpnet-base-v2\", \"content\": True, \"objects\": True})\n",
    "\n",
    "# Create an index for the list of text\n",
    "embeddings.index([(uid, text, None) for uid, text in enumerate(data)])\n",
    "\n",
    "print(embeddings.search(\"dishonest junk\", 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an index for the list of text\n",
    "embeddings.index([(uid, {\"text\": text, \"length\": len(text)}, None) for uid, text in enumerate(data)])\n",
    "\n",
    "# Filter by score\n",
    "print(embeddings.search(\"select text, score from txtai where similar('hiking danger') and score >= 0.15\"))\n",
    "\n",
    "# Filter by metadata field 'length'\n",
    "print(embeddings.search(\"select text, length, score from txtai where similar('feel good story') and score >= 0.05 and length >= 40\"))\n",
    "\n",
    "# Run aggregate queries\n",
    "print(embeddings.search(\"select count(*), min(length), max(length), sum(length) from txtai\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "# Get an image\n",
    "request = urllib.request.urlopen(\"https://raw.githubusercontent.com/neuml/txtai/master/demo.gif\")\n",
    "\n",
    "# Upsert new record having both text and an object\n",
    "embeddings.upsert([(\"txtai\", {\"text\": \"txtai executes machine-learning workflows to transform data and build AI-powered semantic search applications.\", \"object\": request.read()}, None)])\n",
    "\n",
    "# Query txtai for the most similar result to \"machine learning\" and get associated object\n",
    "result = embeddings.search(\"select object from txtai where similar('machine learning') limit 1\")[0][\"object\"]\n",
    "\n",
    "# Display image\n",
    "Image(result.getvalue(), width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### txtai Other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Student:\n",
    "    def __init__(self):\n",
    "        print('Object Instantiated')\n",
    "    def __call__(self,firstName):\n",
    "        print(f'Object is called here with {firstName}')\n",
    "\n",
    "obj=Student()\n",
    "obj('Mantra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from txtai.workflow import Workflow, Task\n",
    "workflow = Workflow([Task(lambda x: [y * 2 for y in x])])\n",
    "list(workflow([1, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/neuml/txtai#egg=txtai[graph,pipeline,similarity] datasets ipyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "from txtai.graph import GraphFactory\n",
    "\n",
    "# Create graph\n",
    "graph = GraphFactory.create({\"backend\": \"networkx\"})\n",
    "graph.initialize()\n",
    "\n",
    "# Add nodes\n",
    "nodes = [(0, \"dog\"), (1, \"fox\"), (2, \"wolf\"), (3, \"zebra\"), (4, \"horse\")]\n",
    "labels = {uid:text for uid, text in nodes}\n",
    "for uid, text in nodes:\n",
    "  graph.addnode(uid, text=text)\n",
    "\n",
    "# Add relationships\n",
    "edges = [(0, 1, 1), (0, 2, 1), (1, 2, 1), (2, 3, 0.25), (3, 4, 1)]\n",
    "for source, target, weight in edges:\n",
    "  graph.addedge(source, target, weight=weight)\n",
    "\n",
    "# Print centrality and path between 0 and 4\n",
    "print(\"Centrality:\", {labels[k]:v for k, v in graph.centrality().items()})\n",
    "print(\"Path (dog->horse):\", \" -> \".join([labels[uid] for uid in graph.showpath(0, 4)]))\n",
    "\n",
    "# Visualize graph\n",
    "nx.draw(graph.backend, nx.shell_layout(graph.backend), labels=labels, with_labels=True,\n",
    "        node_size=2000, node_color=\"#03a9f4\", edge_color=\"#cfcfcf\", font_color=\"#fff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On the fly NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fsner\n",
    "!pip install -U pip setuptools wheel\n",
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import warnings\n",
    "from fsner import FSNERModel, FSNERTokenizerUtils, pretty_embed\n",
    "\n",
    "query_texts = [\n",
    "    \"Does Luke's serve lunch?\",\n",
    "    \"Chang does not speak Taiwanese very well.\",\n",
    "    \"I like Berlin.\"\n",
    "]\n",
    "\n",
    "# Each list in supports are the examples of one entity type\n",
    "# Wrap entities around with [E] and [/E] in the examples.\n",
    "# Each sentence should have only one pair of [E] ... [/E]\n",
    "\n",
    "support_texts = {\n",
    "    \"Restaurant\": [\n",
    "        \"What time does [E] Subway [/E] open for breakfast?\",\n",
    "        \"Is there a [E] China Garden [/E] restaurant in newark?\",\n",
    "        \"Does [E] Le Cirque [/E] have valet parking?\",\n",
    "        \"Is there a [E] McDonalds [/E] on main street?\",\n",
    "        \"Does [E] Mike's Diner [/E] offer huge portions and outdoor dining?\"\n",
    "    ],\n",
    "    \"Language\": [\n",
    "        \"Although I understood no [E] French [/E] in those days , I was prepared to spend the whole day with Chien - chien .\",\n",
    "        \"like what the hell 's that called in [E] English [/E] ? I have to register to be here like since I 'm a foreigner .\",\n",
    "        \"So , I 'm also working on an [E] English [/E] degree because that 's my real interest .\",\n",
    "        \"Al - Jazeera TV station , established in November 1996 in Qatar , is an [E] Arabic - language [/E] news TV station broadcasting global news and reports nonstop around the clock .\",\n",
    "        \"They think it 's far better for their children to be here improving their [E] English [/E] than sitting at home in front of a TV . \\\"\",\n",
    "        \"The only solution seemed to be to have her learn [E] French [/E] .\",\n",
    "        \"I have to read sixty pages of [E] Russian [/E] today .\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "tokenizer = FSNERTokenizerUtils(\"sayef/fsner-bert-base-uncased\")\n",
    "queries = tokenizer.tokenize(query_texts).to(device)\n",
    "supports = tokenizer.tokenize(list(support_texts.values())).to(device)\n",
    "\n",
    "model = FSNERModel(\"sayef/fsner-bert-base-uncased\")\n",
    "model.to(device)\n",
    "\n",
    "p_starts, p_ends = model.predict(queries, supports)\n",
    "\n",
    "# One can prepare supports once and reuse  multiple times with different queries\n",
    "# ------------------------------------------------------------------------------\n",
    "# start_token_embeddings, end_token_embeddings = model.prepare_supports(supports)\n",
    "# p_starts, p_ends = model.predict(queries, start_token_embeddings=start_token_embeddings,\n",
    "#                                  end_token_embeddings=end_token_embeddings)\n",
    "\n",
    "output = tokenizer.extract_entity_from_scores(query_texts, queries, p_starts, p_ends,\n",
    "                                              entity_keys=list(support_texts.keys()), thresh=0.50)\n",
    "\n",
    "print(json.dumps(output, indent=2))\n",
    "\n",
    "# install displacy for pretty embed\n",
    "pretty_embed(query_texts, output, list(support_texts.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with MindsDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'target']\n",
    "\n",
    "df = pd.read_csv('/workspace/PythonExp/data/adult.data', header=None, names=columns)\n",
    "df.target = df.target.map({' <=50K': 0, ' >50K': 1})\n",
    "df.to_csv('/workspace/PythonExp/data/data.csv', index=False)\n",
    "\n",
    "\n",
    "test = pd.read_csv('/workspace/PythonExp/data/adult.test', header=None, names=columns, skiprows=1)\n",
    "test.target = test.target.map({' <=50K.': 0, ' >50K.': 1})\n",
    "test.to_csv('/workspace/PythonExp/data/test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.read_csv('/workspace/PythonExp/data/export.csv')\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = predictions.target\n",
    "predicted_target = predictions.predicted_salary\n",
    "\n",
    "cm = confusion_matrix(target, predicted_target)\n",
    "(tp, fp), (fn, tn) = cm\n",
    "ax = sn.heatmap(cm, annot=True, fmt='g')\n",
    "ax.set_ylabel('Real')\n",
    "ax.set_xlabel('Predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLFlow Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version -->1.29.0\n"
     ]
    }
   ],
   "source": [
    "print(f'Version -->{mlflow.version.VERSION}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run() as run:\n",
    "    mlflow.log_param('gaj','param')\n",
    "    mlflow.log_metric(\"score\",\"100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from random import random, randint\n",
    "from mlflow import log_metric, log_param, log_artifacts\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Log a parameter (key-value pair)\n",
    "    log_param(\"param1\", randint(0, 100))\n",
    "\n",
    "    # Log a metric; metrics can be updated throughout the run\n",
    "    log_metric(\"foo\", random())\n",
    "    log_metric(\"foo\", random() + 1)\n",
    "    log_metric(\"foo\", random() + 2)\n",
    "\n",
    "    # Log an artifact (output file)\n",
    "    if not os.path.exists(\"outputs\"):\n",
    "        os.makedirs(\"outputs\")\n",
    "    with open(\"outputs/test.txt\", \"w\") as f:\n",
    "        f.write(\"hello world!\")\n",
    "    log_artifacts(\"outputs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('uiexp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e401a5a0a82dd3f6793fe48247ce39efc51ddc7576ea94bee0589417d4c87d49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
