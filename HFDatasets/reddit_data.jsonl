{"sub": "investing", "title": "Followup- Here are the results so far from my algo's price calculations on the symbols you gave me:", "selftext": "Please don't put any weight on these!  Its just for fun.  I made this algo myself..I don't know what I'm doing... its far more complicated than need be, and I don't even know what its doing anymore.\n\nNow I'm just testing it for the first time...and having some fun.\n\n|*Asset*|*Actual Price*|*Calculated Price*|*% Change*|\n|:-|:-|:-|:-|\n|FRLN|0.7522|0.114058535|\\-85%|\n|HYMC|1.43|0.921447084|\\-36%|\n|SQQQ|46.56|35.08529881|\\-25%|\n|KOLD|8.78|7.087276735|\\-19%|\n|MOS|65.4|55.48180539|\\-15%|\n|UI|277.02|245.3351418|\\-11%|\n|AAL|19.14|17.00989343|\\-11%|\n|TGT|232.7|210.6066256|\\-9%|\n|FLMN|6.86|6.398027457|\\-7%|\n|REGN|671.33|627.7051901|\\-6%|\n|MSFT|285.65|268.8981373|\\-6%|\n|CC|32.64|31.09223336|\\-5%|\n|BHP|68|64.93559832|\\-5%|\n|KMB|140.48|135.0943617|\\-4%|\n|ARCC|20.85|20.08539081|\\-4%|\n|PFE|49.86|48.10944975|\\-4%|\n|MAS|54.7|52.88982538|\\-3%|\n|GGB|5.63|5.450538112|\\-3%|\n|V|216|209.8209104|\\-3%|\n|TWTR|48.79|47.50760913|\\-3%|\n|TSLA|896|875.6372586|\\-2%|\n|STAG|39.71|38.80885253|\\-2%|\n|ITOT|93.92|93.08233041|\\-1%|\n|GOOG|2326.92|2307.16525|\\-1%|\n|ROK|256.79|255.0798264|\\-1%|\n|RDBX|3.4|3.382586899|\\-1%|\n|NOK|5.08|5.06265949|0%|\n|AAPL|158.77|158.3036962|0%|\n|BLL|81.01|81.36138508|0%|\n|HI|40.74|41.39205932|2%|\n|ASO|37.51|38.27378177|2%|\n|BAC|36.5|37.39420669|2%|\n|MU|67.3|69.13824116|3%|\n|CSCO|49.6|50.97448964|3%|\n|SCHY|24.39|25.14476674|3%|\n|AMD|86.3|89.50602306|4%|\n|HASI|40.56|42.43610909|5%|\n|TSM|91.4|95.7883795|5%|\n|ADBE|403.27|422.9537504|5%|\n|ZIM|57.5|60.36086427|5%|\n|INTC|45.6|48.12631788|6%|\n|INTC|45.6|48.12631788|6%|\n|JPM|122|128.921095|6%|\n|VZ|48.68|51.45133178|6%|\n|NET|92.87|98.281279|6%|\n|CBL|28|29.69499759|6%|\n|WBA|44.25|47.66109726|8%|\n|CELH|54|58.17653303|8%|\n|MMM|144.21|155.6670324|8%|\n|ATER|5.2|5.657450001|9%|\n|SBUX|74.88|81.48248541|9%|\n|LUMN|10.5|11.48311605|9%|\n|HBI|13.7|15.00536269|10%|\n|DB|10.24|11.23550725|10%|\n|FB|207.09|228.4724841|10%|\n|UBER|31.1999|34.49288882|11%|\n|ILMN|296.184|327.545313|11%|\n|GM|38.91|43.06569438|11%|\n|PKE|11.79|13.11316978|11%|\n|DIS|116.7|130.1778382|12%|\n|NVDA|188.3|211.9036967|13%|\n|IVT|30.57|34.41726576|13%|\n|FL|29.6|33.67027133|14%|\n|UMC|7.85|8.940326714|14%|\n|OPY|32.15|36.90363169|15%|\n|AGNC|11.22|12.89140747|15%|\n|GME|129.84|150.3780619|16%|\n|ETWO|7.5|8.799146294|17%|\n|T|19.13|22.65981112|18%|\n|CNF|3.1|3.682833415|19%|\n|BGFV|15|17.83488333|19%|\n|RCUS|24.98|30.10472664|21%|\n|BBBY|14.85|18.05023732|22%|\n|SNDL|0.5325|0.649718687|22%|\n|LEU|27.89|34.04760548|22%|\n|DOCS|44|53.72307412|22%|\n|LAKE|15.89|19.43861225|22%|\n|ARKQ|56.9|69.63677574|22%|\n|WBD|19.22|23.99154274|25%|\n|NERD|18.28|22.93968386|25%|\n|PAYA|5.5|6.976766697|27%|\n|CMPR|56.73|72.21911496|27%|\n|HIMS|4.65|5.925673999|27%|\n|BABA|89.47|114.2001117|28%|\n|DOCU|83.43|107.1609693|28%|\n|PYPL|85.45|110.6818926|30%|\n|XM|19.3|26.15302849|36%|\n|SE|83.99|115.4268668|37%|\n|PLTR|10.8|14.9322183|38%|\n|ARKK|49|68.1552281|39%|\n|XSPA|0.99|1.392123053|41%|\n|ARKW|65.84|92.71455701|41%|\n|TLRY|5.09|7.171070224|41%|\n|NSTG|19.6|28.02821939|43%|\n|CCM|1.5201|2.187256252|44%|\n|BB|5.68|8.224211744|45%|\n|UWMC|3.55|5.144659787|45%|\n|CRSR|15.14|22.05357414|46%|\n|ARKF|22.1|32.23507113|46%|\n|DDD|12.4|18.12539306|46%|\n|ARKG|35.73|53.0237355|48%|\n|AMC|15.92|23.64803253|49%|\n|NVAX|46.74|69.67407471|49%|\n|PINS|20.18|30.31352909|50%|\n|MTTR|5.67|8.575453621|51%|\n|ACB|2.96|4.490906924|52%|\n|TDOC|35.37|54.23178646|53%|\n|HOOD|9.83|15.1202152|54%|\n|ASAN|28.2|43.7092835|55%|\n|BBIG|2.29|3.694369895|61%|\n|RBLX|31.2|50.35916471|61%|\n|KSCP|4.18|6.775821994|62%|\n|STEM|7.77|12.75783628|64%|\n|SAVA|21.18|35.06261643|66%|\n|YCBD|0.685|1.147051623|67%|\n|DKNG|13.97|23.45954861|68%|\n|DRIO|5.04|8.541728298|69%|\n|AI|17.22|29.50844699|71%|\n|NNDM|2.84|5.088082738|79%|\n|RIOT|11.27|21.01636478|86%|\n|CELP|0.45|0.928761522|106%|\n|GREE|6.4|13.23168811|107%|\n|CLOV|2.67|5.551201339|108%|\n|OCGN|2.28|4.922086375|116%|\n|INO|2.7801|6.024544459|117%|\n|GNLN|0.3684|0.816221307|122%|\n|WISH|1.77|4.063528603|130%|\n|GAN|3.72|8.637601448|132%|\n|BEST|0.406|0.984611052|143%|\n|FUBO|4.04|12.75504967|216%|\n\nthe other post: [https://www.reddit.com/r/investing/comments/udklo1/name\\_a\\_stock\\_and\\_ilk\\_tell\\_you\\_the\\_fair\\_price\\_per/?utm\\_source=share&amp;utm\\_medium=web2x&amp;context=3](https://www.reddit.com/r/investing/comments/udklo1/name_a_stock_and_ilk_tell_you_the_fair_price_per/?utm_source=share&amp;utm_medium=web2x&amp;context=3)", "upvote_ratio": 1.0, "id": "t3_ue0sp0", "created_utc": 1651168048.0}
{"sub": "investing", "title": "This 2 hr Mohnish Pabrai lecture is better than 90% of my Ivy League Investing Classes", "selftext": "Mohnish Pabrai is the Warren Buffett of India (or maybe Warren Buffett is the Mohnish of India). In [this](https://youtu.be/9tGjXPhnp-s) video he lays out the 10 Commandments of investing in clear, plain language. Useful for casual investors or for someone looking to start their own fund some day. \n\nRecommend listening on 1.5 speed.", "upvote_ratio": 0.5, "id": "t3_ue0q26", "created_utc": 1651167857.0}
{"sub": "investing", "title": "Vital Energy $VUX has \u201cmore than doubled production\u201d but the market hasn\u2019t noticed this yet \u2013 it was not yet in financials. Easy cashflow, more growth potential and a future stock multibagger using Q4 2021 numbers financials and 900 BOE/D production + higher oil prices of $100 WTI today", "selftext": "Share Price: 0.40\n\nMarket Cap: 32 Mln\n\nShares Outstanding: 81 Mln\n\nShares Diluted: 82.2 Mln\n\nManagement Ownership: &gt;50%\n\nQ4 is for October-December 2021. WTI oil prices have gone up a lot since so Q1 January-March have WTI market prices of somewhere $90 and at the moment we are seeing +$100. Let\u2019s remember that viewing the Q4 financials in the MD&amp;A. **Q1 Financials will be out in a month, end of May**.\n\nVital Energy has been able to more than double its production due to 3 new wells as stated in the 5 October 2021 press release (enclosed down below). From 385 barrels in Q2 to 911 barrels daily in Q4. Furthermore, the company owns many more properties, as stated in the MD&amp;A, and has more potential sites for new wells. 4 of the 8 properties are in production, meaning there isn\u2019t a dependency on one location/property. The cost structure is easy as after the Operational Netbacks the company has only limited expenses. Cashflow in Q4 was used to pay back a loan, note 11, of a total 3.8 Mln and make the balance sheet even cleaner.\n\nConsidering the high cash flow due to low costs and high oil prices, together with &gt;50% insider ownership, this is not a risk.\n\nAs can be seen below, the operational netback in Q4 was $70 dollars with a revenue of $84 per BOE. The average WTI was $77 in Q4. Let us remember that this is October \u2013 December and right now oil price is at $100 WTI.\n\n\\*Please check on Sedar MD&amp;A\n\n  \n\n**Balance Sheet and Income Statement**\n\n**\\*Please check on Sedar**\n\n**Insider Ownership**\n\nManagement owns &gt;50% of the shares so the float is tightly hold \n\n[https://www.vitalenergyoil.com/investor/stockqoute.html](https://www.vitalenergyoil.com/investor/stockqoute.html) \n\n**Oil Proved Reserves 31 December 2021 \u2013 they have more locations for drilling left**\n\n**\\*Please check on Sedar**\n\n**Valuation**\n\nIf you use the IWC (Microcap ETF) P/E ratio for the full year 2022 profit per share \u2013 expected based on production, oil price and cost structure - the stock is worth a lot more. That is for a company with a good balance sheet and a clear cashflow, not something you find in most microcaps as with this ETF. Therefore, using a P/E of 10-15 sounds very reasonable to me. This leaves out the potential growth of new oil wells. [https://stockanalysis.com/etf/iwc/](https://stockanalysis.com/etf/iwc/) \n\nConsidering the production only started to increase since August, and oil prices are now at $100 WTI, I see a completely different picture for FY 2022.\n\n**Oil Industry**\n\nI\u2019m positive towards oil as the world still needs oil. From 2011-2019 the world energy consumption increased by 10% and oil consumption source increased at the same rate according to this source. [https://ourworldindata.org/grapher/global-energy-substitution?country=\\~OWID\\_WRL](https://ourworldindata.org/grapher/global-energy-substitution?country=~OWID_WRL)\n\n***\\*Disclaimer: This is my personal opinion, please do your own homework***", "upvote_ratio": 0.25, "id": "t3_udxph6", "created_utc": 1651159883.0}
{"sub": "investing", "title": "Keep averaging down or wait?", "selftext": "So there's obviously a lot going on right now in the world between inflation, interest rates, and the Ukraine conflict. The usual advice is just keep averaging down and if your time frame is long enough it won't matter, but that being said it'll certainly sting a little to keep buying if we are headed south for the next couple years. How does everyone feel right now in this climate and are you doing or considering doing anything differently?", "upvote_ratio": 0.6, "id": "t3_udx78k", "created_utc": 1651158549.0}
{"sub": "investing", "title": "The future of credit card companies vs. ApplePay, Cash App, Venmo, etc.", "selftext": "I see everything moving into (onto) your smartphone, no one carries cash anymore and and no one under 30 carries a credit card, it seems.\n\nVisa and AX reported solid earnings and expect a rebound in spending yet I am not sure how long they can continue to fend off all these digital payment processors (and cryptos), what\u2019s the bull and bear thesis on these credit card companies, say 5-10 years down the line?", "upvote_ratio": 0.38, "id": "t3_udww2l", "created_utc": 1651157761.0}
{"sub": "investing", "title": "GDP Unexpectedly contracts in Q1 2022", "selftext": "[https://www.bea.gov/sites/default/files/2022-04/gdp1q22\\_adv.pdf](https://www.bea.gov/sites/default/files/2022-04/gdp1q22_adv.pdf)\n\n&amp;#x200B;\n\n **DP annualized, quarter-over-quarter:** \\-1.4% vs. 1.0% expected \n\n&amp;#x200B;\n\nThe decrease in real GDP reflected decreases in private inventory investment, exports, federal government spending, and state and local government spending, while imports, which are a subtraction in the calculation of GDP, increased. Personal consumption expenditures (PCE), nonresidential fixed investment, and residential fixed investment increased (though not as much as expected).", "upvote_ratio": 0.91, "id": "t3_uduelf", "created_utc": 1651150732.0}
{"sub": "investing", "title": "Why I\u2019m long on long read DNA sequencing ($PACB, LON:$ONT)", "selftext": "I work in the biotech field and want to lay out why the 2022 biotech sell-off has created an unparalleled opportunity to buy cheaply into technology leaders in a growing field. Specifically, I want to talk about two companies involved in DNA sequencing: the US-listed Pacific Biosciences ($PACB) and the UK-listed Oxford Nanopore Technologies (LON:$ONT). These are household names for people working in biotech but not widely known outside of the field. I will refer to them as PacBio and ONT in the main text below.\n\n**TL;DR:** PacBio and ONT are significantly undervalued leaders in long read DNA sequencing. Together they hold a duopoly in the long read sequencing market, with sizable technology-based moats that even the sequencing Goliath Illumina ($ILMN) with 80% market share has never been able to overcome. In the last 6 months, PacBio has lost 80% of its value and ONT has lost 45% of its value, providing excellent entry points. Bucking the stock price decline, in 2021, revenues grew 65% for PacBio and 94% for ONT as these companies scale up. Together PacBio and ONT own less than a 10% share of the DNA sequencing market, which is currently worth $5.8B. But both their market share as well as the total addressable market are likely to grow rapidly over the next ten years. Going long on PacBio and ONT therefore has a high probability of yielding a 10X return on investment.\n\n# What is DNA sequencing?\n\nFor those who went to school before the Human Genome Project started in 1990 and those who forgot their high school biology, DNA sequencing is the process of reading out the nucleic acid sequence of a molecule of DNA. You know that stuff inside all of our cells which encodes our genetic blueprint. Importantly, this DNA is rich in valuable information. Some of the most important types of information we can extract from DNA sequence are:\n\n* genetic mutations that contribute to disease like cancer and other traits\n* genetic relationships to other sequences (i.e., who\u2019s the father, or which kind of virus is this?)\n\nIt\u2019s useful to remember that roughly [half of common diseases](https://royalsocietypublishing.org/doi/10.1098/rspb.2015.1684https://royalsocietypublishing.org/doi/10.1098/rspb.2015.1684), and [most rare diseases,](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6279436/) are heritable. For example, the risk of cystic fibrosis and breast cancer is well understood to be passed down from parents to their children. In cancer patients, different types of tumors also have different types of characteristic DNA mutations that can be [screened and diagnosed with DNA sequencing](https://humgenomics.biomedcentral.com/articles/10.1186/s40246-019-0220-8). As we've probably all read in the last few years, DNA sequencing has also been crucial for tracking the evolution of SARS-CoV2 and its variants. The bottom line here is that DNA sequencing provides vital information for improving public health. Information that governments, insurance providers and hospitals are willing to pay good money for. For those outside of the world of biology, I cannot understate how fast biotechnology and precision medicine are developing and how fundamental DNA sequencing is for that.\n\n# What is the total addressable DNA sequencing market?\n\nThe DNA sequencing market had a size of [$5.8B in 2021](https://investor.pacificbiosciences.com/events-and-presentations). Together with the development of precision healthcare and genetic surveillance of pathogens, this market is expected to grow at a fast clip. Though I don\u2019t put much stock into exact predictions, some reports put the DNA sequencing market compound annual growth rate at [\\&gt;10% from 2020 to 2027](https://www.grandviewresearch.com/industry-analysis/dna-sequencing-market#:~:text=The%20global%20DNA%20sequencing%20market%20is%20expected%20to%20grow%20at,USD%2011.21%20billion%20by%202027). The key thesis here is that not too far in the future, it will become routine healthcare policy to sequence and securely store the DNA of most individuals in developed countries. On top of that, I and others expect that sending tumor biopsy samples for sequencing and surveilling wastewater and other environments by sequencing will become routine, greatly expanding the sequencing market. Because DNA sequencing is a fundamental step in lots of modern biotech labs, one way to view DNA sequencing companies is as a pick-and-shovel play on the biotech sector. PacBio estimates a [$40B+ total addressable market for DNA sequencing](https://investor.pacificbiosciences.com/static-files/9ee0d712-6974-4c44-83e9-5af75541add3) and I think that\u2019s realistic considering today\u2019s trends.\n\n# What\u2019s the competitive landscape in DNA sequencing?\n\nDNA sequencing is not a crowded field, with only a handful of companies playing a significant role. One reason for this is that, similarly to the semiconductor industry, the sequencing industry has a high barrier to entry due to technological complexity. Illumina ($ILMN) remains the 600 pound gorilla in the DNA sequencing space with a market cap of 50B and [2021 revenues of $4.5B](https://investor.illumina.com/news/press-release-details/2022/Illumina-Reports-Financial-Results-for-Fourth-Quarter-and-Fiscal-Year-2021/default.aspx#:~:text=Fiscal%20year%202021%20consolidated%20results,%243%2C239%20million%20in%20fiscal%202020). This translates roughly to a market share of 80% based on the estimated size of the 2021 market. With revenues of [$130M for PacBio](https://www.pacb.com/press_releases/pacbio-announces-fourth-quarter-and-fiscal-year-2021-financial-results-3/) and [$177M for ONT](https://nanoporetech.com/about-us/news/preliminary-results-year-ended-31-december-2021-guidance-update-0), these companies are the Davids to Illumina\u2019s Goliath. Today, PacBio commands a market cap of only $1.5B and ONT is in a similar but more highly valued ballpark of $3.5B. Despite the fact that PacBio has been around since 2004 and ONT since 2005, they have not yet had a significant impact on Illumina\u2019s dominance.\n\nThere are a few other companies worth mentioning to get the full picture, though I consider them less important for this thesis. In China, MGI is a large sequencing provider that reverse engineered Illumina\u2019s technology and remains banned from the US market and has not penetrated the European market. One of the biggest biotech instrument producers Thermofisher ($TMO) makes the IonTorrent DNA sequencer, but it\u2019s not competitive on price or data quality so is rarely used. There are also two new US companies (Singular Genomics $OMIC and the private Elements Biosciences) that will be launching sequencing instruments similar to Illumina\u2019s in the next year or so. For those interested in more details, Keith Robinson is a scientist with a [great blog](http://omicsomics.blogspot.com/) on these and other sequencing technologies.\n\n# Why do we need long reads from PACB and ONT?\n\nWhen comparing sequencing technologies, the three main quality metrics we care about are sequence length (measured in bases), sequence accuracy and the cost per sequenced base. The longer, the more accurate, and the cheaper a sequence is, the better. I should state here that fundamentally, Illumina, PacBio and ONT use completely distinct sequencing technologies, each with some individual advantages. What makes PacBio and ONT unique compared to all the other sequencing companies is their *long reads*. All other technologies are based on *short reads*. As someone who has worked a lot with sequencing data, I can tell you that here size matters. Without getting too technical, DNA sequences from Illumina and the other sequencing companies are usually 50-500 bases in length while PacBio gives lengths around 10,000 bases and ONT comes packing 10,000-30,000 bases (sometimes up to several million bases).\n\nBut why do we care about the sequence length, you ask? Well imagine the DNA sequences are pieces of a puzzle. The difference in getting answers from long read sequencing data and short read sequencing data is like the difference between putting together a 10 piece puzzle and a 100 piece puzzle. Not only that, the short read puzzle is usually missing 10% of its pieces. The reasons for this are technical and related to long stretches of repetitive sequence that make it hard to get the whole picture with short reads. There are notable exceptions where long reads are not needed, including liquid biopsy where we are sequencing short fragments of cancer DNA often less than 200 bases in length. But what matters is that for most applications, when using Illumina data, there\u2019s a moderate amount (perhaps [roughly 25%](https://www.sciencedirect.com/science/article/pii/S0002929722000659)) of genetic variation that remains hidden. Some of this hidden variation can cause disease. For example, Evan Eichler, a well-known scientist at the University of Washington, emphasized in a recent article that 34% of all disease-causing variation is made up of variants that are [larger than a single base change](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6681822/), which are more difficult to detect with short reads. We can conclude that long reads offer critical advantages over short reads in clinical and other applications.\n\n# How cost-competitive are long reads?\n\nThe extra length of ONT and PacBio does come at a cost. The key cost metric is the price of a human genome (typically at 30x coverage, which means you sequence each base 30 times on average to make sure you don\u2019t miss anything). Cost estimates can vary widely but sequencing expert [Albert Villela](https://twitter.com/AlbertVilella) has put together [a public tabular comparison of different providers](https://docs.google.com/spreadsheets/d/1GMMfhyLK0-q8XkIo3YxlWaZA5vVMuhU1kg41g4xLkXc/edit?hl=en_GB&amp;hl=en_GB#gid=1569422585), from which I extracted some ballpark ranges.\n\n|*Sequencing provider*|*Cost per human genome*|\n|:-|:-|\n|Illumina/MGI|$1,000-$6,000|\n|ONT|$2,000-$6,000|\n|PacBio|$5,000-$40,000|\n\nAlthough I have to stress that cost estimates vary a lot, clearly Illumina has a lead on cost. However, if we are making clinical decisions affecting people\u2019s health, arguably we healthcare providers may pay a premium to make sure they get the most complete possible information. On top of this, the cost of ONT and PacBio have come down considerably in the last few years and their pace of innovation here has outstripped Illumina. ONT actually reported in their 2021 annual results that they can already generate [a $345 human genome](https://nanoporetech.com/sites/default/files/s3/investors/reports/ONT%20FY21%20Results%20Presentation%20FINAL%2022.3.22.pdf) and are developing a system to go as low as $235, which would be groundbreaking if this translates to prices for users on the ground.\n\nI like to think of Illumina as similar to Intel: a long time market monopolist with juicy margins ([Illumina gross margin 69% for the last four years](https://ycharts.com/companies/ILMN/gross_profit_margin), nice) that has lost its innovative edge and kept iterating on the same short read sequencing technology. ONT has already come dangerously close in cost to Illumina, and has a real chance of becoming cheaper than Illumina in future.\n\n# Long read accuracy - still the Achilles heel?\n\nWe need to now talk about sequence accuracy. What is this? Just like us humans, all sequencing methods make mistakes. These take the form of incorrect bases in the DNA sequence, like typos in a text. For many applications a few errors don\u2019t matter, but when we care about detecting very specific genetic mutations related to disease, we want to avoid even a small number of errors. In the past, Illumina and others have downplayed the threat from long read sequencing by citing its lower sequence accuracy. And until recently it was uncommon to use PacBio or ONT data alone because you needed additional Illumina data to correct the mistakes in the long reads. In the last few years, this has changed as PacBio introduced high-fidelity HiFi reads and [ONT introduced its latest Kit 14 chemistry and duplex sequencing](https://nanoporetech.com/sites/default/files/s3/investors/reports/ONT%20FY21%20Results%20Presentation%20FINAL%2022.3.22.pdf). Illumina no longer has a monopoly on accurate high-throughput reads as the table below shows and [a recent study](https://www.biorxiv.org/content/10.1101/2021.10.27.466057v2.full) nicely lays out.\n\n&amp;#x200B;\n\n|*Sequencing provider*|*Sequencing accuracy*|\n|:-|:-|\n|Illumina/MGI|99.9%|\n|ONT|95%-99.9%|\n|PacBio|98%-99.9%|\n\nAll these error rates seem low, but remember when you sequence a human genome with 6.4 billion bases, a 99.9% error rate means 6.4 million errors. Even a 0.01% improvement is therefore a big deal. There is some fineprint to the table above and its based on manufacturer numbers and my own experience with the data and reading of the literature, but overall the three platforms are converging on [relatively similar error rates](https://www.biorxiv.org/content/10.1101/2021.10.27.466057v2.full]), undermining Illumina\u2019s previous advantage. As a testament to this, short reads are no longer routinely used to supplement long reads in many published scientific studies. Interestingly, there is also a lot of interest from companies like Google to apply machine learning for improving sequencing accuracy. One study by Google and PacBio was able to [reduce errors by 42%](https://www.biorxiv.org/content/10.1101/2021.08.31.458403v1.full). These advances suggest to me that long reads can continue to make gains in accuracy and become the gold standard.\n\n# Beyond DNA sequence - detecting epigenetic modifiers\n\nTalking of machine learning, there is another neat feature of PacBio and ONT that I haven\u2019t touched on: machine learning based epigenetic analysis. Bear with me. You may be aware that there is another layer on top of DNA sequence that can affect our genes, this is epigenetics. In practice it means there the DNA molecules get chemically modified, for example through the addition of a methyl group. When you slap a methyl group or some other modification on the DNA it can help turn genes on or off, making it important for some diseases and traits. Ok, biology lesson is over and I can get to the point. The data generated by PacBio and ONT actually allows [detection of DNA modifications like methylation](https://twitter.com/DRBFX/status/1517189018768883713). You get this information for free, together with the DNA sequence. All you need is the right machine learning algorithm and you can read off the epigenetics together with the genetics. Previously, the main way to study methylation was with a method called bisulfite sequencing that uses Illumina short reads and requires special sample preparation. It\u2019s easier and just as, or more, accurate to do it with just PacBio or ONT.\n\nAs an investor, epigenetics is particularly exciting because unlike genetics it changes for each person over time. A person\u2018s DNA only needs to be sequenced once, but we may want to track changes in methylation in different parts of the body over time once we understand the implications for health and aging. I hope that ka-ching sound is ringing for you too. Epigentics, like liquid biopsy for cancer and pathogen surveillance, means repeated revenue over time. Right now, there isn\u2019t a huge market for epigenetic analysis and compared to the DNA sequence this is mostly a \u2018nice to have\u2019 but not key feature. But as we understand epigenetics better and demand picks up, this feature could fuel further growth for PacBio and ONT.\n\n# How ONT stands out\n\nOne final advantage specific to ONT is its speed (measured as bases sequenced per hour) and the portability and flexibility of its instruments. ONT sequencing gives results in real-time and is generally faster than all the other technologies. A [recent study by Stanford scientists and Google](https://www.nejm.org/doi/full/10.1056/NEJMc2112090) showed that ONT can diagnose a patient by sequencing their genome in less than 8 hours. All other methods would take over a day to get the same results. Not all sequencing is time-critical, but in a clinical setting the speed of ONT gives it an edge. Incredibly, the ONT sequencing machines range in size from the dimension of a smartphone (MinIon) to a mini fridge (PromethION), while PacBio machines are fridge-sized and Illumina instrument sizes range from mini fridge to fridge. Finally, one recent ONT development that really blows my mind is their MinION Mk1D\u200b, which combines [a MinION sequencer with an Apple iPad Pro](https://nanoporetech.com/products/minion-mk1d) so you can sequence on the go! This isn\u2019t out yet and may never be a big seller, but I think it underlines how portable and exciting the ONT technology is.\n\nAlthough this point is more subtle, I also want to mention the extraordinary Read Until capability of ONT. This allows sequencing to stop when a target sequence (e.g. a specific disease-related mutation) is found and to reject non-target sequences before they get fully sequenced. If we consider the problem of finding a needle in a haystack, with PacBio and Illumina, you sequence every straw of hay and then look for the needle, whereas ONT can immediately discard straws until it finds the needle in real time, saving time and money.\n\nThese unique advantages of ONT are the reason that I,  [together with other scientists in a twitter poll](https://twitter.com/TJesse62/status/1509927475077394438), think that ONT sequencing will ultimately prevail over PacBio thanks to its longer reads, greater portability and lower cost.\n\n# Financial fundamentals and valuation\n\nDespite impressive 2021 yearly revenue growth of [65% for PacBio](https://www.pacb.com/press_releases/pacbio-announces-fourth-quarter-and-fiscal-year-2021-financial-results-3/#:~:text=Fiscal%20year%202021%20results,with%20%2434.3%20million%20in%202020) and [94% for ONT](https://nanoporetech.com/about-us/news/preliminary-results-year-ended-31-december-2021-guidance-update-0), both companies still have negative earnings per share, losing $181M and $213M in 2021 respectively. Profitability may still be years away, as the leadership of both companies advises. Still, as shown below these are high margin businesses and can become profitable with scale.\n\n|*Sequencing provider*|*Gross profit margins (2021)*|\n|:-|:-|\n|Illumina|69%|\n|ONT|55%|\n|PacBio|45%|\n\nSome good news is that in 2021 both companies secured additional cash to fuel their growth. PacBio raised money (via convertible senior notes) from Softbank and reported cash and equivalents of $1B in the last quarter and ONT raised money through an IPO and reported $0.8B.\n\nNow a word on valuation. Valuing unprofitable biotech companies based on financials is tough. With a price-to-sales ratio (2021 revenues divided by today\u2019s market cap) of 12 for PacBio and 20 for ONT, the long read companies are comparable to Illumina that comes in at 12 but more expensive than a mature biotech company like Thermofisher at 5. Importantly though, these multiples are nothing crazy like the &gt;50 price-to-sales ratios of biotech companies like Twist Biosciences ($TWST), BioNano ($BNGO) and even PacBio that we saw last year.  Today PacBio is worth less than the $8/share that Illumina offered in its foiled [acquisition attempt in 2018](https://www.pacb.com/press_releases/illumina-to-acquire-pacific-biosciences-for-approximately-1-2-billion-broadening-access-to-long-read-sequencing-and-accelerating-scientific-discovery/) and ONT is worth much less than the [$5.4/share IPO price](https://www.reuters.com/business/oxford-nanopore-eyes-47-billion-market-value-london-debut-2021-09-30/).\n\nThese are unprofitable biotech companies in the early stages of growth. It\u2019s likely that this factor alone is the main reason these companies have been sold off (or sold short) as fears of inflation and recession swirl. For long-term investors, this presents a considerable opportunity.\n\n# Company leadership\n\nI don\u2019t have many particular thoughts on the ONT and PacBio leadership, but my overall impression based on company performance, interviews and Glassdoor reports is relatively positive.\n\nONT has been led by its co-founder Gordon Sanghera, Ph.D. since 2005. The consistency and the scientific experience of the leadership here is a benefit for me. I think Dr Sanghera has substantial skin in the game and will execute on the long term vision for the company. He also holds anti-takeover shares to help ensure ONT can fulfill its potential rather than be gobbled up by a company like Illumina or Thermofisher.\n\nPacBio\u2019s CEO Christian Henry has an MBA background and was a long-time Illumina executive before taking the job at PacBio in 2020. He brought along a number of other Illumina employees to pick up key roles at PacBio. I think Mr Henry has made some aggressive moves, like acquiring short read sequencing company Omniome and sample preparation firm Circulomics. I\u2019m particularly hoping that the Illumina experience of the new PacBio leadership will help them to get their technology out there and drive sales, which is what Illumina excels at.\n\n# Risks\n\nAny savvy investor will realize that investing in unprofitable biotech companies is considerably more risky than a global market ETF. For PacBio and ONT specifically, there are several risks that could lead to a decline in the stock price.\n\nThe main long-term risk is competition. Illumina is aiming to provide their own [synthetic long read sequencing technology](https://www.illumina.com/science/technology/next-generation-sequencing/long-read-sequencing.html). While this has been touted by some stock analysts, I share the [skepticism of other scientists about Illumina\u2019s technology](http://omicsomics.blogspot.com/2022/01/illumina-teases-two-glittering-enigmas.html), which relies on stitching together short reads. A more realistic threat may be that Illumina drops their short read prices since they can no longer compete on sequence accuracy or length. This could take some sales from long read companies, but for many applications the benefits of true long reads are essential. PacBio and ONT are also direct competitors and it\u2019s likely that one of them will emerge as a winner sooner or later. I\u2019m hedging against this by holding both. My personal assessment is that currently PacBio has a more 'production-level' product and better customer service but ONT has the superior technology.\n\nA short-term risk is that macro headwinds further depress prices of growth stocks in 2022. A potential hedge against this is to spread out investments over this year. Although I may well be wrong, I personally think we are close to the bottom on biotech stocks based on reasonable valuation multiples and continued growth in the face of supply chain issues and other headwinds. \n\nFinally, both companies have high institutional ownership. ARK Genomics is the biggest PacBio shareholder with 10% ownership and the IP Group owns 14% of ONT. I don\u2019t see a major long-term risk in this, but ARK and IP Group could eventually decide to dump their stock. Some might also see the high institutional ownership in both companies as a vote of confidence. For instance, Oracle bought a $190M stake in ONT last year and Tencent was also an early investor. All this is good to know, but overall the ownership of PacBio and ONT doesn\u2019t impact my investment decisions in either direction.\n\n# Conclusion\n\nPacBio and ONT have unique industry-leading technology and are poised for rapid growth in the expanding market of DNA sequencing. With a total addressable market estimated at $40B and combined 2021 revenues of $300M, PacBio and ONT have lots of room to aggressively grow their market share against incumbent Illumina. Long reads are becoming longer, cheaper, and more accurate and provide fundamentally more useful information than Illumina\u2019s  short reads that currently dominate the market. My long-term thesis is that long read sequencing technology won\u2019t entirely displace short reads, but will increase its market share tenfold or more in the long term. The 2022 biotech sell-off has provided an attractive entry point to initiate a long position. And that\u2019s why I\u2019m going long on long reads.\n\n*Disclaimer: I\u2019ve allocated 8% of my portfolio to ONT and 5% to PacBio. I aim to hold over 10 years, rebalancing if changes in technology or valuations justify it. Do your own DD and let me know your thoughts.*", "upvote_ratio": 0.83, "id": "t3_udtyqm", "created_utc": 1651149297.0}
{"sub": "investing", "title": "In conditions of high inflation, gold shines more than ever.", "selftext": "The inflation is rising faster that even I thought. But what is [inflation](https://www.investopedia.com/terms/i/inflation.asp)?  Contrary to common belief, inflation is not rising prices. The prices  of goods measured in money i.e. gold or silver are pretty much the same if not lower than they were 60 years ago.\n\nInflation is the loss of the purchasing power by a currency. Currency. What we  have in our pockets is not money. It's currency. It has been like that  since '60s when the world decided to switch to paper because... well,  nobody knows why. It doesn't make sense to pay with inflatable paper.  Before that we had money. It was gold and silver for thousands of years  in comparison with just 60 years of FIAT(paper currency). Now, for a  silver half dollar before 1965 you have to pay more than 20 times more  of its face value in fake dollars. Read that outloud for yourself. It  sounds funny.\n\nI believe that with  offical inflation at 8,5% (we all know that this is a lie) the system  and what I call \"The Great Fiat Experminent\" are doomed to fall apart.  We're well beyond the point of beginning of the process. How many years  will it take to crash completely? Nobody knows. But one thing is sure.  It will end. History shows that there's not been a single paper currency  in the world that survived.\n\n Look how gold has risen since abandoning the precious metals standard:\n\n[https://www.gold.org/goldhub/data/gold-prices?gclid=CjwKCAjw9qiTBhBbEiwAp-GE0RE659EmfOrZCtxgjlcihDeccZ4ZCq8EpmuOiw9LZx8GiwUA1W1ITxoCIv4QAvD\\_BwE](https://www.gold.org/goldhub/data/gold-prices?gclid=CjwKCAjw9qiTBhBbEiwAp-GE0RE659EmfOrZCtxgjlcihDeccZ4ZCq8EpmuOiw9LZx8GiwUA1W1ITxoCIv4QAvD_BwE)\n\n You, English speakers even say: \"As good as gold\". And, indeed, it is  good. Pandemic, war, hyperinflation, you name it. It's a long term investment  counted in years but is safe and reliable. \n\nI would say that physical silver is even a better investment due to its insane ratio of \"paper to physical\" (345:1) on COMEX.\n\nPlease educate yourselves and your dearest. This post only scratches the surface. Some more info:\n\n[https://www.youtube.com/watch?v=DyV0OfU3-FU](https://www.youtube.com/watch?v=DyV0OfU3-FU)\n\n \"When money dies\" by Adam Fergusson\n\n[https://www.reddit.com/r/Wallstreetsilver/comments/nwzadx/a\\_comprehensive\\_compilation\\_of\\_all\\_due\\_diligence/](https://www.reddit.com/r/Wallstreetsilver/comments/nwzadx/a_comprehensive_compilation_of_all_due_diligence/)", "upvote_ratio": 0.23, "id": "t3_udr4ez", "created_utc": 1651138351.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - April 28, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.75, "id": "t3_udqpod", "created_utc": 1651136468.0}
{"sub": "investing", "title": "Stock spilt after buyback", "selftext": "Hi I\u2019m wondering how a stock spilt after a buyback affects a stock, doesn\u2019t the stock spilt essential return the floating shares back to the market that the buyback did?\n\nWondering for stocks like Google and Amazon, considering Google announced a $70b buyback but also a stock spilt of 20:1.", "upvote_ratio": 0.22, "id": "t3_udlqr1", "created_utc": 1651116763.0}
{"sub": "investing", "title": "Name a stock, and ilk tell you the fair price per share.", "selftext": "I got a new algorithm I'm having fun with, and want to crank some more assets through.  Let me know what you are watching and.......................................... I'll value it....................... don't take my advice though....................... just for fun.\n\nedit: I posted results so far over here: https://www.reddit.com/r/investing/comments/ue0sp0/followup_here_are_the_results_so_far_from_my/?utm_source=share&amp;utm_medium=web2x&amp;context=3", "upvote_ratio": 0.4, "id": "t3_udklo1", "created_utc": 1651113197.0}
{"sub": "investing", "title": "I need to make at least $400-500+ a week with investing.", "selftext": "I can daytrade or swing trade or whatever. The method isn't important as I have unlimited time all day to focus on the market. I also have (currently) about $30k in my investment portfolio, but can move another $10k or so if needed. I just don't know what way is best to do it. I've dabbled with daytrading here and there and have had some lucky days but have also lost far more than I've made due to the risky nature of daytrading. I'm not experienced enough to consistently make that kind of money daytrading without major losses offsetting my profits. I've had good luck on some swing trades but at times waiting for an exit position takes longer than is ideal and wouldnt bring a consistent stream of weekly income. What would you guys do in my position? Should I take a trading course? Is there a safer investment that can generate this kind of steady income on a regular basis? I'm not looking to make a living off of this but would like to supplement my normal salary if possible.", "upvote_ratio": 0.2, "id": "t3_udjaaf", "created_utc": 1651109169.0}
{"sub": "investing", "title": "What's going on with financial stocks like $BAC and $SCHW?", "selftext": "What's going on with financial stocks like $BAC and $SCHW? \n\nI picked them especially BAC bc of the supposed rate increases that were going to be happening from the FED, and were seen as \"safety\" stocks in a volatile market.\n\nInstead they have become slowly and surely (not in major drops but a slow trickle down) my 2 worst positions down 20% in both with a decent % of my portfolio invested in them.\n\n\nI am concerned but at the same time don't want to take big losses on them but what concerns me is it is off no news. When the market goes up, these 2 especially $SCHW still barely move.", "upvote_ratio": 0.4, "id": "t3_udgwy6", "created_utc": 1651101935.0}
{"sub": "investing", "title": "Facebook shares spike on better-than-expected quarterly earnings", "selftext": "Article: https://www.cnbc.com/2022/04/27/meta-fb-q1-2022-earnings.html\n\n&gt;Shares of Facebook parent Meta jumped 18% in extended trading on Wednesday after the company reported earnings that topped estimates even as revenue was disappointing.\n\n&gt;The after-hours rally on Wednesday still leaves the stock way down for the year. As of the close, the shares had lost almost half their value in 2022.\n\n&gt;In addition to its earnings figure, Facebook also exceeded expectations for average revenue per user. But almost every other key metric was a miss, including monthly active users.\n\n&gt;Revenue rose 7% in the quarter, the first time in Facebook\u2019s 10-year history as a public company that growth has landed in the single digits. Analysts were expecting 7.8% growth.\n\n&gt;For the second quarter, Facebook forecast revenue of $28 billion to $30 billion, trailing the $30.6 billion estimate of analysts surveyed by Refinitiv. The company said in the release that the guidance reflects continued trends from the first quarter, including soft revenue growth that \u201ccoincided with the war in Ukraine.\u201d\n\n&gt;Facebook changed its name to Meta in October, reflecting CEO Mark Zuckerberg\u2019s effort to push the company towards a future that includes working, playing and studying in a virtual world.\n\n&gt;Facebook\u2019s family of apps, including the core app, Instagram and WhatsApp, accounted for 97.5% of revenue in the quarter. The remaining $695 million came from Reality Labs, the part of the company that\u2019s attempting to build products for the metaverse.\n\n&gt;In the family of apps business, net income dropped 13% from a year earlier to $11.48 billion. Reality Labs lost $2.96 billion in the period compared with a loss of $1.83 billion in the first quarter of 2021.\n\n&gt;Facebook lowered its total expenses guidance for 2022 to somewhere between $87 billion and $92 billion, below its earlier estimate of $90 billion to $95 billion. It expects most of that expense growth to be driven by its family of apps segment, followed by Reality Labs.\n\n&gt;Other social media companies similarly pointed to macroeconomic factors impacting their advertising revenue. Snap CEO Evan Spiegel called the first quarter \u201cmore challenging than we had expected.\u201d The company said some advertisers had paused advertising campaigns after Russia\u2019s invasion of Ukraine in February. Google-owned YouTube grew just 14% in the first quarter, far below the 25% analysts had expected.\n\n&gt;Digital ads could also be impacted by inflation and Apple\u2019s recent privacy changes on iPhone operating systems, which Meta CFO Dave Wehner previously predicted would result in a $10 billion revenue hit in 2022, though he acknowledged that figure was an estimate.\n\nShares are up 20% in After Hour trading.\n\nDisclaimer: I have [long positions in $FB](https://www.wealthly.com/share/5RH4FH), and I was partially influenced to hold my positions after reading [this comment](https://www.reddit.com/r/investing/comments/u8w2wa/is_meta_platforms_inc_fb_a_value_pick_or_value/i5o4859/) from /u/living-pineapple-589. Good call so far sir.", "upvote_ratio": 0.88, "id": "t3_udec7z", "created_utc": 1651094731.0}
{"sub": "investing", "title": "To Everyone Asking if Buying Twitter is Free Money", "selftext": "First, I understand this may be the first merger you've been exposed to. As much as it grinds my gears that this question is only asked about Twitter and not the countless other mergers that go through I'll provide some data on the strategy of buying companies that will be acquired.\n\nIs it free money? Yes and no.\n\nFirst, let's look at a fund that focuses primarily on buying companies that have had an announcement regarding their acquisition.\n\nIQ Merger Arbitrage ETF (MNA)\n\nThis ETF has returned 2.68% annualized over the last ten years and 2.49% since inception. If you exclude fees and assume you do this yourself, over the last ten years it has done 3.56% annualized.\n\nIs this free money? If you consider anything with a positive return to be free money, then yes. However I should remind you the S&amp;P 500 has done around 14% annualized over the same time period.\n\nThe strategy fails for the same reason the martingale roulette strategy fails. You make very little money with each 'success' and your failures eat many 'successes' given how large the drawdowns are when mergers get cancelled. When mergers get cancelled they almost always immediately lose whatever premium they received from the merger announcement. From personal experience, these companies rarely perform well after a failed merger as well.\n\nHope that helps.\n\nTLDR\n\n [ImpressiveCitron420](https://www.reddit.com/user/ImpressiveCitron420/) wrote a great TLDR that simplifies things:\n\nIt's not free money. It's money in exchange for risk. There's a risk that the merger won't happen (as OP says). This is what the premium is paid out for and why the price is not at the buy out price currently.\n\nThe problem however, is that on a risk adjusted basis, this gives poor returns compared to the SP500.\n\n&amp;#x200B;", "upvote_ratio": 0.86, "id": "t3_udcsye", "created_utc": 1651090701.0}
{"sub": "investing", "title": "Does anyone have any advise for dividend investing? id like to learn as much as I can about it.", "selftext": "Looking to get into dividend investing and im unsure how to do it. The market has me kind of nervous and returns are not as high right now anyway. I am trying to maximize my investments and was wondering if I could do this with dividend investing. Am I able to make a decent amount of money with dividend investing or is it not that great? Id like to put some in my Roth to help it grow as well as invest in a taxable account to generate some secondary income. Does anyone have luck with this or know anyone who posts blogs on the topic?", "upvote_ratio": 0.7, "id": "t3_udcsw1", "created_utc": 1651090696.0}
{"sub": "investing", "title": "Are there still any arkk fans out there?", "selftext": "ARKK has dipped severely back to its price circa 2019. Despite this, if you look from 2016-2019, you can see that someone who had invested in the fund would have 3x their money. Wondering if anyone is still bullish on this fund for the future and if anyone is buying the dip. Seems like the perfect time to buy, but sentiment is currently dead.", "upvote_ratio": 0.73, "id": "t3_udc7md", "created_utc": 1651089167.0}
{"sub": "investing", "title": "What\u2019s the best option for losses in the stock market.", "selftext": "My initial investment in Robin hood stocks were 15k and after the elections I have lost up-to 80% in some of my stocks totaling up to 7k in losses total. Would it be smarter to throw the rest of what I have left off and put it into a professional investments to try and make over 80% to try and at least make it back and then write all the rest off in losses which won\u2019t meet up to my standard deduction? Or keep it in and hope that everything goes back up?", "upvote_ratio": 0.75, "id": "t3_udayiy", "created_utc": 1651085839.0}
{"sub": "investing", "title": "Share Dilution. Is it good for retail investors?", "selftext": "Can anyone give a legit answer on how this is good for retail investors. I got legit banned from my favorite stock sub for calling this out. I guess I was spreading FUD? This specific stock was pumping like crazy beginning of 2021 and the CEO added something like 1 billion shares on top of the 1 billion outstanding already, in a time span of 6 or 8 months i want to say. Now this stock is at risk of being delisted as it\u2019s been below a dollar for a while now. It almost seemed like the CEO took everyones money and doesn\u2019t give 2 poopies about the retail investor.", "upvote_ratio": 0.45, "id": "t3_ud9y89", "created_utc": 1651083104.0}
{"sub": "investing", "title": "What investment classes are doing well?", "selftext": "I\u2019ve heard diversification is the key to weathering the storm but I\u2019m having a hard time seeing what investment classes are doing well. \n\nI have small/mid/large cap stock, gold, crypto, reits, and real estate. It\u2019s all down the past couple weeks or so", "upvote_ratio": 0.3, "id": "t3_ud9ec4", "created_utc": 1651081654.0}
{"sub": "investing", "title": "Fidelity 401k - need an investment critique", "selftext": "My company\u2019s 401k is through Fidelity. I (35) have been contributing the annual max to the following three funds:\n\n- Fidelity Growth Company Commingled Pool Class 2\n- Blackrock Equity Index Fund\n- Blackrock Russell 2500\n\nLast year my 1Y Rate of Return was crazy good, like 40-45%. This YTD it\u2019s currently -17.8%.\n\nI don\u2019t consider myself comfortably well-versed on the topic and I\u2019m not interested in frequent changes of my 401k account, but should I continue to take no action and ride it out, or should I perhaps be making changes?\n\nTIA!", "upvote_ratio": 0.4, "id": "t3_ud8mpg", "created_utc": 1651079574.0}
{"sub": "investing", "title": "Plant-based Sector needs a bit of TLC? Let's get into it", "selftext": "My last post BRIEFLY touched on the sector and how I feel. Essentially, I feel (a) anyone invested in the space now is simply early to an inevitable party (one that only serves vegan burgers, of course) (b) the downturn we have seen is essentially the result of OVERALL economic consternation (rising rates, war, etc.) and rotation to the \"ol' reliable\", such as commodities. Let's look at the data:\n\n&amp;#x200B;\n\n&gt;\\- Plant-based food dollar sales grew six percent in 2021 (*3x faster than overall food sales*) to a market of *$7.4B*  \n&gt;  \n&gt;\\- That growth was pretty good considering pandemic challenges and supply chain choke points  \n&gt;  \n&gt;\\- As people begin to care more about social justice and sustainability, the indication is that their wallets follow suit  \n&gt;  \n&gt;\\- Plant based category share is increasing *(4% increase of dollar share &amp; 79% 3-year dollar share growth)*  \n&gt;  \n&gt;\\- Units of plant-based foods (individual products sold) are up compared to overall units of food  \n&gt;  \n&gt;\\- Plant based meals &amp; cheeses sales grew by *9% ($513M) &amp; 7% ($290M)* (a case for a company like **$VEGI** | **$VGGIF** to be a good investment)  \n&gt;  \n&gt;\\- 62% of U.S. households are now buying plant-based products (*repeat consumers sit at 79%*) (think of companies that have Amazon-like marketplaces, like **$VEJI** | **$VEJIF**)  \n&gt;  \n&gt;\\- Millennials and Gen Z (*47% of the population*) will grow in spending power, and they care about eating habits and the consciousness of those habits (think of your annoying Vegan friend)\n\n&amp;#x200B;\n\nAll in all, I am saying what I have said before...this industry is coming. whether you like it or not, plant-based &amp; sustainable living industry will move from the fringes to the main stream. The key will be not if you enter this foray, but how and what horse you pick. Companies like **$VEGI, $BYND** make ready-made plant-based meals, that are affordable and accessible (which I think are MAJOR factors you need to change eating habits as history has taught us). A company like **$VEJI** makes it easy and break down barriers to buying a multitude of plant-based products. Again, the key here is consumer choice and ease of process(es).", "upvote_ratio": 0.7, "id": "t3_ud7yzj", "created_utc": 1651077878.0}
{"sub": "investing", "title": "Portfolio/modelling advice: I want to replicate New Age Alphas H factor algorithm which back solve growth expectations implied by current share prices. Over what time horizon should growth implied by share prices be measured? Can this be standardised to compare growth expectations across companies?", "selftext": "I wondered if  anyone could assist. I am trying to build a portfolio that assesses the growth rate implied by company share prices. \n\nThe criteria for inclusion/exclusion of a company is based on whether the implied growth seems too high relative to historical growth. If this is the case then this company would be marked down and those with more achievable growth rates would be marked up. This is essentially  the same as the F factor method proposed by New Age Alpha  [New Age Alpha Hedge Fund (newagealphahf.com)](https://www.newagealphahf.com/) (Refer to the quote at the bottom which outlines their strategy at a high level)  \n\n\nFirstly, I love this investment thesis and hence want to replicate it. My issue however is that each company likely has a different time horizon for growth. So how are New Age Alpha controlling for this? Do they look one year ahead, four years ahead, or some other number of years?\n\nTo take Tesla as a stylised example: revenue growth was 70% in 2021 and has been 50% on average since 2017. \n\nI built a hypothetical DCF that solves to the current share price of $916 per share. My model uses 3 stages of growth ( 3 years of constant high growth \\[at the most recent growth rate\\] followed by a 6 year declining growth period to finally, a terminal value which uses 2% inflationary growth). This isn't necessarily my opinion of Tesla growth trajectory but serves as an example for now.\n\nThe model tells me that the revenue growth priced into the Tesla share price is 70% growth for 3 years, followed by a linear decline to inflationary growth. This amounts to a 9 year horizon.\n\nI would then determine statistically - based on the last 4 years of Tesla growth, do I think this is achievable? If yes, then I would give Tesla a high number and keep Tesla in my portfolio as I think the share price adequately reflects what the company is expected to achieve.\n\nThe issue is, I chose this growth trajectory at random. Should I be standardising the time horizon across companies to compare them? If I chose a much shorter time horizon then Tesla would need to exhibit higher growth, and vice versa, if I chose a longer time horizon then they would need lower growth to justify the share price.\n\nCurious how the guys at New Age Alpha are dealing with this but their website isn't very helpful. \n\nNew Age Alpha Prospectus states as follows:\n\n \"The H-Factor algorithm measures how human biases affect stock prices and calculates and assigns H-Factor Scores to each component of the S&amp;P 500\u00ae Index. Using a probability-based approach, the algorithm determines the probability that the company will not deliver growth to support its stock price. The algorithm compares the company\u2019s implied revenue growth rate, which is calculated by using the company\u2019s stock price, current and historical financial statements, market data, and other publicly available financial information, such as the company\u2019s revenue, against the company\u2019s historical revenue growth rates to determine how likely the company is to deliver the growth in revenue implied by its stock price. The algorithm is capable of utilizing inputs from different valuation models, such as revenue, cash flow, multiples, comparable, and earnings-based models. Under the algorithm methodology, a high H-Factor Score means that a stock is relatively overpriced and has a higher probability of not delivering growth to support the stock price (*i.e.,* according to the methodology, the stock is a loser). Conversely, a low H-Factor Score means that, according to the methodology, a stock is relatively underpriced and has a lower probability of not delivering growth to support the stock price.\"\n\nTLDR: Over what time horizon should growth implied by share prices be measured? Can this be standardised to compare growth expectations across companies?", "upvote_ratio": 0.33, "id": "t3_ud5twu", "created_utc": 1651072260.0}
{"sub": "investing", "title": "Data on portfolio rebalancing methods?", "selftext": "Has anyone seen any data that shows long-term performance for different rebalancing methods? For example, does rebalancing monthly perform better than yearly, or does rebalancing when a portion of the portfolio has grown past a certain percentage outperform?", "upvote_ratio": 0.5, "id": "t3_ud5cyc", "created_utc": 1651071035.0}
{"sub": "investing", "title": "Microbiome/ gut health stocks?", "selftext": "Having to learn about the microbiome in relation to my own health issues has showed me how important and yet underappreciated the microbiome actually is in all aspects of human health. I'm about to start diving into the investing side, does anyone have any companies they recommend me to start looking at?", "upvote_ratio": 0.54, "id": "t3_ud5az2", "created_utc": 1651070880.0}
{"sub": "investing", "title": "[WSJ] Facebook Parent Meta Expected to Post Slowest Revenue Growth Since IPO", "selftext": "https://www.wsj.com/articles/meta-platforms-facebook-fb-q1-earnings-report-2022-11651022191\n\nFacebook parent Meta Platforms Inc. is expected to post its slowest revenue growth on record as the company navigates growing competition for users and privacy headwinds in its advertising business.\n\nMeta\u2019s stock price was battered in February when it posted quarterly results that showed a sharper-than-expected decline in profit, a gloomy revenue outlook and a drop in its daily active users.", "upvote_ratio": 0.91, "id": "t3_ud5ayv", "created_utc": 1651070879.0}
{"sub": "investing", "title": "Lucid Motors - Saudi Risks?", "selftext": "In case you didn't know around most shares of Lucid are held by the Saudi Kingdom (around 60%).\n\n&amp;#x200B;\n\nRegarding this fact several questions came up to my mind. \n\n&amp;#x200B;\n\nWhat does it mean to you that the Saudis own most of the shares? What risks do you see? Could the Saudis take the company private any day if they like to do so? If your return would be negative you won't be able to hold the shares anymore to turn positive again. Or are we somehow protected because - as far as I know - the registred office of Lucid is in the US? \n\n&amp;#x200B;\n\nIs it possible that in the future Lucid could get rid of the Saudis? How?\n\n&amp;#x200B;\n\nRegardings this risks it may make people rethink about their (potential) investment.\n\n&amp;#x200B;\n\nI'm curios about your thoughts. Thanks!", "upvote_ratio": 0.64, "id": "t3_ud4qyu", "created_utc": 1651069329.0}
{"sub": "investing", "title": "Cost Efficient Small or Mid Cap Emerging Market ETF or MF Recommendation?", "selftext": "I want to weight my portfolio a little more in EM, preferably more in the Value side. I'm not well versed enough in emerging markets to say what region of the world I prefer to be more heavy in, so any ideas there would be appreciated. Any suggestions?", "upvote_ratio": 0.6, "id": "t3_ud3ym4", "created_utc": 1651067170.0}
{"sub": "investing", "title": "Betting/shorting against academia", "selftext": "I realise that \"academia\" is a very vague thing to ask if you can short, but I'm absolutely convinced that it represents a bubble that will eventually burst. \n\nTo me, the fees people are paying for degrees that are becoming more and more meaningless, high grades becoming the usual rather than the exception, the money spent on pumping oit meaningless publications, online learning becoming the norm in and out of universities, all of these are building to a point where people will see that this model is not functioning \n\nAnd so I'm asking for advice, if anyone has any idea on a way to bet on this knowledge in the form of an investment? Or if anyone has any thoughts on the subject? Thanks!", "upvote_ratio": 0.24, "id": "t3_ud3pz3", "created_utc": 1651066483.0}
{"sub": "investing", "title": "Is it worth buying Google or Amazon as a retail investor at their current prices? They are so expensive for just 1 stock.", "selftext": "At prices like 2,373.00 USD (Google) and 2,787.82 USD (Amazon) it seems like the return is very minimal for such an expensive stock, could it really go even higher than those prices? It seems like these are investment opportunities for only the rich to make any actual gains on.\n\nI believe I heard they were doing a split this year, I suppose retail investors should wait until then?", "upvote_ratio": 0.26, "id": "t3_ud3mwh", "created_utc": 1651066237.0}
{"sub": "investing", "title": "Treasury Direct Account Locked", "selftext": "Has anyone had success unlocking their treasury direct account?\n\nI logged in yesterday and it prompted me for 9(!!!!!) secret questions after I entered my password. I must have Mis-answered or had the case wrong because it then locked my account. \n\nI\u2019ve tried calling the number they gave me, and was on hold for 4 hours yesterday. Has anyone had any luck getting through?\n\nedit: OP's log 3 hours 21 minutes, I have made contact with a very overwhelmed sounding employee", "upvote_ratio": 0.81, "id": "t3_ud3f65", "created_utc": 1651065605.0}
{"sub": "investing", "title": "What are some modern European investment funds for retail clients?", "selftext": "I'm looking for investment opportunities across the money management industry in Europe and I'd appreciate your advice!\n\nWhat I hope to find is a fund with focus not so much on index investing but rather a modern approach with focus on tech/disruptive innovation/Web3.\n\nA great example is [Titan.com](https://titan.com/), backed by some solid VCs like a16z. Unfortunately, it's available only for US citizens.\n\nETFs are also an option as long as they are available for European citizens.\n\nThanks!", "upvote_ratio": 0.6, "id": "t3_ud3d6u", "created_utc": 1651065444.0}
{"sub": "investing", "title": "What do we think about Toast TOST?", "selftext": "It has come down considerably as of its IPO, I am in IT in the hospitality industry and EVERYONE is ditching traditional POS systems for Toast. I feel at the current value it may be time to make a move. As they start to integrate with hotel PMS systems they should get much more business as well. We are all so sick of NCR and Oracle Micros, there is much more on the table for available customers.", "upvote_ratio": 0.68, "id": "t3_ud1llf", "created_utc": 1651059861.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - April 27, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.93, "id": "t3_ucz6h6", "created_utc": 1651050068.0}
{"sub": "investing", "title": "New-ish to Investing but Really Want to Get Into it", "selftext": "Hi, I\u2019m 22F (23 in a week) and I just bought my first I-Bond. Only $25 though, just to get started. I\u2019ve been investing in the stock market since I was 18 and made my first crypto investment at 15. I want to also invest in real estate but that wasn\u2019t a dream until I bought my first home at 21. I\u2019ve \u201cnickled and dimed\u201d investing by buying $5 of stocks here and there. So far I\u2019ve made a whopping $130 doing that (never took my money out so technically haven\u2019t made anything). Big money over here, I know, don\u2019t get too jealous now /s. \n\nEven though I\u2019ve been investing for almost a decade, I really want to get more than my feet wet, I just don\u2019t know where. If I\u2019m being honest I\u2019m really scared of taking risks. I\u2019m enamored and in love with the thought of investing and the whole thought of \u201cfree money\u201d: I follow influential investors and read their books but just feel like they\u2019re super lucky and I\u2019ll never have that luck. \n\nI don\u2019t have thousands just lying around, so where can a basically broke person (compared to other posts I see) invest? I\u2019m trying to do research for when I get back into the work force after I graduate from college and can really get the ball rolling with having an income again. \n\n\nObviously I know that I can\u2019t really get a cheat sheet to this stuff, I\u2019m just wondering if anyone was once someone like me when it came to investing and got over their fears and didn\u2019t absolutely lose all their money. \n\nTL;DR I have a pretty small portfolio but not completely ignorant to what investing is and I have a general idea of what\u2019s all out there. I do not work at the moment but I do have $1,000 a month income and I am a young person that can\u2019t seem to get over the fear of risk even though I\u2019ve been investing since I was 15. I would like to know good options for investing when I start working again and can put more than chump change towards an investment and possibly a way to get over the fear of risk.", "upvote_ratio": 0.65, "id": "t3_ucyhlh", "created_utc": 1651046866.0}
{"sub": "investing", "title": "If buying the dip is most profitable, why do dips even happen?", "selftext": "Basically title,\n\nWe know that buying stocks when they are falling gives better returns than buying at all time highs. If that\u2019s the case, why does the market dip so hard? For me personally I know that I buy more stocks now than a couple of months ago, why isn\u2019t everyone doing the same?", "upvote_ratio": 0.35, "id": "t3_ucwxbl", "created_utc": 1651040194.0}
{"sub": "investing", "title": "Considering switching from Morgan Stanley to Fidelity", "selftext": "Morgan Stanley currently charges me 1% a year, and according to Fidelity\u2019s website, they charge 0%. Is this accurate? How can there be 0% fees and commissions? Are there any hidden fees?\n\nAlso, if I was to switch my holdings to another brokerage I won\u2019t get taxed on that right ?", "upvote_ratio": 0.7, "id": "t3_uctmwc", "created_utc": 1651028130.0}
{"sub": "investing", "title": "Fidelity announced this morning (2022-04-26) that they will be the first major retirement plan to allow investors to put bitcoin in their 401k plans (Investopedia).", "selftext": "\nFidelity announced this morning (2022-04-26) that they will be the first major retirement plan to allow investors to put bitcoin in their 401k plans.\n\n&gt; Fidelity Offers Bitcoin as 401(k) Investment Option\n\n&gt; Will become first provider to allow cryptocurrency in 401(k) accounts\n\n&gt; By MARK KOLAKOWSKI Published April 26, 2022\n\n&gt; Fidelity Investments has become the first retirement plan provider to allow cryptocurrencies in the 401(k) accounts that it services, starting with Bitcoin (BTC). This investment option will become available by mid-2022 to 23,000 employers that use Fidelity to administer their retirement accounts. Since Fidelity has $11.3 trillion in assets under administration (AUA), making it the largest retirement plan provider in the U.S., this move represents a major milestone in the mainstreaming of crypto.1\n&gt; \n&gt; Dave Gray, head of workplace retirement offerings and platforms at Fidelity stated: \"There is growing interest from plan sponsors for vehicles that enable them to provide their employees access to digital assets in defined contribution plans, and in turn from individuals with an appetite to incorporate cryptocurrencies into their long-term investment strategies.\" Business software provider MicroStrategy Incorporated (MSTR) reportedly will be the first employer to offer Bitcoin as an investment option in its employee retirement plans.1\n&gt; \n&gt; KEY TAKEAWAYS\n&gt; Fidelity will offer Bitcoin as an investment option in 401(k) plans starting sometime in mid-2022.\n&gt; How much plan participants can put into Bitcoin will be determined by their employers, but a maximum of 20% is likely.\n&gt; This is likely to give a major boost to the mainstreaming of cryptocurrency.\n&gt; However, the U.S. Department of Labor has \"serious concerns\" about crypto in retirement plans, due to the high risks.\n&gt; Fidelity Account Details\n&gt; Initial reports indicate that investors in the Bitcoin-eligible retirement plans administered by Fidelity will be able to allocate up to 20% of their accounts to this investment option, although this figure may change. Moreover, the cap on Bitcoin investments will be determined by the employer.23\n&gt; \n&gt; Fees for Bitcoin-eligible accounts reportedly are planned to range between 0.75% and 0.90% of assets, with the exact amount to depend on the amount invested and the employer. Additional fees, particularly per-trade fees, reportedly also will be charged.23\n&gt; \n&gt; Mainstreaming Crypto\n&gt; Fidelity's move would allow first-time crypto investors to obtain Bitcoin without having to make a separate account on a crypto exchange. This is likely to become a major boost to acceptance of crypto as an investment alternative. In the subset of retirement plans represented by 401(k) plans, Fidelity held an estimated $2.4 trillion of assets as of 2020, making it the third-largest provider in this segment.2\n&gt; \n&gt; In November 2021, Fidelity launched the first regulated offering in Canada that offered Bitcoin custody and trading services for institutional investors. Fidelity next launched two publicly traded bitcoin funds in December 2021 on the Toronto Stock Exchange (TSX). In 2022, Fidelity has launched similar products in Switzerland and Germany.2\n&gt; \n&gt; 'Serious Concerns' From Department of Labor\n&gt; In March 2022, the U.S. Department of Labor (DOL) warned that cryptocurrencies were speculative and volatile investments with inflated valuation. The DOL expressed \"serious concerns\" about providers offering cryptocurrencies in retirement plans. The DOL also stressed that providers must offer adequate information to potential investors about the risks involved in cryptocurrency investing, including the volatile prices and the evolving regulatory environment.2\n&gt; \n&gt; \n&gt; Role of Microstrategy\n&gt; As noted above, business analytics software provider MicroStrategy reportedly has signed on to become the first employer offering Bitcoin in its 401(k) plans administered by Fidelity. That company holds billions of dollars in Bitcoin, and its founder Michael Saylor is a staunch supporter of cryptocurrency through numerous tweets on the subject.2", "upvote_ratio": 0.91, "id": "t3_ucsilw", "created_utc": 1651024439.0}
{"sub": "investing", "title": "Be careful when calculating what interest you'll be receiving on I-Bonds, you may not beat inflation!", "selftext": "I've seen a lot of recommendations of I-Bonds, but a fact that I haven't seen mentioned is that, unless you hold the bond for a minimum 5 years, selling your bond will lose you the last 3 months of interest. This means that the shorter timeframe you hold the bond for, the more of an effect this will have on your nominal return.", "upvote_ratio": 0.35, "id": "t3_ucs5hn", "created_utc": 1651023265.0}
{"sub": "investing", "title": "VGT losing Visa, Mastercard, and PayPal", "selftext": "So I recently learned that the MSCI index that VGT tracks is moving the three stocks I listed, as well as lots of others, out of the tech sector. I really liked VGT as a tech tilt *because* it has these \u201cfinancial\u201d stocks. What do you all think about the restructuring? How much do you think it will affect VGT?", "upvote_ratio": 0.79, "id": "t3_ucs0am", "created_utc": 1651022790.0}
{"sub": "investing", "title": "Thoughts on why bitcoin's trend is anchored to the sell-off of technology stocks", "selftext": "I've been hearing recently that the sell-off in tech stocks is causing a sell-off in Bitcoin and other digital assets. What I want to know is, do cryptocurrency mining companies play a fundamental role in Bitcoin, Ethereum, and other digital assets' value? and perhaps the sell-off in the exchanges where these companies are listed is what's causing the correlation between bitcoin and tech stocks?", "upvote_ratio": 0.36, "id": "t3_ucqpsa", "created_utc": 1651018710.0}
{"sub": "investing", "title": "Real Yields Wade Toward Positive Territory, Denting Stocks", "selftext": "https://www.wsj.com/articles/real-yields-wade-toward-positive-territory-denting-stocks-11650934492?mod=hp_lead_pos4\n\nYields on government bonds are catching up with expected inflation after years of lagging behind it, a threat to the speculative stock-market bets that proliferated in the era of rock-bottom rates and economic stimulus.\n\nBond yields that trail inflation push investors to seek an alternative; many found it in the stock market, powering a surge in risky assets.\n\nOften known as real yields, the yields on TIPS fell deeply negative at the start of the pandemic, meaning investors were guaranteed to lose money on an inflation-adjusted basis if they held the bonds to maturity. That helped power a surge in stocks by pushing investors toward riskier assets for better returns. \n\nNow analysts expect that time to end, with central banks pulling back from their efforts to stimulate economic growth by holding rates ultralow and buying bonds. Many now expect the Fed to fight inflation with a series of rapid rate increases, including a half-percentage point move next month.\n\nThat rapid shift in expectations has dented shares of low-profit tech companies and speculative wagers including Cathie Wood\u2019s flagship ARK Innovation exchange-traded fund. The ETF targets companies it believes offer the greatest potential for innovation such as Zoom Video Communications Inc. and Coinbase Global Inc. It gained popularity in 2020 when the Fed cut rates and investors chased high returns in riskier places. Known by its ticker ARKK, the fund has plunged 20% since the beginning of April, bringing its year-to-date decline to 44%, as of Monday.\n\n\u201cFor the first time in a while fixed income probably looks attractive relative to riskier assets like the stock market,\u201d said Lisa Hornby, head of U.S. multi sector fixed income at Schroders.", "upvote_ratio": 0.73, "id": "t3_ucqayl", "created_utc": 1651017428.0}
{"sub": "investing", "title": "Thoughts on APPH, controlled environment agriculture?", "selftext": "I\u2019ve been buying APPH down from about $8,  at $4 and have this seems to be a more reasonable price. I\u2019m considering going big on this, it basically has a perfect ESG profile and demand will likely be increasing as they flesh out more types of fruit and vegetables. It\u2019s definitely a long term play, just wanted some thoughts", "upvote_ratio": 0.5, "id": "t3_ucpf64", "created_utc": 1651014703.0}
{"sub": "investing", "title": "Robinhood Lays Off 9% Of Full-Time Employees After Decline In Users", "selftext": "[https://blog.robinhood.com/](https://blog.robinhood.com/)\n\n*Robinhood CEO Vlad Tenev shared the below to Robinhood employees following a company-wide meeting to discuss the changes.*\n\nToday we made the difficult announcement that we are letting go approximately 9% of our full-time employees. While this decision was necessary, it was not one we undertook lightly, and I\u2019d like to share our rationale.\u00a0\n\nAs you know, throughout 2020 and H1 2021, we went through a period of hyper growth accelerated by several factors including pandemic lockdowns, low interest rates, and fiscal stimulus. We grew net funded accounts from 5M to 22M and revenue from \\~$278M in 2019 to over $1.8B in 2021. To meet customer and market demands, we grew our headcount almost 6X from 700 to nearly 3800 in that time period.\n\nThis rapid headcount growth has led to some duplicate roles and job functions, and more layers and complexity than are optimal. After carefully considering all these factors, we determined that making these reductions to Robinhood\u2019s staff is the right decision to improve efficiency, increase our velocity, and ensure that we are responsive to the changing needs of our customers.", "upvote_ratio": 0.98, "id": "t3_ucp8pg", "created_utc": 1651014170.0}
{"sub": "investing", "title": "Microsoft earnings report", "selftext": "\u00b7        Revenue was $49.4 billion and increased 18%\n\n\u00b7        Operating income was $20.4 billion and increased 19%\n\n\u00b7        Net income was $16.7 billion and increased 8% GAAP (up 13% non-GAAP)\n\n\u00b7        Diluted earnings per share was $2.22 and increased 9% GAAP (up 14% non-GAAP)\n\nhttps://www.microsoft.com/en-us/investor/earnings/fy-2022-q3/press-release-webcast", "upvote_ratio": 0.96, "id": "t3_ucowsu", "created_utc": 1651013234.0}
{"sub": "investing", "title": "Fundrise v. Saving for Down Payment", "selftext": "Hello!  I am a 25 year old from Columbus Ohio.  I have about 20k saved up right now, but want to continue saving for a larger down payment for my first investment property, between 6-12 months from now.  I also really like the look of Fundrise and utilizing that as an investment fund for myself.  Do you think saving that 1k I want to put into fundrise is worth for a down payment or should I get some initial money in there to start the account growing? \n\nThanks!", "upvote_ratio": 0.7, "id": "t3_ucnsc0", "created_utc": 1651010059.0}
{"sub": "investing", "title": "Research on retail investors", "selftext": "I am conducting a research project for my high school class on how the stimulas checks have created in influx of retail investors and how it has affected the US economy compared to prior recessions. I am running a t - test to compare the 2020 recession to previous recessions (2008, 2000) and I was wondering if there is anywhere to get reliable data on the number of retail investors in the respected year?", "upvote_ratio": 0.6, "id": "t3_ucngxb", "created_utc": 1651009159.0}
{"sub": "investing", "title": "Tiktok is Starting to Hit the Bottom lines of Google (Youtube) and Meta. Will this turn into potential existential threats?", "selftext": "A few months ago, I had asked about the impact of tiktok on the bottom lines of youtube and meta [in a post](https://www.reddit.com/r/investing/comments/rjsgql/can_tiktok_impact_the_bottom_lines_for_meta/). Given the last few earnings, it looks like tiktok is starting to have impacts on **both** meta and youtube. Given the decent miss in earnings in youtube it is fair to say they are feeling the heat (it may also be due to reopenings).  However I think it's mostly due to tiktok. \n\n&amp;#x200B;\n\n[https://money.yahoo.com/video/tiktok-clearly-taking-share-youtube-205032494.html](https://money.yahoo.com/video/tiktok-clearly-taking-share-youtube-205032494.html)\n\n&amp;#x200B;\n\nFurthermore looking at the data provided by sensortower, shows alarming insights.\n\n&amp;#x200B;\n\n[https://go.sensortower.com/rs/351-RWH-315/images/Sensor-Tower-Q1-2022-Data-Digest.pdf?fbclid=IwAR0TOjzGP5P5qysEwVz3rNyDAkZN0-fQuhu3ND-yX58iPbYPlpWbfTgSkeE](https://go.sensortower.com/rs/351-RWH-315/images/Sensor-Tower-Q1-2022-Data-Digest.pdf?fbclid=IwAR0TOjzGP5P5qysEwVz3rNyDAkZN0-fQuhu3ND-yX58iPbYPlpWbfTgSkeE)\n\n[https://go.sensortower.com/rs/351-RWH-315/images/Sensor-Tower-Q4-2021-Data-Digest.pdf?fbclid=IwAR0TOjzGP5P5qysEwVz3rNyDAkZN0-fQuhu3ND-yX58iPbYPlpWbfTgSkeE](https://go.sensortower.com/rs/351-RWH-315/images/Sensor-Tower-Q4-2021-Data-Digest.pdf?fbclid=IwAR0TOjzGP5P5qysEwVz3rNyDAkZN0-fQuhu3ND-yX58iPbYPlpWbfTgSkeE)\n\n[https://go.sensortower.com/rs/351-RWH-315/images/Sensor-Tower-Q1-2021-Data-Digest.pdf?fbclid=IwAR0TOjzGP5P5qysEwVz3rNyDAkZN0-fQuhu3ND-yX58iPbYPlpWbfTgSkeE](https://go.sensortower.com/rs/351-RWH-315/images/Sensor-Tower-Q1-2021-Data-Digest.pdf?fbclid=IwAR0TOjzGP5P5qysEwVz3rNyDAkZN0-fQuhu3ND-yX58iPbYPlpWbfTgSkeE)\n\n&amp;#x200B;\n\nYoutube downloads are starting to decrease. Meta seems to be stagnant (mainly being carried from India which has tiktok ban). Meanwhile tiktok is increasing. Extrapolating the trend it shows that Tiktok could potentially pose existential threats to youtube and Meta. However I am not sure if there is a strong replacement for messaging which Meta still has a huge market share of.", "upvote_ratio": 0.74, "id": "t3_ucnbjq", "created_utc": 1651008735.0}
{"sub": "investing", "title": "Alphabet Q1 earnings miss despite in-line revenue", "selftext": "already down 7%. LOL, tommorow is gonna be a blast.  don't catch any falling knives. \n\noh yeah, paypal, already down nearly 85% reports tommorow. The slightest miss and we're gonna have fun tommorow. \n\nGoogle's parent company Alphabet ([GOOG](https://finance.yahoo.com/quote/GOOG?p=GOOG&amp;.tsrc=fin-srch), [GOOGL](https://finance.yahoo.com/quote/GOOGL)) reported first-quarter sales that were roughly in-line with estimates, with the tech giant showing resilience in its key search advertising and cloud businesses. However, earnings came in lower-than-expected as costs mounted, and growth in the tech behemoth's YouTube business slowed sharply compared to last year.\n\nShares of Alphabet dropped more than 4.5% in late trading following the results.\n\nHere were the main metrics from Alphabet's report Tuesday afternoon, compared to consensus estimates compiled by Bloomberg:", "upvote_ratio": 0.91, "id": "t3_ucmdx4", "created_utc": 1651006185.0}
{"sub": "investing", "title": "Coinbase 8.7 PE ratio, how come it is such undervalued?", "selftext": "Basically the title, how come Coinbase, a software company, has such a low PE ratio? In the current overvalued market Coinbase seems to be dramatically undervalued. I get it, the whole Nasdaq is cooling down (almost 4% at the time of writing this post), but a lot of these companies have insane valuations and no profits. This seems not to be the case with Coinbase, yet its stock price is following the down trend. Why?", "upvote_ratio": 0.66, "id": "t3_uclytm", "created_utc": 1651005037.0}
{"sub": "investing", "title": "Thesis, from Worth Charting: It's time to postpone all buying right now. Question: what are good defensive moves?", "selftext": "[This CNBC interview with Worth Charting's Carter Worth](https://www.youtube.com/watch?v=izo5Nh2AEB4) seemed especially solid to me. What do you guys think of the thesis that it's time to postpone all buying right now?\n\nDoes this really mean it's best to be in cash right now despite inflation? What are good defensive alternatives? It's sounded like treasury bonds may be a good buy.", "upvote_ratio": 0.79, "id": "t3_uclr35", "created_utc": 1651004432.0}
{"sub": "investing", "title": "Thoughts on Netflix as a short term opportunity?", "selftext": "Curious what you all think about Netflix. IMO it's a company primarily valued via sentiment, and I think that's reflected both in it's explosive rise and recent slide.\n\nThat said, there may be a dead cat bounce, or maybe even a slow, steady recovery from the current position. They expect subscriptions to continue to drop, by a million accounts in the next quarter. But if nothing else, they're crafty when it comes to squeezing money out of existing customers. They even got me with their UHD/4k upgrade. They're going to be cracking down on sharing creds, but I think that'll be a wash in terms of impact on subscribers.\n\nThe last time I saw a stock I though had some real chops drop this far, this fast, was when I was eyeballing Boeing at under 100 during the Covid fall out. Kicking myself for that one.\n\nWhat do you all think? I think what they're missing at this point is a staple - they don't have a Marvel, or a LOTR, or a GoT franchise to spin off of. Their original content is hit or miss, and they don't get blockbuster titles until they've been available via on demand streaming services for some time. In short, they seem to be fighting over table scraps.\n\nDoes anyone else see an opportunity here? They're down 50% in the last month. Is there an upside?\n\nEDIT: This sub is a trip. 16 thousand views, zero upvotes (50% upvote rate). Didn't create it for either, but those two stats together are pretty funny.", "upvote_ratio": 0.5, "id": "t3_uci7je", "created_utc": 1650994960.0}
{"sub": "investing", "title": "US ETFs for UK based investors", "selftext": "Hi all,\n\nRetail investor, technically from the UK (using Interactive Brokers UK).\n\nI've recently been looking at Divident ETFs &amp; wanted some help finding the correct tickers:\n\n\\*\\*What are the UK tickers for VYM &amp; SPHD? Preferably in USD but accessible by UK retail investors\\*\\*\n\nFor example, VUAA is the \"UK investor\" version of the VOO (i think)\n\nAnd for future reference, how and where do I find out this information for myself on IB? New to the platform so not sure how to navigate this yet. Or better yet, on Google. thanks!", "upvote_ratio": 0.57, "id": "t3_uci2cf", "created_utc": 1650994578.0}
{"sub": "investing", "title": "Best place to park money if equities are tanking but inflation is increasing?", "selftext": "Pretty straight forward.\n\nWe are seeing SPY fall hard but currently diesel is parabolic indicating high inflation across all goods coming back with a vengeance (after last month showing inflation easing). \n\nWhere is the best place to put your money in this situation?", "upvote_ratio": 0.82, "id": "t3_uchnfp", "created_utc": 1650993473.0}
{"sub": "investing", "title": "Thoughts on the SVOL ETF?", "selftext": "This is an interesting ETF from Simplify and I'm curious to hear thoughts and critiques. \n\nIt collects roll yield from the Vix curve giving a mid teens yield. Price moves inverse to vix with 1/4 exposure and has an options spread to cap extreme spikes.\n\nThere is downside risk if you buy when VIX is low. But if VIX is historically high like today the risk reward seems better than bonds or stocks to scoop this up. \n\nThis high yield is always suspicious, though, and I'm curious for other perspectives on what I might be missing. \n\nhttps://www.simplify.us/etfs/svol-simplify-volatility-premium-etf\n\n[https://www.simplify.us/case-study/high-income-yield-efficiently-harvesting-equity-volatility-premium](https://www.simplify.us/case-study/high-income-yield-efficiently-harvesting-equity-volatility-premium)\n\n[https://www.nasdaq.com/market-activity/funds-and-etfs/svol/dividend-history](https://www.nasdaq.com/market-activity/funds-and-etfs/svol/dividend-history)", "upvote_ratio": 0.71, "id": "t3_ucfixz", "created_utc": 1650987831.0}
{"sub": "investing", "title": "Is this twitter deal free money?", "selftext": "Hi Friends,\n\nI am looking at this twitter deal. Elon is buying all of the shares at 54.20 but right now the stock is trading around 50.20. That's around an 8% premium. Why shouldn't I put all of my portfolio into twitter until the sale and then move everything back to my normal portfolio???\n\nThanks\n\n&amp;#x200B;\n\nAlso, I am struggling to find the closing date for this transaction. When are the shares being called?", "upvote_ratio": 0.78, "id": "t3_ucesl3", "created_utc": 1650985922.0}
{"sub": "investing", "title": "Why not leverage SPY instead of individual stock pick for \"risk\"?", "selftext": "So much sentiment and data proves it's nearly impossible to beat the S&amp;P500 with stock picking. So why not use 2x, 3x, and options as leverage as \"risky\" bets for a portfolio? \n\nEven doubling the S&amp;P500's performance with a leveraged ETF at 10-15% of a portfolio seems like a better deal than picking an individual stock which could collapse just as easy as a leveraged ETF, but at least you have the S&amp;P500's strong history of returns on your side.", "upvote_ratio": 0.77, "id": "t3_ucczgt", "created_utc": 1650980918.0}
{"sub": "investing", "title": "Investing in TBF or TBX (shorting bonds)", "selftext": "Shorting bonds seems like an obvious play to me right now. With high inflation continuing, it seems obvious that the Fed will have to raise rates. Many people also suspect they may have to raise them more than they are currently forecasting. ETFs like TBF (short 20 year treasuries) and TBX (short 7-10 year treasuries) allow you to short government bonds. There are also leveraged versions at TBT and PST, respectively. \n\nI would love to hear counterarguments against these trades.", "upvote_ratio": 0.82, "id": "t3_ucbtt1", "created_utc": 1650977402.0}
{"sub": "investing", "title": "Disney+ has the best potential of all the streaming services", "selftext": "Why? Because Disney has way better integration with the rest of their products and offerings. When Netflix makes a movie they get revenue only from the subscribers it can bring in. Meanwhile Disney+ has potentially:\n\n* Theatrical release (for movies)\n\n* Disney+ release\n\n* Merchanise sales\n\n* Disneyland parks sales\n\n* Games and other potential spin off media\n\nDisney also owns Hotstar and ESPN, so they stand to be much stronger when it comes to sports.\n\nHow can Netflix ever compete with this? How can any streaming service compete with this? Apple TV is currently cheapest but it also seems to be sold at a loss to get market share. Even with all the free iPhone and Mac advertisement their shows generate, can they actually compete with Disney+ long term? Right now most people seem ok with having several services, but what happens when they are all 20+ USD/month and have more than enough content each? \n\nAs I see it Netflix has replaced the cinema -&gt; dvd model with something that makes far less money per project. They just started at a time when streaming licenses were cheap. Now they are starting to pay a large premium for their projects and its forcing them to constantly raise prices.\n\nMeanwhile Disney is already profitable without the streaming, Disney+ is almost free money for them. Meaning when the competition gets rough they can be far cheaper than anyone else.\n\nI would like to hear you guys opinions.", "upvote_ratio": 0.39, "id": "t3_uc8y70", "created_utc": 1650967040.0}
{"sub": "investing", "title": "What was the most trendy no-brainer investment 30 years ago?", "selftext": "These days the popular go-to advice is \"Just buy VOO.\" So what was the equivalent of that in the 1990's and 2000's respectively? What was the thing most educated investors recommended to people in general as the safest most reliable and yet also most potentially beneficial investment the way they refer to VOO today?\n\nDid those things turn out to be the obvious no-brainer outcomes that everyone assumed they would be?", "upvote_ratio": 0.93, "id": "t3_uc8niv", "created_utc": 1650965720.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - April 26, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.81, "id": "t3_uc86o5", "created_utc": 1650963668.0}
{"sub": "investing", "title": "Elon Musk has not become the new owner of Twitter", "selftext": "Aside from the fact that the SEC needs to approve the sale, shareholders have to approve the sale. Their AGM is in a month (assuming this gets included in the main AGM and they don\u2019t have a special meeting/proxy). I\u2019m not saying it isn\u2019t likely, I\u2019m saying there\u2019s still a very legitimate possibility that this never goes through.", "upvote_ratio": 0.42, "id": "t3_uc72li", "created_utc": 1650958470.0}
{"sub": "investing", "title": "Selling 2 shares of stocks from 2 tax lots in a single transactions - 1 share at a loss and 1 share at a gain", "selftext": "Let\u2019s say you bought 1 share of $X @$50 and another share @60, and held them for more than 1 year.\n\nThen you sold both in a single transaction @$58, meaning at an overall gain of $6.\n\nNow a question for taxes:\n\nWhen you file taxes, would IRS think regard you as realizing:\nA) just a long term capital gain (+$6), or\nB) both a long term capital gain (+$8) and a long term capital loss (-$2) at the same time?\n\nThis will be significant if these number are much bigger and you have many tax lots, which seem pretty typical if you DCA?\n\nEdit: Better wordings", "upvote_ratio": 0.44, "id": "t3_uc4pzu", "created_utc": 1650948710.0}
{"sub": "investing", "title": "iBonds for UK citizens with a ITIN", "selftext": "I saw this video on iBonds and looked interesting: [https://youtu.be/san9EVb\\_pKc?t=508](https://youtu.be/san9EVb_pKc?t=508) (timestamped)\n\nCan a UK citizen with a ITIN (which sometimes is substituted with a Social Security) purchase this? \n\nAnd is there a UK equivalent to iBonds? They look pretty good for this current climate, albeit based on my rather limited abilities to DD this myself.", "upvote_ratio": 0.71, "id": "t3_uc47ua", "created_utc": 1650946835.0}
{"sub": "investing", "title": "What investments to make for the following scenarios?", "selftext": "I have had consistent success stock picking. Another one of my picks looks like it may be forced into a sale, and so the question becomes what to do with the extra cash?\n\nI have considered how my investments will do in many different environments. I think they will have a hard time in the following scenarios.\n\n1. Deflation/low inflation recession with rising interest rates\n\n2. Deflation/low inflation recession with falling interest rates\n\n3. Deflation/low inflation recession with stable interest rates\n\nWhat businesses could do well in these scenarios?\n\nWhat investments could do well?", "upvote_ratio": 0.67, "id": "t3_uc47nx", "created_utc": 1650946816.0}
{"sub": "investing", "title": "Does anyone have experience with Tribevest?", "selftext": "Supposedly the platform helps with some of the logistical issues. Formation, documents, the software they use and some other stuff. But does anyone have experience with it? Is it worth the money? It looks like it comes out to $350 a year which doesn't seem bad with member contributions. I run an LLC already but I thought of doing this as a different sort of venture. Basically I wanted to test this with a handful of trusted people, but I doubt the reviews on their website are real.", "upvote_ratio": 0.33, "id": "t3_uc248v", "created_utc": 1650939977.0}
{"sub": "investing", "title": "5 questions related to I Bonds", "selftext": "1. I attempted to create an account but received an email saying \u201cWe are having difficulty verifying the information you provided when opening your account.\u201d and want me to mail an authorization form to them and receive an account approval in 10-15 business days prior to me being eligible to purchase an I bond. Why so? Is there any way to work around this?\n\n2. My spouse was able to open an account without issues and purchased one for $10,000. At this point, can my spouse gift me one for $5,000 prior to the month end (because my account is on hold)? MAking the total bonds purchased from her account = $15000\n\n3. I should have purchased 10 x $1000 bonds. If I didn\u2019t do that, and possess 1 x $10,000, must I sell the entire thing when the time comes? Or can I simply sell just $1000 worth at first and leave the remaining $9000 in the bond? And continue to sell  in chunks as I please?\n\n4. We have 2 kids - a 3 year old and a 1 year old. They both have SSNs. Can I purchase I Bonds using their SSN with an intent to sell the bonds in an year or two without tax issues? If this is OK, should we file taxes for my young kids after sale? Also, must we use the profits from the sale of their I Bond towards their benefit only? Like toys\u2026?!\n\n5. I have an EIN and an LLC. Must the funds for the bond come from the company\u2019s business bank account or can I fund the bond with money from my personal bank account?", "upvote_ratio": 0.79, "id": "t3_uc1v5k", "created_utc": 1650939186.0}
{"sub": "investing", "title": "Taxes - what do you do about tax withholding from your investment income?", "selftext": "I'm curious what people do with respect to taxes and their (after-tax) investments.     \n\nfor 2021, some mutual funds I have did pretty well (like the rest of the market), and paid out most of their dividends and capital gains in the last quarter.  As a result, I got hit with a pretty big tax bill, and owed some penalties.  \n\nAs a result, I changed my W-4 to withhold an extra amount per paycheck.  \n\nWhat are other people doing?  Do you have the brokerage withhold taxes on any investment income?  Make quarterly payments?  Something else?", "upvote_ratio": 0.5, "id": "t3_uc1mf0", "created_utc": 1650938441.0}
{"sub": "investing", "title": "What happens to shares when private company goes public?", "selftext": "I invested some money into a private company through netcapital. I assumed once the company has an ipo I would be able to freely trade as I would with a stock on robinhood or another brokerage. Does anyone know how it works when a private company on netcapital goes public? Thanks!", "upvote_ratio": 0.43, "id": "t3_ubx5nc", "created_utc": 1650925189.0}
{"sub": "investing", "title": "do people invest in existing individual franchises?", "selftext": "I was looking into absentee franchise investments because I wanted another source of passive income besides stocks (of which the market is not doing well now). I got the idea because I live in a city and I know of food service places like Starbucks, dunkin, popeyes, etc, whose specific locations have been open for the past 10-15 yrs and remain wildly successful because they are in close proximity to hospitals, schools etc. And look like they will continue to be successful for years to come. I wanted to know if there was a way to invest in those existing specific locations without having to buy stock of the overall company or being involved in managing it. I thought that absentee franchise investing was how to do it but to my understanding , that involves you still having to open the franchise and find the workers. Any clarification is appreciated!", "upvote_ratio": 0.56, "id": "t3_ubwzce", "created_utc": 1650924709.0}
{"sub": "investing", "title": "If I am a NEET can I consider my all my earnings as losses and don't pay taxes?", "selftext": "Lets say I started investing from 2020\n\nI invested 2000 dollars\n\nCan I consider those 28 months as losses?\n\nLets say minimum salary is 1000 dollars/month so I am in 28 000 dollars deficit since I didn't work for doing trading.\n\nAnd lets say in march I hit the jackpot and earned 30 000 dollars.\n\nSo my deficit is 30 000 dollars and my earnings are 30 000 dollars, can I do that?\n\nOr do I have to pay taxes on the 28 000 dollars?\n\nTo be honest the time I dedicated to trading I would have used it to look for a job yet I operated on loss doing trading until I hit the jackpot, can I do that so to don't pay taxes?", "upvote_ratio": 0.2, "id": "t3_ubw31z", "created_utc": 1650922294.0}
{"sub": "investing", "title": "I have an EIN but I don't use it. Can I still buy another I-Bond for this month?", "selftext": "I bought an I bond and locked in the 7% rate as an Individual.\n\nI have an EIN but I don't use it. Can I still open up a Sole Proprietor entity account and lock in another 10k?\n\nIt seems so good to be true but I'm worried if there's some legal catch or something lol", "upvote_ratio": 0.74, "id": "t3_ubuj2i", "created_utc": 1650918210.0}
{"sub": "investing", "title": "(Honest) Why invest in crypto?", "selftext": "I keep hearing about crypto but I don't understand it so I'm asking for some help. So why do people invest in it? I know all investing is kind of like gambling but with stocks there's some tangible thing you get. Most stocks pay out dividends and in theory if a stock is undervalued then some very rich people will buy the stock because they want a partial ownership of the company itself which creates a scarcity that drives up prices. So there's something tangible there (ownership plus dividends) but with crypto I'm confused. What do you get? I hear people invest in it cause it's the \"future currency of the world\" but then wouldn't it be a bad investment because for a currency to function I'd have to stop all its volatility and become mostly stagnant like the us dollar. I also hear that people invest in it cause they are investing in the block chain technology but bitcoin isn't a company and they don't have any patents so how is it investing in the block chain? Is crypto just a pyramid scheme/gambling ring or am I missing something. Is it a worthy investment for someone a bit more risk adverse?", "upvote_ratio": 0.83, "id": "t3_ubuhjy", "created_utc": 1650918101.0}
{"sub": "investing", "title": "When do you anticipate pulling the trigger on dumping cash back into the stock market?", "selftext": " Just want to start a discussion and hear different peoples perspectives, speculations and strategies. What is your current strategy? Buying in slowly on the way down? Trying to time the bottom and lump some cash you've been saving or maybe catch the upswing early enough? How long do you see this correction/recession lasting? Weeks? Months? What are you doing with cash in the meantime to help preserve its value?", "upvote_ratio": 0.78, "id": "t3_ubsq9m", "created_utc": 1650913584.0}
{"sub": "investing", "title": "Elon Musk to Acquire Twitter", "selftext": "https://www.prnewswire.com/news-releases/elon-musk-to-acquire-twitter-301532245.html\n\n&gt; Twitter, Inc. (NYSE: TWTR) today announced that it has entered into a definitive agreement to be acquired by an entity wholly owned by Elon Musk, for $54.20 per share in cash in a transaction valued at approximately $44 billion. Upon completion of the transaction, Twitter will become a privately held company.\n\n&gt; Under the terms of the agreement, Twitter stockholders will receive $54.20 in cash for each share of Twitter common stock that they own upon closing of the proposed transaction. The purchase price represents a 38% premium to Twitter's closing stock price on April 1, 2022, which was the last trading day before Mr. Musk disclosed his approximately 9% stake in Twitter.\n\nThe stock currently trades at $51.", "upvote_ratio": 0.9, "id": "t3_ubsnxd", "created_utc": 1650913425.0}
{"sub": "investing", "title": "Life Insurance - Feedback", "selftext": "Hello,\n\nI am currently going through the process of considering if I need to opt into getting life insurance. I am at a cross roads on whether to consider term or Index Universal Life Insurance. One life insurance agent that I did reach out too did state that if I wasn't able to contribute $500 at a minimum per a month then an IUL plan would not be suitable if that minimum per a month cannot be met. Is that true? Otherwise, I would just consider a term life insurance plan that can allow a seamless conversion to an IUL plan later on. Would you advise against an IUL plan? I have heard that money is better suited elsewhere (ROTH IRA, 401k) because of the fee's within the plan &amp; the plan could also be structured in such a way to maximize the agents commission. The life insurance agent that I spoke to through email mentioned to max out both my Roth IRA &amp; Roth 401k before considering an IUL because they will have a greater upside. I am 27 by the way. I do have life insurance through my employer but I am not sure if they offer an IUL &amp; or term life insurance that can be converted to an IUL later on. So, I will have to look into that. Any &amp; all recommendations, comments &amp; or suggestions are welcome. \n\n&amp;#x200B;\n\nThanks.", "upvote_ratio": 0.67, "id": "t3_ubrkyt", "created_utc": 1650910539.0}
{"sub": "investing", "title": "How do I compete with wholesale investors and \u201cas-is\u201d companies?", "selftext": "Title is self explanatory - I am an individual investor with enough cash to buy my first property at around $300k. But as I am getting started, I\u2019m beginning to get intimidated by these TV commercials and signs etc everywhere for \u201cwe buy ugly houses\u201d or \u201csell us your ugly house\u201d etc. How can an individual even compete with these firms who have a team of cold callers and marketing?", "upvote_ratio": 0.4, "id": "t3_ubr3qk", "created_utc": 1650909274.0}
{"sub": "investing", "title": "Municipal bonds: Why is nobody talking about this?", "selftext": "State and local gov'ts got tons of COVID money, so are they all in good shape? During the 94 cycle Orange County actually went bankrupt.\n\nI'd think as rates rise municipal bonds would take a beating and some of these gov'ts would get in trouble but I feel like I'm wrong because literally nobody seems to be talking about it.", "upvote_ratio": 0.76, "id": "t3_ubq9rn", "created_utc": 1650907062.0}
{"sub": "investing", "title": "Where is the best place to park money during times of inflation, if you believe a bear market or crash is coming? Doesn't seem like any options?", "selftext": "It's well known and if anything we already see inflation taking place, groceries, gas, goods, prices all increasing. Typical rule is that cash isn't a great asset to own during inflation, since dollars will become worth less in terms of power. \n\nIf we believe a housing crash is coming and the stock market has even more room to fall, where would the best place to put money then. Seems like cash isn't recommended, and housing nor stocks doesn't seem to make sense, from the perspective of someone who believes it's going to fall more. Doesn't seem like too many places left to put it, aside from niche assets, like art, trading cards, but speaking more from actual common assets, many can relate too, unless there's something I'm now missing? \n\nEdit: To maybe clarify, since I know some might totally misunderstand, this post isn't to say a crash is definitely coming, but more to just discuss some ideas of best moves, if someone believed it was coming. \n\nIf someone believes they'll lose a game of sports betting, they'll prob sit still and not play, it doesn't mean if they did play, they'll lose the game, but more that since they don't believe it, they might as well move on to other avenues. In investing however, doing nothing would be holding cash, which apparently doesn't seem as advised either.", "upvote_ratio": 0.72, "id": "t3_ubpaf6", "created_utc": 1650904501.0}
{"sub": "investing", "title": "All about Leafly / $LFLY \ud83d\ude0a", "selftext": "*Note: This is incomplete and I'll try to continue to finish this throughout the week*. Hopefully we can have an in depth discussion about this company. \ud83e\udd1d  If anything, I hope this post can give some more information on the cannibis marketplace as well as potentially a stock to keep on the watchlist.\n\nTo start, I want to say that I am long this stock and am bullish, primarily because I think there are just way too many potential catalysts to not be bullish a marijuana ancillary stock, especially since the sector has been beaten down. I prefer to be in an ancillary stock as they are exposed to less legal risk and currently operate in a less competitive market than the producer/dispensary market. With that said, I am also fully aware that my timing may be off, given the upcoming recession and higher costs of raising capital. But I believe marijuana legalization will continue, and at the end of the day, I\u2019d rather be in a stock I like than try to time the market.\n\n**So let\u2019s see what Leafly is all about**! \ud83d\ude0a\n\n# Quick Overview\n\nEverything I talk about here will be expanded upon as we walk through the DD.\n\n[Leafly](https://www.leafly.com/) basically is a cross between a wiki/media company and online marketplace for weed. It makes its money currently from two sources, **retailers and brands**. Retailers are basically dispensaries that list their store on the website (for a subscription fee) and hope local customers order from their store. Brands are basically companies that produce products related to weed, such as edibles, vapes, etc\u2026 Brands can list their products on the website as well (for a fee) and or pay for advertisements. Currently, Leafly does not make any money from \u2018consumers\u2019 which are people like you and I who go on the website to check out stuff and purchase products.\n\nThe marketplace aspect is like Amazon, Uber Eats, Doordash, etc\u2026 a business that most are very familiar with. The wiki/media website you can think of as a news website or social media platform. And so as you can imagine, given the current methods of revenue, site traffic is very important for Leafly.\n\n# More About Leafly\n\nLeafly has been around since 2010 (operating a similar business model as today) but has had somewhat of a bumpy ride until now. In 2019 and 2020, due to poor leadership and management, the firm downsized its workforce by 50% and the current CEO, Yoko Miyashita, who has a legal background, took over. However, this turned out to be the turning point for the company! Looking at the comments on Glassdoor, Blind, and Indeed, it is clear that poor management was the root cause of the turmoil, and that those dark periods are over. For example, here is a [comment](https://www.reddit.com/r/weedbiz/comments/ibtlnk/comment/g2s47fg/?utm_source=share&amp;utm_medium=web2x&amp;context=3) on Reddit about Leafly from 2 years ago.\n\n&gt;Leafly used to have some of the best, coolest, and most passionate people in the cannabis industry. Then they laid off anyone who voiced concerns over their atrocious mismanagement, refused to promote from within, hired a bunch of bumbling asshats to run the company into the ground. None of the execs ever knew anything about cannabis, let alone felt passion for legalization.Signed, a former passionate employee who once lived and breathed for that company. May they watch their empire crumble and burn.\n\nAnd now this is what they are [saying](https://www.glassdoor.com/Reviews/Leafly-Reviews-E840667.htm?sort.sortType=RD&amp;sort.ascending=false&amp;filter.iso3Language=eng) on Glassdoor:\n\n&gt;Tons of smart people here. I don't think I've ever been surrounded by as many talented people at one company before. Lots of autonomy in how to get the work done. Very little management blockage (if at all). Good work life balance - show up for your meetings and get your work done in the agreed amount of time and you can be on your own schedule for the most part. Really good at being remote - don't miss the office one bit It never gets old talking about weed (real topic - selling weed for cheaper)\n\nAfter downsizing in 2020, Leafly grew headcount by 70% in 2021 and are still looking to hire engineers and sales people today.\n\nAnyways, with Yoko at CEO (who\u2019s passionate about weed), and once again with the desire to grow, in 2022, Leafly went [public](https://www.businesswire.com/news/home/20220204005386/en/Leafly-and-Merida-Merger-Corp.-I-Announce-Closing-of-Business-Combination) via merger with a venture capital / private equity firm.   As for whether or not the VC is looking for easy money or will remain invested for the long haul remains to be seen, at least it gives me a bit more confidence in Leafly as an investment.\n\n# Business\n\nAs mentioned before, Leafly basically functions as a media site and marketplace. On the media side of things, Leafly provides consumers with information about the latest strains and products, news, and reviews.\n\nLeafly currently makes money from retailers and brands.\n\n* Retailers are the licensed storefronts and delivery services that sell cannabis products to consumers. Example of retailer listing: [https://www.leafly.com/dispensary-info/the-re-up](https://www.leafly.com/dispensary-info/the-re-up).\n* Brands are the licensed producers of cannabis products or accessories that are made available for sale to consumers. Example of brand listing: [https://www.leafly.com/brands/five/products/five-rosin-gummies-100-solventless-cbd-thc-hemp-extract-gummies](https://www.leafly.com/brands/five/products/five-rosin-gummies-100-solventless-cbd-thc-hemp-extract-gummies)\n\nCurrently, a large portion of the revenue is subscription based, which is something you'd love to hear. According to their recent 10K filing, Leafly had roughly 10,500 retail listings on our platform, of which over 5,000 were paid. Additionally, Leafly had 9,500 brand listings on our platform, which they are in the process monetizing. Leafly believes there are currently 18,000 brands in the market, meaning that over 50% of the brands in existence are using Leafly. Leafly also claims that, approximately 53% of legal retailers in North America are paying subscribers on the Leafly platform.\n\n**In other words, Leafly has 50+% of all available brands and retailers using the platform**.\n\nLeafly offers different tiers of subscrption services and also allow brands/retailers the opportunity to purchase advertising add-ons, which would be things like advertisements and premium positioning on the website.\n\nCurrently, Leafly's ARPA (average revenue per account) is around $636. The recently lowered some subscription fees to try to increase the number of retailers using the platform and increase market penetration, which makes sense for a growing company.\n\n**International Expansion**: Leafly\u2019s content is available internationally, but the company does not have plans right now to expand outside of the United States and Canada. Other countries that have decriminalized medical marijuana include Germany, Australia, Mexico, and Jamaica.\n\nStrategic Initiatives\n\nSome of their 2022 goals include:\n\n* Hiring more engineers and sales people - this is one of their biggest impediment to growing right now.\n* Improving the bidding feature - basically allows retailers to easily bid for premium ad placement\n* Offer more features like menu merchandising (sponsored ads on retailer menus), delivery gateway\n* Consumer Personalization - basically recommend better products to consumers and offer a more personalized experience on the website\n* Making additional improvements to POS integrations to automate menus\n* Provide retailers with more and better data\n* Create a loyalty program for customers.\n\n# Valuation\n\nFor the full year 2022, we are revising our revenue projections to between $53 million and $58 million representing 29% growth over 2021 at the midpoint\n\n**As a reminder, this guidance does not factor in any new markets that have not begun legalized sales, including the largest East Coast markets like New York and New Jersey that are in the process of setting up their adult use recreational markets. With the revised revenue projections, we\u2019ve been very thoughtful of about our operating expenses.**\n\n# Competitors\n\nIMO, there really is only one other competitor that I would deem competitive enough and similar enough to be a competitor to Leafly, and that is Weedmaps. Sure Dutchie is probably the dominant provider of POS experience (think Shopify), Dutchie's marketplace is far from impressive and I doubt anyone actually goes to Dutchie to buy weed. And Leafly, as of today, is not involved in the POS business.\n\nAdditionally, while there definitely are other websites that offer information on weed strains and products and/or are marketplaces, none of them (outside of Weedmaps) come close to Leafly in terms of depth, quality, number of reviews, and MAUs (Monthly Active Users). According to [similarweb](https://www.similarweb.com/website/leafly.com/competitors/) competitors such as Wikileaf and Allbud (who both offer information on strains) do not even come close to Leafly in terms of the number of visits and other site usage metrics. [Alexa](https://www.alexa.com/siteinfo/leafly.com#section_traffic) also shows a similar story, with Leafly ranked 9620, Weedmaps ranked 8,728. For reference, Hightimes is ranked 69,584, so no one else is close.\n\n**A common criticism of Leafly\u2019s business model is that Google, Shopify, Instagram, Tiktok, Yelp, Doordash, Uber, Amazon, or etc... will eventually make Leafly obsolete**. While it is true that these other companies probably can do a better job than Leafly (i.e. Google is better for finding and reviewing dispensaries) at the end of the day, companies have a lot of legal hurdles and risks to deal with if they want to get into the marijuana business. It is my belief that, as long weed is not legalized Federally, the larger business like Doordash or Shopify will NOT enter the weed business. And because of this, as long as site traffic to Leafly is high, Leafly may be one of the few locations where brands can advertise their products. Leafly gets to take advantage of this current situation.\n\nAdditionally, even today, you almost never see advertisements for alcohol or tobacco on popular social media platforms. Even Leafly itself can\u2019t advertise on places like Gooogle, Amazon, Microsoft, Meta and Twitter today, which means retailers and brands probably can't either. There's reason to believe even if weed was legalized, the competition may still not be that stiff for Leafly.\n\n# Industry\n\nThe legal cannabis industry is estimated to be $40 billion to $50 billion in sales by 2025, which is double the market size of 2020. By 2030, cannabis is estimated to grow to $70 billion with some estimates as high as $100 billion in sales. There are currently [18](https://www.statista.com/chart/6681/the-states-where-its-legal-to-smoke-marijuana/) states that allow for recreational use and 37 states that have medical marijuana laws.\n\n# Future Catalysts\n\nThere are several future catalysts that could propel Leafly's stock price.\n\n**Federal Legalization**: This is the most obvious, the most well known, but also the least likely catalyst that will happen. I understand never say never, but I don\u2019t think any of us will see Federal Legalization in our lifetimes. I won\u2019t get into too much detail, but the two main reasons are politics and money.\n\n* **Politics**: While weed legalization is widely popular across the country, neither party wants to give the other a win, so we will never get enough votes. Unless a single party wins 60 Senate seats and their party controls the presidency, it won\u2019t happen.\n* **Big Pharma**: Big pharma is slated to lose A LOT of [money](https://www.latimes.com/opinion/story/2022-04-20/marijuana-recreational-use-drugmakers-lobbying) if weed was legalized, so they will try their best to not make it happen. We saw during Covid how much power Big Pharma has and they will do anything for money. Just do a quick Google search on why Big Pharma is against legalized weed and you'll see.\n\n**But don\u2019t fret, I actually think this is a blessing in disguise**. As long as weed is illegal on the Federal level, bigger companies such as Uber Eats, Google, Meta, Amazon, probably won\u2019t enter the industry, giving Leafly room to operate. In fact, I\u2019m so strongly opinionated about this that I would probably sell my Leafly holdings the day when weed is passed Federally as that basically opens the door for everyone to easily come into the business.\n\n**State Legalization**: This is the most likely catalyst that will help Leafly out. While Federally we won\u2019t see legalization any time soon, individual states will continue to legalize it every year. Leafly, according to its analyst presentation is well positioned on the East Coast, and that is where the next slew of stated legalizations will occur.\n\n**Speed Of State Legalization**: This is slightly different from the above - this talks about how quickly a market can be developed after legalization. Believe it or not, once a state legalizes marijuana, there\u2019s a long and drawn out process of licensing and what not, so it actually takes some time for a market to be developed. The longer we have to wait, the longer it\u2019ll take for retailers to list on the website and for there be enough competition on the website for retailers to bid each other for premium spots. In fact, the reason Leafly downgraded their guidance in their Q4 call was because this pace was slower than expected for some states (like Illinois). If this pace speeds back up, it will be a positive catalyst.\n\nFor example, In Illinois there are about 185 licenses tied up in litigation.\n\nIf Illinois were to pick up the pace and New York and New Jersey get everything set up, it's only a matter of time before Leafly will raise their guidance. Leafly claims to have a strong foothold in Illinois and is ready to onboard the second the retailers get their licenses. Also in their Q4 call, they mentioned the 2022 guidance:\n\n&gt;Does not factor in any new markets that have not begun legalized sales, including the largest East Coast markets like New York and New Jersey that are in the process of setting up their adult use recreational markets.\n\n# To Be Continued...", "upvote_ratio": 0.55, "id": "t3_ubme3b", "created_utc": 1650896767.0}
{"sub": "investing", "title": "Small Cap Value is out performing the market, and out-performed the market in the 1970s.", "selftext": "Not financial advice, do your own DD.\n\nWhile no one can predict what the market will do, history shows that the market tends to rotate in and out of industries, asset classes, and domestic/foreign markets in regards to outperformance and underperformance.\n\nCurrently, many are unsure of how to proceed in this investing environment after a decade of unprecedented growth and innovation that witnessed large cap tech industry stocks leap ahead the rest of the market. Now, however, with gas prices, inflation, and interest rate hikes have many likening this period to the 1970's, in which the US stock market underperformed (especially large cap growth). A mere .25% interest rate hike and the end of QE have been enough to cause the NASDAQ to correct by almost 20% - and more rate hikes are on the way.\n\nWhile I'm still personally bullish on tech long term when we eventually rebound, I am opening a position in Small Cap Value. I believe that should we really witness another market rotation and decade of underperformance such as the 1970s, that Small Cap Value will do well. In the 1970's it outperformed massively, and currently IJS, an ETF that tracks Small Cap Value stock index, is down -6.17% YTD. Not great, but so far is beating more conventional investments such as cash (-7.5 Jan and -7.9 Feb 2022)  the NASDAQ (-18.99) and the US Market as a whole (-11.83%). When eventually we do recover, Small Cap Value historically as been an asset class that does best in \"recovery environment\" and \"a young bull market\"\n\n**TL;DR;** For me personally I believe now isn't the time to sell tech stocks at a loss, as Tech is still way forward. However, now is the time I'm opening a position in Small Cap Value to make money this next decade should my guess be correct that it will resemble the conditions of the 1970's.", "upvote_ratio": 0.65, "id": "t3_ubl1ru", "created_utc": 1650892932.0}
{"sub": "investing", "title": "Twitter set to accept Musk's $43 bln offer - sources", "selftext": "From the article:\n\n&gt; NEW YORK, April 25 (Reuters) - Twitter Inc (TWTR.N) is poised to agree a sale to Elon Musk for around $43 billion in cash, the price the chief executive of Tesla Inc (TSLA.O) has called his \"best and final\" offer for the social media company, people familiar with the matter said.\n\n&gt; Twitter may announce the $54.20-per-share deal later on Monday once its board has met to recommend the transaction to Twitter shareholders, the sources said. It is always possible that the deal collapses at the last minute, the sources added.\n\nhttps://www.reuters.com/technology/exclusive-twitter-set-accept-musks-best-final-offer-sources-2022-04-25/\n\nThis was a fast turnaround from their \"poison pill\", if true.", "upvote_ratio": 0.95, "id": "t3_ubkklq", "created_utc": 1650891536.0}
{"sub": "investing", "title": "Comparable Merrill Lynch mutual funds similar to the VTSAX, VTIAX, &amp; VTWAX without transaction fees?", "selftext": "I had to move my portfolio over to Merrill Lynch because my wife got a new job at BofA.  Maybe I just need to get used to ML UI, but I don't like it compared to Charles Schwab at all.  On top of that, I had to dump my CS proprietary mutual funds, and ML is charging a transaction fee for vanguard mutual funds.  Do I have any alternatives non transaction fee for my Roth IRA?", "upvote_ratio": 0.5, "id": "t3_ubkc0v", "created_utc": 1650890769.0}
{"sub": "investing", "title": "Finding mutual fund holdings data", "selftext": "I'm working on a research project and trying to find a dataset of mutual funds including their holdings (and the tickers of the holdings). Any ideas where I can find this dataset, preferably for free? :)\n\nI'll be sure to share the project on reddit after I finish building it, but this is a key piece of data!\n\nThank you!", "upvote_ratio": 0.25, "id": "t3_ubj8cp", "created_utc": 1650887176.0}
{"sub": "investing", "title": "Recession and tech stocks", "selftext": "All,\n\nConsidering ongoing macro economic and geo political issues, which tech stock would you actually buy or consider buying in next days?\n\nLow cap + high growth stocks will be almost out of question. High interest rates and recession worries will put a strain on borrowing and high spend on customer acquisitions.\n\nMid - high cap on B2C side will suffer as well. This will put strain on companies like Apple since people would delay upgrading their devices. Companies such as AMD, qualcomm, intel with significant B2C exposure will also suffer. Netflix is already getting hammered.\n\nMid-high cap B2B. Relatively these companies should be in much better position. Companies are not going to stop using Microsoft products or discontinue the aws cloud or stop spending on Google advertisment. Sure, their growth rates will suffer but it's hard to think that their existing hold over market will be affected.\n\nI'm thinking of readjusting my portfolio and would appreciate your inputs. Generally, I'm a fan of the tech sector from a long term perspective and would still invest in it at current conditions. I would still stick to active investing rather than ETFs since i assume accrual returns on ETFs would also be much lower.", "upvote_ratio": 0.67, "id": "t3_ubi2qk", "created_utc": 1650882971.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - April 25, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.93, "id": "t3_ubgork", "created_utc": 1650877269.0}
{"sub": "investing", "title": "Do I smell 2008? Can the housing market really be so hot?", "selftext": "Hi All\n\nBeen reading some posts about housing in the US. I'm based in Europe and we are seeing ridiculous housing prices.\n\nMy house was just valued at a crazy amount and there seems to be nothing on the market.\n\nI'm in Slovakia which is bordering Ukraine so there is also  an increased demand from Ukrainians who want to buy or rent l.\n\nI've been contacting real estate agencies for both rent and for sale. The arrogance is unbelievable. When I call, they simply reply an SMS with the prices and ask if I want it without even viewing.\n\nSo, I took a look at bankruptcies in the EU and they are down!\n\nInterest rates are at about 1 - 1.5% here at the moment but I've seen they are creeping up. I'm sure, when there is a 2 or 3 percent increase, people will start defaulting. \n\nWe are thinking to sell our house and just rent for a few years because this feels like a bubble. We will keep watching the market and try to buy land, or maybe a mixed bag of properties. \n\nNow, I know we can't really predict the market but could anyone suggest some indicators I could check? How do you make decisions regarding real estate investments?\n\nEdit: Just to be clear, when I say 2008, I mean just housing crashing part. I have quite a good understanding of the 2008 financial crash, and it is understood that the reason for the next crash is very unlikely to be for any of those reasons, I mean more in regards the outcome of such a crash - houses being sold a fire sale prices, defaults.etc.", "upvote_ratio": 0.65, "id": "t3_ubdrs4", "created_utc": 1650864904.0}
{"sub": "investing", "title": "U.S. stock futures sink, suggesting more possible losses Monday", "selftext": " Hope conditions improve when the earnings coming out.\n\n*Wall Street is bracing for a busy earnings week, with quarterly reports due from* [*Apple Inc.*](https://www.marketwatch.com/story/apple-gave-investors-more-than-100-billion-last-year-how-much-more-is-coming-11650654829?mod=home-page&amp;mod=article_inline) *Facebook parent* [*Meta Platforms Inc.*](https://www.marketwatch.com/story/facebooks-meh-year-could-get-better-it-just-isnt-11650656197?mod=home-page&amp;mod=article_inline) *Google parent* [*Alphabet Inc.*](https://www.marketwatch.com/story/amid-a-storm-in-tech-sector-google-is-staying-relatively-dry-and-happy-11650658503?mod=home-page&amp;mod=article_inline) *Amazon.com Inc. and Microsoft Corp. among others. Investors will also keep an eye on Twitter Inc. which reports earnings Thursday and on Sunday* [*was reported to be re-evaluating Elon Musk\u2019s takeover bid*](https://www.marketwatch.com/story/twitter-taking-a-second-look-at-elon-musks-takeover-offer-11650830958?mod=home-page&amp;mod=article_inline)*.*\n\n*On Friday, the Dow shed about 981 points, or 2.8%, marking* [*its worst daily percentage drop since Oct. 28, 2020*](https://www.marketwatch.com/story/stock-futures-drop-and-bond-yields-climb-following-hawkish-comments-by-feds-powell-11650622790?mod=home-page&amp;mod=article_inline)*, according to Dow Jones Market data. The S&amp;P 500 index \u00a0slid 2.8% and the Nasdaq Composite Index \u00a0tumbled 2.6%.*\n\n*For the week, the Dow was down 1.9%, the S&amp;P 500 fell 2.8% and the Nasdaq dropped 3.8%, according to FactSet.*\n\n*Fed Chairman Jerome Powell added to the worries of jittery investors Thursday by* [*signaling support for a larger, 50-basis-point rate hike*](https://www.marketwatch.com/story/powell-backs-moving-more-quickly-on-interest-rate-hikes-11650562888?mod=mw_latestnews&amp;mod=article_inline&amp;mod=article_inline&amp;mod=article_inline) *at the Fed\u2019s May meeting.*", "upvote_ratio": 0.92, "id": "t3_ubce2u", "created_utc": 1650859697.0}
{"sub": "investing", "title": "The Fed Wants to Raise Rates Quickly, but May Not Know Where to Stop", "selftext": "https://www.wsj.com/articles/the-fed-wants-to-raise-rates-quickly-but-may-not-know-where-to-stop-11650792603?mod=hp_lead_pos5\n\nFederal Reserve Chairman Jerome Powell is shifting monetary tightening into a higher gear. His goal sounds straightforward\u2014lift interest rates to \u201cneutral,\u201d a setting that neither spurs nor slows growth.\n\nBut there\u2019s a catch: Even in normal times, no one knows where this theoretical level is. And these aren\u2019t normal times. There are good reasons to think the ground beneath the central bank\u2019s feet is shifting and that, after accounting for elevated inflation, neutral may be higher than officials\u2019 recent estimates.\n\nAt their meeting next month, officials are set to approve plans to shrink their $9 trillion asset portfolio and to raise their benchmark rate by a half percentage point. They are poised to follow with another half-point in June.\n\n\u201cWe\u2019re going to be raising rates and getting expeditiously to levels that are more neutral, and then that are actually tightening policy if that turns out to be appropriate, once we get there,\u201d Mr. Powell said during a panel discussion last week.\n\n---\n\nMy point in posting this is specifically their \"Price Pressures\" chart depicting inflation.  It's a pretty significant spike.\n\nYou can see in that chart that from 1970-1982, when the Fed last had to fight significant inflation, there were more recessions in those twelve years than in the forty that followed, 1982-2022.  \n\nA lot of people have noted that every cycle seems to involve the Fed allowing for easier and easier money, and this article puts it all together...the Fed could do this, indeed you can argue the Fed did the right thing doing this, because inflation was contained.  That is no longer the case.  This new state of affairs likely presages an aggressive rate hiking campaign that will not be like anything we've seen during the extended bull market of 1982-present day.\n\nTLDR:  1) Easy money is over, 2) Don't fight the Fed.\n\nThis is what Warren Buffett refers to as the tide going out, and it looks to be going out in a big way.  Likely growth stocks with marginal to negative earnings will continue to get hit the hardest.", "upvote_ratio": 0.86, "id": "t3_ubc9td", "created_utc": 1650859268.0}
{"sub": "investing", "title": "How to not lose money, from wars/major conflicts", "selftext": "Hey everyone, first post\n\nI am a tech investor-growth investor. I believe firmly an event/conflict to happen, that will increase the chip-plastics-metal manufacturing-labor market shortage significantly.\n\nIf I believe this event to happen in 2 years, how should I prepare my portfolio now to make out as + as possible at that time?\n\nI have heard shorting the countries ETFs might be a good option, diversifying into raw materials at countries not involved in the conflict.\n\nPlease let me know your thoughts. I\u2019m a bit old timey with my investment strategies, but I\u2019m willing to commit to learning some more in effort to avoid taking a hypothetical huge hit.", "upvote_ratio": 0.27, "id": "t3_ubae7u", "created_utc": 1650852897.0}
{"sub": "investing", "title": "are we on pace to have a decade like the 1970s in the market?", "selftext": "There were 4 years in which the dow was in red with the highest being -27% in 1974 and the lowest being -3% in 1978. Overall, not a great year for the markets. Obviously, this year is probably lost with inflation fears and the war in ukraine. what r your thoughts about this decade being similar to the 1970s?\n\n1978 -3\n\n1977 -17\n\n1974 -27\n\n1973 -16", "upvote_ratio": 0.84, "id": "t3_ubacuj", "created_utc": 1650852772.0}
{"sub": "investing", "title": "Understanding Vanguards Margin .. my holdings turned into two holdings", "selftext": "Hello guys,\n\nI made my brokerage account to a margin account, and alot of my holdings have now been split into the stock name + stock name (cash). Like for example  VTSAX, and VTSAX(Cash), AMD, AMD (Cash)...why is this?\n\nI don't anticipate using margin any time soon but just wanted it avaliable... so basically if I only use whatever I deposit into my settlement fund, vanguard wont charge me any interest just because I have a margin account right? For example if I have 10k in my settlement fund, and I buy assets &lt; 10k in value, I wont be charged any interest, correct? Only if I go &gt;10K.\n\nThanks", "upvote_ratio": 0.44, "id": "t3_ub9j7j", "created_utc": 1650850105.0}
{"sub": "investing", "title": "Need Opinion on the current situation", "selftext": "Hello all,\n\nI have been reading some articles and would like to understand some aspects better.\n\nPlease correct me for wherever I might be wrong.\n\nRecently US Fed Reserve stated that they will stop buying MBS (mortgage backed securities) which in my mind means will stop buying debt from the market.\n\nThey, also have increased and will increase the rates to borrow and essentially make it costly to borrow.\n\nSo, my question is who in this time would fill the gap that Fed was, by buying the excess MBS and Treasuries Bonds as they are now not having cheap access to money.\n\nPlease feel free to correct any conclusions and theories of mine.", "upvote_ratio": 0.79, "id": "t3_ub840g", "created_utc": 1650845625.0}
{"sub": "investing", "title": "Who makes the decision to dilute company shares?", "selftext": "Hi everyone,\n\nI was recently reading about the dangers of share dilution. However, there is a fundamental question I couldn't find an answer to anywhere.\n\nWhen a company dilutes its shares by offering more to the market, who is the decision-maker? Do shareholders vote on it? Do all of them vote on it or just some of them? Or is it a decision from management? If so, who in management?\n\nIf someone has some real-life examples or stories of this, that would be much appreciated. In particular, I'd like to learn how it affected you as an investor.\n\nWhat are some things a shareholder can do to mitigate the risks of share dilution?\n\nDo shareholders have foreknowledge of a company's intent on issuing more shares? If so, can't one be \"blindsided\" by a sudden decrease in one's share value?\n\nAnother thing I was wondering is does any of this differ in the early stage of a company vs later on? For instance, if an angel investor invests in a company for 50%, does he get decision-making power in terms of whether the company stock is diluted? What about if someone (theoretically) owns 50% of the stock of a public company. Is it the same thing?\n\nMy apologies if any of this reaks of ignorance. I am new to this whole thing and the amount of technical terms, legalities and information is a little overwhelming. I feel like I need to take a course to even understand the basics of how the financial and investment worlds work. Any resources would be greatly appreciated.", "upvote_ratio": 0.73, "id": "t3_ub61ty", "created_utc": 1650839237.0}
{"sub": "investing", "title": "Starting an online business. Need advice.", "selftext": "I'm planning on starting an online business where I assist smaller/medium sized businesses with certain tasks. Such as creating a website for them, SEO, and so on. Basically anything to get them into the digital landscape.\n\nHowever, I'm having issues with two questions:\n\n1) What kind of small/medium sized businesses can I look into? That is, what niche are mature and needs to be in the digital landscape to further reap profits?\n\n2) What kind of services can I provide to these aforementioned businesses? I'm not looking for anyone to provide me an exhaustive list, but perhaps link me to an article or help me get started with my research? So far from my research, website design/creation, SEO, ads, etc. are something that would solve many problems for many offline firms.\n\nThankful for feedback!", "upvote_ratio": 0.1, "id": "t3_ub5sj5", "created_utc": 1650838473.0}
{"sub": "investing", "title": "Retirement income minimum amount", "selftext": "There was a post on here or r/stocks or somewhere not too long ago and it was someone asking about dividend stocks and in it he said he needed \\*k dollars per year in retirement. It was a low enough number by American standards but I figured from the post that the person was not in the US.\n\nNow he got a lot of slack about the number, people telling him he didn't know what he was talking about and he hadn't factored in x,y and z. I had been meaning to write a post asking what people think they need as a min in their part of the world. Obviously cost of living in NYC is not the same as Thailand.\n\nAlso interested in people work it out....\n\nThis is the way I would do it.\n\nI find my cost of living as of now being  a middle-aged man with a small family and being the only income.\n\nI take my wages and subtract my mortgage and what I put towards savings/stocks per year (obviously in retirement I will no longer save and my mortgage will be finished). I end up with 25K, we have two cars so I take a couple of K away for the second car, let's say 23k. Take away the taxes I pay and it's more like 15k.\n\nWhere I live there's a state pension of about 12k. Myself and my wife will both get this so that's 24k tax free.\n\nSo I already live on less than the state pension and even if everything goes to shit and pensions got halved then I could still get by with very little savings.\n\n&amp;#x200B;\n\nNow I know there will be people thinking 'what about contingencies', 'what if you or your family get sick and face huge hospital bills' what about when you get alzheimers and have to pay for a care home'\n\nThe MINUMUM I am talking about does not include things that may not happen, that is the idea of this post....to ask what the minimum is to have a comfortable life, food on the table, roof above your head, etc.\n\nSo what's your figure per year?\n\n&amp;#x200B;\n\nBtw the amounts above are euro.\n\nEDIT: Thanks to the people who gave a figure, it was interesting to see that US retirement figures seem to be way above what people in European countries might need, medical insurance/costs seems to be one of the biggest factors that causes people concern.\n\nIn Ireland I would struggle to find anyone amongst my peers that has expenses of 100k+ per year, maybe if you had 3 or more kids in University...? \n\nAnyway thanks for the replies and it was an interesting eye opener for me...happy investing", "upvote_ratio": 0.62, "id": "t3_ub5sdg", "created_utc": 1650838460.0}
{"sub": "investing", "title": "I bought 75k worth of stocks on Friday at Close", "selftext": "I spent 30k on tesla and 20k on SPY, and 25k on Apple at the end of the day today for a long term hold, how do you think i will do? Any changes? I feel like i should have waited for some of their earnings next week but i saw this dip the last couple days in the market and had to capitalize on it.\n\nI have another 25k on the side to buy any dips if things continue to drop, and will be continually adding between $500-1000 a week on these positions for the foreseeable future. This is now my whole portfolio.", "upvote_ratio": 0.28, "id": "t3_ub1ylo", "created_utc": 1650827622.0}
{"sub": "investing", "title": "How to know in advance whether a foreign company pays qualified dividends?", "selftext": "From Investopeida ([link](https://www.investopedia.com/terms/q/qualifieddividend.asp)):  \n\"*A foreign corporation qualifies for the special tax treatment\u00a0if it meets one of the following three conditions: the company is incorporated in a U.S. possession, the corporation is eligible for the benefits of a* [*comprehensive income*](https://www.investopedia.com/terms/c/comprehensiveincome.asp) [*tax treaty*](https://www.investopedia.com/terms/t/taxtreaty.asp) *with the United States,\u00a0or the stock is readily tradable on an established securities market in the United States. A foreign corporation is not qualified if it is considered a* [*passive foreign investment company*](https://www.investopedia.com/terms/p/pfic.asp)*.*\"\n\nI don't know where to even start to look this information up for the stocks I'm interested in. Specifically talking about China Petroleum and Chemical (SNP), but in general, how can I figure this out?", "upvote_ratio": 0.67, "id": "t3_ub0vrh", "created_utc": 1650824596.0}
{"sub": "investing", "title": "Honest question about liquidity", "selftext": "Hi! I've been searching everywhere for an answer but haven't found one yet so I thought maybe someone here might be able to help me. Does anyone know how to measure the liquidity of a company per day? (i.e compaies with a Liquidity per day &gt; 250,000).  I'm familiar with other liquidity ratios but couldn't find anything about this one in particular. If someone could help me I'd really appreciate it. Cheers!", "upvote_ratio": 0.75, "id": "t3_uazwu2", "created_utc": 1650821905.0}
{"sub": "investing", "title": "Does anyone else remember Motley Fool's \"Ready-Made Millionaire\"?", "selftext": "Perhaps 15 years ago, MF promoted an investing scheme called \"Ready-Made Millionaire\" what they made sound like an investment opportunity of a lifetime.\n\nNow in 2022, I can't find a mention of it anywhere on the internet.\n\nDoes anyone else remember this, and if so what happened to it?\n\nEDIT: This person found mention of it!\nhttps://www.reddit.com/r/investing/comments/uazg01/does_anyone_else_remember_motley_fools_readymade/i61mzo6?utm_medium=android_app&amp;utm_source=share&amp;context=3", "upvote_ratio": 0.92, "id": "t3_uazg01", "created_utc": 1650820537.0}
{"sub": "investing", "title": "IRA withdrawal tax questions", "selftext": "Hello,\n\nI understand it\u2019s not a good idea to take money out of IRA. But sometimes shit happens and you need cash. \n\nMy question is regarding tax. If I chose to pay 35% tax upon withdrawal of the fund, will I still owe more at the end of the year?\n\nThank you", "upvote_ratio": 0.71, "id": "t3_uaz3dc", "created_utc": 1650819577.0}
{"sub": "investing", "title": "Why does Vanguard seem to be the only brokerage that doesn't offer pledged asset loans?", "selftext": "Both ETrade and Charles Schwab will let you take out loans against your holdings at super low rates. This is convenient for short term expenses: you don't have to sell stuff and pay capital gains, and your holdings can continue to grow.\n\nFor some reason Vanguard doesn't offer this at all, the closest they can get is a max of $50k against your 401k, and a margin account (which has way higher rates and other tax implications).", "upvote_ratio": 0.44, "id": "t3_uay0jr", "created_utc": 1650816429.0}
{"sub": "investing", "title": "SMG: Value or Value Trap?", "selftext": " Ticker: SMG\n\nScott's Miracle Grow\n\n[https://finviz.com/quote.ashx?t=SMG&amp;ty=c&amp;ta=1&amp;p=d](https://finviz.com/quote.ashx?t=SMG&amp;ty=c&amp;ta=1&amp;p=d)\n\n\\-P/E Ratio: of 11.72 (Historically SSMG has been above 15)\n\n\\-Debt to Equity Ratio of 3.90\n\n\\-They don't seem to have a lot of cash on hand\n\n[https://www.macrotrends.net/stocks/charts/SMG/scotts-miracle-gro/cash-on-hand](https://www.macrotrends.net/stocks/charts/SMG/scotts-miracle-gro/cash-on-hand)\n\n\\-They have a portion of the business as Hawthorne, which supplies marijuana growers with equipment. Its possible this portion of the companies breaks away from SMG in the future (from last earning call)\n\ntrading at $105\n\nIs it value or value trap?", "upvote_ratio": 0.84, "id": "t3_uaxck4", "created_utc": 1650814548.0}
{"sub": "investing", "title": "shorting the market crash", "selftext": "So everybody and their mother knows the market is about to blow. But holding cash doesn't seem right with high inflation actively devaluing our savings. So I want to actively short the crash to strike a profit. What needs to be done. Spy puts? Or should I try a leveraged bear ETF like SQQ? Or maybe even short stocks directly?", "upvote_ratio": 0.26, "id": "t3_uat6l2", "created_utc": 1650801375.0}
{"sub": "investing", "title": "Is the Mutual Fund NAV the average SE between all collected companies?", "selftext": "Considering that the NAV is calculated as ( total value of all the cash and securities (assets) - liabilities)/ Shares outstanding, which is the same as Net-assets or SE / shares outstanding.\n\nI can't seem to find any confirmation for this and am genuinly curious if that is what it is.", "upvote_ratio": 0.76, "id": "t3_uaroyg", "created_utc": 1650795352.0}
{"sub": "investing", "title": "Clarity on CD Interest Payments", "selftext": "Good morning,\n\nCD terms are giving me a headache. I could use some help understanding how interest is paid out. Here are the details of one investment I am looking at, a 2 year CD:\n\n[Pay Frequency](https://scs.fidelity.com/webxpress/help/topics/help_definition_p.shtml#paymentfrequency)MONTHLY\n\n[Coupon](https://scs.fidelity.com/webxpress/help/topics/help_definition_c.shtml#coupon)2.700\n\n[Maturity Date](https://scs.fidelity.com/webxpress/help/topics/help_definition_m.shtml#maturitydate)05/06/2024\n\n[Bond Type](https://scs.fidelity.com/webxpress/help/topics/help_definition_b.shtml#bondtype)CD\n\n[Interest Accrual Date](https://scs.fidelity.com/webxpress/help/topics/help_definition_i.shtml#interestaccrualdate)05/04/2022\n\nIf I buy one of these for $1000, am I to understand that:\n\na) I will be paid $27 (2.7% of 1000) total each **year**, and some fraction of that amount each month until maturity \n\nOR\n\nb) I will be paid $27 each **month** until maturity  \n\n\nOne of these outcomes is much better than the other! But I do not want to leap into an investment on a misunderstanding! Thank you to anyone who reads and responds.", "upvote_ratio": 0.77, "id": "t3_uarm2u", "created_utc": 1650795016.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - April 24, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.79, "id": "t3_uaqng5", "created_utc": 1650790868.0}
{"sub": "investing", "title": "Where is Vanguard stock held?", "selftext": "I've heard that The Vanguard Group, Inc has an interesting structure, as described by Investopedia:\n&gt; The company is owned by its funds. The company\u2019s different funds are then owned by the shareholders. Thus, the shareholders are the true owners of Vanguard.\n\nI'm curious how this works and more importantly - how is this validated?\n\nI looked at SEC filings and when I looked up reports of fund investments I didn't see any funds owning The Vanguard Group, Inc.\n\n\nFor example, lets look at the [Vanguard Explorer Fund Investor Shares \\(VEXPX\\)](https://investor.vanguard.com/mutual-funds/profile/overview/vexpx)\n\n\nSEC Document: [\\[NPORT-P\\] Monthly Portfolio Investments Report \\(2022-04-01 00:00:00\\)](https://sec.report/Document/0001752724-22-080508/)\n\n\nAm I missing something? (Completely possible - I'm no pro)\n\nI don't see how the company is owned by the funds - am I looking in the wrong place or how is this validated?", "upvote_ratio": 0.45, "id": "t3_ualmiy", "created_utc": 1650770383.0}
{"sub": "investing", "title": "Where can I find historical earnings beats/misses?", "selftext": "Hello everyone, I was wondering if there was anywhere I can view earnings beats and misses as a statistic. After NFLX last week I'd like to be able to check before committing to earnings plays.\n\nI understand I can view earnings individually but I'm mostly looking for consistent earnings beats and misses in an easy to understand format. This has come to mind after NFLX earnings and wish I had a heavier position knowing now they had missed 14 of 16.\n\n  \nThanks in advance!", "upvote_ratio": 0.8, "id": "t3_uaku98", "created_utc": 1650767581.0}
{"sub": "investing", "title": "combatting potential bad yr of investment growth", "selftext": "I recently got into investing..decided to use a robo advisor through Schwab to start me off in Dec. 2021. I do understand the portfolio set up for me....but it's not looking too great. \n\nI have a 401K and maxed out HSA. Already doing the right things there. I have 8 months in emergency savings. \n\nSo, anyways, how are you battling the not so great market right now when it comes to your investments? I'm looking for guidance as I'm going to start breaking away from the roboadvisor. \n\nI have $3k remaining per month to invest.", "upvote_ratio": 0.59, "id": "t3_uakqg6", "created_utc": 1650767206.0}
{"sub": "investing", "title": "Wash Sale Question TD Ameritrade", "selftext": "I am kinda new to wash sales.  I bought an ETF for $14 a share; the DRIP over the course of a few months brought the price per share down to $13.71 (the ETF DRIPs at NAV which is lower than the share price).  I ended up selling all my shares at a share price of $13.85 (for a profit).  So while it was below the initial share price of $14; it was still higher than the cost basis share price including DRIP shares of $13.71.\n\nThen 2 weeks later, I buy more shares of the same ETF at a lower price and TD Ameritrade is calling it a wash sale.  So even though the shares were sold for a profit, they were not sold at or above the initial share procurement price of $14.  I tried to call TD Ameritrade today, but was told to call back Monday.  I know the wash sale rules are a little vague, but wanted to see what you folks thought about this scenario.", "upvote_ratio": 0.74, "id": "t3_uak97o", "created_utc": 1650765543.0}
{"sub": "investing", "title": "Can anyone help me understand Blackstone's offer for ACC and it's implications for me?", "selftext": "tl;dr, ACC is being bought out and I own quite a few shares--what implications does this have for me and what should I do right now?\n\nAs per this article ([https://www.cnbc.com/2022/04/19/blackstone-to-buy-american-campus-communities-in-12point8-billion-deal.html](https://www.cnbc.com/2022/04/19/blackstone-to-buy-american-campus-communities-in-12point8-billion-deal.html)) \" Blackstone will buy American Campus Communities for $12.8 billion \\[...\\]  The per-share price of the all-cash deal is $65.47\"\n\nI've liked this REIT for a long time and have been adding shares quarterly to my portfolio.  Seems to be a good value and continues to be--the dividend it pays doesn't hurt either and has been on auto-reinvest.  What exactly does this mean for me?  If the deal goes through, will the shares disappear from my account and case appear?  Will I get shares of Blackstone instead?\n\nIf the price per-share is 65.47, but it is still trading lower (64.80 at last close) does it make any sense for me to buy into more shares next week as was originally planned?  I'm not guaranteed a 67c per share markup when the deal goes through right?   Otherwise everyone would be buying it at this \"discount\".  Also, I'm guessing there will be another dividend payment before the deal is finalized so maybe buying does make sense?\n\nAny help is appreciated!", "upvote_ratio": 0.69, "id": "t3_uahp8q", "created_utc": 1650756845.0}
{"sub": "investing", "title": "Increasing interest rates, P2P resurgence?", "selftext": "Over the past few years debt has been really cheap. As everyone knows, rates are going up with the fed saying a 50 basis point increase is possible to tame inflation, with more increases to come. With traditional lending becoming more and more expensive, I'd like to know everyone's opinion on P2P lending and if you think that it will make a comeback. I've been racking my brain trying to think of what could be the next \"big investment\" and am wondering if this is the route to go down. I have some history investing with Lending Club and it wasn't awful. I think I made around 5-6% after all of the defaults and charge-offs. I don't know if I have complete confidence in the market right now and think we could be heading towards some type of recession. One of the downsides of P2P is the length of time that your money is locked in but, if you have extra cash laying around maybe it's worth a shot? Would love to hear some opinions.", "upvote_ratio": 0.65, "id": "t3_uagew2", "created_utc": 1650752788.0}
{"sub": "investing", "title": "My thoughts on current macroeconomic state", "selftext": "So the Federal Reserve is going to hike rates to cool the economy and inflation. Inflation itself will tamp down demand, and so will the rate hikes. Housing prices are already inflated, add in some rate hikes, the demand for housing should drop some, or at least not go up anymore. \n\nUnemployment numbers suggests a strong economy right now. So the rate hikes should not cause too much issue. At most mild recession.\n\nI am thinking of 2 scenarios regarding the stock market:\n\n1) rate hikes till neutral (2%) or slightly above neutral (2.5-3%) and inflation gets controlled. Inflation and rate hikes are milder than believed. Stock market stabilizes and rebounds from current level and the doomsday scenario that some are parroting here, doesn't occur. \n\n2) rate hikes way higher than neutral because inflation hard to control, PE compresses, stock market crashes. Inflation then gets under control and US economy enters severe recession. The Fed starts to lower rates again, PE expands again, those companies that survived the recession would see their stock price go back again.\n\nI think in either scenario the stock market will do fine over the medium term, whether its 2 or 5 years, and well in the longer term. If someone wants to invest today instead of waiting until the picture on rate hike and inflation clears up, they should not use margin at all and they should not use options, unless the stock market does crash and the price of good companies becomes absurdly good. But they must be absurdly good that only then do leveraging thru margin or options becomes far less risky and far more rewarding. \n\nOtherwise, simply having cash invested will allow you to take advantage of scenario 1 if things aren't as bad as thought, or ride out scenario 2. Having 100% cash will allow you to take advantage of scenario 2 but you may miss out on a good portion of scenario 1.", "upvote_ratio": 0.82, "id": "t3_uaec5v", "created_utc": 1650746467.0}
{"sub": "investing", "title": "About exercising voting rights and points to vote for, any thoughts?", "selftext": "I'm starting to get voting forms for different companies I've invested into and some points are... off. Any thoughts on below?\n\nSo, for example, our favourite dead horse we like to kick - $FB - voting letter proposes to vote **Against** Shareholder proposal regarding:\n\nreport on lobbying  \nhuman rights impact  \nchild sexual exploitation online  \nreport on community standards enforcement  \nan independent chair  \netc.\n\nBasically, $FB does not want shareholders to know (recommending to vote \"Against\") what agreements they have with gov't (lobbying) or whether or not there is child porn on facebook or whether or not they sensor free speech (standards enforcement) or who is running the company (indep. chair)?\n\nIt's not just FB, there are similar voting points in BRK, LMT, etc. that recomend voting Against those. Basically, they want us to vote against their transparancy and being honest with shareholders? \n\nOr am I misunderstanding something?", "upvote_ratio": 0.54, "id": "t3_ua9hd4", "created_utc": 1650732221.0}
{"sub": "investing", "title": "How to $%#\u00a3 trade one stock for another on fidelity?!", "selftext": "Newbie question:  I have an account in fidelity.com.  It\u2019s the weekend.  I want to input an order for market open, to sell all I have of one stock, and use the proceeds to buy as many shares as possible of another stock.\n\nThis has to be the 3rd most common trading desire in the world, but I cannot for the life of me find a way to do it on Fidelity without waiting two days for settled cash in my account and then noticing an e-mail notification that I\u2019m allowed to place my new order.\n\nDo other brokerages allow \u201cbuy what you can when the money is available\u201d orders?\n\nDo other brokerages allow a trade like this /without/ waiting two days in between?", "upvote_ratio": 0.47, "id": "t3_ua9fm5", "created_utc": 1650732077.0}
{"sub": "investing", "title": "Best place to hold part of an emergency fund?", "selftext": "My wife and I are pretty diversified. We have solid 401k\u2019s that we continue to contribute to, we have a personal investment fund with some stocks and lots of ETF\u2019s that we drip into every month, we have a small savings (primarily for vacations and bills), and an additional emergency fund of about $70,000. \n\nWith inflation that emergency fund is painful to hold. We\u2019re thinking of cutting the fund down to 3 months worth money and stashing the rest somewhere safe but accessible (I.e. where we can move it over if need be without too much hassle. Doesn\u2019t need to be overnight, but would want to access it within a week or so).\n\nNot trying to beat inflation here, just lessen it from holding cash. What are the options? Is there a way to hold bonds without actually purchasing long term bonds? Like bond ETF\u2019s? I\u2019ve been hearing about iBonds and was wondering thoughts on them or other ideas?", "upvote_ratio": 0.88, "id": "t3_ua8jth", "created_utc": 1650729611.0}
{"sub": "investing", "title": "If you can't stomach S&amp;P going to 2800, you should sell now", "selftext": "Although it would be brutal, it wouldn't be the first time S&amp;P has full retraced a massive move over a long period of time\n\nI'm just putting out this PSA, because I can tell reading the hopium and copium here a lot of ya'll aren't prepared or hedged for that downside\n\nAlso, consider and prepare for the economic implications of such a move (i.e. you losing your job)\n\nthe downvote % should give you an understanding of why nobody respects this subreddit", "upvote_ratio": 0.4, "id": "t3_ua7x1k", "created_utc": 1650727831.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - April 23, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.78, "id": "t3_ua19uj", "created_utc": 1650704468.0}
{"sub": "investing", "title": "What is the next tech company that you believe will have negative growth or lose subscribers (like Facebook and Netflix)?", "selftext": "When Facebook and Netflix both posted the first time they lost subscribers for a quarter their stock dropped by 20-30% respectively. Maybe this will end up being a domino effect and more tech companies will follow suit. \n\nFor example Amazon might\u2019ve been a good bet. But unfortunately they\u2019ve diversified to other areas like groceries, cloud computing, streaming, gaming, etc\u2026\n\nWhat other tech companies do you believe are not diversified enough, that you believe will end up posting negative growth like Netflix and Facebook?", "upvote_ratio": 0.85, "id": "t3_ua044f", "created_utc": 1650699494.0}
{"sub": "investing", "title": "Dow Tumbles Nearly 1,000 Points as Stocks Extend Selloff", "selftext": "https://www.wsj.com/articles/global-stocks-markets-dow-update-04-22-2022-11650613003?mod=hp_lead_pos1\n\nWorries about slowing corporate earnings and the Federal Reserve\u2019s plans to rapidly raise interest rates dragged the Dow industrials to their worst day since 2020.\n\nInvestors this week parsed first-quarter financial results from a range of firms in search of clues about the health of the economy, the consumer outlook and companies\u2019 ability to cope with inflation. Of the companies that have reported so far, about 80% have beat analyst expectations, according to FactSet, which has helped provide some stability to the U.S. stock market.\n\n\u201cUsually when the economy\u2019s slowing down, or there is a perception it\u2019ll slow down, there are obvious sectors to hide in. Those traditional sectors aren\u2019t as safe from an earnings basis as they are historically because they still are going to have negative impacts from inflation,\u201d said Tavis McCourt, institutional equity strategist at Raymond James.\n\nThe Dow Jones Industrial Average posted its worst one-day percentage change since October 2020, losing 981.36 points, or 2.8%, to close at 33811.40. The S&amp;P 500 dropped 121.88 points, or 2.8%, to 4271.78, while the Nasdaq Composite fell 335.36 points, or 2.5%, to finish at 12839.29. \n\nThe recent rise in government-bond yields showed signs of steadying, with the yield on the 10-year Treasury note ending Friday at 2.905%, down two of the past three trading days. Yields staged a climb earlier Friday before reversing course. Bond yields rise when prices decline.\n\nHealthcare stocks are often considered defensive, with money managers betting that consumers will pay medical bills before making discretionary purchases. The S&amp;P 500\u2019s healthcare sector fell 3.6%, its worst day since June 2020.\n\nConcerns about inflation and the pace of monetary tightening by the Fed also remained at the forefront of investors\u2019 minds this week. On Thursday, Fed Chairman Jerome Powell gave investors a clear signal that the central bank is ready to tighten monetary policy more quickly and indicated that it was likely to raise interest rates by a half-percentage point at its meeting in May.\n\nA rate increase next month, following the Fed\u2019s quarter percentage point increase in March, would mark the first time since 2006 that the central bank increased its policy rate at back-to-back meetings.\n\nMr. Powell\u2019s comments injected fresh volatility into a stock market that has been whipsawed this year by the war in Ukraine, soaring inflation and rising Covid-19 cases in China.\n\n\u201cThe market is finally internalizing and factoring in the reality that the Fed really means what it says and it\u2019s not going to back down,\u201d said Tim Courtney, chief investment officer of Exencial Wealth Advisors. \u201cSomebody had a saying, and it\u2019s pretty\n good: \u2018You don\u2019t fight the Fed when the Fed is fighting inflation.\u2019\u201d\n\nIn commodities, Brent crude, the international benchmark for oil, fell $1.68 a barrel, or 1.6%, to $106.65. It fell 4.5% this week.\n\n---\n\nJust want to add, there's been some negative correlation between oil and the rest of the market, but today everything was bloody red.", "upvote_ratio": 0.95, "id": "t3_u9x6bn", "created_utc": 1650687667.0}
{"sub": "investing", "title": "Ex employer keeps moving 401k without notifying me", "selftext": "I have about 180k in a 401k from an ex employer. I don't really trust him.  He has moved it three times in the past three years without telling me, leaving me to track it down.  It's now with  Millennium\u00a0Trust, and I think I need to move it out.  I can use Fidelity, Schwab, or a TSP, or maybe open an account at Millennium.   Any advice?", "upvote_ratio": 0.74, "id": "t3_u9x35b", "created_utc": 1650687351.0}
{"sub": "investing", "title": "At what spread above treasury rates do Short Box Spreads typically get filled?", "selftext": "If I wanted to short a box spread as a cheap source of margin loan, how much should I price the rate of the box above treasury yields?\n\nAlso a few questions:\n\n1) Does the maturity effect this e.g. Does a 3 year box have a higher/lower spread than a 1 month box?\n\n2) How does the spread effect the time it takes to get filled. So what spread would get filled almost instantly? After 1 hour? A couple hours? A day or longer etc.?\n\n3) What is the minimum spread I could get if I just walked the rate up from 0% and could take as much time as needed to get filled?\n\n3) Are the spreads marked to market every year so that a portion of the interest is deductible as a capital loss each year even if the maturity of the box is several years?", "upvote_ratio": 0.83, "id": "t3_u9wxod", "created_utc": 1650686827.0}
{"sub": "investing", "title": "Understanding the return wording", "selftext": "A construction revenue sharing investment I'm looking at suggests \"3x return on the initial investment amount\"\n\nFor example, would that mean for a $10,000 investment I make an additional $30,000 and get back my $10,000 leaving me with $40,000 or that I make $20,000 and get back my initial $10,000 and walk off with a total $30,000?", "upvote_ratio": 0.33, "id": "t3_u9tdyh", "created_utc": 1650675187.0}
{"sub": "investing", "title": "Investing in stocks as an International Student in the UK", "selftext": "I am a Colombian student studying in the UK and I want to buy stocks with some spare money I have in my UK bank account. I do not know almost anything about taxes in the UK so I do not know if I have to pay taxes for any profits I get. Does anyone know what resources could help me with this issue?", "upvote_ratio": 0.33, "id": "t3_u9t9cs", "created_utc": 1650674769.0}
{"sub": "investing", "title": "How do you read 10-K filings to make the analysis more efficient?", "selftext": "Most  analysts I talk to, like reading financial reports of public companies and claim to understand the numbers quite well. But not everyone likes  to read long stories behind those numbers hidden in 10-K reports.\n\nReading long lines of unstructured text data is just cumbersome.\n\nQuestions:\n\n1. How do you read 10-K filings to make the analysis more efficient?\n2. What terms and phrases do you look for?", "upvote_ratio": 0.86, "id": "t3_u9l06p", "created_utc": 1650651305.0}
{"sub": "investing", "title": "Do Stock Prices affect Cashflows?", "selftext": "With all the noise being made around Netflix stock crashing, I had a question. Do Stock Prices of a company affect Cashflows? The way I understand it, they shouldn't have much to do with each other. I understand that a stock crash might happen due to factors that also reduce Cashflow, but I'm talking about a Direct Relationship.", "upvote_ratio": 0.78, "id": "t3_u9pm3o", "created_utc": 1650663942.0}
{"sub": "investing", "title": "KRBN, thoughts? Global trend towards carbon tax, carbon \"markets\" for trading carbon credits.", "selftext": "https://cleantechnica.com/2022/04/20/denmark-announces-aggressive-carbon-tax/\n\nKRBN rallied by 5% two days ago after months of no movement. It's an ETF I've been following closely. Does anyone have opinions of this? Seems to be picking up some steam of finance discussion forums due to new policies that are being rolled out regarding carbon emissions taxes.\n\nAlso seems to be a trend that the entire western world, and even China, are moving forward. A \"carbon tax\" where \"credits\" are traded and can be bought and sold by individual investors on a carbon \"market\"", "upvote_ratio": 0.85, "id": "t3_u9nqyn", "created_utc": 1650658819.0}
{"sub": "investing", "title": "What copper company do you think has the potential that I should invest in?", "selftext": "I just noticed that Gold and silver cost 400 and 5 times more than copper, respectively. The reasons can be largely explained by both material scarcity and by enduring public opinion. But, The good thing about copper is that the cheaper a commodity is, the more volatile its price can be. Gold isn\u2019t going to double in value in a month, not from a starting price of over $1,000 an ounce. Copper very well could. The problem now is I\u2019m not sure where can I put my investment into. Hoping I\u2019m not yet too late, recently have been emphasizing the possible significant increase in copper so I thought it would be nice to put some money on it.   \nThe question is, where? Hope you guys can help me out a bit here.", "upvote_ratio": 0.82, "id": "t3_u9nqf5", "created_utc": 1650658783.0}
{"sub": "investing", "title": "The FED plans to shrink the balance sheet - support for the stock market gone?", "selftext": " \n\n\ud835\ude4f\ud835\ude5d\ud835\ude5a  \ud835\ude41\ud835\ude40\ud835\ude3f \ud835\ude65\ud835\ude61\ud835\ude56\ud835\ude63\ud835\ude68 \ud835\ude69\ud835\ude64 \ud835\ude68\ud835\ude5d\ud835\ude67\ud835\ude5e\ud835\ude63\ud835\ude60 \ud835\ude69\ud835\ude5d\ud835\ude5a \ud835\ude57\ud835\ude56\ud835\ude61\ud835\ude56\ud835\ude63\ud835\ude58\ud835\ude5a \ud835\ude68\ud835\ude5d\ud835\ude5a\ud835\ude5a\ud835\ude69 -  \ud835\ude68\ud835\ude6a\ud835\ude65\ud835\ude65\ud835\ude64\ud835\ude67\ud835\ude69 \ud835\ude5b\ud835\ude64\ud835\ude67 \ud835\ude69\ud835\ude5d\ud835\ude5a \ud835\ude68\ud835\ude69\ud835\ude64\ud835\ude58\ud835\ude60 \ud835\ude62\ud835\ude56\ud835\ude67\ud835\ude60\ud835\ude5a\ud835\ude69 \ud835\ude5c\ud835\ude64\ud835\ude63\ud835\ude5a?\n\nThe  Federal Reserve (US central bank) wants to reduce their bond holdings  by shrinking the balance sheet around $95 billion a month.  In other  words, they want to sell a lot of bonds back to the market.\n\nUntil  recently, the FED did the opposite, buying a lot of bonds to support  the economy and the stock market via QE (\"quantitative easing\"), aka  \"money printing\".  This measure, combined with an indicated interest  rate hike of 0,5%, aims to fight the rampant inflation.\n\n\ud835\ude44\ud835\ude62\ud835\ude65\ud835\ude61\ud835\ude5e\ud835\ude58\ud835\ude56\ud835\ude69\ud835\ude5e\ud835\ude64\ud835\ude63\ud835\ude68 \ud835\ude5b\ud835\ude64\ud835\ude67 \ud835\ude69\ud835\ude5d\ud835\ude5a \ud835\ude68\ud835\ude69\ud835\ude64\ud835\ude58\ud835\ude60 \ud835\ude62\ud835\ude56\ud835\ude67\ud835\ude60\ud835\ude5a\ud835\ude69\n\nIt is impossible to say what the exact results will be for the stock market, but a look at recent history might help.\n\nThere  are strong arguments for the thesis that the high stock market gains of  the last decade were mainly driven by growing levels of debt and the  FED's QE.  If quantitative easing (QE) combined with low interest rates  drive stock prices higher, the opposite (rising interest rates and a  shrinking balance sheet) can't support the stock market as well.\n\nOctober  2017 was the last time the FED started to shrink the balance sheet  (\"quantitative tightening\") by selling $50 billion of bonds per month  back to the market. This finally pushed up 10-year US Treasury Bond  yields up above 3% (less demand for bonds leads to higher yields). At  that point, investors panicked and the stock market fell by 20% the  first time since the big financial crisis in 2008.", "upvote_ratio": 0.58, "id": "t3_u9k1eh", "created_utc": 1650648631.0}
{"sub": "investing", "title": "Is Margin Trading wise for your average investor?", "selftext": "I have an investment account and a Roth IRA. I don't want to trade with leverage for any sort of crazy options play or anything, just your standard Long time frame, buy-and-hold, equity investments. Would it be wise to invest with leverage? Do you think that if I do trade with leverage I should invest lower risk than I normally would invest for my age to try and offset the risk associated with leverage or just stay the course and enjoy the additional buying power?", "upvote_ratio": 0.52, "id": "t3_u9jjkv", "created_utc": 1650647261.0}
{"sub": "investing", "title": "if everything is priced in, then why......?", "selftext": " then why did Hang Seng index fall over 40% right after the handover to China in JUNE 1997 even though the arrangements started around a decade ago and everyone knew the timing of the handover? \n\nshouldn't it be effectively priced in? what do you think about it?", "upvote_ratio": 0.33, "id": "t3_u9ik9g", "created_utc": 1650644672.0}
{"sub": "investing", "title": "Is there any investment that's safe right now?", "selftext": "With bonds getting slaughtered, and stocks continuing their slow bleed, is there anything safe right now? I always thought that common sense stated that bonds were supposed to be the \"safer\" and less volatile investment, but it doesn't seem be the case any more.", "upvote_ratio": 0.89, "id": "t3_u9gnv1", "created_utc": 1650639604.0}
{"sub": "investing", "title": "What is the best way to handle taxes on vesting shares?", "selftext": "I will be having shares vesting and I\u2019ve heard the taxes we need to pay out of pocket and there\u2019s basically no way around it. Is there a way I could move shares vested into a 401k and be taxed on them later? I thought you only had to pay taxes on anything you personally take out. \n\nAny help would be appreciated.\n\nEdit: I appreciate all of the help and insight that everyone has shared with me. I had heard that there were no other options with regard to how I could handle RSUs but didn\u2019t believe it 100%. I guess I will have to sell to cover taxes and suck it up.", "upvote_ratio": 0.64, "id": "t3_u9fbtk", "created_utc": 1650635917.0}
{"sub": "investing", "title": "Investing in foreign company - common stock vs depository receipt?", "selftext": "When investing in a company based outside of your home country there may be an option to choose between Common Stock and Depository Receipt. What are the pros/cons to each of these?\n\nEx In USA - brokerages show BASF has:\n\nBASFY  (Depository Receipt)\n\nBFFAF (Common Stock)\n\nThey both have a similar dividend but show different stock prices.", "upvote_ratio": 0.5, "id": "t3_u9dk61", "created_utc": 1650630722.0}
{"sub": "investing", "title": "How do you invest with your values?", "selftext": "I've been investing in ETFs since 2017, mostly in S&amp;P500, and recently realised how many companies that includes which I'd rather not support (weapons, oil and gas, etc). Even the 'ESG' or 'Green Energy' ETFs still have ExxonMobil and others... Honestly looking into it I saw it's all marketing bs. Betterment/Acorns sustainable options are the same.\n\nI'd rather not create my own S&amp;P500 minus the Co's I don't like by buying 100s of stocks...\n\nHave you faced this problem? Any suggestions for how to keep diversification without ETFs?", "upvote_ratio": 0.44, "id": "t3_u9diqq", "created_utc": 1650630596.0}
{"sub": "investing", "title": "Did you invest in TSLA? Let's see how TSLA has grown into a listed company with a market value of $1 trillion? Judging from its acquisition case.", "selftext": "**Have you invested in Tesla stock? Today its market value has exceeded $1 trillion. How did Tesla grow into a $1 trillion public company? Let's look at its acquisitions.**\n\nTesla was founded in 2003, with a revenue of $53.823 billion in 2021 and a market value of $1.08 trillion. Tesla is still a relatively young company, but it has quickly grown into the most valuable automaker, backed by a series of key acquisitions. The main purpose of these acquisitions is to increase manufacturing capacity, increase the speed and efficiency of operations, and reduce costs.\n\nBelow, we'll take a closer look at 5 of Tesla's most important acquisitions.  \n\n\n**1. Sun City Company**\n\n\uf0b7 Type of business: Solar\n\n\uf0b7 Acquisition price: $2.1 billion\n\n\uf0b7 Date of acquisition: November 21, 2016\n\nSolarCity was founded in 2006 by two cousins \u200b\u200bof Elon Musk. The idea for the company was that of Musk, who also provided initial working capital and served as chairman. SolarCity designs, manufactures and installs solar energy systems and sells solar power. In August 2016, Tesla announced that it had reached an agreement to acquire SolarCity, with the merger closing later that year. Tesla said the merger would create the world's first vertically integrated sustainable energy company, leveraging the synergies created by combining Tesla's energy storage with SolarCity's solar power. Although the merger was approved by a majority of Tesla shareholders, some Tesla investors later sued the company. They claim that the real motive for the deal was to bail out SolarCity, which was struggling financially at the time.\n\n**2. Maxwell Technologies**\n\n\uf0b7 Type of business: energy storage and power transmission products\n\n\uf0b7 Acquisition price: $207 million\n\n\uf0b7 Acquisition date: May 16, 2019\n\nThe company was founded in 1965 as Maxwell Laboratories Inc. It went public in 1983 and then changed its name to Maxwell Technologies Inc in 1996. The manufacturer of energy storage and power delivery solutions specializes in ultracapacitors, devices capable of holding more energy than standard capacitors. Its products are used in a range of applications including transportation, industrial and grid energy storage. In May 2019, Maxwell was acquired by Tesla. Musk has said in the past that supercapacitors will be a breakthrough in electric vehicle production. He is also optimistic about Maxwell's efficient process for producing battery components, which could drastically reduce the cost of Tesla's electric vehicles.\n\n**3. Groman Engineering Co., Ltd.**\n\n\uf0b7 Type of business: automated manufacturing systems\n\n\uf0b7 Acquisition price: $135.3 million\n\n\uf0b7 Acquisition date: January 3, 2017\n\nGermany-based Grohmann Engineering was founded in 1963 by Klaus Grohmann. The company specializes in the design and development of automated manufacturing systems. When Tesla announced its acquisition of Grohmann in November 2016, the company had 700 employees and had grown revenue at an average annual rate of 6% over the past 20 years. Tesla completed the acquisition in January 2017 for $135.3 million. This total cost includes an initial cash payment of $109.5 million and an additional $25.8 million paid in the first quarter of 2020 as part of an incentive compensation arrangement. Tesla said the acquisition will help the electric car maker increase the speed and efficiency of its manufacturing process, which will significantly reduce expenses.\n\n**4. Perbix Machinery Co., Ltd.**\n\n\uf0b7 Type of business: automated manufacturing equipment\n\n\uf0b7 Acquisition Price: Financial terms not disclosed; estimated value of $10.5 million.\n\n\uf0b7 Acquisition date: November 7, 2017\n\nFounded in 1976, Perbix specializes in the design and manufacture of custom, highly automated manufacturing equipment. It was acquired by Tesla in 2017. Earlier in 2017, Tesla acquired Grohmann Engineering, a German company doing a similar business. The acquisition of Perbix gives Tesla more control over vehicle production by making more auto parts in-house. The acquisition is another step in Tesla's push to optimize and accelerate its electric vehicle production process, following the acquisition of Grohmann.\n\n**5. Haiba System Co., Ltd.**\n\n\uf0b7 Type of business: Manufacturer of automated liquid dispensing and filling systems\n\n\uf0b7 Acquisition price: Financial terms not disclosed\n\n\uf0b7 Date of acquisition: 2019\n\nCanada-based Hibar Systems was founded in 1974. The company specializes in the design and manufacture of high-precision dispensing pumps and filling systems, including automatic vacuum filling systems for lithium-ion batteries used in electric vehicles. The exact timing of the acquisition is unclear, as Tesla did not announce the acquisition. But in October 2019, Tesla listed Hibar as one of its subsidiaries in an Oct. 2 filing with the Canadian government, according to several publications in the U.S. and Canada. Tesla's interest in the company is more explicit. Hibar will allow the electric car maker to produce batteries in-house. The move will help reduce key operating expenses with which Tesla co-owns and operates a battery factory in Nevada.\n\n&amp;#x200B;\n\nThis is part of what makes Tesla a great company. They know their shortcomings and quickly acquire excellent companies in related industries to make up for their shortcomings. In the end, the behemoth achieved its own market value of $1 trillion, and it is still growing.\n\nThe just-announced Q1 financial report reached an explosive growth of $18.7 billion. In the future, we believe that Tesla will get better and better.\n\nWhat do you think?", "upvote_ratio": 0.42, "id": "t3_u9d6sq", "created_utc": 1650629507.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - April 22, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.79, "id": "t3_u9a65v", "created_utc": 1650618068.0}
{"sub": "investing", "title": "Investing on Acorns \u2014 what are the consequence of changing to a more aggressive portfolio at a loss?", "selftext": "\nI started investing a few dollars at a time under a moderately aggressive portfolio in January. All in all, I\u2019ve lost money, but I\u2019m actually wanting to change to a more aggressive portfolio since I\u2019m young and have pennies to burn.\n\nI know that switching portfolios can have adverse tax consequences \u2014 is this something I\u2019d need to worry about in my situation?", "upvote_ratio": 0.61, "id": "t3_u947xo", "created_utc": 1650595177.0}
{"sub": "investing", "title": "Fed\u2019s Powell Seals Expectations of Half-Point Rate Rise in May", "selftext": "https://www.wsj.com/articles/feds-powell-could-seal-expectations-of-half-point-rate-rise-in-may-11650533444?mod=hp_lead_pos1\n\nFederal Reserve officials have broadly signaled a desire to raise interest rates to levels that don\u2019t provide stimulus\n\nFederal Reserve Chairman Jerome Powell signaled the central bank was likely to raise interest rates by a half percentage point at its meeting next month and indicated similar rate rises could be warranted after that to lower inflation.\n\nA rate increase in May, following the Fed\u2019s decision to lift rates from near zero by a quarter percentage point last month, would mark the first time since 2006 that the central bank increased its policy rate at back-to-back meetings. A half-point increase would be the first such move since 2000.\n\nThe Fed has indicated it will also formally announce plans at the May 3-4 meeting to begin shrinking its $9 trillion asset portfolio in June, a double-barreled effort to remove stimulus to curb price pressures that are at a four-decade high.\n\n\u201cIt is appropriate in my view to be moving a little more quickly\u201d than the Fed has in the recent past, Mr. Powell said Thursday. \u201cI also think there\u2019s something in the idea of front-end loading\u201d those moves.\n\nInvestors in interest-rate futures markets have in recent weeks bet on increases of a half percentage point, or 50 basis points, at each of the Fed\u2019s next two meetings. \u201cMarkets are processing what we\u2019re seeing. They\u2019re reacting appropriately, generally,\u201d Mr. Powell said, though he said he wasn\u2019t endorsing any particular market pricing. Still, he concluded, \u201cFifty basis points will be on the table for the May meeting.\u201d\n\nMr. Powell warned of growing supply-and-demand imbalances in the U.S. labor market that some economists worry could fuel a wage-price spiral that drives inflation higher as workers bid up wages.\n\nIn July 2019, as the unemployment rate was falling to a half-century low of 3.5% but inflation drifted below the Fed\u2019s 2% target, Mr. Powell dismissed concerns that the labor market might be overheating. \u201cTo call something hot, you need to see some heat,\u201d he said.\n\nToday, wage growth is running at its highest levels in years and the labor market has tightened rapidly, with the unemployment rate tumbling to 3.6% in March from 5.9% last June. \u201cIt\u2019s too hot. It\u2019s unsustainably hot,\u201d Mr. Powell said. \u201cIt\u2019s our job to get it to a better place where supply and demand are closer together.\u201d\n\nMr. Powell said the Fed is focused above all else on bringing down inflation. \u201cEconomies don\u2019t work without price stability,\u201d he said.\n\nThe Fed is trying to engineer a so-called soft landing in which it slows growth enough to bring down inflation, but not so aggressively that the economy slips into a recession. \u201cI don\u2019t think you\u2019ll hear anyone at the Fed say that that\u2019s straightforward or easy. It\u2019s going to be very challenging,\u201d Mr. Powell said.", "upvote_ratio": 0.97, "id": "t3_u92olg", "created_utc": 1650590295.0}
{"sub": "investing", "title": "I do not understand how dividend long term investing works.", "selftext": "I understand some companies either give a yearly or quarterly dividends. But I don't see how it can increase so much. Example: Company A gives a 2% dividend, and annually the dividend increases by 3-4% ish. Over a span of 40 years wouldn't the dividend payout be extremely high? Also, how does dividend reinvesting work? I understand that it when you receive dividends you reinvest it into the stock, but when I do that on a calculator, you barely make anything compared to the money you make on the stock alone without the dividend. Or I'm getting this all wrong, anyone help? Thx.", "upvote_ratio": 0.7, "id": "t3_u9218a", "created_utc": 1650588297.0}
{"sub": "investing", "title": "Fertilizer Market- Very Bullish Weekly Prints Coming In $MOS $NTR $CF - *Mini Post* please add if you have more insights \ud83d\ude4f", "selftext": "\nFertilizer Market- Very Bullish Weekly Prints Coming In $MOS $NTR $CF - *Mine Post*\n\nThis market is extremely bullish reaching all time record breaking highs! And with earnings around the corner for fertilizer producers are going to be bullish imo. If you look back at charts of 2007-08 these companies are much more valuable today then they were in those years. Especially with global food shortages and rocketing inflation globally. With the war and Russia being a key exporter in this market. The sanctions and higher natural gas prices will further increase the price of fertilizer in the next month or two. This is due to the fact that it\u2019s fertilizer season from March-June. Companies like Mosaic, Nutrien, CF and other fertilizer companies will produce recording breaking quarterly statements imo. \n\n\nThese Are The Recent Weekly Fertilizer Friday Print Prices:\nApril 15- 1158.98\nApril 8- 1149.25\nApril 1- 1269.76\nMarch 25- 1270.40\nMarch 18- 1248.09\nMarch 11- 1137.56\nMarch 4- 948.72\n\nHere are some recent links: \n[link 1](https://www.dtnpf.com/agriculture/web/ag/crops/article/2022/04/20/phosphate-prices-reach-time-high)\n\n[link 2](https://www.dtnpf.com/agriculture/web/ag/crops/article/2022/04/20/phosphate-prices-reach-time-high)\n\n[link 3](https://www.bnnbloomberg.ca/rising-fertilizer-costs-are-catching-up-to-rice-farmers-threatening-supplies-1.1753446.amp.html)\n\n[link 4](https://www.bloombergquint.com/amp/onweb/fertilizer-price-surges-43-to-fresh-record-as-supplies-tighten)", "upvote_ratio": 0.59, "id": "t3_u90p7k", "created_utc": 1650584159.0}
{"sub": "investing", "title": "What investment advice would you give to an immortal being?", "selftext": "First of all, as a declaimer I must make this very clear. I have NOT split my soul into 7 pieces. There is NOT a portrait of me in the attic that keeps getting older and older. I have NOT kidnaped a princess with magical hair. And I have NOT been bitten by a vampire. This is just a question in general and I am not asking for personal advice for myself.\n\nAnyway .....\n\nI know if my time horizon is short, say I am buying a house soon or I am near to retirement, I should not be messing around with stocks because the year-to-year has too much volatility. If my time span was longer then stocks would be a good idea because I would have time to even out any one year variance. But what if my time horizon was REAL long? Investing in a business would seem as if it would switch back to being a bad idea once more. I cant imagine a business that would still be around 1 000 years into the future. No. Not even Tesla.\n\nSo if stocks are out of the picture, the next thing I thought of was gold. Bear markets in stocks can last decades but if your start when you are young then you could just wait out a bear market. Likewise, bear markets in gold can last centuries, but an immortal could just wait out these bear markets in a similar manner. But I am a history buff and I know that gold as an investment is a relatively new idea. Lots of cavillations did value gold, but they valued it as a status symbol rather than as an investment. Like a Lambo or a Rolex. You don't 'invest' in a Lambo. You buy it to show off how rich you are and rub it in the noses of all your haters. Kings and emperors had gold to gloat, not to invest.\n\nMy guess is the best option would be real estate. Even when you go back a thousand years, the land owners were the rulign class. It has a proven track record over the needed span of time. Buy a house and the neighborhood turns to shit, and a human would never be able to wait out the bear market for the neighborhood to come back. But an immortal  could just depend on invadable population growth and know that cities will always expand.\n\nIs there any other suggestions? Or things I am missing?", "upvote_ratio": 0.39, "id": "t3_u8zlkn", "created_utc": 1650580918.0}
{"sub": "investing", "title": "Once in a lifetime opportunity", "selftext": "With the recent sell off in a lot of stocks, we are going to be faced with a once in a lifetime opportunity to accumulate some of the world\u2019s most cutting edge companies. Now is the time to be building your watch lists of companies that you have strong conviction in over the next 5 years +. Don\u2019t fret over the short term price fluctuations as some of the major market cap companies are guaranteed to rebound in the future. Now is the time to be accumulating positions in companies that have long term potential. Trying to time the market now will be difficult and accurately timing a bottom is almost impossible, even with some of the most advanced charting technology the world has ever seen. We could see further price depreciation but the lower we go, the greater the opportunity. Stocks are a long term play. Don\u2019t expect to get rich overnight as that\u2019s not realistic for most people. Sure, some people get extremely lucky or maybe are just talented investors but for the most part, this is about the long term game. Some stocks take 3-5 years or more to return good returns. The best thing in my opinion that you could do right now are to accumulate stocks in companies that you have the highest conviction in. Now is the time to set aside for these plays that will be life changing for you in the future. Your future self will thank you for having the fortitude to begin accumulating stocks in great companies. I personally invest in the following companies: AAPL, AMZN, GOOGL, WMT, JPM, UPS, XOM, PLD, UNH, NEE, COIN, NEM, BHP. These are the largest market cap companies from each sector (minus coin) giving me complete exposure to all sectors. I encourage you all to create a diversified, yet concentrated portfolio such as this to give you the greatest opportunity at success, while limiting your risk through diversification. Nothing is guaranteed but I believe in these companies and will continue to accumulate stocks in these companies for the remainder of my lifetime. If the stocks go further down, then this is a further opportunity to lower my cost basis in some amazing companies. But enough with the rant and please tell me about some of the holdings you have and why you hold them. I am always interested in learning more about great companies. Good luck everyone and don\u2019t let these price swings get you down. Look at them as an amazing, once in a lifetime opportunity to accumulating some killer companies.", "upvote_ratio": 0.2, "id": "t3_u8zj9i", "created_utc": 1650580739.0}
{"sub": "investing", "title": "Is Meta platforms Inc (FB) a value pick or value trap?", "selftext": "Despite earnings going from $23.5 Billion in June 2020 to $39 Billion in December 2021, the stock price has dropped by 18% in that time period and has halved since September. I understand there are concerns around a slight decrease in users and the uncertainty of the metaverse, but it feels like this is an overcorrection by the market. \n\nWhat are your thoughts on this, is it a buy, or to be avoided?", "upvote_ratio": 0.87, "id": "t3_u8w2wa", "created_utc": 1650570979.0}
{"sub": "investing", "title": "Why aren't Fed rate hikes priced in already?", "selftext": "The Fed has been saying since November that they would hike rates 6 times during 2022, sometimes 0.25 and sometimes 0.50. By now, this is old news. The message hasn't changed. Yet, every single time anything is said about this, the general market seems to have a fresh panic sell-off, with growth stocks (e.g. the segment of the biotech industry represented by XBI) dropping to record lows. The recovery in-between news cycles doesn't cover the lost ground.\n\nWhy aren't the Fed rate hikes priced in by now? What would it take for this trend to bottom out?", "upvote_ratio": 0.92, "id": "t3_u8upa6", "created_utc": 1650567164.0}
{"sub": "investing", "title": "Any explanation for PLUG\u2019s plummet today?", "selftext": "I\u2019m trying to figure it out but as much research and news as I can find is leaving me stumped. These guys have been on the up for years with smart financials, the emergence of green energy, their expansion into new markets (Renault) and building bigger deals with current customers (Wal-Mart) so what gives?! What the hell happened? \n\nHoping somebody with better understanding of the markets/investing can explain why this stock which is relatively stable just took a nose dive despite a lot of positives for it lately.", "upvote_ratio": 0.71, "id": "t3_u8tp73", "created_utc": 1650564402.0}
{"sub": "investing", "title": "So, this Twitter thing. The stock's barely moving, but Musk says he's got the bucks. What's up?", "selftext": "It's seemed unwise in the past 5 years to bet against Musk. He seems ready to takeover Twitter, which has been the most underperforming mega platform with the least amount of tweaks to its strategy towards growth. Musk has (annoyingly so) become a conservative media darling, thus garnering some to complain that he's killing free speech...and there's the poison pill. But I figured the stock would have some big swoops by now. What am I missing?", "upvote_ratio": 0.39, "id": "t3_u8ta2c", "created_utc": 1650563263.0}
{"sub": "investing", "title": "US performance in the future", "selftext": "If the US stock market continues to outperform international stock markets at the rate that it has recently, then wouldn't the US will be 99% of the world\u2019s market cap eventually (in the next 10-20 years)? Is this even possible? If not possible, would that suggest that the rational choice would be to invest more in a Total World Market Index Fund in the near future than in S&amp;P or total US market index funds?", "upvote_ratio": 0.78, "id": "t3_u8q6q0", "created_utc": 1650554906.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - April 21, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.67, "id": "t3_u8j6po", "created_utc": 1650531669.0}
{"sub": "investing", "title": "Sustainable energy and tech?", "selftext": "Hi all. Very new to investments. But I\u2019d like to go bigger. I\u2019ve done chump change to see how my gut-feelings pan out. \n\nHas anyone worked in Lumen stock? I\u2019m seeing an enormous downtrend, but they advertise as being the next big thing. Pretty typical, but I\u2019m new. \n\nHas anyone had good luck with sustainable energy and/or tech? AI and edge computing seem promising\u2026", "upvote_ratio": 0.64, "id": "t3_u8fpa1", "created_utc": 1650517025.0}
{"sub": "investing", "title": "Am I being silly by having many index funds?", "selftext": "Something about my investing habits, i tend to not like to put all my eggs into one basket, to an extreme in many ways. So I think I might be off on investment strategy, or maybe others do this and i'm not crazy, i'm just wondering.\n\nBut I like to invest in multiple index funds, energy, medical, S&amp;P 500, total market, foreign markets, etc. Okay, I don't own funds in all of those, but i want to. \n\nSo I'm wondering if I am crazy to have a few of these funds up and running, then deciding weekly/monthly which index fund I feel good about investing in. I know you can never really beat the market, but like when we were at ATH, i felt like I wanted to invest in value stocks, so i directed my money into those index funds instead. When the pandemic hit, I felt like medical would be good. \n\nI'm guessing regardless, i'll be fine in the long run. I'm a saver, doing basically good things based on my risk tolerance at the time, but I still wonder if I'm being wrong here. Would it be better to just pick one or two funds, dump it all in those, wait?\n\nFor more information, I have like 20-30 years before I would need it, so my risk tolerance is long term.", "upvote_ratio": 0.72, "id": "t3_u8bicd", "created_utc": 1650503297.0}
{"sub": "investing", "title": "Why do we measure the market by the S&amp;P? Why not the Russel? Or the Dow? And, what happens to the rest of other markets when the S&amp;P performs poorly?", "selftext": "With all the talk of the S&amp;P being overvalued or that its too high or that its going to correct... I'm starting to wonder: why is there so much more focus on the S&amp;P? \n\nI see a dozen charts a week showing how the S&amp;P performed since some arbitrary date in time. Why don't we give that same focus to the Dow? the Russel? \n\nI never hear how small cap stocks performed historically. Not a single chart! Same goes for mid cap. What about developing markets? We keep talking about the S&amp;P and there's soooooo much more to the economy than the largest 500 US companies. \n\nI feel like there should be more talk outside the S&amp;P 500. Does anyone know why that's not a thing? What are your favorite indicators that don't involve the S&amp;P?", "upvote_ratio": 0.5, "id": "t3_u8b64c", "created_utc": 1650502264.0}
{"sub": "investing", "title": "Google Earnings Report Discussion Thoughts?", "selftext": "GOOGL is expecting to announce their earnings for Q1 2022 on April 26, 2022.\n\nLast time during the Q4 2021 ER, GOOGL shot up from the $2500 support level to as high as $2960. It was not as huge as previous ERs but it announced a 20-to-1 stock split which played a huge part. The $2500 support level is very strong as it has been tested around 5 times already with each time leading to a nice reverse.\n\nHowever, many people believe that GOOGL will report negatively after 4 consecutive positive ER already.\n\nI\u2019m just creating this post to create some discussion about Google and what other people think will happen to their earnings.", "upvote_ratio": 0.79, "id": "t3_u8atco", "created_utc": 1650501161.0}
{"sub": "investing", "title": "What was it like being invested through the great recession?", "selftext": "What did the markets do, what did you do? Did stop losses even matter, did the prices plummet after hours? Did it change how you invest? I only have two anecdotes. I know a guy who lost his daughter's college/future fund. It was around 100k. He was devastated and said he'd never invest again. Then my father, who was in mutual funds. The crash didn't hurt him much, which was as he'd planned, but he figured his colleagues who were in stocks would probably recover in time, and they probably did. There were probably not nearly as many retail investors then as now, so may have been harder to go cash.", "upvote_ratio": 0.9, "id": "t3_u88ij2", "created_utc": 1650494319.0}
{"sub": "investing", "title": "Is this the beginning for Netflix to feel the side effects due to lack of portfolio diversification?", "selftext": "Netflix is the streaming leader, but being a leader in a domain eventually you reach a point of saturation especially with competitors on your heals. They are attempting to become the entertainment goto while they have the concentration of eyeballs on their platform. They could steer customers to other entertainment content, games, music, twitch like content, vr, passive or active interaction on their platform, etc... However if their diversification efforts do not bear fruit and the streaming competitors are eating at their user-base, the stock would reflect that.", "upvote_ratio": 0.65, "id": "t3_u86ynn", "created_utc": 1650489979.0}
{"sub": "investing", "title": "Tesla's Q1 results; 3.3B net income, 19% operating margin, 33% auto gross margin. Reduced debt from 1.4B to 88 MILLION.", "selftext": "[Link to the Q1 financials.](https://tesla-cdn.thron.com/static/IOSHZZ_TSLA_Q1_2022_Update_G9MOZE.pdf?xseo=&amp;response-content-disposition=inline%3Bfilename%3D%22TSLA-Q1-2022-Update.pdf%22)\n\nP/E is now ~130 and EPS for Q1 was 2.86. Forward P/E is ~90.\n\nTesla is an absolute financial monster and by retiring the debt they are saving ~200 million in headwinds due to interest payment. They sit on 17.5B cash while earning 3B+ in net income per quarter. Operating margin went from 14.6% in Q4 21 which was fantastic already, to 19.2% Q1 22...\n\nEDIT; Customary edit, thanks for all the gold. :)", "upvote_ratio": 0.96, "id": "t3_u86h5b", "created_utc": 1650488613.0}
{"sub": "investing", "title": "Stock Market is a Casino, but Can't Cash your Chips - Only Sell your Seat?", "selftext": "I read this description of the stock market somewhere, I'm wondering if you guys think it's accurate. If not, why not?\n\nImagine there's a casino, and you're sitting at a table with $500 worth of chips in front of you. However, you can't take out your chips, exchange them for cash and walk away. Instead, you can only sell your seat to someone else. Only when the casino shuts down one day (maybe 400 years from now, who knows?), will you get cash for your chips - if there's any left after paying the casino debts etc.\n\nQuestion: If someone with $500 worth of chips on the table offers to sell you their seat for $200, would you consider it a bargain and buy it immediately to take their place, or will you refuse the offer?\n\nWould you agree with this characterization of the stock market?", "upvote_ratio": 0.17, "id": "t3_u86538", "created_utc": 1650487722.0}
{"sub": "investing", "title": "How valuable is Tax Loss Harvesting for a fresh grad?", "selftext": "I just graduated college and have been looking to consolidate my investments into a robo advisor. I\u2019ll be living in Colorado earning about 81k gross and I have around 50k saved up (I also have no student debt which I am absurdly grateful for!) When I look around between different choices, the feature of Tax Loss Harvesting comes up quite a bit. I understand the concept, to realize a loss to offset taxable income, but I\u2019m not sure if it\u2019s worth it for myself. \n\nHow can I approach gauging the value of TLH to determine if it\u2019s worth paying an extra management fee for it?\n\nAs an aside, I\u2019m mostly looking between Vanguard Digital, Betterment, and SoFi for digital advisors, I\u2019d love to hear anyones experience with these as well!", "upvote_ratio": 0.43, "id": "t3_u85usl", "created_utc": 1650486955.0}
{"sub": "investing", "title": "Evaluating Pension buyback vs investing", "selftext": "Hey everyone, I am a teacher and have the option of buying pensionable service as an investment. Here are the facts:\n\n-32 years old\n\n-plan to retire in 25 years (age 56, 2047) \n\n-1 year of pension service is 8917 dollars(its partially subsidized)\n\n-I'm not in great health, so let's assume I will die 5 years before the average. You may think this is a poor assumption, willing to hear counter arguements\n\n--current income is 86000, current pay cap is 100k.will need to assume what this will be by 2047\n\n-pension is 2% of average 5-year top salary per year.\n\n-pension is 70% cost of living adjusted\n\nInvestment I'm comparing it against is investing that 8917 into low cost index fund(VT)\n\nAny thoughts are appreciated!", "upvote_ratio": 0.6, "id": "t3_u854ck", "created_utc": 1650484911.0}
{"sub": "investing", "title": "Anyone use a Private Mailbox for their investment accounts?", "selftext": "Like if you don't have a solid address in the US or you are an expat or you spend most of your time out of the US and come back to visit friends, have you used one of those Private Mailbox services? They seem great in terms of holding and forwarding mail as needed, but it's the address I really need.", "upvote_ratio": 0.5, "id": "t3_u83zh8", "created_utc": 1650481813.0}
{"sub": "investing", "title": "Understanding the \u201cBuy, borrow, die\u201d strategy.", "selftext": "Diving into this subject a bit lately. I think I get the basic idea as outlined here: https://www.peoplestaxpage.org/buy-borrow-die-1\nBut I am curious about a couple of things\u2026 hypothetically of course\u2026 \n\nHow much (in appreciating assets) does one actually need to pull this off? Is a stock portfolio worth 1M getting say 10-20% annually enough? \n\nThen, hypothetically one could get a line of credit for at least $250k with a 5% fixed rate. Blow it all on a cheap condo. Never pay it off till you die as in step 3.\n\nBut if person still has +-30 years left on the planet, the 250k with compounding interest will have ballooned substantially by then. But I guess so too has their 1M stock portfolio. So I suppose as long as the returns on the portfolio continue to be greater than the accruing debt, it\u2019s viable?  \n\nAnd how does a person get a line of credit from their brokerage that does not require at least SOME type of payment along the way? How does someone actually do this?", "upvote_ratio": 0.48, "id": "t3_u82zjd", "created_utc": 1650479075.0}
{"sub": "investing", "title": "Invested in a private company that recently sold.", "selftext": "I had the opportunity to invest in a private company I was working for right at the end of 2019. The company has been around for a couple of decades but recent leadership made some bad decisions so they opened up funding again in 2019. I bought $25K in restricted stock and took a $25K pay cut in order to get some common stock. Covid happened and anything that could go wrong, did and I left the company towards the end of 2020. \n\nI just learned they sold on 2/1/22 most likely at a loss or at best, breakeven. I have only heard this through the grapevine and haven't heard anything from the Board of Directors, the VC, or anyone else from there. The corporation that acquired them didn't disclose what they paid for the company. My question is, it hasn't even been 90 days yet but as a shareholder, shouldn't I get some kind of paperwork showing either the gains or losses I took? Even if it was a loss, I would think they would have to disclose that so I can take that into consideration next year for taxes. This is my first time going through something like this so it's all very new.", "upvote_ratio": 0.88, "id": "t3_u80pz4", "created_utc": 1650472918.0}
{"sub": "investing", "title": "Streaming reset. When does it hit bottom? Netflix, Disney, paramont.,", "selftext": "Seems like we are in a very volatile situation with the Netflix reset. Will the contagion from Netflix bring the other streaming services down more?   \nIt seems to me that Disney may be at rock-bottom. But then again it keeps going lower.\nAny thoughts regarding a good time to look at buying some streaming services?\nI don\u2019t own any Netflix. For the record at age 61 I don\u2019t even watch Netflix which is why I probably never purchased it. But I guess that Bill Ackman is much smarter than me because he was talking about it being a great stock when it was $350", "upvote_ratio": 0.93, "id": "t3_u7zf3i", "created_utc": 1650469397.0}
{"sub": "investing", "title": "Wealth Management &amp; Tax Loss Harvesting Benefits for ~30Y/O?", "selftext": "I got married a year ago, and with the joint income between my wife and I we've realized that we have more and more to invest. Because of this, we've started meeting with a financial advisor (through Fidelity) for a little over a year now. A topic that occasionally comes up with \"wealth management\". We're considering pulling the trigger on it, but as someone who's used to being a DIY person for most aspects, it's hard to convince myself to pay someone for this service.\n\n**Background:**\n\n* We're both young (around 30)\n* We're both putting the allowable 20,500 to our 401(k) - all going to ROTH\n   * We're considering doing Mega Backdoor ROTH IRA this year\n* We're above the limit for contributing to ROTH IRA\n   * We're considering doing Backdoor ROTH IRA this year\n* We each have our separate 401(k) accounts, ROTH accounts, and individual investment accounts.\n* This past year, we set up our first joint account for investing\n* So far a majority of our investments have been in Fidelity's low-cost index funds that track pretty well with S&amp;P-500\n* We have enough in our bank accounts as a \"rainy day fund\" to last us for a while\n* We're looking at our investments as \"long term\" (probably retirement). We're not really planning to pull anything out at any point. \n\n**Wealth Management Considerations:**\n\n* We're planning to add about 100k to a joint account - we're debating between having this be managed or just let it ride low-cost index funds\n* The management fee would be 0.4% annually (deducted quarterly)\n* Discussing with family who does something similar (also through Fidelity) and they say that once the fee is accounted for, it comes out similarly to the S&amp;P - sometimes higher, sometimes lower, but long-term is relatively close.\n* Assuming they match the S&amp;P (after the fee), the main \"perk\" we'd get out of this seems to be \"tax loss harvesting\"\n\n**My Questions:**\n\nI've been reading up on things, as I typically like to have a basic understanding of everything I'm doing. That said, I'm still new to most of this, thus have some questions surrounding all of this.\n\n* For starters I guess, is a 0.4% rate for a wealth management service through Fidelity a good rate? From what I can tell it is based on the numbers I've seen online.\n* More specifically to the \"is this a good rate\" is: What benefit would I really see from having this service?\n* Is \"Tax Loss Harvesting\" really beneficial in my situation?\n   * From what I've gathered, TLH essentially will help defer the tax burden from capital gains. If I just ride index funds (and not cashing out), then wouldn't capital gains be quite minimal, thus having no need for TLH?\n      * I see how TLH can have a huge benefit if your reshuffling your portfolio and/or having constant buys/sells. But doesn't that benefit mostly go away if you're just in something like FXAIX or FSKAX?\n   * There's a potential to save around $700 a year in taxes by using losses to offset some of our income. That's a semi-fixed number that doesn't rise with the amount in the account though. So, wouldn't we hit a breaking point where the fee is costing  us more than this offset is gaining?\n\nThese are all things I could ask the advisor (and will), but I figured an outside opinion wouldn't hurt.", "upvote_ratio": 0.73, "id": "t3_u7zccr", "created_utc": 1650469194.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - April 20, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.74, "id": "t3_u7s5yb", "created_utc": 1650445269.0}
{"sub": "investing", "title": "Ways to automatically detect changes in companies' fundamental ratios' directions?", "selftext": "Just finished a self-study course, which included stock valuation and companies' fundamentals.\n\nOne specific thing was, to try to see trends of increases or decreases over time, in a company's fundamentals (P/B, P/E, profit, etc.); and to take note of reversals in those trends as possible indicators of action to take.\n\nFor example, if Company 1 made $90 in profit one year, $120 the next period, $150 the next period, then down to $100 the next period. Or if their P/E went $20, $18, $17, $25.\n\nIs there any sort of service the internet that goes through financial data, to detect these trends like these, and indicate when there's been a reversal in a fundamental's direction?", "upvote_ratio": 0.4, "id": "t3_u7k7sv", "created_utc": 1650415924.0}
{"sub": "investing", "title": "Utilities Stocks and Inflation", "selftext": "I\u2019ve read in a number of places online that utilities stocks don\u2019t do very well during periods of high inflation. The reasoning is that the increases in energy costs for these companies can\u2019t be instantly passed down to consumers by instantly raising prices by a large amount because many of these companies are regulated by state and local governments that put a restriction on how quickly prices can be raised.\n\nHowever, VPU (Vanguard utilities ETF) has outperformed the market by a large margin over the past year. I am completely confused by this. Can anyone enlighten me? Much appreciated!", "upvote_ratio": 0.78, "id": "t3_u7fy4d", "created_utc": 1650403536.0}
{"sub": "investing", "title": "Netflix Q1 EPS $3.53 vs. $2.89 Est.; Q1 Revs. $7.87B vs. $7.93B Est. Stock is down -20% AH", "selftext": "* Shares of Netflix cratered more than 23% on Tuesday after the company reported a loss of 200,000 subscribers during the first quarter.\n\n* It\u2019s the first time the streamer has reported a subscriber loss in more than a decade.\n\n* Netflix blamed increased competition, password sharing as well as inflation and the ongoing Russian invasion of Ukraine for the stagnant subscriber growth.\n\n* For the current quarter, Netflix said it expected an even steeper decline in new users. The streamer said it sees subscribers declining by 2 million in the fiscal second quarter, whereas consensus analysts were looking for a gain of 2.4 million. (h/t /u/Tellon)\n\n[Sub Growth](https://imgur.com/a/cYtSopG)", "upvote_ratio": 0.97, "id": "t3_u7f3jz", "created_utc": 1650401217.0}
{"sub": "investing", "title": "McDonald's As Inflation Hedge", "selftext": "I am trying to hedge against inflation and thought McDonald's stock might be a good idea. My reasoning behind this is:\n1. In essence, they are a real estate company and generate much of their profits through leases to franchises\n2. As a worldwide company, international revenue will protect against possible devaluation of the US Dollar\n3. In a recession people who want to still eat out may choose lower cost options. This could be further exacerbated by rising gas/electric bills incurred by home cooking\n4. In control of output price so can increase prices if required\n5. Frequent dividend payment\n\nI've put 10% of my total portfolio in so far, but am interested in your thoughts before investing any more\n\nMany thanks,", "upvote_ratio": 0.81, "id": "t3_u7dnu9", "created_utc": 1650397403.0}
{"sub": "investing", "title": "Investing in Crypto ETF Funds", "selftext": "I understand the theory behind crypto currency, but not to the degree that I've done any research on it.  But now that some foreign governments are using it, the US is dropping hints and even WalMart is trying to get their own version going....  I'm paying a little more attention to various articles on the subject.\n\nA recent [announcement](https://www.etftrends.com/new-fidelity-etfs-offer-access-to-crypto-esg-bonds-and-the-metaverse/?utm_source=Yahoo&amp;utm_medium=referral&amp;utm_campaign=ReadMore) about Fidelity offering new Crypto ETF's caught my eye earlier this week.  It looks like these EFT's focus on the infrastructure surrounding crypto, not the actual currency itself.  Fidelity makes no secret to the fact these funds came about because of the desires of younger investors. This makes a little more sense in my eyes, kind of akin to investing in gold mining and processing companies rather than gold itself.  ***What do you investors think of this kind of opportunity?***  The release date is April 21st.", "upvote_ratio": 0.38, "id": "t3_u7dl81", "created_utc": 1650397207.0}
{"sub": "investing", "title": "Prudent to invest in a single investor/developer?", "selftext": "Hi there,\n\nI'm turning to this community for some advice for a friend of mine (this is actually a friend, not the whole \"asking for a friend\" thing).\n\nHe was approached by a developer/investor that's also a good friend of his and asked if he wanted to invest in some land deals this developer had on the horizon.\n\nHe is apparently investing in some land near Universal Studios Orlando, as well as opening up a fast food franchise. The problem is that this guy has no oversight, board of directors, or anything else. It'd be just him.\n\nHe doesn't really have a proven track record, either. My friend just really trusts him, and I guess I'm just trying to find some objective reasons why he should or should not invest with him.\n\nHope this makes sense, and I appreciate any advice you can offer!", "upvote_ratio": 0.43, "id": "t3_u7deyv", "created_utc": 1650396747.0}
{"sub": "investing", "title": "Can someone explain this bullshit to me? Result of PFOF?", "selftext": "I was about to sell an option on RH and saw the price at $1.10 but then was prompted to reset my password. By the time I reset my password, the option price had tanked to $.62 and I thought, ok maybe I was just seeing old data as RH has it's bugs. Decided to try and sell the options anyway at $.65 with the bid/ask .55/.70. I left the offer up for about 30 seconds with no execution. Decided to cancel the offer and then add the option to my watch list so I could see price history.   \nhttps://imgur.com/KSwwJWu \n\nI saw it before it started to respike, and thought \"wow I guess I should buy some options instead\". Saw the bid/ask .55/.75. Everytime from there on out, as soon as I bid, w/e I bid, there was suddenly tons of bids at that price. The bid level did not change before I bid. After a couple of bids, the ask sat a $1.20.  \n\n\nI ask you, how is it possible that my canceled sell didn't execute if any of those bids actually exist?", "upvote_ratio": 0.33, "id": "t3_u7axyh", "created_utc": 1650390321.0}
{"sub": "investing", "title": "Nvidia SEC filings - Item 1 and 7 - word frequency count analysis - 2011 \u2013 2022", "selftext": "Just though I would share some interesting word frequency count for Nvidia for years 2011 - 2022.\n\nInteresting transition from \"products\" to \"AI\" and now to \"software\".\n\nHere\u2019s a link to the post:\n\n[Nvidia - Word Frequency Count](https://www.reddit.com/r/codingeconomist/comments/u287hp/nvidia_financial_report_word_frequency_count/?utm_source=share&amp;utm_medium=web2x&amp;context=3)", "upvote_ratio": 0.87, "id": "t3_u7auyt", "created_utc": 1650390113.0}
{"sub": "investing", "title": "Do dividends actually create any additional value?", "selftext": "You often hear that the best investment advice is to put your money into a fund, not to touch it, and to re-invest the dividends. \n\nBut if the price of a stock goes down by the value of the distribution, what is the value of a dividend at all?\n\nFor example, if I buy Company A for $90 and it goes up to $100. Then they issue a dividend of $3. The share price drops to $97 and I reinvest the $3 dividend so I'm back at $100.\n\nHow has this provided me with any value, besides creating a taxable event?", "upvote_ratio": 0.76, "id": "t3_u79hdk", "created_utc": 1650386493.0}
{"sub": "investing", "title": "In an economic recession, what can you invest in?", "selftext": "Prior to further speaking on my point, mods and everyone, please note that I attempted to thoroughly look for the answer to my question on here but I\u2019ve had no success in finding that. If there is, please guide me there. \n\nI\u2019m sure most people have the understanding that we are already in the process of an economic recession/crash. What are the best moves to make during an economic recession, in regard to investing in stocks/businesses?\n\nI did some research and ETFs, Real Estate, Dividend stocks were the way to go, but I wanted to get the opinion of my fellow redditors. What are the best moves to make in your opinion? \n\nFor some reason I\u2019m reminded of The Big Short movie and the potential for good investment. \n\nThank you in advance", "upvote_ratio": 0.44, "id": "t3_u7871l", "created_utc": 1650383110.0}
{"sub": "investing", "title": "Fannie Mae forecasts U.S. economy to fall into modest recession next year", "selftext": "\"Our updated forecast includes an expectation of a modest recession in the latter half of 2023 as we see a contraction in economic activity as the most likely path to meet the Federal Reserve\u2019s inflation objective given the current rate of wage growth and inflation.\" \n\nhttps://www.fanniemae.com/research-and-insights/forecast/inflation-rate-signals-tighter-monetary-policy-and-threatens-soft-landing", "upvote_ratio": 0.96, "id": "t3_u74jsr", "created_utc": 1650373066.0}
{"sub": "investing", "title": "Can you please check my long term investment plan and answer a few questions?", "selftext": "Hi,\n\nI   have finally started investing in stocks and would like if you guys  can  check my long term assumptions and answer a few questions I have.\n\nI   am 30yo and my NW is \u00a385k. I have got \u00a375k invested in V3AM. I plan   contribute \u00a31k/month until I will have enough money to retire.\n\nMy questions are:\n\n1. what   is a realistic (or the most probable based on historical returns, I   should say) total real rate of return I can expect on the V3AM? With   'total' I mean including reinvesting dividends and with 'real' I mean   after inflation. At the moment I am using 8% in my calculations.\n2. in   general I see that a 100% stocks portfolio tends offer the highest  rate  of returns compared to a portfolio mixed with bonds. However I   understand that as I get older, should the market crash, I won't have   enough time to wait for it to recover so, at some point, I should swap   some stocks with bonds. Is there a 'rule' to follow for this? My main   concern is to buy bonds too early and miss out on higher gains.\n3. what   I am not clear in particular is: (should I manage to retire early)   should I plan to swap some stocks with bonds based on my 'early   retirement' date or based on my predicted life expectancy? I am into   FIRE and the simulations tend to show that the portfolios with the best   success rate are the 100% stocks ones.\n\nThank you.", "upvote_ratio": 0.45, "id": "t3_u74e42", "created_utc": 1650372580.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - April 19, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.91, "id": "t3_u70nv8", "created_utc": 1650358869.0}
{"sub": "investing", "title": "Etoro CopyTrade for New Investor?", "selftext": "I want to get into a small amount of low to medium risk investing, and have little to no experience. I don't want to spend a whole lot of time on managing it, however I'm happy to spend a couple hours a week if need be. Would a properly researched and setup Etoro CopyTrade/Copy Portfolio be a good option for long term? Is there a better option for an Australian resident? I've also heard good things about index funds, what ratio of these two should I be going for?", "upvote_ratio": 0.38, "id": "t3_u6yrgl", "created_utc": 1650350776.0}
{"sub": "investing", "title": "Why I bonds could be bad for an emergency fund", "selftext": "I've been considering migrating my emergency fund into I bonds by purchasing the 10k every year, and after each year has cleared, withdrawing 10k from my current emergency fund (high yield savings). Here are my thoughts:\n\nFirst some math on the difference - for each 10k in I bonds instead of a high yield savings account (assume 0.6%), you will gain...\n\n* this year: 8.37% - 0.6% = 7.77% = $777\n* at target inflation: 2% - 0.6% = 1.4% = $140\n\nFor years like 2022, which are not the norm, it's clearly a nice bonus. The amount becomes much less impactful when we're at or close to target inflation. Long term, I would characterize the additional earnings as nice to have, but nothing that will meaningfully impact your overall financial state.\n\nHowever, during however many years it takes to build an appropriate emergency fund in I bonds, you must also keep the remainder plus the currently seasoning 10k in an alternate source. This is 10k that could otherwise be invested.\n\nStill, once your emergency fund is fully transferred over you're in good shape, right? Well yes, unless you have to actually use it. First, it's a little harder to access. I couldn't find much on this, but it seems like best case you could access your money in 2-3 days. Not terrible, but not truly liquid. It's likely a good idea to keep some amount of your emergency fund in something immediately available like a high yield savings account anyways. Second, if you need to drain your emergency fund, which is what it's there for, you are now back to square one and need to spend however many years building it back up again. The extra 10k you need to hold will likely be even more of an inconvenience right after a large unexpected expenditure.\n\nWhile I was originally keen to jump in on I bonds for my e-fund purposes, I'm now seriously questioning if it's actually worth it.", "upvote_ratio": 0.2, "id": "t3_u6su48", "created_utc": 1650330755.0}
{"sub": "investing", "title": "Buying after a major companies failure.", "selftext": "I\u2019m 30 and only invest in mutual funds, but I\u2019ve always wanted to invest $500-$2000 in companies that take a higher hit from a PR issue or failure. For example I render when Boeing was sub $100 and thinking this is it. This is going to be a money maker. I didn\u2019t do it.\n\nDoes anyone have any data on something like this and the chances of Major gains vs company bankruptcy? Boeing was my example because I see the chances of them going down completely seemed impossible bc of their military contracts alone", "upvote_ratio": 0.83, "id": "t3_u6sgb7", "created_utc": 1650329625.0}
{"sub": "investing", "title": "Question about bonds and rising rates", "selftext": "I am down about 10% on my Vanguard BND fund in the last 12 months. I thought that bonds basically meant that you loan your money and get a very small premium at the end of the term. Will these funds recover eventually if I keep holding, assuming the interest rates stabilize?", "upvote_ratio": 0.82, "id": "t3_u6qe5p", "created_utc": 1650323688.0}
{"sub": "investing", "title": "Which index fund(s) do you hold?", "selftext": "So after almost 15 years investing in individual stocks (and doing quite well, mind you) I\u2019ve recently switched to an \u2018all-in index fund\u201d approach.  I\u2019ve found that trying to pick individual stocks to beat the market takes up far too much of my time\u2026 especially as I continue to age.\n\nI don\u2019t trust my ability to beat the market with stocks over the long term nor do I desire to spend even a fraction of the time I\u2019ve spent conducting DD.  Also, I\u2019ve found that I\u2019m quite bi-polar acting in the market.  Oftentimes switching in and out of names in attempts to time the market or easily being spooked by negative news headlines.  So\u2026 I\u2019m now in VOO.\n\nLegends like Buffett have said time and time again that you can plant your money in the S&amp;P and sit on your hands.  And, I of course agree. But I\u2019m interested in what others who take a similar approach, in only holding index funds, hold in their portfolio.\n\nSo\u2026 do you guys mind sharing? Are you all in on one fund or many? And which ones?\n\nThanks ahead of time for the discussion!", "upvote_ratio": 0.88, "id": "t3_u6q0tz", "created_utc": 1650322694.0}
{"sub": "investing", "title": "Jim Cramer's, 04/14 Mad Money Stock Recommendations", "selftext": "These are Jim Cramers recommendations from his 04/14  Mad Money Show\n\n|Company|Ticker Symbol|Recommendation|Stock Price|\n|:-|:-|:-|:-|\n|1-800-Flowers (FLWS)|FLWS|buy|$13.40|\n|AT&amp;T (T)|T|sell|$19.54|\n|American Express (AXP)|AXP|buy|$181.16|\n|Bank of America (BAC)|BAC|buy|$37.57|\n|Carmax (KMX)|KMX|negative|$91.79|\n|Clearfield (CLFD)|CLFD|buy|$59.02|\n|Freeport-McMoRan (FCX)|FCX|positive|$49.19|\n|General Electric (GE)|GE|buy|$90.83|\n|Green Brick Partners (GRBK)|GRBK|sell|$19.05|\n|Halliburton (HAL)|HAL|positive|$40.76|\n|Howmet Aerospace (HWM)|HWM|buy|$35.48|\n|IBM (IBM)|IBM|positive|$126.56|\n|Innovative Industrial Pro (IIPR)|IIPR|buy|$169.68|\n|Johnson &amp; Johnson (JNJ)|JNJ|buy|$179.90|\n|Johnson Controls (JCI)|JCI|buy|$61.53|\n|Lithia Motors (LAD)|LAD|buy|$290.64|\n|Netflix (NFLX)|NFLX|negative|$341.13|\n|ProLogis (PLD)|PLD|buy|$162.70|\n|Procter &amp; Gamble (PG)|PG|buy|$158.57|\n|Schlumberger (SLB)|SLB|positive|$43.25|\n|T-Mobile US (TMUS)|TMUS|positive|$132.96|\n|Tesla (TSLA)|TSLA|buy|$985.00|\n|Textron (TXT)|TXT|buy|$68.65|\n|Travelers Companies (TRV)|TRV|positive|$184.24|\n|United Continental (UAL)|UAL|positive|$45.13|\n|United Rentals (URI)|URI|buy|$331.07|\n|Verizon (VZ)|VZ|positive|$53.83|\n|Virgin Galactic (SPCE)|SPCE|sell|$8.99|\n|Zoom Video Communications (ZM)|ZM|positive|$110.31|", "upvote_ratio": 0.52, "id": "t3_u6poyz", "created_utc": 1650321779.0}
{"sub": "investing", "title": "Any thoughts on stomp capital?", "selftext": "I was looking to pick up some real estate investing without the management work.  Stomp capital looks interesting, they essentially get zoned as a STR and build up resort properties.  They don't really have any history, though I don't see anything negative about the founder, who's been in the business for a while.", "upvote_ratio": 0.3, "id": "t3_u6plyb", "created_utc": 1650321544.0}
{"sub": "investing", "title": "Vanguard retirement ETFs for person with little pension.", "selftext": "I'm currently an expat and have kind of neglected my retirement since leaving the comfort and safety of company and state retirement plans. I'm now looking to ensure the security of my future by regularly investing (mainly for retirement, but once that's seeded possibly for a mortgage to). Having set up a brokerage account, what are people's thoughts on the vanguard retirement funds? My research does they seem to be OK, but 100 brains are better than one.\n\nThanks for your thoughts", "upvote_ratio": 0.67, "id": "t3_u6o19s", "created_utc": 1650317298.0}
{"sub": "investing", "title": "Value with momentum investing 20.81% cagr", "selftext": "Has anyone researched this strategy which is back-tested by a phd? specifically the portion titled \"Value\u00a0(and then Momentum) Investing Portfolio Results\". The Cagr is staggering at 20.81% between 1974-2014.\n\n[https://alphaarchitect.com/2015/03/the-best-way-to-combine-value-and-momentum-investing-strategies/](https://alphaarchitect.com/2015/03/the-best-way-to-combine-value-and-momentum-investing-strategies/)\n\nThe strategy I'm highlighting essentially filters for the top decile of stocks based on simple value ranking then filters top half of those based on momentum, with monthly rebalancing. The specific definitions of value and momentum used here are contained in the article.\n\nI really favor this as my long term investment strategy for my roth ira and would love to hear everyone's critiques to ensure I'm not missing valid reasons to not pursue.\n\nTo preempt some common counterpoints:\n\n\\-there should be no excessive tax impact as it would be through a roth ira.\n\n\\-no management fees as i will filter stocks once a month\n\n\\-no trading fees as is the norm with brokerages nowadays\n\n&amp;#x200B;\n\nMy main concern is forward testing this strategy. I'm not tech savvy enough to conduct this project. Has anybody else tested this beyond 2014? One index that follows somewhat of a similar strategy is the s&amp;p smallcap 600 high momentum value index which has annualized returns of 14.92% for the 10 years preceding march 31,2021 (roughly the same or a touch better than sp500) - however the big caveats are much less frequent rebalancing and I believe it is factor weighted vs equally weighted. Point being: factor premia such as momentum and value are diluted with less rebalancing, but this shows even without the monthly rebalancing the fundamental idea of trending value has legs in recent years.", "upvote_ratio": 0.72, "id": "t3_u6nf1j", "created_utc": 1650315600.0}
{"sub": "investing", "title": "new US Gov I-bond interest rates for April 2022 - 9.62%", "selftext": "don't know if this was already noted somewhere but this is a nice inflation hedge: looks like US Gov I-Bond interest rates will jump from a healthy 7.12% to a very nice 9.62% this month, for at least the next 6 months. https://keilfp.com/blogpodcast/i-bond-rate-november-2021-to-april-2022/", "upvote_ratio": 0.96, "id": "t3_u6milx", "created_utc": 1650313248.0}
{"sub": "investing", "title": "Hot Economy, Rising Inflation: The Fed Has Never Successfully Fixed a Problem Like This", "selftext": "https://www.wsj.com/articles/inflation-jobs-fed-recession-economy-11650294297?mod=hp_lead_pos9\n\nThe Federal Reserve is setting out to do something it has never accomplished before: reduce inflation a lot without significantly raising unemployment.\n\nCentral bank officials think it is possible with calibrated interest rate increases that slow booming demand just enough to take steam out of an overheated economy. But even one of the Fed\u2019s closest allies, U.S. Treasury Secretary Janet Yellen, sees the risk of failure. \u201cIt will require skill and also good luck,\u201d the former Fed chair said in public comments in Washington last week.\n\nDuring the past 80 years, the Fed has never lowered inflation as much as it is setting out to do now\u2014by four percentage points\u2014without causing recession. In this case, the central bank will need a number of factors out of its control to break its way.\n\nStill, Fed officials can find reason for both optimism and caution from history. In seven different episodes during the past 80 years, inflation has fallen as much as the Fed bank wants it to drop now, with varying outcomes. The episodes suggest that the desired scenario is theoretically possible though the risk of failure is high, especially because the bank is chasing inflation that already exists, rather than addressing the problem before it arises as it did in some earlier episodes.\n\n\u201cNo one expects that bringing about a soft landing will be straightforward in the current context\u2014very little is straightforward in the current context,\u201d Fed Chairman Jerome Powell said last month. The central bank, he added, faces a \u201cchallenging task.\u201d\n\nDuring the early 1980s, the U.S. experienced a classic hard-landing as economists dub it\u2014falling into a deep recession with double-digit unemployment after the Fed pushed its benchmark interest rate to nearly 20% to tame stubbornly high inflation that had been rising for more than a decade.\n\nThe U.S. had less severe, but bumpy landings during the 1950s, characterized by short-lived inflation spikes and recessions. During that period, the unemployment rate rarely got very high even when economic output contracted.\n\nThe 1970s delivered aborted landings, when inflation fell and then lurched higher, beset by outside shocks such as OPEC oil embargoes and policy missteps including a central bank that hesitated to raise interest rates aggressively.\n\nThe U.S. has had soft landings, too, most recently in 1994. Fed Chairman Alan Greenspan sharply raised rates to 6% in February 1995 from 3% one year earlier, and the unemployment rate kept going down. Unlike 1994, however, the Fed today is trying to reduce inflation that is already too high rather than prevent it from rising, as Mr. Greenspan did back then.\n\nIn the scenario Fed officials mapped out, their benchmark interest rate will rise to around 2.75% by the end of next year, just above estimates of a rate that neither spurs nor slows growth.\n\nThey project inflation will drop to slightly above 2% by 2024, a rare four-percentage-point decline in less than three years. They see economic output growing at a rate between 2% and 3% while unemployment holds below 4%.\n\nJohn Taylor, an economist at Stanford University who is the author of an influential policy-setting rule of thumb called the \u201cTaylor rule,\u201d says his formula calls for the Fed to set interest rates at 5% right now. Because the Fed is unlikely to lift rates so dramatically in one year, he said officials instead ought to raise rates to 3% by December and signal more increases after that unless inflation comes down.\n\n\u201cThis is not the only time in history that they\u2019ve been behind, but they are strikingly behind,\u201d said Mr. Taylor. \u201cThey need to catch up and do it in a systematic and understandable way.\u201d\n\nThe Fed\u2019s success will depend on several factors outside its control. Those include whether global energy supplies recover from the shock of Russia\u2019s invasion of Ukraine, reducing energy prices; whether sidelined U.S. workers rejoin the labor force, easing the labor shortage and wage pressures; whether Chinese plants reopen in the face of more Covid-19 lockdowns, clearing supply bottlenecks; and whether Covid itself recedes for good in the U.S., ending related pandemic-related economic disruptions.\n\nThe Fed\u2019s job will be easier if these supply constraints ease. If they don\u2019t, the central bank will need to push rates higher to squeeze demand, with a risk of more damage to the economy.", "upvote_ratio": 0.89, "id": "t3_u6m6bj", "created_utc": 1650312356.0}
{"sub": "investing", "title": "What are your thoughts on CLM and its 17% dividend?", "selftext": "I already have some stock in CLM for 3-4 years now. it has been giving a monthly dividend of 25% for my cost. \n \nIt has dropped by about 20% in the past two weeks or so. it's sitting at 17% DIV. I was wondering if I should AVG up or not.\n \nHave you done any DD on it?", "upvote_ratio": 0.81, "id": "t3_u6levk", "created_utc": 1650310399.0}
{"sub": "investing", "title": "Best approach to Bond ETFs / Funds, currently? (Lost 12% so far &amp; trying to understand what I need to know in the future)", "selftext": "I'm hoping to get some 1) understanding (education) about bonds and maybe even 2)advice (action?) about best next steps with a bond ETF that's not doing well.\n\nI just started investing a year ago, so I've been investing slowly, mainly to solidify my own education and understand the best investment approach for myself moving forward. The current bond situation seems like a perfect \"learning opportunity\" for me, to learn from more seasoned investors, and feel more confident moving forward, when/if I decide to keep investing in bonds.\n\n**First question: A BETTER UNDERSTANDING OF THE BOND BIG PICTURE?**\n\nI purchased SCHQ (Schwab's bond ETF), in September 2021, thinking I was \"hedging\" against the stock market, since that's what bonds do, right? (I had only been researching investing for a few months at that point)\n\nClearly I was not really educated enough about bonds, bond funds, and bond etfs. It's gone down 12% since then.\n\nSo can anybody share:\n\n\\- What should I have looked at, back in 2021, to determine if investing in a bond ETF would have been a good move at the time?\n\n\\- When is it better to invest in a bond ETF/fund vs just a bond?\n\n\\- What are the best ways to invest in bonds, as a portfolio hedge? (Corporate bonds, long term bonds, short term bonds, ETFs, etc?)\n\n\\- Was there something else I was overlooking when I decided to purchase SCHQ, that a more seasoned investor would have thought of?\n\n**Second question: BEST NEXT STEPS FOR UNDERPERFORMING BOND ETF?**\n\nI guess in a nutshell: What would you do if you currently owned SCHQ and it was down 12%? Would you just hold? Sell now before it goes lower?\n\nI know these are a lot of questions. I appreciate *any* reply to *any* of them. And I ALWAYS do my own due diligence and research before taking any advice, and I never take advice blindly. I just think the redditors here for sure have valuable information to share on this topic!\n\nThank you!\n\nEDIT: Thought of one more question: **Is NOW a great time to be purchasing MORE of bonds or bond ETFs, since the prices are so low?** ", "upvote_ratio": 0.63, "id": "t3_u6ldec", "created_utc": 1650310287.0}
{"sub": "investing", "title": "What happens to secondary US Bonds in the event of a foreign debt crisis?", "selftext": "The WSJ is reporting increased concerns of a debt crisis in developing countries resulting from increased US Treasury rates and inflation.\n\n&amp;#x200B;\n\nAs someone who has a sizable stake in $TMF (20+ year Treasury 3x Leveraged ETF), what effect would a debt crisis in other countries have on the US secondary bond market?\n\n&amp;#x200B;\n\nI would think that a default on US debt by foreign countries would tank both the primary and secondary bond markets. The primary market would get hit less, due to decrease in demand for Treasure bonds, and the secondary market would get slaughtered because all these bonds already purchased that are sitting in the ETF are now defaulted on.\n\n&amp;#x200B;\n\nI'd love to hear others' thoughts. \n\n&amp;#x200B;\n\nLink to article:\n\n[https://www.wsj.com/articles/ukraine-war-deepens-debt-woes-across-developing-world-11650187803?mod=flipboard](https://www.wsj.com/articles/ukraine-war-deepens-debt-woes-across-developing-world-11650187803?mod=flipboard)", "upvote_ratio": 0.88, "id": "t3_u6f648", "created_utc": 1650294010.0}
{"sub": "investing", "title": "Which investment themes will grow the most over the next 5-7 years?", "selftext": "Which 4 of the following stock/ETF investment themes do you believe will increase most in value/price over the next 5-7 years:\n\nElectric, Autonomous Vehicles &amp; LIDAR; Electric Battery Technology;\nGenomics, Personalized Medicine &amp; Longevity; 5G;\nFintech;\nRenewable Energy; \nSemiconductors;\nLithium, Nickel and Battery Commodities;\nRare Earth Mineral Commodities;\nEcommerce; or\nLab Grown Meat and Seafood?\n\nAll opinions, contributions and commentary are greatly appreciated?\n\nThanks!", "upvote_ratio": 0.79, "id": "t3_u6jisv", "created_utc": 1650305437.0}
{"sub": "investing", "title": "Am I in an echo chamber or are index funds really the best?", "selftext": "I'm nearing 30 and have recently started looking doing more with my savings than just a HYSA. I found r/personalfinance and r/Bogleheads about 6 months ago and have been following their advice pretty closely (setting up a Roth IRA and investing in a 3-fund portfolio). However, I am wondering if, as a relatively younger investor, I should be investing in riskier things? I completely agree with the Bogleheads philosophy of not trying to time the market and that index funds usually do out-perform everything else \u2014 but I can't help but think that seems too easy. I also became slightly suspicious when I learned that John Bogle was the founder of Vanguard \u2014 of course he wants to sell us index funds!\n\nI'd love to hear your anti-index fund and anti-Bogleheads opinions, to help me make more informed decisions!\n\n&amp;#x200B;\n\nEdit: Thank you everyone for so many responses! I will try to read through everything and comment.", "upvote_ratio": 0.91, "id": "t3_u6k3de", "created_utc": 1650306906.0}
{"sub": "investing", "title": "Can someone in plain English explain the change in OTC that Vanguard sent me?", "selftext": "Full email:\n\n\u201cAn update on over-the-counter securities\n                                                                                                                 \nDear Vanguard Client,\n\nBeginning April 28, 2022, Vanguard will no longer accept purchases and transfers in of most over-the-counter (OTC) securities. This change allows us to better support a targeted, enduring suite of products and services rooted in Vanguard's time-tested investment philosophy and built to help secure the long-term success of investors.\n\nAccording to our records, you either currently hold at least one of these restricted securities in your portfolio or have traded them in the past.\n\nWhat does this mean for the securities I hold?\n\nYou can continue to hold and sell your positions in these securities. You can also make additional purchases of a small selection of global American Depositary Receipts (ADRs). Here's how you can determine if you hold an ADR that won't be restricted.\n\nAll three of the following criteria must apply to the ADR:\n\n- Consists of a five-letter ticker symbol that ends in \"Y.\" This confirms it's traded OTC.\n\n\n- Has a market capitalization of over $300 million (in U.S. dollars). Market capitalization is the total market value of a company's outstanding shares.\n\n\n- Belongs to the top three tiers of the OTC markets (Pink Current, OTCQB, and OTCQX). These markets are up to date with disclosures and listing requirements.\n\nLearn more about the OTC markets and individual securities at otcmarkets.com.\n\nThank you for investing with Vanguard.\u201d", "upvote_ratio": 0.9, "id": "t3_u6eido", "created_utc": 1650292238.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - April 18, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.76, "id": "t3_u68nc5", "created_utc": 1650272469.0}
{"sub": "investing", "title": "Questions about stock options when a company is being bought out.", "selftext": "The company I work for is being bought.  I have 750 vested options out of 1,000.  My strike price is roughly 65% of the current price and roughly 55% of the price they stocks will be purchased for when the deal closes.   \n\nMy questions are;\n\nWhat happens to the other 250 options that haven't vested?  \nIf I don't exercise my vested options will they get bought at the purchase price or do I lose them?\nWhat's the best move here?  \n\nThere's roughly 2-3 months before the deal finalizes and my other 250 options vest in September.", "upvote_ratio": 0.76, "id": "t3_u65sta", "created_utc": 1650260452.0}
{"sub": "investing", "title": "Looking for advice about ETF investing", "selftext": "Greetings investors,\n\nI've just started investing in ETFs in the last few and was hoping I could get your thoughts on my (small) portfolio so far. To start with, I want to identify long term holdings and I'm not sure how many ETFs I should invest in for this. Also, if yall have any advice on things like portfolio redundancy, please do tell me. Right now what I have is:\n\n* VOO\n* MGK\n* IUSG\n* VUG\n\nI was thinking of also investing in QQQ. I know this is a favorite of many, does it benefit my portfolio if I add it? I also want to invest in healthcare, and in that regard I have identified XLV. \n\nYour thoughts? Are all these appropriate for long term holding?", "upvote_ratio": 0.56, "id": "t3_u65jut", "created_utc": 1650259455.0}
{"sub": "investing", "title": "Help structuring LLC with investing/trading in mind", "selftext": "Hello fellow aspiring entrepreneurs, I have a few questions about how to structure my business.\n\n\n I originally pursued the idea of investing as an LLC so I could form a investment fund, and shield my gains.\n\n\nHowever, upon research, I discovered that simply holding investments in the LLC doesn\u2019t qualify me for any special treatment. My S/O then had the idea to open an LLC for my side hustle in order conduct my normal business dealings &amp; gain favorable tax treatment by writing off expenses related to travel, equipment and training.\n\n\nThus the idea then eventually evolved into building an actual business that has an investment fund for the purposes of paying for business expenses using dividends/capital gains, and securing the fund as collateral if necessary.\n\nI plan use on keeping the accounts separate from my personal investment activities.\n\n**How can I best accomplish this?**\n\nEdit: Added a missing sentence.", "upvote_ratio": 0.45, "id": "t3_u64nu5", "created_utc": 1650256109.0}
{"sub": "investing", "title": "Stocks vs Bonds vs Commodities", "selftext": "The goal here is to determine the expected return of bonds, stocks, and commodities given current market conditions.\n\nI previously addressed stocks vs bonds here: [https://www.reddit.com/r/investing/comments/on11bs/bonds\\_vs\\_stocks\\_and\\_short\\_term\\_returns\\_now\\_is\\_the/](https://www.reddit.com/r/investing/comments/on11bs/bonds_vs_stocks_and_short_term_returns_now_is_the/). Since then the stock market appears to have peaked, as I expected. However, the bond market is also having a rough go of it. Bonds continue to have a higher expected return than stocks, but neither are expected to be good.\n\nFor this reason, I\u2019ve been casting about for another place to invest. Real-estate is expensive as well, and I already own plenty. However, I have been hearing that commodities may be the place to be, so I decided to investigate further.\n\nI\u2019ve reworked my analysis, and put it at [https://docs.google.com/spreadsheets/d/1BSHryGSdk3idC\\_izY14qAa7zdSm\\_3LLsZA2MNybyAIg/edit?usp=sharing](https://docs.google.com/spreadsheets/d/1BSHryGSdk3idC_izY14qAa7zdSm_3LLsZA2MNybyAIg/edit?usp=sharing). I\u2019ve removed all mentions of real dollars and am now discussing nominal dollars and returns only, as looking at real dollars doubles the number of columns in the spreadsheet and lowers the correlations I find (because I do not know how to predict inflation). Coupons and dividends are reinvested, as before.\n\nI\u2019ve also changed how things are organized so that stocks, bonds, and commodities are on a more equal footing. Bonds are just one possible investment, rather than the reference point for stocks.\n\n**I use the following predictors / valuation measures:**\n\n*Stocks:*\n\nJesse Livermore's Average Investor Equity Allocation (AIEA) has the best performance that I\u2019ve found.\n\nBriefly, AIEA is (Stocks Owned) / (Stocks + Bonds + Cash Owned) by the entire market. It can be thought of as a fear/greed indicator, where higher values are greedy and lower values are fearful.\n\nAIEA is discussed here: [https://www.philosophicaleconomics.com/2013/12/the-single-greatest-predictor-of-future-stock-market-returns/](https://www.philosophicaleconomics.com/2013/12/the-single-greatest-predictor-of-future-stock-market-returns/), and the raw data is available here: [https://fred.stlouisfed.org/graph/?g=qis](https://fred.stlouisfed.org/graph/?g=qis).\n\nI find a historical R\\^2 of 0.86 and a current expected 10 year CAGR of -0.8 +/- 2.5%.\n\n*Bonds:*\n\nThe mechanism used for the model is that the bonds held are traded each month for new bonds at the new interest rate. This takes into account the effect of changing interest rates on the value of bonds held, but not taxes or fees. Coupon payments are reinvested. The given interest rate for the current ten year bond is a good (though not perfect) predictor for what the ten year return will be. The data comes from [http://www.econ.yale.edu/\\~shiller/data/ie\\_data.xls](http://www.econ.yale.edu/~shiller/data/ie_data.xls)\n\nI find a historical R\\^2 of 0.88 and a current expected 10 year CAGR of 2.1 +/- 2.0%\n\n*Commodities:*\n\nI did not find any good suggestions in my searching, and tried a wide range of possibilities. The best predictor that I found was GSCI/GDP. GSCI is S&amp;P\u2019s commodity index, with records back to 1970 here: [https://tradingeconomics.com/commodity/gsci](https://tradingeconomics.com/commodity/gsci). GDP is the U.S. gross domestic product, with data here: [https://fred.stlouisfed.org/series/GDP](https://fred.stlouisfed.org/series/GDP). This implies that there is a natural level of investment in commodities, compared to GDP. If you know of something better, please let me know.\n\nI find a historical R\\^2 of 0.64 and a current expected 10 year CAGR of 13.5 +/- 10%. The high uncertainty here is largely because the correlation got markedly worse in 2005, around the time of global financial crisis. Commodities suffered a bubble and pop, with a peak in June of 2008. In addition to the demand fluctuation related to the global financial crisis, this was the time that massive oil and natural gas investment was occurring, which ended up crashing the prices until recently.\n\nSo it is clear that Commodities have a higher expected return going forward. However, much of this depends on whether you believe that the correlation breakdown in 2005 is permanent or temporary. Personally, I am confident enough to put a sizeable commodities investment in DBC, a commodities futures fund, but not going all in.\n\n**Back tests:**\n\nAs before, I worked out an optimal back tested strategy. With this I find the following CAGRs, from January, 1970 to January, 2021.\n\nStocks only: 10.93%\n\nBonds only: 7.18%\n\nCommodities only:6.79%\n\nStock and Bonds: 13.42%\n\nStocks and Commodities and Bonds: 17.18%.\n\nFor each of these, I use the indicators from six months prior to the current date. This happens to give better results, as well as being possible without a time machine. I switch the entire portfolio to the best investment once a month, where the best investment has a fudge factor. That is, if expected stock return &gt; expected bond return + 0.8%, I go with stocks. Otherwise I go with bonds. If I wish to look at commodities, If find that it almost never a good idea to choose commodities over stocks. However, if expected bonds return &gt; expected commodity return + 0.5%, I stick with bonds. Otherwise, I stay go with commodities.\n\nBy the time I get to commodities, the model feels over fit, which means that it should used with caution. However, we can still stick with the conclusion that commodities are cheap right now, and likely have much more upside.\n\n**Warnings**\n\n* Past performance is no guarantee of future results. All back testing requires diligence about when and how past rules may break down in the future.\n* AIEA measures a natural level for stock vs bond ownership. This is not a fundamental parameter, with an underlying theory about why the natural level should be where it is, or will remain the same in the future.\n* Though GSCI/GDP feels like it could be fundamental parameter, it leaves out important phenomena, like the changing structure of the economy, as well as paradigm shifts in commodity extraction technology.\n* I don't take into account taxes or other transaction costs.\n* My analysis starts in 1970, because that is when the commodities data becomes available. This is a smaller data set than I \u2018d like.\n* I\u2019m using the S&amp;P 500, 10 year US treasuries, and GSCI because they have long data sets, not because I consider them to be optimal investments.", "upvote_ratio": 0.73, "id": "t3_u64m1i", "created_utc": 1650255915.0}
{"sub": "investing", "title": "Which trading strategy Long-Term Capital Management (LTCM) used?", "selftext": "I recently read about the Long-Term Capital Management (LTCM)  .\n\nCan anybody know which strategy they use to get 40% annual return?\n\nI want to know all of their strategy. Each and every. Is it possible to find which trading method they are  using??\n\nI find one \"When Genius Failed: The Rise and Fall of Long-Term Capital Management\". It is more dramatic information rather than technical. I want to know every technical details. Can anybody tell very details of their method ???\n\nAny book?? Any Online material ??", "upvote_ratio": 0.27, "id": "t3_u63w5c", "created_utc": 1650253389.0}
{"sub": "investing", "title": "Inflation might temper sooner than expected", "selftext": "I was firmly in the camp that inflation and rate hikes are going to be severe this year. Then I thought some more, and wonder if the inflation will die down sooner than I thought (meaning the prior inflated prices will stay, but new inflation going forward will be tempered.)\n\nThe reason is 2-fold: \n\n1) inflated prices are taking away real purchasing power from consumers. This will hit their pocket books, and wages have never grown faster than inflation in the past decade. Even with this year larger than usual raise, it's still behind inflation.\n\n2) household debts are pretty high, making households sensitive to even small rises in interest rates (same goes for US debt.) this will again depress spending.\n\nas long as the US government doesn't hand out more money or forgive tons of debt, the debt overhang effectively act as a deflationary factor.\n\nMy greatest fear, however, is that if monetary tightening is not enough, there will still be speculative money floating around, causing continued asset prices to rise unreasonably, especially in housing.\n\nI have another scenario for rate hikes which I didn't consider before: it's middle of the road between low rates and runaway inflation, and very high rates, moderate to severe recession and low inflation--rate hike to about 3-4%, inflation rate decelerating to 5-6%, mild recession, average 30-year mortgage rates to hit 6-7% this year, and stock market PE to shrink moderately. So not like a catastrophic scenario.\n\nAnybody poke some holes in my theory?\n\nAnd I hear people saying that interest rate needs to be higher than inflation rate in order to tamp down inflation. I don't understand the reasoning behind it. Why does it require 9+% interest rate to battle a 9% inflation rate? If someone can enlighten me on this, that would be appreciated.", "upvote_ratio": 0.49, "id": "t3_u63673", "created_utc": 1650250948.0}
{"sub": "investing", "title": "How to trade SPX option during night time?", "selftext": "[https://go.cboe.com/24x5](https://go.cboe.com/24x5)\n\nAbove link shows that it can be traded nearly 24x5. But I don't see how to trade it outside normal trading hours on brokerage trading platform. I have TD Ameritrade account and webull account.\n\nThanks.", "upvote_ratio": 0.33, "id": "t3_u60tak", "created_utc": 1650243432.0}
{"sub": "investing", "title": "Bond Rout Promises More Pain for Investors", "selftext": "https://www.wsj.com/articles/bond-rout-promises-more-pain-for-investors-11650122040?mod=hp_lead_pos3\n\nThe worst bond rout in decades shows few signs of abating, threatening further pain for both investors and borrowers.\n\nRising Treasury yields are in many ways a reflection of a robust economy. A big reason why many investors expect continued high inflation in the near term is that households are flush with cash and eager to spend their money on travel and leisure activities as they begin to worry less about the Covid-19 pandemic. The labor market is also, by some measures, the tightest in decades, giving workers leverage to demand better wages and confidence that they can always find a different job if they lose their current one.\n\nThese forces, though, are precisely why the Fed has been trying to push up bond yields by promising a rapid series of interest-rate increases\u2014an effort whose urgency hasn\u2019t been diminished by a modestly encouraging inflation report last week. Many investors are saying they expect bond prices to continue to fall this year, and ***some contend it won\u2019t be clear that the central bank\u2019s message is getting through until stock prices suffer more serious declines.***\n\nTreasury yields largely reflect expectations for short-term rates over the life of a bond. They in turn set a floor on borrowing costs across the economy. The Fed, now, wants borrowing costs to rise to slow consumer demand and bring down inflation\u2014and it is succeeding at least in the first of goals, with the average 30-year mortgage climbing last week to 5% for the first time since 2011.\n\nOne hope of some bond investors is that surging consumer prices, coupled with higher borrowing costs, could slow consumer demand in relatively short order. In that case, the Fed could keep tightening monetary policy, but officials wouldn\u2019t feel the need to raise their rate forecasts further, allowing bond yields to stabilize.\n\nGuessing the final destination of interest rates is extremely difficult, he said, but ***one sign that the Fed might need to do more than currently expected is that stocks, as a whole, have only experienced modest declines, with the S&amp;P 500 down 7.8% year-to-date.***\n\n---\n\nCaveat Emptor people.  \n\n#Don't fight the Fed.", "upvote_ratio": 0.82, "id": "t3_u60cpp", "created_utc": 1650242000.0}
{"sub": "investing", "title": "What's the quickest way for a Gen Z'er to own real estate in this economy?", "selftext": "I've been investing in the market for a year and I've been studying the overall markets/stock market since I was 18-19 ish. I'm very conservative and figured I'd rather learn more since my small amount of money wouldn't matter in the short term. eventually I started working more and I went from basically no debt, barely any Net worth to a now no debt, 10k net worth within a year (investing made me really see the value in money so I worked more to spam buy VTI and other such things)\n\nI wanted to build a good source of passive income that's truly useful to me combined with being able to cycle it from my stocks portfolio to real estate to stocks again, in a sort of loop. and real estate sounds like the best bet, as well as giving me a place to stay. However, we all know how hard it is to buy a house right now, and especially if you're just starting from scratch like I am. I am wondering if it's better to just tune off the breaks for stock market investing and to move my efforts to real estate but the stock market seems so simple compared to real estate, and I really would love to be so proud of owning a home or anything for that matter. But the sad reality is, mostly everybody around my age of 21-22 will never own anything and most think they won't own anything either. \n\nI do not want to be a self fulfilling prophecy and I want to try as hard as I can. But how does one get started? is this even the right sub for this or should I try a real estate focused sub?", "upvote_ratio": 0.5, "id": "t3_u5zpo6", "created_utc": 1650239983.0}
{"sub": "investing", "title": "Tax efficient investing after retirement accounts", "selftext": "So I have the benefit of being able to fully max my 401(k), IRA, and HSA each year.  Generally speaking, I split my contributions between traditional and after-tax dollars.  My employer does not permit after-tax contributions to my 401(k), so I can't participate in a MEGA backdoor Roth.\n\nAdditional contributions that I make are going to a taxable brokerage account.  I've done well in 2022 and I'm looking at a large tax bill for next year's taxes.  There's probably nothing I can do about 2022, but what are some alternative ways to set up to reduce your tax bill on investment gains for relatively frequent trading?\n\nBESIDES the obvious answer of \"don't sell your investments to trigger a realized, taxable gain\", are there other legal entities such as partnerships or setting up a corporation that could help reduce taxes on gains from year to year?  Other methods?", "upvote_ratio": 0.83, "id": "t3_u5ys58", "created_utc": 1650237068.0}
{"sub": "investing", "title": "Bonds vs. Stocks Discussion", "selftext": "This is a mostly discussed topic, Bonds vs choosing Stocks. Financial advisors said you should invest 60% stocks 40% bonds to mitigate the risk from stocks. WallStreetBets people think you should go full on options, which I'm not very comfortable with that much risks. How many people here are actually following 60-40 rule? And is this investment strategy really effective? Can anyone shed a light please?", "upvote_ratio": 0.5, "id": "t3_u5yolp", "created_utc": 1650236771.0}
{"sub": "investing", "title": "All options for borrowing against home equity?", "selftext": "Thinking about investing a second home. I want to explore borrowing the equity on my current home to partially fund the down payment. What are all of my options? I heard about some methods and understand the risks. I just want to learn what all possible options I have here to make an informed decision in the future. Thanks ahead of time", "upvote_ratio": 0.44, "id": "t3_u5ybm3", "created_utc": 1650235727.0}
{"sub": "investing", "title": "Investments that Cash flow that arent Real Estate?", "selftext": "Basically the title.\n\nWhat kind of investments are out there that generate consistent income that don't involve the purchase, maintenance and upkeep of real estate?\n\nI.e. You invest in it, it generates money on a monthly/quarterly/time goes here basis if you play your cards right, and generally doesn't swing wildly in the amount it returns to you", "upvote_ratio": 0.86, "id": "t3_u5y549", "created_utc": 1650235174.0}
{"sub": "investing", "title": "Minimum income needed to invest comfortably to make 1 million before 30?", "selftext": "Hello, 20M software engineering major here GameStop made a lot of people hundreds of thousands, and I know that trying to get rich quickly comes with its risks, but what minimum income is needed to invest comfortably to meet this goal? Also, how would I get there? GameStop? Crypto? Real estate? Options?", "upvote_ratio": 0.23, "id": "t3_u5y2t1", "created_utc": 1650234981.0}
{"sub": "investing", "title": "What type of account should I invest in if I want to retire early?", "selftext": "I am confused on which type of investing accounts I should invest my money in if I plan to use the 4% rule and live off my Investments. I currently have a regular brokerage account and a Roth IRA but am concerned if I can live off these when I retire due to the taxes on the regular brokerage and the limits on the Roth. Should I open a Traditional IRA or max out Roth then invest in my regular brokerage? \n\nThanks!\n\nBackground: 19M wanting to retire before 35. Reinvest all dividends.", "upvote_ratio": 0.43, "id": "t3_u5xsq1", "created_utc": 1650234155.0}
{"sub": "investing", "title": "Vanguard LifeStrategy 100% Equity or VUSA (S&amp;P 500 UCITS ETF) or..", "selftext": "Hi all, \n\nI live in the UK and I have been regularly investing \u00a3300\nwhich is 16% of my salary every month in\nthe Vanguard life strategy 100% Equity\nfund for a year. At the moment I have\naround \u00a310k (as I have transferred some\nlump sums | had saved). I am not sure if I\nshould keep investing in this blended fund\nor a lower fee ETF?\nI am now 24 years old and my intention is\nto just keep investing a fixed percentage\nevery month but I am now unsure which\noption is the best. Another option is the\nFTSE global all cap.\n\nAny help, is much appreciated!", "upvote_ratio": 0.61, "id": "t3_u5vpwp", "created_utc": 1650228056.0}
{"sub": "investing", "title": "How do you manage risk in your portfolio?", "selftext": "Is there a good way to measure how risky your current portfolio is? I\u2019ve looked at indicators like beta, sharpe ratio, and sortino ratio in the past, but I\u2019m not sure how to actually use it to manage my portfolio.\n\nAre there other indicators for things like interest rate risk? Or is managing risk really just about understanding the individual companies and sectors you\u2019re invested in?", "upvote_ratio": 0.73, "id": "t3_u5vdw4", "created_utc": 1650227044.0}
{"sub": "investing", "title": "Special Dividends question", "selftext": "Stock  is paying 2.13 special dividend. Stock is $3.10 buy side. If i buy 10,000 shares the dividend is $21,000 . So a purchase of $31,000 turns into $52,000 after dividend payout. Barring massive price drop, this seems like a solid investment. Am i missing something?\n\nThanks for input.", "upvote_ratio": 0.7, "id": "t3_u5sr4f", "created_utc": 1650219212.0}
{"sub": "investing", "title": "Is it reasonable to expect S&amp;P 500 ETF return rates to continue averaging ~10.5% in the future?", "selftext": " I know that Michael Burry, who predicted the 2008 recession, has said that he thinks it is a \"dangerous bubble\" and that he is expecting an \"epic crash.\"\n\nDo you agree? Moreover, is it reasonable to expect S&amp;P 500 ETF return rates to continue averaging \\~10.5% in the future?", "upvote_ratio": 0.87, "id": "t3_u5tkb4", "created_utc": 1650221634.0}
{"sub": "investing", "title": "i started investing with little money.", "selftext": "Hey everyone i have a question. so i started investing with little money. I put my first 500$ in vanguard etf. I intend to put like 50 every month. Im 25 and i have a long term plan to do it for like 15-20 years or something. Do you think its worth it. I think the amount is really small but thats what i can invest unfortunately.", "upvote_ratio": 0.88, "id": "t3_u5r59s", "created_utc": 1650214535.0}
{"sub": "investing", "title": "Thanks to Margin I made $300k in TSLA but I don\u2019t like margin calls. Wouldn\u2019t a home equity loan be better?", "selftext": "It sucked when brokerage sold part of my positions when TSLA dropped 20% then recovered in a few days but not prior to Fidelity selling some of my position. \n\nTo avoid this shouldn\u2019t a home equity loan be a better choice? No margin call and lower interest rate ! Yes I know about collateral risk I\u2019ve considered that so don\u2019t waste your time schooling me into a risk I\u2019m willing to take.\n\nHome equity loan better to margin loan? What\u2019s your experience?", "upvote_ratio": 0.17, "id": "t3_u5odbb", "created_utc": 1650206343.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - April 17, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.72, "id": "t3_u5j6gg", "created_utc": 1650186069.0}
{"sub": "investing", "title": "Confused about choosing Accumulation or income S&amp;P 500 fund", "selftext": "HI all, hoping someone can help with this.\n\nNot sure whether to choose an accumulation or income fund option. I understand the differences between the two but wondered am I better off selecting Income,  and setting a manual reinvest in the broker app. This way I could potentially turn off compounding if I ever needed the dividends paid out. Does that make sense as an option and would there be any issues here?", "upvote_ratio": 0.53, "id": "t3_u51j0t", "created_utc": 1650126804.0}
{"sub": "investing", "title": "How do I calculate margin requirements with portfolio margin? [Example below]", "selftext": "On [tdameritrade's website](https://www.tdameritrade.com/investment-products/margin-trading/portfolio-margin.html) they give this example of how to calculate the margin requirement of a Collar trade.\n\n* Long 10,000 shares of\u00a0 XYZ @ 97.73\n* Long 100 XYZ April 95 Puts @ 1.02\n* Short 100 XYZ April 105 Calls @ .40\n\nThey say the margin requirement is 33,500. How is this formulated?Can someone work me through the math?", "upvote_ratio": 0.44, "id": "t3_u526tf", "created_utc": 1650128685.0}
{"sub": "investing", "title": "YTD Top 25 Stocks S&amp;P 500: Take a look and see how many of you were in this Sector?", "selftext": "Here are the top 25 stocks in the S&amp;P 500 year to date.  Take a look at the sectors and first figure out how much you have invested in these sectors and compare your returns YTD to these.   None of these moves are a surprise as most prognosticators outside of Cathy Woods were telling us this is where we should be concentrating since 3rd Qtr of 2021.   Stocktwits provides this information for free each week as well as the ability to share your thoughts on your stocks via a twitter like format.\n\n   \nhttps://imgur.com/OfIBbuX", "upvote_ratio": 0.79, "id": "t3_u57akr", "created_utc": 1650143384.0}
{"sub": "investing", "title": "Is P/E ratio still a good indicator for deciding which stocks to invest in?", "selftext": "I remember reading long ago that 15:1 was a good P/E ratio for a company. \n\nI just saw that Disney is trading at 77:1 and who knows about many other high flyers. I understand that some companies are in growth mode, and may not even have earnings, but what about all the others?\n\nI understand there isn\u2019t just one metric, but spending $77 to generate $1 in earnings sounds crazy. Does it make sense to start by focus on companies with low P/E ratio?", "upvote_ratio": 0.91, "id": "t3_u559cn", "created_utc": 1650137406.0}
{"sub": "investing", "title": "Investing in Colombian Emeralds?", "selftext": "Anybody invest in emeralds? Thinking about diversifying a bit into them. \n\nThoughts on investing in Colombian emeralds? How liquid would they be, and how has their track record upscale (or lack thereof) been? Could not find much info when Googling its's performance history.", "upvote_ratio": 0.32, "id": "t3_u51w6y", "created_utc": 1650127857.0}
{"sub": "investing", "title": "What will happen if an online broker just disappear?", "selftext": "Hey. I am new in the world of investing and i want to make my first steps although a little afraid.   \n\n\nI want to buy an etf for long term but my only problem is what will happen if let's say Degiro website just disappear? If the go broke and just close the site. How am i safe? If i can't login to my account to retrieve anything?  \n\n\nThis is the only thing stopping me right now from buying.", "upvote_ratio": 0.62, "id": "t3_u4wn1v", "created_utc": 1650111651.0}
{"sub": "investing", "title": "Upcoming phosphorus shortage (?) as an investment opportunity", "selftext": "Howdy folks, \n\nI was listening to a podcast last night (it's not in English so I'm not going to link it here) with a paleo-climatologist talking about global warming/climate change and all that interesting and often controversial stuff. Not the point. One thing he noted, however, is the availability and use of phosphorus for food production. He claimed that humans cannot artificially synthesize P and that it is crucial for food production and that it will be the new oil in a decade.\n\nSome studies and articles related to this:\n\nhttps://www.nature.com/articles/s41467-020-18326-7\n\nhttps://www.nationalgeographic.com/science/article/farmers-are-facing-a-phosphorus-crisis-the-solution-starts-with-soil\n\n\nHas anyone here done some research on this topic? Is this an investment opportunity worth pursuing, in your opinion?", "upvote_ratio": 0.85, "id": "t3_u4wke3", "created_utc": 1650111368.0}
{"sub": "investing", "title": "Is there a way to evaluate bond investment opportunities like using the Graham number formula for evaluating stock investments? I am looking at the Canadian Government Bond Index ETF, as I thought a bond index would be a good place to start. Any help?", "selftext": "From my point of view, bonds are more difficult to understand than stocks in terms of assessing their inherent value, but that is probably because I have mainly been working with stocks in my portfolio. Can anyone suggest resources to learn more about the bond market?", "upvote_ratio": 0.73, "id": "t3_u4w6id", "created_utc": 1650109950.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - April 16, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.82, "id": "t3_u4tt04", "created_utc": 1650099669.0}
{"sub": "investing", "title": "how does one invest in MIT?", "selftext": "Just guessing the engineers and department  at MIT aren't publicly trade but I'd like to [invest in this](https://news.mit.edu/2022/thermal-heat-engine-0413#:~:text=Engineers%20at%20MIT%20and%20the,that%20of%20traditional%20steam%20turbines.)", "upvote_ratio": 0.33, "id": "t3_u4n9vy", "created_utc": 1650073986.0}
{"sub": "investing", "title": "Are there fees when trying to buy Schwab ETFs through your VG brokerage?", "selftext": "Interested in buying some SCHD into my vanguard personal brokerage account. But I can\u2019t tell if VG will charge me a fee for doing this or not. If they do, then I guess it wouldn\u2019t be worth it for just doing a share here and there when I get some extra cash.  I also have a Robinhood account. Would I just be better off buying it there?  Thanks.", "upvote_ratio": 0.4, "id": "t3_u4kxlm", "created_utc": 1650066144.0}
{"sub": "investing", "title": "Ten Members Of International Stock Manipulation Ring Charged In Manhattan Federal Court", "selftext": "Global \u201cPump-and-Dump\u201d Scheme Targeted Retail Investors and Generated Over $100 Million in Illicit Proceeds:\n\nhttps://www.justice.gov/usao-sdny/pr/ten-members-international-stock-manipulation-ring-charged-manhattan-federal-court\n\nThis is just one of the juicy parts:\n\n&gt;First, the defendants and their co-conspirators secretly amassed control of the vast majority of the stock of certain publicly traded companies that were traded on the over-the-counter (\u201cOTC\u201d) market in the United States. Second, the defendants and their co-conspirators then manipulated the price and trading volume for these stocks, causing the share price and trading volume to become artificially inflated, through coordinated trading and false and misleading promotional campaigns that they funded.\n\nI am *really* curious on which stocks on the OTC market were impacted. I wonder how many people on /r/pennystocks fell victim to it.\n\nThe whole indictment is an interesting read, I recommend reading the full thing.", "upvote_ratio": 0.97, "id": "t3_u4ko6u", "created_utc": 1650065305.0}
{"sub": "investing", "title": "Are PLTR and AFRM good picks for dollar cost averaging?", "selftext": "Peter Thiel is one of the founders of Palantir. I just read his book Zero to One, and it was no secret that he\u2019s a genius with uncanny foresight and a data driven mind - but reading his book solidified it for me. He outlined how Palantir has done some truly mind blowing things in terms of data mining. And with the web becoming evermore interwoven into our lives, being able to invest in a company in that industry seems like a good choice. The appeal of Affirm lies in the fact that it\u2019s in online sales. Folks shop online continuously, Affirm helps them do it when they can\u2019t quite make payment for the things they really need.", "upvote_ratio": 0.41, "id": "t3_u4kbq5", "created_utc": 1650064192.0}
{"sub": "investing", "title": "Rules for withdrawing contributions from Roth IRA account?", "selftext": "Hello all.\n\nAs I await for my tax advisor to get back to me, I hit some financial issues and am in need of some immediate funds.  I am not clear on the rules for withdrawing contributions for a Roth IRA account.\n\nMy account is more than 5 years old.  Am I allowed to withdraw contributions only without incurring any penalty?  I can't imagine you would have to pay taxes on this since this was taxed money that was invested to begin with.\n\nAre there any other gotchas I have to worry about?  I would think that I would have the option to choose what kind of withdraw to make, such as FIFO, but I'm not getting any of those options when I get to the point of withdrawing.\n\nAny help would be appreciated.  And I know this isn't the ideal situation (my apologies if this is the wrong sub - figured the majority would know best in here).  Thanks.", "upvote_ratio": 0.73, "id": "t3_u4j1p9", "created_utc": 1650060334.0}
{"sub": "investing", "title": "Twitter board adopts \u2018poison pill\u2019 after Musk\u2019s $43 billion bid to buy company", "selftext": "**Note:** The term poison pill refers to a defense strategy used by a target firm to prevent or discourage a potential hostile takeover by an acquiring company. Potential targets use this tactic in order to make them look less attractive to the potential acquirer: https://www.investopedia.com/terms/p/poisonpill.asp\n\n**Article:**\n\nhttps://www.cnbc.com/2022/04/15/twitter-board-adopts-poison-pill-after-musks-43-billion-offer-to-buy-company.html\n\nTwitter adopted a limited duration shareholder rights plan, often called a \u201cpoison pill,\u201d a day after billionaire Elon Musk offered to buy the company for $43 billion, the company announced Friday.\n\nThe board voted unanimously to adopt the plan.\n\nUnder the new structure, if any person or group acquires beneficial ownership of at least 15% of Twitter\u2019s outstanding common stock without the board\u2019s approval, other shareholders will be allowed to purchase additional shares at a discount.\n\nThe plan is set to expire on April 14, 2023.\n\nSuch a move is a common way to fend off a potential hostile takeover by diluting the stake of the entity eying the takeover.\n\n\u201cThe Rights Plan will reduce the likelihood that any entity, person or group gains control of Twitter through open market accumulation without paying all shareholders an appropriate control premium or without providing the Board sufficient time to make informed judgments and take actions that are in the best interests of shareholders,\u201d the company said in a press release.\n\nTwitter noted that the rights plan would not prevent the board from accepting an acquisition offer if the board deems it in the best interests of the company and its shareholders.\n\nMusk already owns a more than 9% stake in Twitter as revealed in a Securities and Exchange Commission filing last week. Soon after his stake became public, Twitter\u2019s CEO announced plans for Musk to join the board. But days later, Musk reversed course and decided not to join the board after all.\n\nIf he had joined, Musk would not be allowed to accumulate more than 14.9% of beneficial ownership of the company\u2019s outstanding common stock.\n\nAlso on Friday, Bloomberg reported, citing anonymous sources, that Twitter brought on JPMorgan to help respond to Musk\u2019s bid. Twitter had already been working with Goldman Sachs and Musk has been working with Morgan Stanley.\n\nSeveral outlets including The New York Post reported Twitter was also fielding interest from Thoma Bravo, though it\u2019s still uncertain a bid will materialize, according to sources who spoke to Reuters.\n\nJPMorgan has history with Musk, suing Tesla over a matter related to his 2018 tweet claiming he had \u201cfunding secured\u201d to take the company private. Tesla later countersued the bank.\n\nJPMorgan, Twitter and Thoma Bravo declined comment.\n\nIn a live-streamed interview at the TED2022 conference in Vancouver on Thursday, Musk laid out his vision for making Twitter\u2019s algorithms more publicly accessible and limiting content moderation.\n\nHe also acknowledged he\u2019s \u201cnot sure\u201d if he\u2019ll actually be able to buy Twitter, though he said he does have \u201csufficient assets\u201d to fund the deal if accepted. Despite his fortune, Musk has much of his assets tied up in equity in his companies including Tesla, meaning he\u2019d likely have to liquidate or borrow against his assets to come up with a large sum.\n\nBut Musk said \u201cthere is\u201d a Plan B if his initial offer to buy the company and take it private, which he called his \u201cbest and final,\u201d is rejected. He declined to provide further details in the TED interview.\n\nOn Friday, Twitter\u2019s former CEO and current board member Jack Dorsey tweeted that \u201cthe real issue\u201d is that \u201cas a public company, twitter has always been \u2018for sale.\u2019\u201d", "upvote_ratio": 0.96, "id": "t3_u4ht6s", "created_utc": 1650056742.0}
{"sub": "investing", "title": "Is there a place to park my money in robinhood (or similar) for a downpayment?", "selftext": "Thanks to previous posts I know that for my situation (want to buy a house in 3-5 years) I should gets bonds etc, but I'm wondering if anyone knows of an option that can be done with robinhood (or similar app since I'm thinking of ditching robinhood anyway) since I like the convenience of seeing all your $ and returns at once, from an app.\n\nBasically the past couple years I've saved 75k putting into what I thought were low-medium risk (Index funds + MSFT, DHR, QQQ, HD, BABA, GOOGL) - overall return is -3%. \n\nDiscover savings account is at 0.05%, better than my -3% investing skills but I'd still be losing $ to inflation. I see a few bond ETFs on robinhood but those all have negative returns somehow....", "upvote_ratio": 0.31, "id": "t3_u4h5js", "created_utc": 1650054843.0}
{"sub": "investing", "title": "Hey everyone i want to start investing.", "selftext": " Hey everyone, so probably this is frequent asked but:\n\nI am 27 years old and i kinda landed a pretty stable job. I have an emergency account and i would like to start investing. Everyone is telling about etfs. I know what they are and everything but i am too scared to start investing as it's my first steps in this world.\n\nI can afford 50-100 dollars per month to add to the etf. Do you think it's worth it starting investing in etfs in my age.\n\nMy plan is to just put some money there and let it grow over the years for like 20-25 years.  I just need the first push from guys with more knowledge than me. Is it really worth starting now.", "upvote_ratio": 0.42, "id": "t3_u4g2k9", "created_utc": 1650051789.0}
{"sub": "investing", "title": "Twitter Brings on JPMorgan to assist in talks with potential buyers", "selftext": "Twitter Inc. has brought on a second investment bank, JPMorgan Chase &amp; Co., to help it respond to Elon Musk\u2019s hostile bid, according to people familiar with the matter.\n\nThe largest U.S. bank started work recently to assist Twitter in talks with potential buyers, the people said, asking not to be identified because the matter is private. \n\n Source:  [https://www.bloomberg.com/news/articles/2022-04-15/twitter-brings-on-jpmorgan-as-adviser-alongside-goldman](https://www.bloomberg.com/news/articles/2022-04-15/twitter-brings-on-jpmorgan-as-adviser-alongside-goldman)", "upvote_ratio": 0.75, "id": "t3_u4fh7b", "created_utc": 1650050073.0}
{"sub": "investing", "title": "Unexplained funds in IRA?", "selftext": "I finished contributing to my Vanguard Roth IRA for 2021 and noticed $750 showed up in the account for 2022 that I did not contribute. I checked my bank account which is the only account attached to my IRA account for transfers and there is nothing showing $750 coming out of there. It also does not show the $750 on my IRA transaction history. \n\nAny ideas?", "upvote_ratio": 0.63, "id": "t3_u4dtdw", "created_utc": 1650045508.0}
{"sub": "investing", "title": "Opinions Needed: Employee Share Buy In Plan. 15% discounted shares. How much should I invest?", "selftext": "Hi everyone!\n\nI get to choose whether to take 1% - 10% to buy 15% discounted company shares. How would you think about which percentage to choose?\n\nIt's my first year working, not much in savings. But I do like my paychecks atm. I appreciate all input\n\nEdit: I want to thank everyone for your help &amp; perspective. It\u2019s very much appreciated. I decided to max out my contributions", "upvote_ratio": 0.91, "id": "t3_u4cupj", "created_utc": 1650042810.0}
{"sub": "investing", "title": "Investing in the Past as Entertainment and Education", "selftext": "Does anyone know of a trading platform that has simulation of the past? Like a sort of 'live historical stock market data paper-trader' where you can make all the plays you wish you would have. Works near identical to a regular paper trading platform, except you can set the date back and fast-forward/rewind through time.", "upvote_ratio": 0.85, "id": "t3_u49d80", "created_utc": 1650033139.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - April 15, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.87, "id": "t3_u43um0", "created_utc": 1650013269.0}
{"sub": "investing", "title": "Where can I simulate the effect of a large market order on the current buy/sell market.", "selftext": "E.g. if I wanted to instantly sell 10,000 shares of a company but only 5,000 buy orders are at the listed price, 2,000 at the next lowest, etc. Obviously I'd get diminishing returns on those sales as the buy orders went down in price.\n\nI know how to see a market depth chart but I haven't been able to find a service or calculator for actually pricing against it. I know NASDAQ level 2 does this but it's private.", "upvote_ratio": 0.7, "id": "t3_u3wn02", "created_utc": 1649985977.0}
{"sub": "investing", "title": "Should I keep it to one brokerage account?", "selftext": "Hi!\nI am still a pretty new investor. I have an IRA with Fidelity and also a brokerage account in which I invest in index funds. I have decided to invest some in individual company stocks and thought about using E*Trade for that because it seems easy. Is this a good idea or should I keep all of my investing (both index funds and individual stocks) in the same brokerage account?", "upvote_ratio": 0.65, "id": "t3_u3wm21", "created_utc": 1649985893.0}
{"sub": "investing", "title": "Need more knowledgeable Bond experts", "selftext": "Well, as rates rise, more people are asking about Bonds.\n\nThere is a subreddit /r/bonds.  There isn't much traffic, or seemingly much knowledge if I'm there with my speculative angle on that market.\n\nBut that can change!  Join me in helping them!  I know there are some actual fixed income people here....\n\nSorry if this breaks the rules.", "upvote_ratio": 0.83, "id": "t3_u3uym5", "created_utc": 1649980745.0}
{"sub": "investing", "title": "What am I missing about $GOOGL?", "selftext": "\nGoogle has:\n\n*Nearly its lowest P/E since 2015\n\n*Still putting up insane growth numbers YoY\n\n*At a roughly 9 month low and very strong support\n\n*Obviously a very strong future with tons of investment in research and development (particularly cloud computing and working on autonomous cars)\n\n*Stock split coming up that could have a bit of upside\n\n\nEspecially with continued earnings growth it just looks like such a good spot. I know it had an insane (65%!) 2021, but the P/E ratio actually went down since then, which would mean the market was just pricing in (and technically underpricing because P/E dropped) the earnings growth throughout 2021. So yea, it looks really good to me I am just wondering what other people's thoughts are on $GOOGL, and if I am missing anything about this because it just seems like an incredibly good deal to me at this spot.", "upvote_ratio": 0.92, "id": "t3_u3tufz", "created_utc": 1649977364.0}
{"sub": "investing", "title": "What are some investment ideas that are atypical but can have great returns?", "selftext": "What are some investment strategies that you have utilized that may take a little upfront work, but have great returns in the long run?\n\nAnd no, mutual funds, stocks, ETFs and buying rental properties have all been talked to death here. I'm more asking about something not everybody thinks about, but are actually great strategies that have worked for you.\n\nThanks.", "upvote_ratio": 0.89, "id": "t3_u3stlu", "created_utc": 1649974366.0}
{"sub": "investing", "title": "Trying to understand 10 year yields, help?", "selftext": "\"The 10-year yield is used as a proxy for mortgage rates. It's also seen as a sign of investor sentiment about the economy. A rising yield indicates falling demand for Treasury bonds, which means investors prefer higher-risk, higher-reward investments.\" - Investopedia. If this is true then why has the 10 year yield gone up and stocks gone down since January? Any help? Seems like the higher the 10 year yield goes the lower the stock market goes. Trying to better understand bonds. \ud83e\udd14", "upvote_ratio": 0.79, "id": "t3_u3pc3z", "created_utc": 1649964624.0}
{"sub": "investing", "title": "I have been looking at either Devon or Marathon as a potential buy", "selftext": "Longer question.  I have been researching the annual percentage increases of either Devon Energy (DVN) or Marathon Oil  (MRO)  and wondered if the community had any thoughts on things to consider for or against for each.  Please delete if this too specific.", "upvote_ratio": 0.46, "id": "t3_u3nr55", "created_utc": 1649960209.0}
{"sub": "investing", "title": "I honestly think Xebec Adsorption ($XBC) is going to triple in 2-3 months. Please read my analysis and see if you agree!", "selftext": " Xebec is now (as of yesterday) the world's largest carbon capture and storage project, with pre-existing operations in hydrogen energy and gas filtration, while also working with 8 European countries (includes Germany, Netherlands, Belgium, Norway) with its subsidiary HyGear to produce green fuel for aviation. Its total assets after debt are \u00a3301.33m (so their market cap of \u00a3205m is actually backed by that and more unlike majority of other firms). Revenue for Q4 was \u00a345.90m and actually reported a PROFIT which was higher than expected (although FY21 net loss margin 18.6%). Forecasts, as they have for months, see it in slight net loss for next year or two, but anything can happen). I've been doing this stuff for a few years now and honestly this is just such a unique opportunity right now. Especially being under $3. My biggest holding by far.\n\nGo check the current 5 year chart and it'll blow your mind.", "upvote_ratio": 0.33, "id": "t3_u3iq4e", "created_utc": 1649946302.0}
{"sub": "investing", "title": "Commodity ETFs - risk tolerance and possible inflation hedge?", "selftext": "Good day all - I'm researching various commodity ETFs in an effort to hedge against inflation.  Currently the majority of my holdings are in stocks (\\~30%) and an investment property (\\~60%).  This is disregarding my 401k which is managed in a diversified lifecycle fund (I max this out and pretend it doesn't exist).  I've done some research (admittedly not enough) and believe that commodities are a good way to hedge against inflation (regarding my non-retirement investments).  Rather than going out and buying copper/cows/soy/gold, I've found several ETF's that would allow me some exposure.  I went to buy shares of JJC (copper futures ETF) through fidelity and had to agree to a \"high risk\" investment via an online disclosure.  A few questions:\n\n1. I know that some investments can lead to theoretical infinite losses (i.e. shorting a stock).  I assume that investing in a commodities ETF (i.e. JJC) limits my loss to my total investment - is this correct?  For example if I purchase $1000 worth of JJC, my maximum loss is $1000?\n2. Are these ETF's subject to bubbles?  I don't mean copper bubbles, but ETF speculation bubbles?  I guess what I'm trying to say is that as ETF positions are purchased, does that money get put into actual commodity purchases by the ETF and more shares issued, or will increased demand in ETF drive up prices of the ETF because no new shares are created?  Hope that made sense. \n3. Are there any good resources you can recommend for getting smart on commodity ETFs?  Why are they considered so risky?\n\nI know these are rookie questions - please don't hurt me. I'd like to purchase more of these (i.e. wheat, soy, gold).  As I understand they are mostly investment tools as they typically inverse stock market gains.  I do believe we're in for a stock market correction but I'd rather hedge than liquidate my stock positions.", "upvote_ratio": 0.74, "id": "t3_u3hexb", "created_utc": 1649942457.0}
{"sub": "investing", "title": "Since everyone is big on I-bonds right now, allow me to make an argument AGAINST them", "selftext": "**While no one can predict the future, I am using historical data to argue that your average rate might be as low as 2.2%.**\n\n[https://www.treasurydirect.gov/indiv/research/indepth/ibonds/IBondRateChart.pdf](https://www.treasurydirect.gov/indiv/research/indepth/ibonds/IBondRateChart.pdf)\n\nIf I'm reading this chart correctly, the first column is your \"fixed\" locked in minimum rate. If you buy a bond now, your \"fixed\" rate will be 0% and will fluctuate each 6 months.\n\nEvery time you buy a bond, the fixed rate is different. The best time period was if you bought between May and Oct of 2000, when the fixed rate was 3.6%. Those people (if they still own their bonds) will be making 10.85% right now, compared to new buyers making 7.12%.\n\nSince the current fixed rate is 0%, I will compare to another time when the rate was 0%. 2008 gives us the the longest time period for comparison, and averages to 2.16%  \n2011 gives us 2.01%  \n2015 gives us 2.16%  \n2020 gives us 3.35%\n\nNow, all this being said, inflation might stay very high for the next 5-10 years, in which case the rate will stay generously high also. But since I hadn't heard anything truly contrarian yet, I wanted to play devil's advocate.", "upvote_ratio": 0.63, "id": "t3_u33khr", "created_utc": 1649892774.0}
{"sub": "investing", "title": "Elon Musk offers to buy Twitter for $41.39 billion", "selftext": "After taking a 9.2% stake in twitter, Elon musk has now offered to buy the entire company for $41.39 billion. This comes after some recent news announcement of Elon Musk not accepting the board seat position at Twitter because of some alleged background check. \n\nAccording to Twitter CEO, Parag Agarwal, Twitter and Elon couldn\u2019t come to an agreement regarding the board. Parag notified his employees that there could soon be noise around the company. Twitter shares closed at a market cap of around $36.7 billion as of Wednesday. Shares are up 11% pre market as of this writing.", "upvote_ratio": 0.93, "id": "t3_u3eaw3", "created_utc": 1649932093.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - April 14, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.78, "id": "t3_u3cxzs", "created_utc": 1649926869.0}
{"sub": "investing", "title": "Has the age of robotics and automation peaked?", "selftext": "I've been in the iShares Automation &amp; Robotics and although I think the future is going to be in these sectors the ETF has been pounded down in recent months. Is this a really badly balanced ETF for this sector, is the future not in these underlying companies or is it just irrational markets now?", "upvote_ratio": 0.2, "id": "t3_u3c85d", "created_utc": 1649923610.0}
{"sub": "investing", "title": "I hardly resist buying EDV (extended term treasury STRIPS).", "selftext": "I hardly resist buying EDV and making it 10% of my portfolio, even though my portfolio is composed of 100% stock index funds according to my long-term investment strategy.\n\nThe news is terrible for the bond market, especially for long-term bonds: inflation is reaching its 40-year high, oil prices are high and could rise even more due to the Russian-Ukrainian war and a possible oil embargo on Russia.  Higher oil prices could lead to even higher inflation.  The Fed has already announced multiple interest rate hikes over the next 3 years. \n\nAnd all this terrible news is already priced in the bonds!\n\n I understand that buying EDV today means market timing, but I hardly resist buying it.", "upvote_ratio": 0.71, "id": "t3_u3at2d", "created_utc": 1649917379.0}
{"sub": "investing", "title": "how do I explain to my smaller sibling that literally everything is investing?", "selftext": " I hear this from them that they do not want to invest and only want to spend their money. So I tried explaining that actually everything is an investment, if you buy a meal you enjoy, you\u2019re investing your money in exchange for   satiety, if you pay to do something you like, that\u2019s an investment too, literally everything is an investment. \n\nI want them to think like this, because ever since I started thinking like this, everything became better. I would seek out the value in everything instead of just buying and selling things, value was something I could finally perceive. And now that crypto is becoming a real commerce tool with projects like Binance Pay and Exeno, I feel like I\u2019ll be spending money much more easily. Since I\u2019ll be paying in crypto directly, I\u2019ll be directly pulling out profit to spend, which is why I need this mindset more than ever. \n\nI need to explain to him that investing early is crucial. And that understanding where your money goes is elemental. There\u2019s one way that I could get him to spend money with the most fulfilment and it\u2019s like that. \n\nInvesting isn\u2019t just buying a stock and selling it, it goes way deeper than that, it\u2019s part of our every day life, and I wish more people understood this.", "upvote_ratio": 0.55, "id": "t3_u392xs", "created_utc": 1649910541.0}
{"sub": "investing", "title": "Alleghany shareholder sues to block $11.6 billion Berkshire buyout over lack of disclosures", "selftext": " Alleghany Corp, which agreed last month to be acquired by Warren Buffett's Berkshire Hathaway Inc, was sued on Wednesday by a shareholder who accused the insurance company of making inadequate and misleading disclosures about the $11.6 billion takeover.\n\nIn a complaint filed in Manhattan federal court, the plaintiff Shiva Stein said Alleghany failed in a proxy statement to adequately explain the financial basis for the \"fairness opinion\" issued by its bankers at Goldman Sachs, which assessed whether the deal was fair to shareholders.", "upvote_ratio": 0.93, "id": "t3_u33xkn", "created_utc": 1649893871.0}
{"sub": "investing", "title": "what will happen when artificial intelligence is used in the markets", "selftext": "I'm sure it is already Ofcourse but I'm talking about when it gets to the point where whoever has the technology is guaranteed to always win. They know exactly where and when the markets will move. Backed by billions of dollars \n\n\nThere has to be and will be point  where tech and government Influence on the markets will just be too much and all the other players stop playing as they see it as fixed . The way the markets have been the last 100 years won't be the future , it just won't work anymore .", "upvote_ratio": 0.38, "id": "t3_u33wj4", "created_utc": 1649893783.0}
{"sub": "investing", "title": "apps similar to My Stock Portfolio?", "selftext": "I currently use Vanguard for all my investing but as everyone knows, their app and reporting is shit.  I love MSP but I have to enter every transaction.\n\nSo, is there a similar app out there but one that can connect to Vanguard?  Would love to here what everyone else uses", "upvote_ratio": 0.81, "id": "t3_u33t10", "created_utc": 1649893483.0}
{"sub": "investing", "title": "Is FB is a solid long term buy?", "selftext": "After the recent price drop it seems unlikely the price could hurdle downward super significantly in the near future, unless the company itself also goes out of business entirely, which also seems extremely unlikely. \n\nI think the real bet here is, if Facebook goes out of business. If they do money will be lost on the investment. If they don\u2019t then long term there likely could be profitability. Even if not to the same extent as just a broad equity index fund", "upvote_ratio": 0.39, "id": "t3_u32n29", "created_utc": 1649890108.0}
{"sub": "investing", "title": "The real reason for \"Record\" profits.", "selftext": "Does anyone else think that it would just make sense that companies report higher profits during a period of inflation? \nSeems like the political spin is greed, but for investing this could show some new signs to look for. I've been thinking about starting to try and make \"inflation-corrected\" numbers when looking at company earnings. What do you guys think is this just obvious, or is their a flaw in my thinking?\n\nThanks for any feedback", "upvote_ratio": 0.37, "id": "t3_u2rd6a", "created_utc": 1649859485.0}
{"sub": "investing", "title": "Question about AT&amp;T spin-off and WBD", "selftext": "I just had AT&amp;T do the spin-off and got new shares of WBD. On my apps. should my $T and $WBD shares both display a new \"average cost\" after a while? Because didn't the spin-off lead to recalculation of the basis for those shares? I hope this question makes sense. Thank you!", "upvote_ratio": 0.85, "id": "t3_u2rckf", "created_utc": 1649859441.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - April 13, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.85, "id": "t3_u2m0sj", "created_utc": 1649840468.0}
{"sub": "investing", "title": "Real Estate vs Index Fund", "selftext": "I just made up my mind to pump money into my life over the next decade. Since Jan I\u2019ve been saving 75% of my income in an account labeled \u201cInvestments\u201d and have narrowed things down to Fidelity index fund FNTEX or rental properties. I can\u2019t see a benefit of rentals over index funds but lots of people buy rentals. Just asking for some perspective. Thanks.", "upvote_ratio": 0.52, "id": "t3_u2hbg2", "created_utc": 1649821549.0}
{"sub": "investing", "title": "Is acorns a legitimate way to invest?", "selftext": "Delete if not okay, but I didn\u2019t want to post to the acorns sub bc of bias\nAs the title says, is acorns an okay thing to use to invest with stocks? For example, about 52% of my money invests into large companies, like Apple and Amazon, then with 10% into midsize companies and the rest is into small companies, some international companies, and a bitcoin etf. Is this relatively good? Because  my investing sucks ass atm and was wondering if this is an okay way to invest my money?", "upvote_ratio": 0.8, "id": "t3_u2gv52", "created_utc": 1649820008.0}
{"sub": "investing", "title": "Buying I-bonds today will yield a guaranteed return of 8.37% in the next 12 months. What is the case for not maxing this out?", "selftext": "Why would I not max this out?  I understand that I-bonds exist to simply keep up with inflation and if the S&amp;P moons I have missed out, but this kind of no risk return is not something I have come across in my short time investing.  The other side of the coin is the S&amp;P could continue to struggle.\n\nSomeone explain to me why I shouldn't throw my money at this.\n\n8.37% comes from 7.12% from the first 6 months and the calculated 9.62% for the second 6.", "upvote_ratio": 0.95, "id": "t3_u2gt5t", "created_utc": 1649819835.0}
{"sub": "investing", "title": "Opened a Treasury Direct Account to purchase I bonds....", "selftext": "Is anyone else doing this?  Is it as easy as it seams?  \n\nHad no issues opening the account and linking my funding account.  \n\nDo I just need to put in $10k and let it go for the year?  And assume that I can put in $10k each year after?  \n\nIf an emergency arrises I can pull the purchase it seems?  Just take a 3 month penalty if held less than 5 years?\n\nIs there anything I missing before I fund?\n\n&amp;#x200B;\n\nThank you!", "upvote_ratio": 0.9, "id": "t3_u2fxcd", "created_utc": 1649817034.0}
{"sub": "investing", "title": "Crypto IRA - are they a good choice for long term holding?", "selftext": "Looking to diversify a small percentage of the growth percentage of my retirement with crypto. Probably about 3% overall. Right now I\u2019m about 90% stocks in my mid 40s. \n\nI have been looking into Crypto IRAs. Any thoughts on these? Are they legit and safe? Should I just buy and hold crypto another way?\n\nI don\u2019t currently have any crypto, just 401k, IRA, and a brokerage account I keep for short term investments. The crypto investment would be a long term hold.\n\nEdit: it\u2019s not a post about investing in crypto but using the crypto IRA as opposed to just buying through another means.", "upvote_ratio": 0.33, "id": "t3_u2cvto", "created_utc": 1649807733.0}
{"sub": "investing", "title": "Which investing website has the best layout?", "selftext": "I'm looking to get into investing, mainly in ETFs and also setting up a Roth IRA. For a while I was trading individual stocks on TD Ameritrade but their website is absolutely terrible and frustrating to use. For the other investment firms (Fidelity, Vanguard, etc), which one has the most simple website for me to trade on? Thank you!", "upvote_ratio": 0.76, "id": "t3_u2b9c8", "created_utc": 1649803042.0}
{"sub": "investing", "title": "Investing in TIPS (treasury inflation protected securities). Good idea?", "selftext": "I\u2019ve been thinking about where to invest my money lately. I\u2019ve been long stocks for 10+ years. \n\nI\u2019ve been thinking about TIPS (treasury inflation protected securities). I don\u2019t understand exactly how they work though. It seems like they are currently paying 7.5% interest annually. With inflation rising to 8.5%, I assume they will now be at 8.5%. Am I missing something? I would be more than happy with 8.5% in the market right now. Why not just get the guaranteed 8.5% from the TIPS. I understand that I\u2019m technically breaking even due to inflation, but I think I\u2019m ok with that for the short term. \n\nHelp me understand if I\u2019m missing something.", "upvote_ratio": 0.69, "id": "t3_u2780b", "created_utc": 1649791647.0}
{"sub": "investing", "title": "Investing 10-20k into googl since December and will continue until split. Opinions?", "selftext": "I\u2019ve been investing 10-20K per month in Google since December and plan to continue that trend until the split happens. I\u2019ll then half that and start throwing more into spy and voo. My thinking is it\u2019s a long term hold and with waymo not even really being talked about yet could be a major upside 4-5 years down the road if their partners start licensing their technology. Also alphabet has some other moonshot potential companies like wing that started delivering items in Houston via drone. \n\nOr am I just a big dummy?  \n\nCurrently holding 22 shares at 2750 cost basis.\n\nEdit- goal is to have 40 shares before split\n\nEdit- my income monthly is about 30-40K depending on commissions that month. Last month I hit 50K but that\u2019s an unlucky trend 30K is the basis there.", "upvote_ratio": 0.69, "id": "t3_u25fu5", "created_utc": 1649786954.0}
{"sub": "investing", "title": "LVMH Q1 results exceed expectations with +29% revenue increase YoY", "selftext": "From LVMH's IR website ([https://www.lvmh.com/news-documents/press-releases/good-start-to-the-year-for-lvmh-2022/](https://www.lvmh.com/news-documents/press-releases/good-start-to-the-year-for-lvmh-2022/)):\n\n&gt;*LVMH Mo\u00ebt Hennessy Louis Vuitton, the world\u2019s leading high-quality products group, recorded revenue of* ***18 billion euros*** *in the first quarter of 2022,* ***up 29% compared to the same period in 2021.*** ***Organic revenue growth was 23%.*** *LVMH had a good start to the year against a backdrop of continued disruption from the health crisis and marked by the dramatic events in Ukraine. All business groups achieved double-digit revenue growth, except for Wines &amp; Spirits, which continued to see supply constraints. The United States and Europe also achieved double-digit revenue growth; Asia continued to grow over the quarter despite the impact of a tightening of health restrictions in China in March.The LVMH Group is closely monitoring developments in Ukraine and the region. Its first priority was to ensure the safety of its employees in Ukraine and to provide them with all the necessary financial and operational assistance.*\n\nMarket's expectations noted at 17,03 bn EUR. Actual revenue was **18 billion euros ($19.59 billion).**([https://www.marketscreener.com/quote/stock/LVMH-MOET-HENNESSY-LOUIS-4669/news/LVMH-1Q-Revenue-Outstripped-Expectations-Despite-Global-Pressures-40035685/](https://www.marketscreener.com/quote/stock/LVMH-MOET-HENNESSY-LOUIS-4669/news/LVMH-1Q-Revenue-Outstripped-Expectations-Despite-Global-Pressures-40035685/))\n\nAccording to IR especially their biggest category Fashion &amp; Leather Goods, including their in-house brands Louis Vuitton &amp; Dior postet strong numbers, as category revenue rose by 35% YoY (30% organically growth) to 9,123 bn EUR, which makes up about half of the total Q1 revenue. Weakest link was the Wines &amp; Spirits compartment that missed double digit growth at 8% increase (2% organic) in revenue that at the same time is the smallest category the company reports in.\n\nNotwithstanding the economic pressures at the moment LVMH stands out with strong numbers. Waiting on the transcript and commentary of their Q&amp;A session. Anybody?\n\nWKN: 853292\n\n*Disclaimer: I am a shareholder of LVMH stock.*\n\n*Crosspost from* /r/stocks : [*https://www.reddit.com/r/stocks/comments/u24upf/lvmh\\_q1\\_results\\_exceed\\_expectations\\_with\\_29/*](https://www.reddit.com/r/stocks/comments/u24upf/lvmh_q1_results_exceed_expectations_with_29/)\n\nEdit: typo, added info", "upvote_ratio": 0.77, "id": "t3_u254b1", "created_utc": 1649786118.0}
{"sub": "investing", "title": "Shorting the Las Vegas Housing Market", "selftext": "For a variety of reasons that would take a long time to explain, I believe the fundamentals of the Las Vegas housing market are terrible and prices will peak in the next several months.\n\nI've looked at a lot of REITs, but all the residential ones I can find are pretty diversified and not concentrated in any one region. \n\nDoes anyone know of any REITs focused primarily on the Las Vegas residential market? Or does anyone have other suggestions on how to short a regional housing market (as opposed to shorting the entire housing market)?\n\nThanks!", "upvote_ratio": 0.75, "id": "t3_u21wbq", "created_utc": 1649777710.0}
{"sub": "investing", "title": "SQQQ as Insurance to tech heavy portfolio?", "selftext": "I have two separate portfolio\u2019s; one long term investing (30k), one short-term \u201ctrading\u201d (2k). In my short-term im heavy tech, and trade basic options such as calls/puts both short and long term (limited long d/t small capital); does anyone here have any experience with SQQQ; if so, opinions on using SQQQ as insurance to a tech heavy/risk heavy portfolio such as the aforementioned? Percentage you might use to insure? For ex, if 1.5k is in tech stocks, would you use $250 in SQQQ (essentially equivalent to $750 (1/2 invested portfolio) as insurance to cover potential loss? \nThoughts/comments?\nIm 24 y.o. so love taking on risk", "upvote_ratio": 0.5, "id": "t3_u20aq4", "created_utc": 1649773386.0}
{"sub": "investing", "title": "I Bonds\u2019s new variable rate will rise to 9.62% with the May reset", "selftext": "Since inflation might be peaking, this might be the highest variable rate for I bonds that we'll see for a while! And probably best to lock in the current 7.12% in April while you can, so that it can have both high rates in the year long minimum holding period.\n\nhttps://tipswatch.com/2022/04/12/i-bondss-new-variable-rate-will-rise-to-9-62-with-the-may-reset/", "upvote_ratio": 0.96, "id": "t3_u1yuea", "created_utc": 1649769308.0}
{"sub": "investing", "title": "What happens to my broker-held shares in case of a merger?", "selftext": "I bought some Discovery stock last week, ahead of the Warner merger, and now that the merger is complete, Warner Bros Discovery is trading. However, my broker account still shows Discovery shares, and I can\u2019t do anything with them. Will the automatically be replaced with Warner Bros. Discovery at some point, or does my broker have to do that manually?", "upvote_ratio": 0.71, "id": "t3_u1yd5b", "created_utc": 1649767901.0}
{"sub": "investing", "title": "Help me convince my wife to properly invest $250k", "selftext": "Quick story: My daughter inherited a house (fully paid for and tenated) that (if sold today) would net about $250k (after closing costs and such).  \n\n\nProblem is, this house was rented in the \"good 'ol boy\" way, where the house is making approximately $1k/mo, but then it has taxes and fixes that drain about $5k/yr.  Net profit is \\~$7k.\n\nI told my wife that this is insane, and that we could sell this house and use the money to help our daughter have a headstart in life. She could make so much more off of the funds from the investment, and she would also have no headaches with the renters or anything like that.  \n\n\nWhat are some ways this could be invested to make more than what it currently returns.  One suggestion I have had is QYLD with DRIP, but I would love to show her three or four other options to really drive the point home.", "upvote_ratio": 0.3, "id": "t3_u1y0xf", "created_utc": 1649766837.0}
{"sub": "investing", "title": "CPI rises 1.2% in March, 8.5% over last 12 months", "selftext": "https://www.bls.gov/news.release/cpi.nr0.htm\n\nYou can see a very specific breakdown per item here: https://www.bls.gov/news.release/cpi.t02.htm\n\n&gt;The Consumer Price Index for All Urban Consumers (CPI-U) increased 1.2 percent\nin March on a seasonally adjusted basis after rising 0.8 percent in February,\nthe U.S. Bureau of Labor Statistics reported today. Over the last 12 months,\nthe all items index increased 8.5 percent before seasonal adjustment.\n\n&gt;Increases in the indexes for gasoline, shelter, and food were the largest\ncontributors to the seasonally adjusted all items increase. The gasoline\nindex rose 18.3 percent in March and accounted for over half of the all items\nmonthly increase; other energy component indexes also increased. The food index\nrose 1.0 percent and the food at home index rose 1.5 percent. \n\n&gt;The index for all items less food and energy rose 0.3 percent in March following\na 0.5-percent increase the prior month. The shelter index was by far the biggest\nfactor in the increase, with a broad set of other indexes also contributing,\nincluding those for airline fares, household furnishings and operations, medical\ncare, and motor vehicle insurance. In contrast, the index for used cars and\ntrucks fell 3.8 percent over the month.\n\n&gt;The all items index continued to accelerate, rising 8.5 percent for the 12\nmonths ending March, the largest 12-month increase since the period ending\nDecember 1981. The all items less food and energy index rose 6.5 percent, the\nlargest 12-month change since the period ending August 1982. The energy index\nrose 32.0 percent over the last year, and the food index increased 8.8 percent,\nthe largest 12-month increase since the period ending May 1981.", "upvote_ratio": 0.97, "id": "t3_u1xyrn", "created_utc": 1649766644.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - April 12, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.76, "id": "t3_u1ukwd", "created_utc": 1649754069.0}
{"sub": "investing", "title": "Bayer reaches $80 mln PCB contamination settlement with Ohio. Is Monsanto buy?", "selftext": "https://www.reuters.com/business/healthcare-pharmaceuticals/bayer-reaches-80-mln-pcb-contamination-settlement-with-ohio-2022-03-24/\n\nIs Bayer/Monsanto a buy? It's been climbing for one month straight and I guess that's because of GMOs given the situation at Ukraine.", "upvote_ratio": 0.6, "id": "t3_u1ua9c", "created_utc": 1649752796.0}
{"sub": "investing", "title": "Qualcomms dividend strategy", "selftext": "All,\n\nRecently qualcomm announced that they would focus on increasing dividends over time instead of share buybacks.\n\nCan someone please explain how this affects things in the longer run and why dividends are better than buybacks, at least for qualcomm?", "upvote_ratio": 0.88, "id": "t3_u1tbp2", "created_utc": 1649748524.0}
{"sub": "investing", "title": "IRA Recharacterization Questions", "selftext": "I funded my Roth IRA for the 2021 tax year in 2021 up to the max. However, my MAGI is over the income limit, so now I'm overfunded and will be charged 6% a year that I'm over. My Roth IRA funds are all in equity and I would rather not sell any to get below the limit. I've already funded my Roth IRA for 2022 so I can't just move funds from 2021 to 2022.\n\nMy question is: if I do an IRA Recharacterization from Roth IRA to Traditional IRA selecting just those assets to get me below the income limit to avoid a penalty do I keep my stock holding/cost basis/acquisition date, etc in the new Traditional IRA account? Or are my stocks liquidated at whatever the market is at at the end of the day I make my IRA Recharacterization request and I now have to reinvest those back into the stocks I had previously in my Roth account?\n\nI hope this makes sense and if anyone has any suggestions on other options that I have available I'd appreciate it.", "upvote_ratio": 0.67, "id": "t3_u1q5mk", "created_utc": 1649736009.0}
{"sub": "investing", "title": "Nine Years Ago, someone here asked what stock to hold for 20 years. Whats your pick for the next 9 years?", "selftext": "https://www.reddit.com/r/investing/comments/1bbbgh/if_you_were_to_buy_a_stock_today_to_hold_for_20/\n\nI saved this thread and looked back to see winners and losers. Some interesting picks here, GOOG, AAPL, along with some \"safer\" picks. Towards the bottom a few folks suggest TSLA, and at the very bottom, back in 2013, with -3 votes, someone suggests buying Bitcoin!\n\nSo... which stocks are you holding for 9, 10 years?", "upvote_ratio": 0.97, "id": "t3_u1ocku", "created_utc": 1649730272.0}
{"sub": "investing", "title": "Carbon capture technology?", "selftext": "What is your opinion of carbon capture technology and it\u2019s future. \n\nI\u2019ve heard a lot of hype surrounding capturing carbon from the atmosphere and storing it in carbon wells etc.\n\nWhat companies are pioneering this industry, especially those listed on the TSX or NYSE.\n\nAnd how viable is it going forward ?", "upvote_ratio": 0.74, "id": "t3_u1nadv", "created_utc": 1649727104.0}
{"sub": "investing", "title": "Advice on what to do with 70k FL lot", "selftext": "Hi all, I\u2019m currently struggling with what to do with a plot of land worth around 70K in a small coastal town in FL (not waterfront or anything though). I\u2019ve been considering various options and am honestly in an analysis paralysis. \n\nI\u2019m 28 years old and rent in a different city in FL, have a well-paying job, and am overall doing well financially. My idea is to scale this investment and not use it for anything personal. \n\nOn one hand, I can build a house on the lot (approx. 300k), save in agent fees and some closing costs, then either sell or rent it out when it\u2019s completed a year later, depending on the market. On the other hand, I can sell it now, have that money on hand to shop around for a single-family home in a different FL city. I could either rinse and repeat or rent it out. The FL market is absolutely crazy right now though. I don\u2019t see it slowing down but if something were to happen, I\u2019d be waiting for the construction to finish and be stuck with an expensive mortgage. I like the idea of having the money to shop around for the right deal but maybe I\u2019m approaching this wrong? Maybe the best thing would be to throw it into a fund or other investment type. Any advice would be greatly appreciated.", "upvote_ratio": 0.33, "id": "t3_u1mgq5", "created_utc": 1649724668.0}
{"sub": "investing", "title": "Kindred Group (KIND SDB) Analysis\u2026 Worth the Gamble?", "selftext": "*Disclaimer:* huge bricks of text coming up...\n\n**Stock Valuation:** \n\nEuropean comps 888 Holdings, Entain Plc, and Flutter Entertainment trade at avg. multiples of 17.9x EBITDA and 3.5x sales.  888 Holdings is half the size of Kindred on a revenue basis, and a quarter of the size on an EBITDA basis. Entain is 2.5x larger than Kindred on a revenue basis, but less than 2x its size on an EBITDA basis.  Flutter is 4.5x the size of Kindred on a revenue basis but only 2.5x its size on an EBITDA basis. These comparisons point to the significantly higher profitability of Kindred vis-\u00e0-vis its peers (discussed further below) US-domiciled comp DraftKings is slightly smaller than Kindred on a revenue basis and will lose approximately $615 million on an EBITDA basis this year.  DraftKings trades at 17x revenue.  \n\n**Profitability Considerations vs. Competitors:** \n\n\u2022\tFanDuel (the US online sports betting segment of Flutter Entertainment) sales and marketing expense % of revenue is 42%.  It will not be EBITDA profitable until 2023.  DraftKings sales and marketing % of revenue is 65%.  They will not be EBITDA profitable until 2024.  DraftKings has publicly stated that it will take 2 to 3 years for each new customer acquisition to generate a positive profit contribution.\n\n\u2022\tKindred\u2019s harnassing of \u201cBig Data\u201d to create efficiencies and \u201cstickiness\u201d in customer acquisition has led to a sales and marketing expense % of revenue of 21%, more than 50% below the ratio of its US competitors.  This is the reason Kindred has EBITDA margins of over 30%.  European comps have EBITDA margins in the mid 20s, and US comps, as shown above, continue to lose money.  These economies of scale have to do with Kindred\u2019s huge head start in entering the online sports and casino gambling market, and its lead in exploiting the data generated and steady infrastructure built up over these years.  Kindred Group was founded in 1997 (same year as 888 Holdings), while Entain was founded in 2004, FanDuel was founded in 2009, and DraftKings in 2012.  Kindred Group and the Europeans clearly saw this opportunity earlier.\n\n\u2022\tKindred generates 67% of its revenue in Western Europe, 31% in ROW (rest of world includes Eastern Europe, the Nordic Countries, Australia, and Asia), and 2% in the US.  (Attached chart shows geographic exposure of peer group)  Kindred, through its Unibet brand, has just won 2 additional state licenses in the US (Virginia and Arizona) and now is in a total of 6 states.  Unibet was recently chosen as the online sportsbook partner of the Pittsburgh Steelers and the Philadelphia Eagles. Generally, each state awards 7 to 8 licenses, and there are still 30 states that have not yet legalized online sports betting and gambling.  The break-down of US states presence is shown below:\n\n&amp;#x200B;\n\n||Number of US States Operating in|States|\n|:-|:-|:-|\n|Kindred Group|6|VA, AZ, NJ, PA, IN, IA|\n|DraftKings|13|NJ, WV, IN, NJ, MI, CO, IL, IA, NC, VA, PA, NH, TN|\n|FanDuel|10| IL, CO, VA, PA, IN, WV, IA, NJ, TN, MI|\n|Entain (BetMGM)  |10|MI, NV, NJ, IA, WV, IN, TN, CO, VA, PA|\n|888 Holdings                                                                              |3|NJ, NV, DE |\n\n\n\n\u2022\tThe take-away here is that Kindred, which is the 3rd largest global online sports betting and casino gaming company, still has a miniscule share of the total market opportunity.  It will generate approximately $1,250 of revenue this year in Europe; $610 million of revenue in ROW; and $40 million in the US.  These represent market shares of 5.2% in Europe, 5.1% in ROW, and 0.3% in the US, which it just entered in 2021.   Growth going forward will be enormous.\n\n**Short-Term Catalysts:** \n\n\u2022\tAs of 09/21/21 Kindred\u2019s revenue had already surpassed the revenue of last year\u2019s entire third quarter.  In the third quarter of 2020 the economy had reopened post-covid, and there were a large number of live sporting events - so Q3 2021 was supposed to be up against very tough comps. The enormous increase in avg. revenue per day seen at the beginning of July was positively affected by the Euro Cup 2020 soccer tournament held through July 11. The key point is that the valuation and profit growth of Kindred are going to look even better than I have described.\n\n**Long-Term Catalysts \u2013 The Balance Sheet, Shareholder Return Yield, and M&amp;A:** \n\n\u2022\tKindred has a 2.5% dividend yield (safe and growing) and a 50 million GBP share repurchase authorization. They have repurchased 16 million GBP shares year-to-date at an average cost of 146.00 SEK. They have an additional 34 million worth of shares to repurchase this year.\n\n\u2022\tAssuming about 300 million GPB of free cash flow in 2021 (using their historical average free cash flow % of EBITDA) and Kindred\u2019s goal of \u201creturning 75% of free cash flow to shareholders every year,\u201d this equates to a total return yield (dividend + buybacks) of 7.4%.  Investors will love to own an appreciating asset with a yield that dwarfs the 10-year US treasury yield of 1.3% and even lower government bond yields in Europe. \n\n\u2022\tKindred\u2019s fortress-like balance sheet and free cash flow retention makes M&amp;A a continual opportunity for them.  They have identified several candidates in the US and believe that most new entrants will fail because they are too late to the party and don\u2019t have the financial resources to build out the infrastructure, brand awareness, regulatory expertise, and data-driven marketing advantage that Kindred possesses.\n\n\u2022\tKindred is itself a very attractive take-over target for brick-and-mortar casino operators facing declining revenue growth, regulatory crackdown in Macau, trends in \u201cstay-at-home consumption,\u201d and an extremely late start at getting into the online gambling sector. Potential buyers include Wynn Resorts, Caesars, Las Vegas Sands, and Penn National Gaming.  A potential take-out of Kindred at a 15x forward EBITDA multiple (DraftKings just offered 19x forward EBITDA for Entain), would value KIND stock at 410 SEK.\n\n&amp;#x200B;\n\n**Major Shareholders:** \n\nCapital Group \u2013                                    7.03% \n\nAvanza Pension \u2013                                 3.35% \n\nVeralda Investment \u2013                          3.00% \n\nUnionen  -                                              2.95% \n\nSwedish National Pension Fund \u2013     2.48% \n\nTIN Funds -                                            1.98% \n\nBlackRock -                                            1.91% \n\nKindred Group Plc -                                   1.82% \n\nLife Insurance Skandia -                            1.71% \n\nBassac GP -                                                  1.71% \n\nNordea Funds -                                           1.66% \n\nJanus Henderson Investors -                    1.44% \n\nGoldman Sachs Asset Management-      1.39% \n\nDimensional Fund Advisors -                    1.32% \n\nEpoch -                                                         1.31% \n\nTotal Top 15 -                                                            35.07% \n\n**Officers and Directors:**\n\n Anders Strom, Chairman and Founder -  3.00% \n\nHenrik Tjarnstrom, CEO -                            0.80% \n\nRest of Supervisory Board -                        0.20% \n\nRest of Executive Team -                             0.30% \n\nO&amp;D Total                                                      4.30%\n\n&amp;#x200B;\n\n**Conclusion:** \n\nI believe that this is a fantastic opportunity.  Kindred operates in one of the fastest growing industries globally (recession and inflation proof) and is the best run company in online sports betting and gambling.  It will generate profit and cash flow margins far in excess of its peers, and will use this cash flow to further strengthen its competitive moat and to return money to shareholders in a way that will give us very high fixed returns on top of massive capital appreciation potential.", "upvote_ratio": 0.71, "id": "t3_u1lk8p", "created_utc": 1649721956.0}
{"sub": "investing", "title": "Should I convert my stock shares to DRS (Direct Registration System)?", "selftext": "I'm kinda scared of a market collapse soon and I'm wondering what will happen to my stock shares if my broker were to collapse. Let's say for example Schwabb was to collapse and fail, my street shares would be toast, right? But if I have DRS shares, I'd still be safe, right?", "upvote_ratio": 0.28, "id": "t3_u1if68", "created_utc": 1649713214.0}
{"sub": "investing", "title": "Realistically, where do you see the stock market in the next 10 years?", "selftext": "Hi there,\n\nWe can all feel the negative atmosphere in the stock market, since the beginning of the year, we've been hearing nothing but bad news.\n\nNow, let's try to look beyond the near term challenges.\n\nWhat are your realistic expectations for the stock market in the next 10 years?\n\nWill it be positive return? Negative return? Neutral?\n\nWhich sectors will outperform and which will underperform?\n\nMy opinion:\nI can't think of a scenario where tech will NOT outperform the market. Even if interest rates hit 10% and a world war breaks out, I think tech will still be standing tall and strong.\n\n(Disclaimer: I'm not a financial advisor nor a professional at all.)", "upvote_ratio": 0.73, "id": "t3_u1hvc4", "created_utc": 1649711789.0}
{"sub": "investing", "title": "Question on how bonds work", "selftext": " I have been trying to tell my parents that letting their money sit in a savings account is one of the worst things they could be doing. For my own personal portfolio, I have nearly all of money in full market etfs like VOO. Of course my parents are much closer to retirement age, and extremely risk-averse. I told them that government bonds would be a good option since they are considered risk-free and wouldn't lose value in a downturn like SPY or VOO. I am also looking at actual government bonds and not a bond etf that could lose value. My question is, how does someone even go about purchasing bonds, and is there a recommended length that people normally purchase (1-year, 2-year, 5 year)? I only really know about stocks and etfs, so the bond market is entirely new to me.", "upvote_ratio": 0.81, "id": "t3_u1gfir", "created_utc": 1649708179.0}
{"sub": "investing", "title": "Curiousity never killed the cat", "selftext": "Have some of the members in this community considered investing on the JSE (South African stocks)?\n\nIf so, which stocks peaked your interest? And what do you base your analysis on?\n\nI'm looking to invest in some stocks that will give me a \"safe\" return over a 1 year period as I'm saving up to buy property. I don't want to put it in the bank as I believe stocks/ETFs will render a larger return on investment.\n\nI was thinking to buy some higher risk stocks like Steinhoff and Renergen and the majority to be safe holdings like a S&amp;P500 tracking ETF and maybe Nasdaq?\n\nYour intellect is much appreciated", "upvote_ratio": 0.66, "id": "t3_u1gcx3", "created_utc": 1649707999.0}
{"sub": "investing", "title": "AT&amp;T-WBD Stock Spinoff Question", "selftext": "For every one share of AT&amp;T, investors were supposed to get .24 shares of WBD. I have 8 shares of AT&amp;T in a portfolio, and only got 1 share of WBD. I get why they wouldn\u2019t distribute fractional shares, but will I get compensated for this? That\u2019s around $20  in value lost.\n\nEdit:\nYou will receive cash in lieu of fractional shares. \n\nThank you, everyone.", "upvote_ratio": 0.78, "id": "t3_u1fv8f", "created_utc": 1649706784.0}
{"sub": "investing", "title": "Calls and Puts. I understand what happens when one exercises a call option", "selftext": "But what happens when one exercises a put option? I started trading options in very very small amounts just to see how they work. I have not exercised an actual call option but I know that you get the 100 shares for the price set in the option. Puts are a totally different animal for me. It took me much longer to understand just how they work and now I get it but I still don\u2019t know what happens if I chose to exercise one. I know the general rule is to sell the options but I am just curious and fairly new to the whole options game.", "upvote_ratio": 0.4, "id": "t3_u1e1op", "created_utc": 1649700869.0}
{"sub": "investing", "title": "Investing in REITs and your thoughts", "selftext": "Only learning about REITs, and just wanted some ideas from you level headed bunch...\n\nDo many of you invest in REITs? On the whole what are the thoughts on these... A good way to diversify and have a little exposure to real estate? As good as owning the real estate?\n\nAny good REITs I can look into? Bonus if it's UK\n\nThanks", "upvote_ratio": 0.74, "id": "t3_u1dqrc", "created_utc": 1649700065.0}
{"sub": "investing", "title": "I'm looking for the name of the extremely successful financial firm that averaged mid/high double-digit yearly returns for several years then became 100% private?", "selftext": "I watched a fascinating hour-long documentary on youtube 1-2 years ago on a channel similar to \"Cold Fusion\" that does documentaries on financial companies and I'm trying to find the documentary I watched 1-2 years ago so I can view it again. \n\nHere are details I remember:\n\n* This company started in the 1980's when 2 guys (possibly 1 was a mathematician?) realized that their price prediction models ***based on technical analysis*** could accurately predict future price movements \n* The company was modestly successful at the beginning but they reinvested their profits into hiring geniuses at math &amp; computer science who all signed NDA's and worked for the financial company to make the models even better which led to huge profits \n* The yearly rates of return were consistently in the 50% - 200% range for at least a dozen years \n* The company was so successful that they kicked out all their investors and fired all their clients and became only available for the investing benefit of its own employees/owners\n\n\n\nThe financial firm may have possibly been Blackstone Inc. or BlackRock Inc. but the wikipedia pages don't seem to be 100% in agreement with the things about the company as told by the youtube video. **Can anyone tell me the name of the financial firm so I have a better chance of finding the documentary I watched?**\n\n___\n\n**TL;DR:** I'm looking for the name of the extremely successful financial firm that averaged mid/high double-digit yearly returns for several years then became 100% private?\n\n___\n\n**Edit: u/Savik519 nailed it! It's \"Renaissance Technologies\" now I just need to find the video!** \ud83d\ude01", "upvote_ratio": 0.76, "id": "t3_u1db9f", "created_utc": 1649698934.0}
{"sub": "investing", "title": "ATT options post spinoff include WBD stock?", "selftext": "I read a lot about the ATT, Warner spin off, and now that it has happened, I am still confused about the call options.\n\nI bought 100 shares of the NEW ATT post spin off. I wanted to sell a covered call, but all the prices are of T1 options, so a 20$ covered call is like 400$ premium. But from what I read , this includes  a 24% of WB. So does that mean if I sell covered calls of the NEW post spinoff Att stock, and I get assigned, the buyer will also get the value of 24% of WB stock from me even though I dont own that stock? It will be like partial naked selling of calls?? Am I understanding this right??", "upvote_ratio": 0.5, "id": "t3_u1d9ue", "created_utc": 1649698834.0}
{"sub": "investing", "title": "Can we actually use this way of thinking while analyzing companies? - From one up on wall street", "selftext": " [https://imgur.com/a/NOlsUIH](https://imgur.com/a/NOlsUIH)  \n\nIt doesn't really make sense to find out net cash per share, and if it was\n\ntrading exactly at that value, thinking ur getting the equity for free ain't correct right.. Or am I missing something? There is no guarantee that the stock would trade at or higher than the net cash per share if it was found out to be below net cash per share right? Why did peter lynch claim this", "upvote_ratio": 0.7, "id": "t3_u1cumz", "created_utc": 1649697709.0}
{"sub": "investing", "title": "State Sponsored 529 vs. Third party (Vanguard)", "selftext": "Hello, just had a newborn son and we are setting up a 529 early for education. We'll be contributing smaller monthly payments but honestly the majority of the funding will come from grandparents who want to keep it pretty well funded (we are very fortunate that they are so generous).\n\n&amp;#x200B;\n\nAnyways I live in Florida and was reading all about the 529 here (which we prefer over prepaid tuition), and as I was about to sign up I saw that Vanguard, and other brokerages, also offer their own 529s. \n\n&amp;#x200B;\n\nDoes anyone have experience with a private one over the state sponsored one? Is there a reason to go with one over the other? It looks to me like Vanguard has way more investment options so I was leaning that way but if there's a reason to stay with Florida I will.\n\n&amp;#x200B;\n\nThanks in advance", "upvote_ratio": 0.82, "id": "t3_u1crzx", "created_utc": 1649697515.0}
{"sub": "investing", "title": "Doing a virtual stock market game in my highschool economics class. Can I get some help?", "selftext": "I think the best choice would be to choose stocks that fluctuate a lot so I can make money fast. No IPOs are allowed in the game. We start with $100k and I think the game lasts for a month give or take. Whoever wins gets their name printed on a big trophy so itd be pretty cool to win. Any helpful suggestions on what to do to win would be much appreciated. Thanks\n\nEdit: we\u2019re not allowed to short stocks", "upvote_ratio": 0.67, "id": "t3_u1c64b", "created_utc": 1649695933.0}
{"sub": "investing", "title": "Is 50/50 Tesla Apple a bad investment?", "selftext": "I already have $250 per paycheck in a Roth IRA with a safer investment but i want to put around $100 a month into something, I was looking at the warren Buffett 90/10 option but I was on a portfolio visualized and 50% apple 50% Tesla had around a 50/60% return, compared to the 90/10 15% return. But is this a bad investment? It feels like it is, but I just don\u2019t know. The high return looks good but I don\u2019t want to lose money", "upvote_ratio": 0.43, "id": "t3_u1bo92", "created_utc": 1649694593.0}
{"sub": "investing", "title": "UK Investors - How to get exposure to Cathie Wood's ARKK ETF", "selftext": "Hi investorers,\n\nAnyone here from the UK managed to invest in ARKK?\n\nI feel like it's a good time to start dollar cost averaging the fund but I can't buy in the UK because \"ARK have not provided key information documents (KIDs)\".\n\nAnyone got any workarounds?\n\nThanks!", "upvote_ratio": 0.31, "id": "t3_u19dmr", "created_utc": 1649688248.0}
{"sub": "investing", "title": "Atlassian future after outage", "selftext": "I work in tech so I'm interested in what people think this [week long outage](https://www.theregister.com/2022/04/11/atlassian_still_down/) for Atlassian products like JIRA and Confluence will have on the company going forward. It appears they have only restored service to 35% of their customers so this is a huge concern for customers going forward.", "upvote_ratio": 0.54, "id": "t3_u18olx", "created_utc": 1649686292.0}
{"sub": "investing", "title": "Explain how Compound Interest could work with Vanguard ETF", "selftext": "Hey guys, so I'm all new to the investing train ( and quite late at 32, but better late than never right? )\n\nI've been following many videos and I've come to the conclusion that investing in VFV ( Vanguard S&amp;P 500 ETF ) is what I want to do ( I already own Apple stocks and buy some every year being an employee there ).\n\nI've heard about the term \" Compounding Interest \" and how it's the key to double/triple your investments over 30 years, but I'm not sure that I fully grasp the concept, and it's frustrating to me.\n\nI live in Canada, so according I would buy 10,000$ worth of VFV right now, that would give me 100 shares, now how would I go about getting compounding interest with that through my TFSA? I don't understand how I can get returns if I'm buying shares, their value will increase, I'm not getting a % of interest return on that amount every year to reinvest in it as far as I know?\n\nPlease enlighten me, I'm lost.", "upvote_ratio": 0.77, "id": "t3_u18cge", "created_utc": 1649685380.0}
{"sub": "investing", "title": "Investing in company\u2019s stock", "selftext": " I currently work for chase bank. And due to naivety when I first started I placed 90% of my portfolio into jpm stock. I currently have about 50k in there contributing 20% every pay cycle and the dividends every quarter are nice. Is this a feasible strategy to continue or should I diversify and if so into what? \n\nThank you", "upvote_ratio": 0.5, "id": "t3_u16ug3", "created_utc": 1649680888.0}
{"sub": "investing", "title": "Selling a property for net of ~$200k, 33yo with a family living abroad, 1031exchange?", "selftext": "In next 2mo I\u2019ll be selling an investment property in US with net of $200k. (Total sale price will be ~$700k, I\u2019m in a fairly high income bracket.)\n\nMain goal is to set up for corporate retirement in 7 years.\n\nWeighing the option to do a 1031 exchange and could use some unaffiliated advice\u2026\n\nPros: wealth accelerator, modest cash, tax deferral. \n\nCons: sellers market, it\u2019s a lot of work to set up, heavy debt.\n\nBackground: \n- I\u2019m a 33 American living abroad. \n- Have other investment properties.\n- Appreciate hard work but have limitations being on the other side of the world. \n- Have dual high income household. \n- Have never done 1031 exchange before", "upvote_ratio": 0.58, "id": "t3_u14r8j", "created_utc": 1649673617.0}
{"sub": "investing", "title": "Do you guys use any forums other than reddit?", "selftext": "I've come across a few non-reddit pages focused on investing, like CafePharma and BogleHeads... Do you guys know / use any others?\n\nCould be generalist pages / forums or more specific ones focused on an area you're interested in. For me that might be green energy.\n\nWould love to hear what else is out there!", "upvote_ratio": 0.73, "id": "t3_u14hik", "created_utc": 1649672512.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - April 11, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.79, "id": "t3_u13az9", "created_utc": 1649667669.0}
{"sub": "investing", "title": "A Platform Where I Can Search, Sort and Analyze Companies / Financials", "selftext": "Hello lads,\n\nI like making my own analysis. So, I need a platform where I can dig into detailed financials of companies. I would like search and sort companies according to spesific financial information. I would like to see details of financials such as investments, cash flows, balance sheets. I'm not speaking of basic number displaying sites. Information must be accurate and detailed (at least as much as possible. For example tradingview has an option of listing company financials. But! When I compare those financials with actual 10k or annual reports, I find major differences). \n\nThank you in advance.", "upvote_ratio": 0.6, "id": "t3_u12q7p", "created_utc": 1649665142.0}
{"sub": "investing", "title": "Is TEC.to still a good buy?", "selftext": "During the pandemic this ETF did really well, over 47% return in 2020 and over 25% return in 2021 but definitely has slowed down in 2022. What are your thoughts about this ETF? Buy, don't buy?\n\nFocus completely on tech companies, with the largest holdings being in Apple, Microsoft, Amazon, Tesla and Google/Alphabet.", "upvote_ratio": 0.57, "id": "t3_u10bhy", "created_utc": 1649654910.0}
{"sub": "investing", "title": "Is there a Broker that allows you to do recurring deposits and distribute the money in your portfolio based on a percentage?", "selftext": "Basically I want to  build my own ETF inside a Broker platform where I can setup allocation percentage and have monthly deposits.\n\nI know I can do the math in a spreadsheet and setup recurring deposits individually for the stock but I am too lazy to do all the work.\n\nMay be there is a better way to automate distribution, would love to learn more.", "upvote_ratio": 0.76, "id": "t3_u0z1hy", "created_utc": 1649650081.0}
{"sub": "investing", "title": "Elon Musk decides not to join Twitter board, says CEO Parag Agrawal", "selftext": "\u201cElon has decided not to join our board. I sent a brief note to the company, sharing with you all here.\u201d - Parag Agrawal\n\nhttps://twitter.com/paraga/status/1513354622466867201?s=20&amp;t=ljgxw1kVvOZi3M-FDeQi3g\n\nhttps://www.cnbc.com/2022/04/11/elon-musk-decides-not-to-join-twitter-board-says-ceo-parag-agrawal.html", "upvote_ratio": 0.95, "id": "t3_u0yfos", "created_utc": 1649647961.0}
{"sub": "investing", "title": "I-Bonds - A great alternative to cash, but not a replacement for equities", "selftext": "I've seen this subreddit give lots of praise to Series I Savinga Bonds, which, for those who don't know, are bonds offered by the U.S gov which are supposed to track inflation and are currently returning a whopping 7.1% annually. I think this praise is rightfully earned, because this level of risk free nominal return is practically unheard of with traditional fixed income investing.\n\nSo as the title states, these bonds are vastly superior to just holding cash in a HYSA, and this is where their praise should stop. But instead, I've seen many contributors to this subreddit replacing their equity exposure with I-Bond holdings stating things like \"7% is practically what you get from stocks, and it's risk free!\". This is flawed thinking, because most of these investors are forgetting that **i-bonds will always target a 0% real return on your investment, while equities investors can expect to earn a non-zero positive real return** (over a long enough time horizon)\n\nIt is true that equity exposure comes with uncertainty, but that is exactly why they are also associated with higher expected returns. If you are looking to build your wealth and increase your buying power, I-bonds are guaranteed to NOT accomplish this.\n\nA personal note: I have transitioned a portion of my cash holdings to I-bonds, and have also continued my usual investment schedule through these high inflationary periods.", "upvote_ratio": 0.79, "id": "t3_u0xj7d", "created_utc": 1649644938.0}
{"sub": "investing", "title": "I-bonds are giving 7% return yet BND is still trending downwards?", "selftext": "I am not gonna pretend to understand the details of how a fund like BND works, but with I-bonds returning 7% I am wondering why BND does not give similar yields? It just keeps dropping. ELI5?\n\nFor context;\n\n[BND](https://investor.vanguard.com/etf/profile/BND)\n\n- 40% of funds are allocated to 1-5 year bonds; 0.3% of funds are &lt;1 year\n\nhttps://www.treasurydirect.gov/indiv/research/indepth/ibonds/res_ibonds_iratesandterms.htm\n\n&gt; The initial interest rate on new Series I savings bonds is 7.12 percent. You can buy I bonds at that rate through April 2022. \n\nIf these bonds are giving such high returns, why are bond ETF's like BND not giving such returns? Is there some kind of lag before they reach those 7% levels? Are they controlled by some other factor?", "upvote_ratio": 0.54, "id": "t3_u0xagu", "created_utc": 1649644122.0}
{"sub": "investing", "title": "Experienced traders I am seeking some enlightenment for future trades. What happened on Friday 4/8 with NDRA.", "selftext": "  [$NDRA](https://stocktwits.com/symbol/NDRA) Soo can someone explain if there was any other info posted on 4/7/22 besides     \nHC Wainwright &amp; Co. changing the PT from $6 to $5 and still holding as a buy?? The last ten minutes of trading on 4/7 the volume was only 37,159. But the opening 1 minute candle on 4/8 had a 13.25 million green candle?? Off of a lower Price Target? Then a sell off till 3:54pm when it hit its new 52 week low of .2923??? The Daily volume was 74 million for 4/8 compared to the average volume of 390,821 shares and a float size of 41,140,000 shares. any thoughts folks?", "upvote_ratio": 0.33, "id": "t3_u0wkii", "created_utc": 1649641818.0}
{"sub": "investing", "title": "Which company is responsible for stocks quotes?", "selftext": "Hi there,\n\nThere's a question that has been on my mind for a long time.\n\nWhich firm or institution is responsible for live sharing all these stock quotes that we see on websites like yahoo and such?\n\nIs it the stock exchange itself? And if so, who acts as an intermediate between the exchange and the websites?\n\nBecause if it's a publicly listed company, it could be a monopoly and I'd love to give them my money :)", "upvote_ratio": 0.58, "id": "t3_u0ux6q", "created_utc": 1649636390.0}
{"sub": "investing", "title": "Short term investments for the next 2.5 years?", "selftext": "Going to be saving heavily (goal is $125k) for the next 2.5 years for a home purchase. I\u2019m trying to position my portfolio understanding that we\u2019re in a high inflation, interest rates rising environment with a potential recession occurring during this time period. \n\nMy number one goal is preservation and some very conservative growth. Right now, I\u2019m in all equities and cash - equities being etfs in health care, financials, utilities, consumer staples, and materials/commodities.\n\nIf I\u2019m investing ~ $2k/month + bonuses, and you were in my position, would you do anything different? What would you do for the next 30ish months?", "upvote_ratio": 0.81, "id": "t3_u0tftm", "created_utc": 1649631774.0}
{"sub": "investing", "title": "chase vs premium bonds vs something else (uk)", "selftext": "hello,\n\n&amp;#x200B;\n\ngot some lump cash that i want to invest to instead of sitting in Cash ISA.\n\n&amp;#x200B;\n\nwhat would you recommend? i heard people keep saying that Chase with 1.5% is great, others rely on safe premium bonds.\n\n&amp;#x200B;\n\nwhat is your opinion? thank you", "upvote_ratio": 0.5, "id": "t3_u0sqa6", "created_utc": 1649629623.0}
{"sub": "investing", "title": "Anyone like Cincinnati Financial $CINF?", "selftext": "Solid Dividend King. Underwriting looks good, been around for a long time.\n\nCurrently trading at ATHs after a +10% run over the last month, but still looks reasonably priced to me. Thought about just going ahead and jumping into a small position on Monday, like a $1000. If it pulls back or trades sideways for awhile, just add a little more down the line.", "upvote_ratio": 0.5, "id": "t3_u0ro48", "created_utc": 1649626601.0}
{"sub": "investing", "title": "roth ira / brokerage asset allocations", "selftext": "im going for the 3 fund portfolio for my brokerage account but im not sure if i should copy that allocation strategy into my roth ira account too?\n\nim going for 80 / 10 / 10 - vanugard indexes, 10 international, 10 bond for my brokerage.\n\nive seen ppl do completely different allocations on their roth to max out gains, not sure if thats the correct way to go.  And what about target date fund 2065? Should i go for it or ignore it? im 27.", "upvote_ratio": 0.66, "id": "t3_u0qm4b", "created_utc": 1649623545.0}
{"sub": "investing", "title": "Blackstones ($BX) yearly outlook in 2022?", "selftext": "They are my largest position, and going into earnings on 21st I\u2019m concerned\n\nThe last rate tightening cycle (2016-2018) saw BX lose ~40% of share value, and that was in a relatively dovish tightening period. Inflation is real now, and a long-term hawkish Fed is an entirely new ballgame for PE.\n\nNot even taking into account the issue with their will Smith producing deal which is small, but a clear loss for their business. \n\nWhat do people think? I am strongly considering selling half my position before earnings", "upvote_ratio": 0.68, "id": "t3_u0qbrh", "created_utc": 1649622712.0}
{"sub": "investing", "title": "Would you invest this money or put it in savings?", "selftext": " I\u2019m going into my senior year of college for engineering and have about $25,000 saved and no debt. I\u2019ll be working a paid internship this summer and my expenses during my next (and final) school year will total about $10,000 after scholarships and grants. I\u2019m considering putting a few thousand into a Roth IRA before April 18th. Is this a good idea or should I just continue to save the money for now?", "upvote_ratio": 0.69, "id": "t3_u0q6yr", "created_utc": 1649622333.0}
{"sub": "investing", "title": "Is this the go-to growth portfolio?", "selftext": "Starting young, most people can focus on growth investing and end up on top.  And around the time of retirement can transition into an income portfolio to live out their golden years.\n\nI have found that QQQ, VOO, and SCHD have the best growth and security for younger investors.  More QQQ will give you a higher return overall but possibly more volatile.  55/30/15 seems to be the best option, from what I can tell in the portfolio visualizer.\n\n&amp;#x200B;\n\nDoes anyone else have a better go-to growth portfolio for young investors to feed and forget and then rebalance once per year?", "upvote_ratio": 0.36, "id": "t3_u0mdzg", "created_utc": 1649611637.0}
{"sub": "investing", "title": "Sell ARKK at a loss for VTI", "selftext": "I had invested about 5k into ARK funds. I lost about 50 percent. My initial idea was that this was a good long term play however given that they are consistently making bad trades and don\u2019t generally have a long term outlook for their stocks, plus the management fee I think it may be wise to take the loss and put the money into something more stable. Anyone have any faith in the long term of ark funds? \n\nWas thinking of buying QQQ OR VTI. Would appreciate any thoughts", "upvote_ratio": 0.82, "id": "t3_u0m4rk", "created_utc": 1649610889.0}
{"sub": "investing", "title": "Employer insurance (hdhp) automatically gave me an HSA, but I'm still on parents plan which isn't hdhp. Can I still contribute?", "selftext": "As the title states, i signed up for insurance via my employer, but I don't have to move to the work location anymore (im remote). \n\nSaid plan offers an hsa which my employer automatically signed me up for but I'm still on a low deductible plan since I'll be at home. (I'm leaving the 2nd plan for now in the event that I have to move in the near future - just as a safety net).\n\nSo can I contribute to the HSA, while also being on LDHP plan? What will happen? Extra taxes/penalties? They pretty much gave it to me after I said I wouldn't be eligible.", "upvote_ratio": 0.6, "id": "t3_u0ll06", "created_utc": 1649609358.0}
{"sub": "investing", "title": "If Russia defaults on its debt, what goes up?", "selftext": "Obviously a lot of stocks go down, but there\u2019s always big institutional investors that move their money from one place to another when something like this happens. There\u2019s also some type of business that would benefit from this. I am not smart enough to figure this out, but I know some of you Redditors are. \nAny thoughts?", "upvote_ratio": 0.85, "id": "t3_u0iqre", "created_utc": 1649600975.0}
{"sub": "investing", "title": "Out of everyone in r/investing, is there an asset that you think nobody else holds except for you?", "selftext": "Maybee I have just been lurking here too long, but everting is all kind of starting to look the same. Tesla + uranium + weed stocks + crypto. Over and over and over again.\n\nThe point of me plugging into a community is to come across new ideas I had not considered before. So lets have em. What is the niche ideas that only you do?", "upvote_ratio": 0.9, "id": "t3_u0gnzj", "created_utc": 1649594015.0}
{"sub": "investing", "title": "Are you holding on to your BOND FUNDS? Or bailing out before they crash further?", "selftext": "I am maxed out in IBONDS for the year and don't want to be 100% stocks.  So up to recently, I had about 40% of my assets in Bond Funds.  (Like BND or AGG). \n\nThe last six months have been terrible for these bond funds.  Many total bond funds are down nearly 10%.  This would not be a big issue for a stock fund.  But it could take years for a bond fund to recover from the largest quarterly drop since the early 1970s.\n\nWhat are you doing with your existing bond funds:\n\n1) Holding out and not selling and hope that in 5-10 years they will recover\n\n2) Cashing out and putting your money in CDs or Individual Bonds\n\n3) Putting all your money in stocks", "upvote_ratio": 0.7, "id": "t3_u0eu0e", "created_utc": 1649586219.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - April 10, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.82, "id": "t3_u0dssj", "created_utc": 1649581270.0}
{"sub": "investing", "title": "Very high borrow rates for multifactor ETFs - why?", "selftext": "Borrow rates for most large, liquid, diversified ETFs are quite low.\n\nAs of the writing of this post and per [iborrowdesk.com](https://iborrowdesk.com),\n\n[SPY](https://iborrowdesk.com/report/SPY) (S&amp;P 500), [EFA](https://iborrowdesk.com/report/EFA) (MSCI EAFE), [EEM](https://iborrowdesk.com/report/EEM) (MSCI Emerging Markets), [QQQ](https://iborrowdesk.com/report/QQQ) (Nasdaq 100) borrow rates are all below 1%.\n\nETFs tracking certain asset classes that are harder to short for retail investors/investors who can't trade OTC have moderately higher borrow fees, like [JNK](https://iborrowdesk.com/report/JNK) (HY bonds) at 3%.\n\nHowever, many factor ETFs, especially multifactor funds, have extraordinarily high borrow rates, especially vs their relatively modest tracking errors. Examples include [LRGF](https://iborrowdesk.com/report/LRGF) (MSCI USA Multifactor) at 21%, [VFMF](https://iborrowdesk.com/report/VFMF) (Vanguard US Multifactor fund) at 31% and [SMLF](https://iborrowdesk.com/report/SMLF) (MSCI USA Small Cap Multifactor) at a whopping 122%. Other factor funds, particularly value, also have relatively high borrow rates (DFAT, AVUV, IVAL for example). Many active funds are expensive to short too ie [FMAG](https://iborrowdesk.com/report/FMAG) (Fidelity Magellan) at 11%.\n\nWhat gives? This doesn't seem to be a particularly new phenomenon, and even though borrow fees aren't fully passed on to longs + not all units can be lent out, wouldn't it be attractive to hold many of these ETFs simply to receive (a portion of) the borrow fees?\n\nExact same story using borrow rates from fintel.io , which doesn't seem to use IBKR, so this strangeness seems pretty robust.", "upvote_ratio": 0.67, "id": "t3_u0bm4y", "created_utc": 1649571364.0}
{"sub": "investing", "title": "Went all in on UNH. Good? Bad?", "selftext": "I put all my money in UNH stock last week and used margin. I know they say not to put all your eggs in one basket. However, I think UNH is very solid and will continue the upward trend especially in a high inflation environment. I plan on holding 3 to 5 years. It appears to average around 23% gains annually. Is this a bad idea?", "upvote_ratio": 0.48, "id": "t3_u080co", "created_utc": 1649557250.0}
{"sub": "investing", "title": "Discussion and Thoughts on Upstart Holdings (UPST)", "selftext": "Hello all, I would like to hear all of your opinions on UPST in the comments, my personal opinion on the stock is that right now is a very good time to buy because it is entering historical support levels and this company has a lot of room to grow in the futures. They reported great earnings and predicting a lot of growth a few months back which sent the stock running up to 160 but failed to break that level and since then has come all the way back down. Please let me know your thoughts on this company and it\u2019s stock.", "upvote_ratio": 0.53, "id": "t3_u05hf5", "created_utc": 1649548377.0}
{"sub": "investing", "title": "Investing in shorts on Canadian housing markets", "selftext": "Like it says in the title I\u2019m trying to add exposure within my portfolio to the Canadian housing market (shorting it rather than investing in it). any funds I can invest in that are doing this without directly shorting myself? I really don\u2019t want the exposure of direct shorting and also I don\u2019t want to get another broker as my current one doesn\u2019t allow retail shorting of stocks. I see the market as way overpriced and feel it\u2019s almost on the verge of falling anyone else? Preferably either US or European markets please.", "upvote_ratio": 0.58, "id": "t3_u03lvx", "created_utc": 1649542304.0}
{"sub": "investing", "title": "Opening 401k, any suggestions?", "selftext": "Hi everyone (some background) I\u2019m 23 years old with some capital I\u2019ve saved up my entire life around 30k, and I\u2019m looking for any help/suggestions for the best dividends stocks, ETFs for multiplying money over time (trying to retire as early as possible). Of course scaling in properly and buying the dip is optimal, but I\u2019m looking for the best dividends or stocks possible; thinking Captain America like MSFT GOOG AMZN TSLA or maybe more riskier equities, but regardless I\u2019m looking for dividend and best stocks for my future. I appreciate any response and feedback!", "upvote_ratio": 0.45, "id": "t3_u02wc0", "created_utc": 1649540152.0}
{"sub": "investing", "title": "Merrill Lynch Investment Management", "selftext": "I have like $30k invested in stocks and it's not doing great.\n\nI have another $100k total in savings that I'm not even collecting interest in.\n\nA ML person called me and is interested in getting me to invest with them in their managed accounts, which costs a .58-1% fee depending on a few things.\n\nIs this a good idea?\n\nI'm thinking I should save 6months of savings from the $100k and give the investor the rest?\n\nHe said I could make 10-18% gains a year depending.", "upvote_ratio": 0.67, "id": "t3_u00w6n", "created_utc": 1649534054.0}
{"sub": "investing", "title": "Any long-term investing advice?", "selftext": "Hello I am new to this sub-Reddit investing and I was wondering if anyone had any good advice or good recommendations for a long-term stock or even crypto investments, for a safe percentage annually. Any recommendations would be helpful although I already know of Berkshire Hathaway and S&amp;P 500 of course.... \nthank you for your time, please no trolls\n\nEdit: THANK You for helping! I will review the percentages in these comments, the non-correlated Index funds were helpful by a handful. However, the HIGHLY correlated Index funds are alright as-well.", "upvote_ratio": 0.52, "id": "t3_u00f31", "created_utc": 1649532587.0}
{"sub": "investing", "title": "Exploring the possibility of Long Term EV plays", "selftext": "In the 2030s many states are passing laws the the only new cars allowed to be sold are EV or hybrids, if this is the case there is a huge potential for businesses that can capitalize on this. I know there are a lot of hyped up companies regarding this like Rivian and Lucid motors but there must be other lesser known companies that make the parts needed for EV cars or other future manufactures of EV cars. Please discuss in the comments any of these possible companies that you know of, I think long term there is big potential opportunity here.", "upvote_ratio": 0.84, "id": "t3_tzymrm", "created_utc": 1649527222.0}
{"sub": "investing", "title": "What prevents me from front-loading shares into a mutual fund that's about to have a capital gains disbursement?", "selftext": "So I'm invested in FPURX, which is a fairly conservative mutual fund, but twice a year they have capital gains distributions, based on how many shares you hold. What would prevent me from selling a bunch of other stocks, and buying a ton of FPURX right before the distribution, collecting a huge capital gains distribution, then selling those shares and rebuying my original positions?\n\nEDIT: Ugh sorry I forgot Reddit has a lot of pedantic members who take things too literally. When I said, \"What prevents me\", what I meant to say is, \"Is it a bad idea to\".", "upvote_ratio": 0.73, "id": "t3_tzygt1", "created_utc": 1649526748.0}
{"sub": "investing", "title": "REITs vs Bonds in Portfolio rebalancing", "selftext": "As a 21 year old I have my IRA invested into a exclusively a huge mix of equity in hopes of getting the annual average return of 10-11 percent from the S&amp;P 500 over the next 30 years. \n\nHowever when I rebalance my portfolio as I edge up on retirement in my 50\u2019s and start to invest in more fixed income securities, I\u2019m wondering which is better. \n\nYes, REIT\u2019s have risk of being illiquid and losing value depending on if it\u2019s publicly traded or not. However bonds also have risk with interest rate swings and are likely even more illiquid. \n\nAlso bonds return 2-3% while REIT\u2019s return 5-6%. And yes there are REIT\u2019s with considerably high fees but you must do your research first. Also you can diversify with REIT\u2019s just like stocks or bonds. \n\nI think they might be a more valuable fixed income investment for retirees needing a steady flow of cash from their investments over bonds", "upvote_ratio": 0.5, "id": "t3_tzvzfo", "created_utc": 1649519439.0}
{"sub": "investing", "title": "S&amp;P becomes first ratings agency to declare Russian foreign currency debt in default", "selftext": "After Russia attempted to make payments on dollar denominated debt in Rubles, S&amp;P Global changed Russia's foreign debt credit rating to the default classification. Russia tried to make the payment on April 4th, but it was blocked under US Sanctions. The Treasury Department had recently granted a waiver in March on such payments, but it's recent denial show an expanded effort to cut Russia off from global borrowing markets. \n\nRussia is in a classification of \"selective default\", in which it only failed to make payments on it's foreign currency debt. It still has a 30 day grace period from the missed payment on April 4th to make investors whole, but S&amp;P believes further sanctions are coming and Russia will not have the ability to honor it's obligations. It's local currency debt is still rated in the category just above \"default\". \n\nInterestingly, S&amp;P stated last month that it would halt ratings on Russian debt. They will no longer issue ratings on Russian debt after April 15th. They had to throw the default grenade into the mix just before they left the room.", "upvote_ratio": 0.97, "id": "t3_tzua4n", "created_utc": 1649514375.0}
{"sub": "investing", "title": "What are the risks of real estate in developing countries?", "selftext": "Probably everyone here is aware that house prices have gone insane in western-ish countries: Canada, USA, West Europe, New Zealand, Sydney and Melbourne, etc. You probably have to be a multi-millionaire to have a serious real estate investment in those countries.\n\nSo how about buying up real estate in currently poor (but quickly becoming rich) countries instead? The selected country should be peaceful, open to the free market, and is getting more loans or financial packages from richer countries. You can choose either houses in big cities (safer, more in-the-money), or in some suburban areas that can be bought for dirt cheap then hope it'll evolve into a city later.\n\nI don't understand why Blackrocks aren't doing this (or do they?). Buying real estate from poor countries getting financial packages from US/Japan/EU/etc practically means they get free money from the governments. I don't know much about finance so please explain how this plan might go wrong.\n\nEdit: turns out there's a lot more risks than just buying something local. Thanks guys!", "upvote_ratio": 0.69, "id": "t3_tztpx9", "created_utc": 1649512639.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - April 09, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.82, "id": "t3_tzpb1y", "created_utc": 1649494870.0}
{"sub": "investing", "title": "ASML: CEO and CTO will depart within 2 years while NVDA seeks to double authorized shares", "selftext": "I just stumble upon some information in regional newspapers that I didn't pick up in international news about the semiconductor sector: both the CEO and CTO of ASML will be leaving in 2 years (not long after the president commissioner left) and they expect a 2 year shortage of key machinery. Recruitement is looking outside the organisation for board members.\n\nI found this after reading that Nvidia seeks approval to double authorized shares to 8 billion. The longer I think about this the weirder it feels. They just did a split and they currently have 2.5B shares outstanding, so there should be more than enough room to do basically whatever, everything but a split.\n\nI also thought the announcement they'd be interested in working together with Intel IDM was pretty weird; on the day Intels CEO spoke to congress. Then we had the announcement Intel, Micron, and Analog Devices joined the Semiconductor Alliance, announcing an agreement to accelerate chip R&amp;D and prototyping to build a more robust domestic industry, on the same day congress was briefed on economic and national security vulnerabilities resulting from a lack of domestic chip production. Not to mention the UCIe 1.0 announcement earlier, in which AMD, Google, Meta, Microsoft, Samsung, and TSMC work together. Intel is playing a key role by \u201cdonating\u201d the initial specification. Nvidia recently announced they'd be adopting the standard as well, while at the same time announcing they'll open up NVLink-C2C to other suppliers.\n\nIs it just a coincidence all these events are happening at the same time?\n\n[https://www.ad.nl/veldhoven/onvervangbare-topman-asml-mogelijk-over-twee-jaar-weg\\~af61dcbe/](https://www.ad.nl/veldhoven/onvervangbare-topman-asml-mogelijk-over-twee-jaar-weg~af61dcbe/)\n\n[https://www.dutchnews.nl/news/2022/03/asml-says-chip-makers-face-a-two-year-shortage-of-key-machinery/](https://www.dutchnews.nl/news/2022/03/asml-says-chip-makers-face-a-two-year-shortage-of-key-machinery/)\n\n[https://www.eetimes.com/chiplets-get-a-formal-standard-with-ucie-1-0/](https://www.eetimes.com/chiplets-get-a-formal-standard-with-ucie-1-0/)\n\n[https://www.reuters.com/business/nvidia-ceo-says-interested-exploring-chip-manufacturing-with-intel-2022-03-23/](https://www.reuters.com/business/nvidia-ceo-says-interested-exploring-chip-manufacturing-with-intel-2022-03-23/)\n\n[https://www.reuters.com/technology/nvidia-seeks-approval-double-authorized-shares-8-bln-2022-04-08/](https://www.reuters.com/technology/nvidia-seeks-approval-double-authorized-shares-8-bln-2022-04-08/)", "upvote_ratio": 0.69, "id": "t3_tzjnyb", "created_utc": 1649471982.0}
{"sub": "investing", "title": "[Reuters] Nvidia seeks approval to double authorized shares to 8 billion", "selftext": "April 8 (Reuters) - Chipmaker Nvidia Corp (NVDA.O) said on Friday it would seek shareholders' approval to increase the number of authorized shares of common stock to 8 billion from 4 billion.\n\nAgain this is not a stock split this is a dilution of shares.  Any insight from my fellow redditors would be great.", "upvote_ratio": 0.95, "id": "t3_tzha5i", "created_utc": 1649463861.0}
{"sub": "investing", "title": "Best Subscription Services", "selftext": "Greetings everyone!\n\nI am looking for a subscription service that:\n\n1) Compiles at least 20 years of 10-k data\n2) I can export the data into excel\n\nBonus if it has an app. \n\nI don\u2019t really care about analysts price target, recommendations, new letters etc. I just want the financial data. I am also fine paying a monthly or annual fee. \n\nI normally pull it from the sec.gov website but my time is becoming more valuable. \n\nA few that I have looked into are: \nYahoo finance plus\nGuru focus\nMacrotrends\nEverything money\n\nJust wondering if any of you have tried any of these services and have any recommendations. \n\nThank you", "upvote_ratio": 0.71, "id": "t3_tzdnr3", "created_utc": 1649452836.0}
{"sub": "investing", "title": "Sticking with Fidelity even tho IKBR has way better margin rates", "selftext": "Ive been thinking about transferring to IKBR from Fidelity because of IKBR's low interest rates but having second thoughts. \n\nI have a roughly 100k account and borrow about 10%. Doing some quick math, I will only be saving a whopping $600 per year on margin interest by moving to IKBR. \n\nSome downsides of moving are that IKBR is trigger happy with margin call liquidations, all my other retirement accounts are already on Fidelity and Fidelity's customer service. \n\nAre there any other benefits of IKBR over Fidelity?", "upvote_ratio": 0.71, "id": "t3_tzcp6e", "created_utc": 1649450117.0}
{"sub": "investing", "title": "Will data center REITs ever be obselete?", "selftext": "Hi there,\n\nI have a question for the tech guys around here, or anyone who has a legit answer, basically.\n\nI have been looking at data center REITs like Equinix &amp; Digital Realty.\nThey seem to have an excellent track record of growth.\n\nHowever, the tech world is constantly changing.\n\nSo the question is, how important are data centers are , and how sustainable are they for the future?\n\nDo you guys think data center REITs will go out of fashion in the next 20 years? Are there any real alternatives? Or will there be a demand for them forever?\n\nAll feedback is welcome.", "upvote_ratio": 0.91, "id": "t3_tzcia9", "created_utc": 1649449573.0}
{"sub": "investing", "title": "Investing in App Harvest (APPH) - interested in your opinion of my analysis", "selftext": "Hello,\n\nI recently did some research on the Controlled Environment Agriculture (CEA) sector. What really interests me about it are the efficiency gains in terms of water/land usage and yield that can be achieved compared to \"traditional agriculture\". I also know that there are still some problems that need to be solved (overall energy usage, types of crop to be grown etc.), but all in all I like the approach and technologies behind it.\n\nAnyway, I don't want to discuss CEA but one player in this sector: [AppHarvest (APPPH)](https://www.appharvest.com/2.0/).\n\nThey are growing mostly tomatoes in one \"of world\u2019s largest indoor farms in Appalachia\" and are planning to expand their business to four indoor farms by H2 2022 growing also strawberries and leafy greens.\n\nLet's say that they had a very rough first year which is also reflected in the stock price and I'm actually not sure if they will be able to recover.\n\n# Net Sales VS Cost of Goods Sold (COGS)\n\nIf you look at their [10-K from 03/2022](https://investors.appharvest.com/node/7926/html#i27acac6cdf284555bb8fb5594f62b21c_91) they report the following for the whole year of 2021:\n\n\\- Net Sales:  **$9.050.000**\n\n\\- COGS: **$41.938.000**\n\nGranted, they encountered some problems in their first year of operations and were not able to grow at full capacity and highest quality as it is shown on page five of their [Q4/2021 earnings presentation](https://investors.appharvest.com/static-files/d77f986b-e813-4542-a84b-cd39411e1a0f):\n\n||Q1 2021|Q2 2021|Q3 2021|Q4 2021|\n|:-|:-|:-|:-|:-|\n|Yield (lbs)|3.8M|8.6M|1.5M|4.4M|\n|Net Price / Lb.|$0.61|$0.36|$0.37|$0.69|\n|Net Sales ($M)|$2.3M|$3.1M|$0.5M|$3.1M|\n\nBut let's assume for a second that 2021 went well and they were able to produce 8.6M lbs of tomatoes each quarter and sell them for $0.69 each. This would give us the following net sales:\n\n\\- 8.6M \\* 4 \\* $0.69 =  **$23.736.000**\n\nSo even under such an assumption their COGS is still almost 2x higher than their net sales which tells me that something seems to be wrong with their business model in general and I have my doubts that a business expansion solves anything here and I'm fearing that the Net Sales to COGS ratio might be similar for the other indoor-farms they are about to put into operation.\n\n# Executive Compensation\n\nAside from the doubts about their overall business model there are also the Exec Comps in 2021 which leave me with a question mark.\n\nThe CEO of AppHarvest earns a basic salary of $250.000 per year (so far so good). But then there are the bonus payments ($1.500.000) and stock awards ($31.282.077) that seem totally exaggerated.\n\nThe same is true for the President which got a salary of $650.000 and stock awards for a sum total of  $22.567.690.\n\nI mean I get that they are highly qualified professionals who probably works their asses off, however, AppHarvest is a startup (founded in 2017) and such salaries and bonus payments don't seem proportional, especially after such a horrible year.\n\n# Conclusion\n\nI'm not going to invest in this company for the aforementioned reasons. However, I still believe that the sector as a whole as a bright future ahead of it and that there are other companies which are a better choice for an investment.\n\n# P.S.\n\nI have posted a comparison of economical and yield projections made by AppHarvest during their [Dec 15, 2020 Analyst Day Presentation](https://investors.appharvest.com/static-files/5fe4670f-144a-4788-9728-01b72186f02b) with their actual results of 2021 in the r/AppHarvest subreddit should you be interested.\n\n[https://www.reddit.com/r/AppHarvest/comments/u1wzpw/appharvest\\_reality\\_check\\_interested\\_in\\_your/](https://www.reddit.com/r/AppHarvest/comments/u1wzpw/appharvest_reality_check_interested_in_your/)\n\n&amp;#x200B;", "upvote_ratio": 0.78, "id": "t3_tz7suu", "created_utc": 1649436374.0}
{"sub": "investing", "title": "Question on future US debt interest payments", "selftext": "Hi all, looking to see some discussion on what the implications of higher interest rates will be, specifically with regards to national debt. \n\nI read an opinion article on cnbc that essentially states that in 2021, 20% of taxes paid went towards interest payments, which went through at an average of 1.5%. \n\nThe author then goes on to explain that with meaningfully higher interest rates, we could easily see a situation in which two thirds of taxes paid in the US end up going towards interest payments. \n\nDo you guys think this is a possibility? How do you think the government will react to this?", "upvote_ratio": 0.71, "id": "t3_tzcag4", "created_utc": 1649448939.0}
{"sub": "investing", "title": "A \"recession shock\" is coming warning announced", "selftext": " 4/8/2022\n\n*I hope we are in not as bad shape.     If it arrives early at least we are prepared.*   \n\n This is from Research Strategist at BOA\n\nThe macro-economic picture is deteriorating fast and \"*COULD*\" push the U.S. economy into ***recession as the Federal Reserve tightens its monetary policy to tame surging inflation***, BofA strategists warned in a weekly research note.\n\n\"'**Inflation shock**' worsening, 'rates shock' just beginning, 'recession shock' coming\", BofA chief investment strategist Michael Hartnett wrote in a note to clients, adding that in this context, \n\nC**ash, volatility, commodities and crypto currencies could outperform bonds and stocks.**\n\nThe Federal Reserve on Wednesday signalled it will likely start culling assets from its $9 trillion balance sheet at its meeting in early May and will do so at nearly twice the pace it did in its previous \"quantitative tightening\" exercise as it confronts inflation running at a four-decade high. [**read more**](https://www.reuters.com/business/feds-qt-plan-then-now-2022-04-06/)\n\n**A large majority of investors also expect the central bank to hike its key interest rate by 50 basis point**.\n\nIn terms of notable weekly flows, BofA said emerging market equity funds enjoyed the biggest inflow in ten weeks at $5.3 billion in the week to Wednesday while emerging market debt vehicles attracted $2.2 billion, their best week since September.\n\nIt was also an eight week of outflows for European equities at $1.6 billion while U.S. stocks enjoyed their second week of inflows, adding $1.5 billion in the week to Wednesday.\n\nThe analysis was based on EPFR data.  \n\nReporting by Julien Ponthus, editing by Karin Strohecker", "upvote_ratio": 0.36, "id": "t3_tz9wcw", "created_utc": 1649442210.0}
{"sub": "investing", "title": "Big Short Keeps Nickel Market on Edge One Month After Squeeze", "selftext": "Bloomberg has been covering this pretty extensively, but, strangely, there has been almost zero discussion on the topic on this sub (of all places).\n\n[Article](https://imgur.com/gallery/V87Rtv3)\n\n[Nickel spot prices L3Y](https://imgur.com/a/h7309SW)\n\n[Price history since 12/2021](https://imgur.com/a/mfVO2LO)", "upvote_ratio": 0.9, "id": "t3_tz6hpw", "created_utc": 1649432870.0}
{"sub": "investing", "title": "Preferred stock - what are the pros/cons?", "selftext": "Preferred stocks often pay higher dividend yields. Why would someone  choose to invest in common stock rather than preferred? Do preferred  stock values typically mirror those the the common stocks? What exactly happens to your investment in preferred stock if the stock is 'called'?", "upvote_ratio": 0.8, "id": "t3_tz4maq", "created_utc": 1649427614.0}
{"sub": "investing", "title": "Happy accident or potiental for meltdown?", "selftext": "So my grandpa was a traditional investor and following his directives I have a decently standard stock portfolio just a little more aggressive than the nasdaq, but on my own bullshit investing in crypto/etc. I\u2019ve found reciprocal success IE Devon(or whatever major stock I\u2019m holding) might be up while crypto is down and vice a verse. Can I just hop moneys back and forth or will it eventually blow up in my face?", "upvote_ratio": 0.26, "id": "t3_tz2u8y", "created_utc": 1649422287.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - April 08, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.81, "id": "t3_tyz9qt", "created_utc": 1649408469.0}
{"sub": "investing", "title": "What's Going On With Natural Gas Equities?", "selftext": "Over the past couple days, WTI and Brent have corrected from their highs. Normally, energy stocks in general are pretty correlated to oil price movement and the broader market. So I have never felt the need to investigate further. \n\nAlso, over the past couple days,  both US natural gas futures and spot have gone bananas (likely a result of US shipping LNG over to Europe in the great energy reshuffling).\n\nHowever, US and Canadian natural gas equity price has not followed nat gas price, but still seem to trade in correlation with WTI. Eg., OVV, LNG, CTRA. If anything, it seems to me that natural gas equities have underperformed their oil counterparts by around 20-50% post-Omicron.\n\nSome could argue that these stocks haven't run because of the expectation that nat gas price will normalize due to the upcoming seasons of the year when demand for natural gas will wane. But that doesn't explain why these pure natural gas plays have also gone down with WTI and other oil plays.\n\nWhy is there not yet a divergence between pure natural gas and pure oil equity price performance? Shouldn't nat gas equities be outperforming oil right now?\n\nu/enginerd03\n\n[**enginerd03**](https://www.reddit.com/user/enginerd03/)", "upvote_ratio": 0.8, "id": "t3_tyu6du", "created_utc": 1649388064.0}
{"sub": "investing", "title": "Selling investments to pay tax debt", "selftext": "I'm in a position where I either need to cash out $15,000 from a taxable retirement account (Betterment), or sell $15,000 worth of Bitcoin to pay 2021 taxes. I'm not sure which is the best option, so I thought I'd pose the question here. I have about $25k in crypto total, and about $25k on Betterment, equally split between a Roth IRA and the taxable account. I guess I'm wondering which sell will I regret less later on.", "upvote_ratio": 0.38, "id": "t3_tytjb1", "created_utc": 1649385930.0}
{"sub": "investing", "title": "Portfolio opinion thanks in advance", "selftext": "I have made many mistakes so far\u2026.. those mistakes have been good. They taught me lessons while i\u2019m younger\u2026. As I look to re establish  my financial fundamentals . Why do we not see more people in favor of dividend investing? I always thought about this a decade ago. I understand now with inflation it may be even more popular. Does anyone have any reasons why I should not be creating a primary income dividend portfolio?\n\nMy plan would be significant dividend portfolio and some growth companies I like the fundamentals of. (I love EV, AI, semis) I\u2019m just looking for where I am wrong to just go all in on dividends? \n\nThanks in advance!", "upvote_ratio": 0.2, "id": "t3_tysl2z", "created_utc": 1649382859.0}
{"sub": "investing", "title": "US mortgage market basics", "selftext": "When a \u2018normal\u2019 residential borrower applies for a home loan in the US, what are the \u2018normal\u2019 types of product available to them? \n\nNormally in the UK, borrowers apply for a fixed rate &lt; 5 years, expecting the borrower will switch onto a comparable fixed rate &lt; 5 years (10 years would be very unusually long period to fix for), or a tracker tied within a % or two of the Bank of England base rate (the primary policy tool for setting interest rate policy in the UK). If the borrower isn\u2019t on such terms, they\u2019ll be charged the lender Standard Variable Rate - considerably higher than fixed or tracker rates, but allowing freedom to switch onto a better rate subject to affordability assessment\n\nHow does the US model tend to operate? I\u2019ve come to understand it\u2019s more long-term (20+yr) fixes, but posts I\u2019ve seen recently seem to suggest that such fixed rates are affected by FED rate policy. Is that right, or have I missed something? \n\nThank you. High-level only - I wouldn\u2019t want to put anyone to spelling it out deep-dive style!", "upvote_ratio": 0.82, "id": "t3_tys3g6", "created_utc": 1649381261.0}
{"sub": "investing", "title": "Does my 401k managing company suck?", "selftext": "Since 8/1/2019 my 401k has a total return of 6.25% and before I called to change it they set 18.6% of all money going into it in a money market fund losing and gaining nothing. Any recommendations? My employer still matches 4% of my checks so I'm keeping it regardless but I can get better returns on my own through low cost ETF's so I'm hesitating on putting more into it per check.", "upvote_ratio": 0.57, "id": "t3_tys0ox", "created_utc": 1649381011.0}
{"sub": "investing", "title": "4-7-22 SPY/ Apple Daily TA", "selftext": "Going into today we had a small gap of about 90 cents to fill left on the daily chart. While GAPs almost always fill an upside gap when we are in what appears to be a short term bear trend can be up for debate as there is no real timeframe for gaps to fill. It does appear SPY fills them two days later usually. \n\nWith that being said we saw the bears take hold again today running the VIX back up to 23.82 intraday and a low of 443.53 before saw a massive (what used to be rare but is now common) mid day reversal of trend. While these trends are getting pretty darn old they are becoming fairly common. Again the hint was the back to back massive red candles at 1pm. From there she just kept falling. That 24 level seems to be key \u201cresistance\u201d right now.  \n\n \n\nAs you can see we are within a clear downward what I call a double channel. We are currently trading within and bouncing upward resistance of the upper channel. \n\nDespite the big energy the bulls came with end of day we saw SPY stop dead in its tracks at 450.5 range and failed to break through that resistance for over an hour before it finally sold off right into candle with a nice fat red candle down to 448.77. \n\nWith the gap being filled and the upper limit holding strong I will be watching for a sell off into after hours and for futures to take a small dip over night. I suspect we could trade around the 446.5 to 44 range after hours. \n\n&amp;#x200B;\n\nUpper resistance for the channel (assuming we stay with in it overnight) puts resistance at 449.5 for open and support at 445.7 \uf0e0 442 \uf0e0 438.\n\n \n\nWhile I am watching for a bearish formation (see the black channel down) I am monitoring for SPY to find support and continue on a bull run (red channel) of course while also monitoring for a bigger overall channel that I have not been able to establish yet. \n\nIf the BEAR channel holds true tomorrows range is 442.3 to 455.5.\n\nIf the BULL channel holds true 445.5 to 460 is tomorrows range. \n\nThe one thing while this chart is very busy I want to point out is the EMAs. Today we saw SPY dip below the 20, 50, and 100 daily EMAs for the 2nd time this week but failed to close below it again. On the upside we rejected quite firmly at the daily 8ema of 450.8. To me the breaking of the 20, 50 and 100 ema AGAIN along with the very firm rejection and red candle into close tonight is bearish for tomorrow and definitely for next week. We also finally have a true downward direction on the daily 8ema. \n\nNow of course if tomorrow bulls can get this up and over the daily 8ema and close above it I would think 460 comes before 440. However, I will be looking for bears to get back on track again tomorrow and close below the daily 50 ema at 445.3. However, a close below the 100ema at 446.75 or the daily 20 ema at 447.21 would also be a bearish indication for next week. \n\nWe also have a confirmed daily MACD sell and RSI still a 53 trending sideways now. \n\n \n\nSame thing with the weekly chart here the bears were able to get it down below the weekly 20 and 8ema which is very bearish and I will look for them to close this candle tomorrow below the 20 and 8ema at 445.38/ 446.64. A close there with a firmly red candle below the doji last week would solidify some red downside next week in my opinion. \n\n \n\nThe VIX has another MASSIVE day today with a total range from peaks to lows of 21.12 to 23.82 or a move of 12% again today. Some very big moves this week with the VIX that\u2019s for sure.  \n\nBefore someone says \u201cYoU cAnT tA tHe ViX\u201d Yes I know this\u2026 duh\u2026 BUT you can watch key levels and right now this month the low has been 18.7 and the high 24.8. What the means to me is closing at 21.56 we are pretty much right in the middle of the current month range. Bulls and Bears both have a chance to capitalize tomorrow.\n\n  \n\nApple had a pretty unexciting day today finishing at a small 0.18% green. We saw it break its key support at the daily 20ema again of 171. However, it too saw an end of day bull run that hard rejected its daily 8ema at 173.72 to close at 172.14. With an already red after market i will look for that to continue over night. SPY bears will like to see Apple close its week out with a candle below the daily 20ema at 171 tomorrow with hopes of seeing 165-168 next week. However, the bulls need a close above the 8ema tomorrow. \n\nRange for tomorrow based off the current black bull channel puts us at 171.3 to 178 (unlimited upside tomorrow). The yellow bear channel (much like SPY) gives a range of 171.3 to 160 (unlimited downside tomorrow).\n\n \n\nApples Weekly chart has a similar break target of below the 8ema of 169.1 tomorrow to ensure a red week next week. \n\n10% challenge-\n\nI had a pretty incredible day today. I had realized gains of 6x my daily profit goal today alone. Heading into tomorrow this is shaping up to be a killer week. I played a 1dte call for -10%, a 1dte put 10%, 4dte put 23%, 6dte put 20%, 1dte put 20% and a 4dte call for 15% today. I am holding a 4dte put over night and also opened a YOLO June 17th 375P today.", "upvote_ratio": 0.5, "id": "t3_tyq51f", "created_utc": 1649375105.0}
{"sub": "investing", "title": "What am I missing in this biotech investment?", "selftext": "I am intentionally not adding the ticker, as I dont want to come off as pumping my shares.\n\nBut I invested in a biotech company that started listing 3years ago. In their pipeline 2 potential drugs for a market that is estimated at 1% of global population. \n\nThis week they got FDA approval to commercialise one of them. And now I try to understand market action.\n\nThey have 19M floating shares. At the day of approval 15M shares changed ownership, still end of dat the share did not rise. \n\nThe next day it even dropped 10%.\n\n6months prior to this approval day share value dropped 60-70%.\n\nHow can a biotech that went from no income, to owning a drug that is approved to commercialise, with that market potential and such a trade volume with that low amount of outstanding shares, not rise in price?\n\nWhat am I missing?", "upvote_ratio": 0.58, "id": "t3_typ690", "created_utc": 1649372240.0}
{"sub": "investing", "title": "Stock market indices vs Fed funds rate hikes", "selftext": "Here's a link to a chart showing how stock market indices (S&amp;P500 and Nasdaq) react to interest rate hikes.\n\n[Fed funds rate vs stock market](https://www.reddit.com/r/codingeconomist/comments/tha1te/fed_funds_rate_vs_stock_market/?utm_source=share&amp;utm_medium=web2x&amp;context=3)", "upvote_ratio": 0.75, "id": "t3_tyl91y", "created_utc": 1649361208.0}
{"sub": "investing", "title": "Visualizing the power of Dollar-Cost Averaging", "selftext": "I'd always heard the phrase, \"time in the market is more important than timing the market,\" but I had never been able to see how it would have played out. A few things I learned while doing my research:\n\nIf you had put some money in VTI consistently since 2013 (when I graduated from college, Go Blue), you would:\n\n1. Be up 68% on your money\n2. Have a max loss of just -3% (March 2020)\n\nHad you done the same with QQQ:\n\n1. 106% Return with **no** periods of negative return\n\nAnd IWM:\n\n1. 40% return with -23% max loss\n\nHad you done the same thing with Bitcoin (just for fun), you would:\n\n1. Be up 344%(!)\n2. Have a max loss of -37%\n\n&amp;#x200B;\n\nTakeaway: Did I do any of these things? No! But if you're a passive investor (like I am aspirationally), there's almost nothing you can do that's more effective than investing **regularly** (ideally in broad-market ETFs). \n\n&amp;#x200B;\n\nStay the course!\n\n&amp;#x200B;\n\nSource (ETFs): [https://factor.fyi/questions/dollar-cost-averaging-the-market-a1ikcvysfi](https://factor.fyi/questions/dollar-cost-averaging-the-market-a1ikcvysfi)\n\nSource (BTC): [https://factor.fyi/questions/dollar-cost-averaging-btc-since-2013-jqrm6p6be8](https://factor.fyi/questions/dollar-cost-averaging-btc-since-2013-jqrm6p6be8)", "upvote_ratio": 0.75, "id": "t3_tyjoly", "created_utc": 1649356861.0}
{"sub": "investing", "title": "Understanding brokered treasury bills", "selftext": "After much digging around I have decided to put a fair amount of cash into Treasury Bills, probably the 52-week variety.  At first I looked into using Treasury Direct, but to be honest it looks like a lot of work.  So I fell back to looking at buying through Vanguard, Schwab and/or Fidelity as I  have accts in all of them.\n\nIn looking at their offerings (and the listing for same  all look similar) I noticed something I don't understand.   And I am loathe to put $$$ where I don't feel I understand.\n\nI will explain my concern with an example.  If I look at the 1yr offerings I expected to see 52-week freshly minted bills that yield about 1.68%, the present rate. \n\nBut there are other items offered.  An example of this would be taking the last year of a 5-year bond with a 2.65 coupon.  This guy yields just over 2%.  I don't understand this. Why would this offering yield more?  To me, since there is no risk involved in these bills/bonds, The yield should be closely related to the remaining duration of the bond, i.e. it should be about 1.7%.\n\nSeeing this makes me think I am missing something.  Can anyone explain?", "upvote_ratio": 0.67, "id": "t3_tyjk4n", "created_utc": 1649356523.0}
{"sub": "investing", "title": "S&amp;P 500 \"Stock Picking\" Thoughts", "selftext": "The S&amp;P 500 is a collection of \"the 500 largest US companies by market cap\". I'll get into why there's quotes there later.\n\nIt is common knowledge among investors that the S&amp;P 500 index beats out active investors who \"pick stocks.\" For a lot of investors, the S&amp;P 500 is commonly referred to as \"the market\" and is often the benchmark to compare other strategies. After all, it looks well diversified because it holds lots of companies from many different sectors.\n\nThe S&amp;P 500 is known as a passive investment. There isn't any managers actively \"picking stocks\", and various implementations (VOO, SPY, etc.) have very little fees.\n\nI do however have a problem with the definition that it is a passive investment. What many may not know is that the stocks within the S&amp;P 500 are actually chosen by a committee. This committee has various requirements for a stock to be included, including fundamentals such as revenues. However, investors are not paying for this, as the committee is a separate entity from the ETFs.\n\nIt may also be surprising to some that Apple makes up almost 7% of VOO. I would guarantee that most passive investors would disapprove of having that much of your portfolio into one stock. Different S&amp;P 500 ETFs may have different allocations.\n\nWhat are your thoughts? Why do we discredit \"stock picking\", but are fine with supposedly \"passive\" ETFs? Why is this committee's fundamental analysis blindly accepted as \"correct\" over other strategies?\n\nLet me know what your thoughts are, and where I get things wrong. I enjoy understanding the nuances of different investment strategies.", "upvote_ratio": 0.71, "id": "t3_tyjdz6", "created_utc": 1649356062.0}
{"sub": "investing", "title": "Dutch housing market: first drop in average selling prices since years", "selftext": "*\"After years of steadily rising house prices, the housing market shows the first sign of cooling. Homebuyers, on average, paid 2.1 percent less for an existing house in the first three months of 2022 than in the previous quarter, realtors' association NVM reports. That's the first quarterly price decrease since the beginning of 2019. Compared to the first quarter of 2021, home prices were still much higher.\"*  \n[https://nltimes.nl/2022/04/07/dutch-housing-prices-fall-first-time-three-years](https://nltimes.nl/2022/04/07/dutch-housing-prices-fall-first-time-three-years)\n\nInterest rates on mortgages are rising sharply while the average adjusted disposable houshold income is dropping sharply due to inflation, which combined with sentiment in the market could mean a turning point on the housing markets. For a variety of reasons the reported decrease in prices isn't as significant as it might seem, but it might be an early sign we're approaching a potential turning point on the housing markets worldwide. Inflation in the Netherlands is rising extremely quick, from 1,4% in July to 11,9% in March, an indication the reduction in adjusted disposable income and rises in interest rates on martgages aren't temporary.\n\nAverage selling prices dropped from EUR 440,000 to EUR 428,000. Publications (Dutch only): [https://www.nvm.nl/wonen/marktinformatie/](https://www.nvm.nl/wonen/marktinformatie/)", "upvote_ratio": 0.95, "id": "t3_tyii6f", "created_utc": 1649353673.0}
{"sub": "investing", "title": "Estimating earnings on dividend reinvestment", "selftext": "I have this idea kicking around in my head that one investment goals to pay for things in 20-30 years is to:\n\n* Purchase sufficient amount of stocks so that the quarterly dividends are enough to purchase at least an additional share each quarter\n* Primarily look into [S&amp;P Dividend Aristocrats](https://en.wikipedia.org/wiki/S%26P_500_Dividend_Aristocrats) and Dividend ETFs that grow at least at the rate of inflation\n* Only consider companies that are likely to exist in 20-30 years (I.E. McDonalds, Lowes, but maybe not smaller petroleum companies)\n* Only consider companies that existed before 2008 and fared well in the last recession (as an indicator that they will survive future recessions)\n* Monitor these stocks, and dump/re-invest elsewhere if they look like they are going to perish\n* Not kick off this model until after the next recession\n\n&amp;#x200B;\n\nThe initial math on what (arbitrary example) $48k of McDonalds stock is worth in 20-30 years in this strategy is fun, but I am sure I am making some mistakes.\n\nHere's some assumptions I am making in the model (read left to right, not PEMDAS):\n\n&amp;#x200B;\n\n* Enough stock was purchased to purchase a full share on the first dividend\n* Partial shares can be purchased\n* The stock price will grow at a rate of (current price) - (initial price) / (quarters in existence) \\* (90%: 10% discount to be more conservative)\n* The dividend yield will be (last 10 average yields) \\* (90%: 10% discount to be more conservative)\n* Taxes on earnings are not included\n\n&amp;#x200B;\n\nThoughts?", "upvote_ratio": 0.81, "id": "t3_tyh7tu", "created_utc": 1649349981.0}
{"sub": "investing", "title": "People who actually know the answer only: Why is the financial sector getting such a beating (some more than tech) in the last few months when rates are increasing?", "selftext": "The financial sector is supposed to go up with increasing rates. Why in the world are bank stocks falling into the depths of hell? \n\nI understand people might be tempted to spend less with high rates, but more people will keep their money in banks and financial sector generally makes more money during rate increases.\n\nSomeone please explain this idiocy to me.", "upvote_ratio": 0.61, "id": "t3_tygc8p", "created_utc": 1649347531.0}
{"sub": "investing", "title": "Satellite Technology Advice", "selftext": " Hi,\n\nso there's been some buzz especially as of late about satellites. Starlink helping Ukraine get service &amp; Amazon getting more involved in sending satellites up. There are already a bunch of great companies and stocks out there that provide satellite info as well, Viasat, Lockhead, Boeing, Maxar, etc. So already some competition. Satellite's seem to be inevitable to helping the world connect to places they couldn't have before (middle of the ocean, better connection while flying, more stable ways to connect when in trouble, like Ukraine). I'm not a aerospace engineer or genius or know too much, but satellite technology seems to be a great tool in the future. Does anyone know of any stocks that help build satellites or satellite technology? or is the only way to invest as of now is a like space etf or one of these stocks that also provide satellite data?\n\nI think it'd be better / safer to choose the stocks that help build these satellites than to choose one and then competition eventually crushes them? or choose one of the few ETF's (UFO, ARKX). I'm looking / willing to hold for a few years. I have tried digging into some of these individual stocks, so maybe one of those options is the one I'm looking for, but I'm wondering if you know of any that I am missing?\n\nThank you!", "upvote_ratio": 0.72, "id": "t3_tye0ju", "created_utc": 1649340922.0}
{"sub": "investing", "title": "What is the official broad term for companies that provide background/unnoticed resources?", "selftext": "As a layperson, I 'm referring to Background companies..\n\nexamples ARE:\n\nUtility companies, infrastructure companies, materials companies (mining. etc), Scientific equipment, Manufacturing equipment, etc.", "upvote_ratio": 0.6, "id": "t3_tyd3ww", "created_utc": 1649338123.0}
{"sub": "investing", "title": "$HMTXF Listed on OTC Market", "selftext": " \n\n&amp;#x200B;\n\nHemostemix Inc. (OTC: $HMTXF) is pursuing a business opportunity to impact the human healthcare industry positively; Hemostemix is considered one of the first Canadian biotech companies that enlightened the world about regenerative medicine. The healing and regenerating properties of stem cells form the basis of ACP-01, which repairs and reconstructs damaged tissue. Another advantage suggests being less invasive because of the therapy\u2019s autologous nature.\u00a0", "upvote_ratio": 1.0, "id": "t3_tybofv", "created_utc": 1649333451.0}
{"sub": "investing", "title": "Food Shortages Coming - where to drop $15k to profit", "selftext": "I keep reading about Ukraine war and how real food shortages are coming. Ukraine makes a lot of the world's wheat, but other than that, why would there be shortages of other food - doesn't make sense to me.\n\nAnyway, What would you buy, right now, if you knew food shortages are coming to the world. I have been told the wheat commodity prices already have this priced in", "upvote_ratio": 0.38, "id": "t3_tybc9c", "created_utc": 1649332298.0}
{"sub": "investing", "title": "Portfolio for a rising interest rate and high inflation environment", "selftext": "I'm thinking about how to position myself for a high inflation and rising interest rates environment. My current thesis is this:\n\n- Rising interest rates, and thereby a rising risk-free rate, are going to compress multiples in the broad stock market. This makes investing in broad index ETFs less attractive as in the past decade, as the returns are likely low if not negative.\n\n- Bonds are losing value when interest rates rise. Therefore investing in bonds before a period of rising rates is not advisable\n\n- Cash loses value quickly due to high inflation\n\nTherefore, I see two possible options for positioning myself:\n\n1. Investing into different markets outside of EU/US where the environment might be different\n\n2. Going overweight into banking sector ETFs. Banks profit from rising interest rates and could come out on top in such an environment. However, if the rising rates lead to a general market sell of and/or a recession the bank stocks might also lose value.\n\nOf course, the whole thesis builds on the assumption that interest rates will be rising for some time and inflation does not magically go away when supply chain issues are resolved.\n\nWhat are your thoughts on this? Am I going wrong somewhere?", "upvote_ratio": 0.91, "id": "t3_tyb0ef", "created_utc": 1649331064.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - April 07, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.73, "id": "t3_ty8uxy", "created_utc": 1649322069.0}
{"sub": "investing", "title": "Thoughts on Floating Rate Funds in the current atmosphere?", "selftext": "Hello r/investing!\n\nMy girlfriend and I have a few thousand shares of RSFYX (Victory Floating Rate Fund) through Raymond James -- our average share cost is $9.17 (current price is $9.16).\n\nThe price of RSFYX has been on a steady decline since its inception, though it pays decent monthly dividends, and we only pay 1.25% our portfolio value to Raymond James in fees annually (about 0.3% each quarter). We were up 2ish% right before the crazy geopolitical news broke. Since then, we were down 2% at one point, but now only down 0.6% (mainly from fees, as our average share price is $0.01 below the current share price).\n\nThe reason I am typing this post is because we are under contract for a new-construction townhome that will break ground in a few weeks. We plan to hold (and continue buying with regular contributions) until we need to sell shares for our down payment, closing costs, and other associated fees. This may be sometime in the next 12-18 months.\n\nEssentially 99% of our saved money is held in these RSFYX shares - emergency fund, new home, furniture, etc. My girlfriend and I share the mentality that although we are taking a slight/safe(?) risk, this savings method is likely to generate better returns than a typical savings account. I am not anxious by any means, but I am curious to hear the community's thoughts on this strategy, especially during this volatility. I watch the RSFYX price daily, it typically never fluctuates more than a couple cents. When the war in the east broke out, it did react rather dramatically compared to its usual behavior, and has been on a slow climb back up since.\n\nDo you think we are better off holding a portion (or all) of this money in cash for the next year and half (or more)? How could the general performance of the economy or Fed interest rate hikes over the next year affect the bank loan interest that floating rate funds deal with? What are your thoughts on floating rate funds in general, and using them as a vehicle for saving money that typically makes decent gains in the long run? Thanks for taking the time to read!", "upvote_ratio": 0.66, "id": "t3_txxqbq", "created_utc": 1649283154.0}
{"sub": "investing", "title": "What happens during a recession?", "selftext": "Simply that, I\u2019m 20 this year and if we do reach a recession, what happens to the stock market? Are there any sectors or stocks that perform or does the whole market tank? \n\nIf everything does go down, maybe you can\u2019t time the bottom but that means you should just sell now, lock in profits and wait for some signs of recovery to buy back in? Assuming you\u2019re a believer that there will be a recession since obviously no one can predict it.\n\nAppreciate any replies!", "upvote_ratio": 0.86, "id": "t3_txwfi8", "created_utc": 1649279581.0}
{"sub": "investing", "title": "CVNA and other struggling online auto retailers", "selftext": "I'm curious about others opinions either in agreement or dissenting. Rode the wave down on puts from the top and still think their Financials suggest downward. Curious if you think also dead cat now or are we at a good support? I'm avg down a bit more on some OTM puts for May 20. Do you see any bullish or bearish signs? Is the auction site a huge thing for them but won't be auctioning cars for a year or so? Etc...", "upvote_ratio": 0.62, "id": "t3_txv8mf", "created_utc": 1649276392.0}
{"sub": "investing", "title": "Will Nasdaq or NYSE ever unhalt the trade of Russian stocks?", "selftext": "Hey guys. \nI hold a huge bag of Russian stocks, yandex, headhunter, and ozon, all of which I have invested long before the Russian invasion on Ukraine. Their trade were all halted on February 28th as T12 halt.\n\nI recently read the news that Russian congress is pushing forward the bill to delist Depository Receipts and exchange them with Moscow stocks. But I don\u2019t think this applies to these internet stocks, which are issued abroad (especially Yandex which is issued in the Netherlands).\n\nI am already down a lot. And I am so worried that I might lose everything. Any insights on how and when Nasdaq or SEC ever allow to trade again?", "upvote_ratio": 0.55, "id": "t3_txt0ps", "created_utc": 1649270388.0}
{"sub": "investing", "title": "Artificial intelligence ETF's", "selftext": "Can anyone tell what is happening in the artificial intelligence market thats causing prices to drop? Big spike down yesterday.\n\nI'm in a few robotics ETF's that are also falling but was about to add some AI ETF's to my portfolio until I noticed the AI ETF's all falling, any information please?", "upvote_ratio": 0.6, "id": "t3_txst9a", "created_utc": 1649269817.0}
{"sub": "investing", "title": "When to contribute to an IRA and when to contribute to a Brokerage", "selftext": "I've been doing a lot of research on IRA and saving in general. Every outlet I've read says to always max out an IRA when you can. I guess my question is why would you want to lock up your money until your 59.5? I understand the tax-advantage you gain from the IRA but as someone who wants to retire early (living frugal and saving 30%+ of income), would I be better using all my savings (besides minimal 401k match) to invest in growth funds in a taxable brokerage?\n\n&amp;#x200B;\n\nI'm curious on your thoughts on the idea and if I'm missing someone big here.\n\n&amp;#x200B;\n\nEDIT: Another part of this is that I feel IRA's are just the governments way of insuring you work till you're in the 60's. What if I don't want too?\n\n&amp;#x200B;\n\nThanks", "upvote_ratio": 0.7, "id": "t3_txsn9o", "created_utc": 1649269362.0}
{"sub": "investing", "title": "Profit Sharing - Being offered a new role in my company with profit shares as part of the offer, I\u2019m seeking advice to get the best outcome for myself from the negotiations.", "selftext": "I handed in my notice at my company a couple of weeks ago. They have asked me to stay on in a new role heading up their new manufacturing business. I\u2019ve never done this before but the new business ties closely with their existing well established business (the one I\u2019m leaving and know inside out).\n\nI\u2019m currently negotiating salary, bonuses etc but profit shares has been mentioned as part of the package as well. \n\nThe current target for the business is to be turning over \u20ac5m at 25% profit by 31rst Dec 2025. The turnover target is achievable but I\u2019m not sure about the profit % target until I get right into the details.\n\nI know board members of the other business have an agreement to get 3% of profits at end of 2025 based on a target.\n\nI suppose my questions are (I know nothing about profit shares):\n\nQ1, How do I cover myself with a deal if they company doesn\u2019t achieve its goals so I don\u2019t end up with nothing?\n\nQ2, If I meet these targets how much should I be looking for?\n\nQ3, What about exceeding the targets ie higher turnover at 25% or even better \u20ac5m + turnover with a higher % profit?\n\nThanks", "upvote_ratio": 0.55, "id": "t3_txsaxl", "created_utc": 1649268427.0}
{"sub": "investing", "title": "wash sale disallowed loss?", "selftext": "So I accidentally did a wash sale last year, was a 30k loss, I get it I cannot claim the loss oh well, the problem is they're counting it as a gain.  Is this a penalty?  I'm paying taxes on a 30k loss as a gain... lol wtf????  Robinhood swears it's correct &amp; to consult a tax professional for further questions.  Whew.  Thoughts?", "upvote_ratio": 0.55, "id": "t3_txs2r3", "created_utc": 1649267825.0}
{"sub": "investing", "title": "Reverse Stock Splits - Why do Investors Hate Them?", "selftext": " \n\nI have noticed that most retail investors hate reverse stock splits even if it means the compnay would be uplisted. Why is that?I am currently invested in a company that will be doing 18-1 split to uplist to NASDAQ. What should I be expecting? I noticed HITI did a 20-1 split for the same reason last year and their stock price soon when to hell.Can you help a brother up by sharing your experience?\n\nThanks", "upvote_ratio": 0.66, "id": "t3_txrjb1", "created_utc": 1649266366.0}
{"sub": "investing", "title": "Former NY Fed Pres Bill Dudley: Fed Might Need to Force Stocks to Fall (Bloomberg Interview)", "selftext": "It's rare to see a Fed official, even a former one, be this explicit about the wealth effect going the other direction. The Fed relied on the wealth effect to boost the economy when stocks were on the way up. (Bernanke famously said so in 2010). Now Dudley says the Fed wants equities lower to slow inflation.\n\nhttps://www.youtube.com/watch?v=Fiiib9oqTB0\n\nEdit: Fed minutes came out at 2pm EDT. Consensus interpretation seems to be dovish. $95B cap on QT phased in over three months.", "upvote_ratio": 0.95, "id": "t3_txr4c5", "created_utc": 1649265245.0}
{"sub": "investing", "title": "Is there a way to convert an index fund to an ETF without selling?", "selftext": "I have some fidelity index funds and want to switch to S&amp;P 500 ESG ETFs, is there a way to do so without selling (realizing gains)? Or am I better off just leaving them and buying ESG ETFs moving forward?\n\nI don't care if ESGs are more volatile or marginally less profitable.", "upvote_ratio": 0.54, "id": "t3_txqmlk", "created_utc": 1649263942.0}
{"sub": "investing", "title": "Question: Can a stretch IRA grow significantly?", "selftext": "\\*\\*Disclaimer: I'm not looking for specific advice about my account, just a general explanation of how an investment would operate.\\*\\*\n\nMy inherited stretch IRA has been bobbing along at roughly the same about for over a year now, so it started me thinking: since I can't put any new money into it, how much can it grow?  Obviously the stock market fluctuates so the actual amount of dollars in the account at any given time will be different.  But without new contributions, would there be any kind of exponential growth?  What's the typical trajectory?\n\nSigned, A Financial Dunce\n\nEdit: Thanks everyone for the great responses!  It was just what I wanted to know.", "upvote_ratio": 0.7, "id": "t3_txpjvc", "created_utc": 1649260983.0}
{"sub": "investing", "title": "Hemostemix Inc. Market analysis", "selftext": "  \n\nHemostemix Inc. (OTCQB: $HMTXF) is a Canada-based biotech company listed on OTC Market with the ticker symbol $HMTXF. The trading volume is marked as 20,950, having a fair amount of a 9.8M market cap. The company started opening today at 0.1392, currently running at a hike of 0.14%.\n\nMore Stock Information: [https://www.otcmarkets.com/stock/HMTXF/overview](https://www.otcmarkets.com/stock/HMTXF/overview)", "upvote_ratio": 1.0, "id": "t3_txp4d2", "created_utc": 1649259860.0}
{"sub": "investing", "title": "Bigger investment in mining needed to meet climate goals, says LGIM - FT", "selftext": "Title says it all, LGIM states we'll need to increase our investments in mining to meet climate goals. For example, copper demand will need to double and nickel demand will need to quadruple to meet the Paris agreement. \n\nTo complete our energy transition our mining industry needs to grow. I think I will sharpen my pencil and take a deeper dive into mining companies. \n\nhttps://on.ft.com/3j3DlBS", "upvote_ratio": 0.74, "id": "t3_txoi2t", "created_utc": 1649258184.0}
{"sub": "investing", "title": "Is there some time span implicitly included when a \"price target\" is announced by an analyst?", "selftext": "I oftentimes read something like \"so-and-so analyst company initiated XYZ coverage with Outperform and target xxx.yy.\"\n\nI  see nothing about a time horizon in this. Is there something like 6  months or 1 year implied? This statement says nothing about the \"speed\"  of reaching that target.\n\nUpdate: Thanks for the various replies. But then what's the point of such a \"price target\"? if there is no time frame associated with the \"target\", it's pointless. I could have said in 2002 \"Amazon will reach 3400\", and at \\*some\\* point in time I would have been right (some 20 years later).", "upvote_ratio": 0.81, "id": "t3_txmhdv", "created_utc": 1649252565.0}
{"sub": "investing", "title": "Balance Sheet To Be Focus Of Fed Minutes Amid Recession Warnings", "selftext": "Shedding light on US central bankers\u2019 hawkish stance, minutes from the Federal Reserve March meeting are expected to reveal a committee leaning towards more aggressive monetary tightening.\n\nLikely to mirror post-meeting rhetoric, the Fed minutes are set to reinforce an urgency to frontload policy and included more details on the balance sheet run-off, according to analysts. \n\nIn March, the Fed raised its benchmark interest rate by just a quarter point as war raged in Ukraine and domestic inflation hit a four-decade high. Since then, price pressures have only increased, and labour market data has shown solid employment growth and an acceleration in wage growth.\n\nWith the Fed pulling no hawkish punches in its policy guidance, investors will scrutinise the minutes for any insights on plans to normalise the balance sheet.\n\nFull link: https://www.livesquawk.com/report/special_balance-sheet-to-be-focus-of-fed-minutes-amid-recession-warnings", "upvote_ratio": 0.81, "id": "t3_txkrxv", "created_utc": 1649247267.0}
{"sub": "investing", "title": "I'm confused which should I choose: Dividends or Real Estate?", "selftext": "My parents are convincing me to invest together as a family. They want us to split the cost and try our luck in either real estate or dividend investing, but we can't decide which. We are weighing the benefits and drawbacks of both options.\n\nMy father suggested that we consider real estate investing because it has been shown to be beneficial in the long run, and if done correctly, it can generate lucrative returns for us. While having our money in the market will undoubtedly require less energy and effort from us, dividend investing will undoubtedly require less energy and effort from us.\n\nI'm not very knowledgeable about this and I really want to support my parents' plan so I've been researching and looking for various platforms that can assist and guide us through our investment journey. I came across [Hedonova](https://www.hedonova.io/?r=1), [Fundrise](https://fundrise.com/), [Carl Inc.](https://moneymade.io/discover/carl-inc), and [Yieldstreet](https://www.yieldstreet.com/) but I'm not sure which one to use. Is it possible that some of you already experimented investing through these platforms?\n\nI don't want this to go to waste because I want to ensure that it will be a reliable source of income for my elderly parents.\n\nI'm excited to hear about your own experiences and specific recommendations for a better strategy.", "upvote_ratio": 0.81, "id": "t3_txitan", "created_utc": 1649239958.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - April 06, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.78, "id": "t3_txhtbr", "created_utc": 1649235669.0}
{"sub": "investing", "title": "Pitfalls of Chinese Equities", "selftext": "## The Big Picture\n\nThere are plenty of different countries and islands which Chinese firms use as a springboard to list on the NYSE or NASDAQ. The concept remains the same, and thus the most prominent and biggest company makes for an intriguing show-by-example: Alibaba. Without a question, Alibaba is huge, has plenty of customers, and offers a wide range of services. E-commerce, artificial intelligence, venture capitalism, Alibaba covers it all. The company went public on the New York Stock Exchange in 2014 and became the largest IPO in history (not just Chinese, but overall). The demand for stocks of the internet giant was huge and the underwriting oversubscribed. While insiders and sophisticated investors understood what the rights of owning BABA stocks entailed, most were blinded by the prospect to get in the action. Although it was an initial public offering, and the name of Alibaba was splashed across it, investors didn\u2019t actually get a piece of the Chinese company but solely a promise to receive profits of the Alibaba Holding Group in the Cayman Islands.\n\nI know. It\u2019s complicated.\n\nChina doesn\u2019t trust foreigners and consequently, Chinese law forbids them to hold strategic domestic assets. No surprise, the nation\u2019s e- commerce flagship belongs to that untouchable group. Wall Street and stockbrokers are aware of that. \u201cSo why the fuck are Chinese equities even listed on the NYSE if we don\u2019t get shit for it?\u201d, you may ask. Good question and the answer is money. To solve the issue of foreign ownership prohibition and still offer a financial product to investors, there is a \"solution\" called variable-interest entity structure (VIE). Even though a VIE doesn\u2019t give investors the right to receive a penny if Alibaba would default, it gives the holder of a VIE the right to receive profits from the emitting entity. In simple terms: Alibaba makes a profit and you as the owner of their \u201cstock\u201d are technically entitled to parts of that profit in the form of dividends. If you think this procedure is an isolated incident then let me reassure you: it\u2019s not. Maybe the islands change to the British Virgin Islands or Bermuda but at the end of the day, Chinese equities in their traditional sense do not exist. To go around the domestic rules in China, companies set up an equity interest scheme (or profit scheme) through which investors can partake in the success of a publicly listed Chinese company.\n\nTechnically you can partake in the profits, but it's important to add that in the history of Chinese companies listed abroad, not a single firm has paid dividend interest to its shareholders. Moreover, the PRC could simply impose a rule that strategic assets are obliged to reinvest their profits instead of paying out profits, thus preventing a negative money outflow. Currently, there is no reason for the Chinese government to interfere in these practices as any IPO in the West just brings in billions of US Dollars for Chinese companies using shell setups in faraway islands. \n\n## Friendly Advice\n\nI bought Alibaba shares because I believed in the business, the numbers, the growth rates. But I sold my shares because these companies were valued at similar rates to their Western counterparts, even though their share structure looked nothing alike. In my opinion, you carry all the risks whilst being subjected to three jurisdictions: the US, the Cayman Islands (or another island state), and China. The complicated setup of onshore and offshore companies also provides countless chances for accounting fraud, thus further reducing the reliability of Chinese corporations' balance sheets. It's virtually impossible to audit or verify their numbers. Alibaba is not worth its market cap. No Chinese company is. And not because their business is bad. It could even be excellent, but the issue lies in their structure, the accounting and the unknown political factor of the Chinese government. At any time the PRC can revoke the possibility that domestic corporations are allowed to engage in certain business practices such as using a variable interest structure. This would imply that, even if Alibaba is constantly growing, their numbers are valid, and investors are anxiously preparing to receive their first dividends, the PRC can just stop these payout, and no one could do anything about it.\n\nI am not condemning the Chinese government to keep such options in place nor am I shocked: The PRC is one of the most consistent governments in terms of reaching its objectives and \u201cServe the People\u201d. Alibaba, Baidu, JD.com, and the likes are still able to raise foreign capital because foreign investors are willing to buy their so-called equities despite the rather shaky fundament they are built on. Stock markets, banks, and brokers are built to sell financial products and earn a commission, thus profiting from the practice. Still, if an investor believes that Chinese equities function the same way as American or European stocks, they are just wrong.\n\nMy advice to you is to sell your Chinese shares and move on. There are better alternatives.", "upvote_ratio": 0.56, "id": "t3_txhoj6", "created_utc": 1649235098.0}
{"sub": "investing", "title": "What will happen to the 30-year loan after the next rate hike?", "selftext": "So we've seen the 30-year fixed rate loan skyrocket 1.5% now since the rate hike. How will the next 0.25% rate hike effect the 30-year fixed rate loan?\n\nWill it increase by another 1.5% over the course of 3 to 4 months or will it increase at a slower rate or not at all? I know this is tied to the 10-year treasury note. \n\nWhat I'm confused on is how the Treasury note will affect the 30-year fixed rate loan during the next rate hike?\n\nEDIT: Seems like this question confused everyone. Of course if you have a current fixed rate loan the rate does not change. I am asking if the current interest rate will continue to rise.", "upvote_ratio": 0.78, "id": "t3_txgbgk", "created_utc": 1649228836.0}
{"sub": "investing", "title": "CoinBase Is Stupid. Crypto currencies are stupid.", "selftext": " **I have no vendetta against anybody from CoinBase, nor do I hold any position in any company. I have been hearing about this crypto crap since 2016/17 when I was starting at grad school, but really didn't care much at that point. More recently, when I felt I had been a victim of some crypto fraud (i.e. my identity had been compromised), I decided to take this personally.**\u00a0\u00a0I could really attack any company that has crypto roots, but since CoinBase is the \"[Amazon](https://www.wallstreetoasis.com/company/amazon)\" or \"Uber\" of its industry (that is to say the most recognizable name in the space), I am focusing my efforts on it. Again, I don't care if Jim Simmons has invested in the company or Warren Buffet did (though he hasn't), if it doesn't make any economic sense, it is not worth investing in.\u00a0\u00a0  \n\n\n1. Retail Users: Retail users are driving bulk of the activity here. 32% of the [trading](https://www.wallstreetoasis.com/finance-dictionary/trading-overview) volume is coming from them and it makes up 94% of the transaction revenue. Retail and institutional guys are lumped under \"verified users\", but their criteria for verified users is ridiculous (it is essentially anybody who has provided an email address or a phone number or some other information). It's really not that time-consuming to open an email account, and as they themselves state, they don't know how many \"unique\" verified users or MTU are on their platform. So for all you know, maybe 100 or 200 folks are driving bulk of the activity here, and they are clearly doing no due diligence checks to verify the identity/finances of the users. I don't know how are Robinhood's onboarding measures different, but Robinhood clearly asks for Social Security number (at least that's what I remember it asking for in 2016), and it seems the barriers to trading here are very low on CoinBase. Why this matters? Because it potentially opens them up to a bunch of lawsuits, which are already in the action and 2) they may potentially be overstating their key business metrics (not potentially, definitely)\u00a0  \n\n\n2.\u00a0Litigation: Company has 3/4 legal proceedings either going against it or has been threatened with. Truly an innovative and cutting-edge company to be dealing with litigation issues at the start of its public life. I can only imagine what other marvelous moments are ahead of it\u00a0\u00a0  \n\n\n3.\u00a0Security: Clearly the company has not been paying much attention to the security hacks that happen on this platform otherwise they would have created a provision for them or had some sort of reserve to deal with these situations. For their crypto asset wallets, they state the risk of loss is remote. How do you reconcile this with the litigation proceedings that the company has going on? There is a disconnect here\u00a0  \n\n\n4.\u00a0Salaries: Maybe I am looking at the wrong place here, but I can't find the cash compensation of the management team, so I am assuming that the salaries they mention their are cash compensations. Brian Armstrong is getting paid $1 million in base, the salary of Zoom CEO (arguably a shitty company too but at least has a relevant use) is $300K. But what surprises me the most is that they don't have a chief security officer or anyone listed there on the management, and 2) the salary for chief legal officer (one of the more important guys in this business) is \\~200K. I am not really factoring in stock options here or any of that stuff because I really think CoinBase is worthless and will be going down. I don't think Brian understands the risks inherent in this business or he would be paying his chief legal officer more here\u00a0\u00a0\u00a0  \n\n\n5. Technology &amp; Development: This is where I am assuming that security expenses are being included, but it's not abundantly clear to me. They say: \"technology and development expenses include personnel-related expenses incurred in operating, maintaining, and enhancing our platform. These costs also include website hosting, infrastructure expenses, costs incurred in developing new products and services and the amortization of acquired developed technology\". I don't think the company regards security of the platform vital enough otherwise they would be drawing more investor attention to it.  \n\n\n6. Valuation: Their valuation of their crypto assets is amusingly funny. I am not an accountant, but I find it ridiculous that they value the assets on their book based off the closing on the last day of the respective period. So where are you capturing the volatility in the crypto markets? Dumb. And. Wrong.\u00a0\u00a0  \n\n\n7.\u00a0Disclosure: As I said before, their disclosure sucks. 31% of the assets on their platform are \"other crypto assets\" and make up 50% of their revenues. But no breakdown on these other assets. I don't know the difference between Bitcoin and Ethereum and Lithium and Sodium and Potassium (all the fucking elements of the Chemistry table), but some of you schmucks might know. However, for a company that is making 50% of its revenue from two assets (Bitcoin and Ethereum), it should talk about each in more detail and what separates them. Nothing. This revenue / asset concentration is nothing to write about.\u00a0\u00a0\u00a0  \n\n\n8. Acquisition: OK I am not dissecting all the acquisitions they've made nor have I researched them. But in January 2022, they made an acquisition of 257 million of which 151 or something was in cash. Why pay in cash if you think think your stock is worth more? Dunno, doesn't make much sense to me.\u00a0\u00a0 \n\n\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\n\nThe fact that this asset class (Bitcoin/Ethereum) is at this price or that doesn't matter. \n\nBefore you plow your money into this crap, ask yourself or whoever is asking you to invest in this crap, the following questions:  \n\n**Valuation:**\n\n* How do you ascribe value to these\u00a0crypto\u00a0currencies (please help me understand the valuation employed and the assumptions that go into it)?\n* What is the difference between Bitcoin, Ethereum and Dogecoin? Why is one valued higher than the other? What is each bitcoin's competitive moat?\n* What investment time horizon do you recommend, and why?\n\n**Legal:**\n\n* How do you ensure that these currencies or the exchanges they're traded on are not immune to either security hacks or other kinds (by now, cases of multi-billion / million dollar hacks seem to be a common phenomenon), how does one ensure that this does not happen?\n* How do you suggest you do due diligence on the parties involved in these transactions, as they seem to be keen on remaining hidden from the public eye?\n\n**Commercial:**\n\n* How further long before all major central banks / governments around the world endorse\u00a0Crypto\u00a0as a part of their overall monetary policy framework? In a situation where it is not commonly accepted by all, what risks do you see playing out?\n* What reserve currency will these\u00a0crypto\u00a0assets be pegged to and why? If USD, why, as bulk of the activity seems to be concentrated in Asia here?\n* How do you differentiate between what is \"fake\"\u00a0crypto\u00a0currency and what is \"real\"\u00a0crypto\u00a0currency unlike the litmus test that exists for commodities (gold etc.)? \u00a0\n\n**Accounting / Taxation / Insurance:**\n\n* How does one ensure transparent and correct financial reporting of these\u00a0crypto\u00a0assets (in terms of accounting)? How should one be reporting the value of these\u00a0crypto\u00a0assets on one's tax returns?\n* What's right mark-to-market approach for these assets?\n\n**Others:**\n\n* How do you convince someone , who's not a sophisticated and / or an accredited investor, that investing in\u00a0crypto\u00a0assets is a better use of capital as opposed to investing in even more hard, tangible assets (real estate) or non-riskier (as well as non-volatile, and more transparent investment options: government bonds, [ETF](https://www.wallstreetoasis.com/finance-dictionary/what-is-an-exchange-traded-fund-ETF) / index funds) \u00a0 \u00a0\n* Why is there a need for crypto / bitcoins when the existing forms of payment (hard cash, soft cash) seem to be acceptable by most? I am really hoping that the answer to this question is not that a bunch of millennials / generation Z want to \"own\" something different, and so everyone jumped on the bandwagon to make that happen. \n\nI doubt you're going to get any seemingly intelligent answer to any of these, which does not make this worth your time/effort/investment. I don't know when will this dooms day arrive, but the price of this thing will soon be down going the gutter. \n\n\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\n\nPS -- I am not here to answer any questions for any of you, nor am I here to engage in any debate. I am just stating this as my opinion. You all can debate among each other. I don't have a position in this company nor have I ever.", "upvote_ratio": 0.47, "id": "t3_txdadf", "created_utc": 1649217290.0}
{"sub": "investing", "title": "Broad question about companies with large balance sheets of one asset.", "selftext": "Poking around twitter I saw a recent tweet by Michael Saylor, CEO of MicroStrategy advertising how the company added another ~4k btc to their already humongous 130k btc balance sheet.  \n\nIt struck me that the price of the company is completely tied to the price of bitcoin, and any value their stated business of software or marketing or whatever would be completely negligible against the volatility of the asset on the balance sheet.\n\nWhat's going on here?  Are there other companies that stockpile a single type of asset into their balance sheet and has its price dictated by that assets fluctuations?  Is this some winkwink way to allow people to buy options against the price of bitcoin?  Feels odd...", "upvote_ratio": 0.68, "id": "t3_txcwuv", "created_utc": 1649216001.0}
{"sub": "investing", "title": "Curious on what you own !", "selftext": "started my Roth at 18 and I\u2019m 20 now I own BND, VOO (majority) VTI and VXUS and just want a good amount of compound interest like everyone else 7% to 8% would be awesome but I\u2019ve looked at my numbers and I know the markets been crazy lately but I\u2019m only looking at a 2% return so I\u2019m curious what you guys own ? Thanks !", "upvote_ratio": 0.71, "id": "t3_txbqg0", "created_utc": 1649212140.0}
{"sub": "investing", "title": "Legitimate investing company", "selftext": "My wife and I are looking to invest some of our savings. After watching so many money scam shows on Netflix has increased my paranoia in trusting people, especially with our money. Is there any useful advice when researching a companies legitimacy? Such as looking up licenses or legal documents that provide assets the company claims to have. Any advice is welcome, thank you.", "upvote_ratio": 0.45, "id": "t3_tx7a4u", "created_utc": 1649198599.0}
{"sub": "investing", "title": "Critical Analysis of Meta", "selftext": "I lead an investment club at my college where we analyze stocks and talk about principles of investing. This week we want to look at Meta.\n\n~~There was a brief period on~~ [~~morningstar.com~~](https://morningstar.com) ~~where Meta was analyzed as being 50% of it's fair value, and then they looked at it again and now it's okay. Does anyone have an explanation for what happened? I understand that the company had earnings below what we were hoping for.~~\n\nEdit: [Morningstar.com](https://Morningstar.com) currently has Meta listed as being 42% below fair value, which is just insane to me. My previous statement was not accurate.\n\nDo you think it's a buy in general?\n\nI'm pretty bullish on the Metaverse, because I've seen what vrchat can do, and the whole situation there reminds me of League of Legends taking over from DOTA. In that situation, Riot gaming basically took the model built by an entire network of community software developers, added their capital and profit motive, and created an extremely dominant and powerful game which has a Netflix series and professional and amateur e-sports teams across the globe.\n\nOn top of that, we live in a post-covid and highly globalized world, where people are much more accepting of using digital interfaces, like zoom, to socialize.\n\nThat being said, I have no idea what the competition looks, like and I don't have a very good read on the company in general.", "upvote_ratio": 0.38, "id": "t3_tx61y0", "created_utc": 1649195161.0}
{"sub": "investing", "title": "Why I am long on coinbase", "selftext": "https://youtu.be/iZYozMeo7dQ\n\nBut seriously, anyone have strong opinions on this stock? I am currently long on it as I think they should have low operating fees in the future and I believe crypto will continue to grow. However, it does seem to be an easy market to get into for competition which is one concern I have.\n\nAppreciate the comments.", "upvote_ratio": 0.21, "id": "t3_tx5j29", "created_utc": 1649193765.0}
{"sub": "investing", "title": "Is investing in ETF/ Index funds really worth it for the long term?", "selftext": "I\u2019m 24 and live with my parents, and I\u2019m in a good spot financially. No debt, and great at saving money. I have been hesitant to invest in an ETF/ Index funds for one main reason.\n\nLet\u2019s say I\u2019m to invest 100 dollars a day into funds such as VOO, VTI, or FXIAX. Essentially in roughly 27-30 years I would have invested enough money to average 100 dollars a day in passive income. My math is more than likely wrong, but I\u2019m just using it as an example. So 100 dollars a day sounds great as passive income, but what will 3,000 dollars a month get me 30 years from now? Inflation will continue to grow, and what will become of my investment?\n\nPlease correct me if I\u2019m wrong, as I\u2019m still learning about the stock market. To me it just feels a little bit pointless to invest so much, and end up getting very little in the future. It\u2019s obviously better than letting it sit in the bank and depreciate value, but would it just be better to save my money and pursue real estate? \n\nSorry if I don\u2019t know what I\u2019m talking about. I just want to get some input, and thoughts of others before I invest money.", "upvote_ratio": 0.33, "id": "t3_tx4a9o", "created_utc": 1649190536.0}
{"sub": "investing", "title": "When do I stop DCA into VTI", "selftext": "Hi all,\n\nMy question is when do I stop dollar cost averaging into VTI? Currently am investing $20 daily into VTI which just seems to bring my average cost up. When is it smart to stop? What kind of strategies do you impose? Everyone seems to enjoy this strategy of dollar cost averaging into an index fund but I\u2019m confused as to how it can bring me large returns if it just brings my average cost higher and higher?", "upvote_ratio": 0.43, "id": "t3_tx3i47", "created_utc": 1649188492.0}
{"sub": "investing", "title": "What is up with the volume on the DJT", "selftext": "I\u2019m looking at my stock app, and the DJT, tracker for the Dow Jones transportation average is currently down almost 3%. What\u2019s really interesting though is that the volume is significantly higher than the average. Average volume is roughly 1.275 million and the volume today is over 90 million. \n\nI\u2019ve read that the freight transportation sector has been found to be a reliable leading economic indicator. The Dow Transports was down 5% last Friday (One of its worst daily losses in history). A study conducted by the Bureau of Transportation Statistics within the US Department of Transportation report that the Bureau\u2019s Freight Transportation Services Index leads slowdowns in the economy by an average of 4 months. The FSI is only reported monthly, but it is highly correlated to the Dow Jones Transportation Index. \n\nDoes anyone know what\u2019s happening today in the DJT? It\u2019s interesting that we\u2019ve still been having significant swings up against a backdrop of broad declines in the transportation sector. Is this just a return to normalcy after huge run-ups post-Covid?", "upvote_ratio": 0.78, "id": "t3_tx3f80", "created_utc": 1649188273.0}
{"sub": "investing", "title": "Investing in india's growth?", "selftext": "Hi there\n\nI have been searching extensively for ways to invest in India's economic growth.\n\nI researched all the ETFs, actively managed, passively managed, CEFs, Emerging market ETFs, all of them.\n\nHowever, none of them were a match for me, as most of them seem to have been trading sideways for decades and India's market PE ratio is also indication some overvaluation.\n\nAfter being stuck in this paradox for weeks, I have stumbled upon india's largest bank: HDFC bank.\n\nIt has a pretty smooth upward trend and good fundamentals on paper.\n\nI would like to hear your opinions on this bank, is it a good long term bet on India's economy?\n\nDo you guys know of a better way to invest in India's growth?\n\nThanks.", "upvote_ratio": 0.66, "id": "t3_tx13k0", "created_utc": 1649182123.0}
{"sub": "investing", "title": "Take a look at HIVE, seems undervalued, doesn't it?", "selftext": "Current price in USD **$2**HIVE is a cryptocurrency mining company listed in the NASDAQ.\n\n1 week RSI(43), 1 day RSI(49), 1 hour RSI(34), 5min RSI(43), 1min RSI(9)\n\n(values in CAD)\n\nRevenue 89 million\n\nNet Income 56 million\n\nFree Cash Flow 13 million\n\nCash 50 million\n\nEquity 169 million\n\nReturn on Assets **42%**\n\nReturn on Equity **56%**\n\nReturn on Capital **34%**\n\nShares Outstanding 409 million\n\nVolume 4.38 million", "upvote_ratio": 0.5, "id": "t3_twzgkr", "created_utc": 1649177713.0}
{"sub": "investing", "title": "Labor shortage (Boomers retiring) and how it will affect future investments? +Supply chain +Inflation", "selftext": "To me the biggest effect of the pandemic is just how many boomers decided to retire and how it\u2019s affecting labor shortage. \n\nAlmost no one wants to take a low wage job since they can work anywhere and can choose. \nArticles claiming trucking companies have a shortage of truck drivers while other articles point to facts about no labor shortage just truckers have their own company/self employed to get better paid. \nAnd I\u2019ve seen posts in job boards for small business owners looking to hire where people respond saying the wages they want to pay is the same as fast food restaurants. Which of course they\u2019ve had to raised because no one wanted to work there for such low pay.\n\nAll that and more seems to be one main issue that now every workers wants to get paid twice as much as before for almost any job. \n\nSo because so many boomers retired and left a bigger supply of all types of jobs, income will raise a lot. Which companies will pass the increases to consumers. Which will then want to get better paid\u2026 and keep going on unevenly for a while.\n\n\nAnd then we have all the stimulus and the fed ok with low interest rates and a soft landing (or just slow to react). \nSo tons of cash=inflation.\n\n\nAnd then the more surprising things (or not so much looking back) of supply chain shocks because now people are going back to work, factories at full capacity, restaurants back again, etc.\n\n\nI feel this labor shortage for low wage jobs is the real kicker. It\u2019s pushing all sort wages up and adding to inflation.\nI think maybe a lot of small business will have to close in masses they can\u2019t pay corporate level price increases. \n\n\nWhat do you see happening? What would be a good investment in this scenario with the labor demographic change?", "upvote_ratio": 0.75, "id": "t3_twyyg5", "created_utc": 1649176368.0}
{"sub": "investing", "title": "How can compound interest be used with index funds?", "selftext": "I understand how with dividend stock you can reinvest your earnings, thus starting the process of compounding, but how does one achieve this with index funds?\n\nI keep hearing about it, but I can't grasp the idea of how this is possible. If I buy one VOO share, then after 10 years, I still only have that 1 VOO share. Yes, it would grow in value with lets say 10% yearly, but with what earnings could I compound my share?\n\nExample: Let\u2019s look at an index fund for which you paid $10,000, assuming it earns 10% per year:\n\n* Year 0: $10,000\n* End of Year 1: $10,000 + (10% x $10,000) = $11,000\n* End of Year 2: $11,000 + (10% x $11,000) = $12,100\n\nSource: [https://www.quora.com/Is-it-possible-to-compound-an-investment-in-an-index-fund](https://www.quora.com/Is-it-possible-to-compound-an-investment-in-an-index-fund)\n\nWhat does the index fund \"earn\" that can be reinvested?\n\nI hope I have asked my question clearly enough. What am I missing? \n\nThanks in advance!", "upvote_ratio": 0.6, "id": "t3_twxm95", "created_utc": 1649172613.0}
{"sub": "investing", "title": "How do I do financial projections for DCF?", "selftext": "hi, \n\nI've recently been picking up the basics of DCF as a way to value stocks and I'm clear on the details that go into the model. A big part of the model comes from the assumptions in revenue / expenses / earnings projections into the future.\n\nWhat can I do to get better at making rational financial projections?\n\nAlso, where can i get more details about analyst financial projections into the future (and even their reasoning behind these numbers if possible?)\n\n&amp;#x200B;\n\nThanks!", "upvote_ratio": 0.63, "id": "t3_twvtd0", "created_utc": 1649167756.0}
{"sub": "investing", "title": "Business cycle leading indicators?", "selftext": "Hi guys,\n\nAnyone ever come across a whitepaper or research on leading indicators for the business cycle and how predictive they've been over time? You always hear about the inversion of the yield curve as one major indicator and then there are a million different interpretations of other factors that you'll hear from talking heads on TV.\n\nWondering if anyone's come across anything else that's been useful? Doesn't have to be a white paper, would be happy to hear people's thoughts on other things they watch and any data to support.", "upvote_ratio": 0.44, "id": "t3_twuqsn", "created_utc": 1649164649.0}
{"sub": "investing", "title": "The FCC has authorized Amazon's \"Project Kuiper\", or broadband-satellite internet unit to deploy 3,2336 satellites over a five-year stretch. 50% must be operational by July 2026 or else Amazon could lose rights.", "selftext": "[Amazon to Spend Billions on Space Launches as SpaceX Ramps Up Satellite-Internet Service](https://www.wsj.com/articles/amazon-to-spend-billions-on-space-launches-as-spacex-ramps-up-satellite-internet-service-11649156400?mod=hp_lead_pos2)  \u2013 *The Wall Street Journal*  \n   \n* Blue Origin (Bezos' company) to conduct 12-27 launches  \n* United Launch Alliance ([Boeing [$BA]](https://finance.yahoo.com/quote/BA?p=BA&amp;.tsrc=fin-srch) &amp; [Lockheed Martin [$LMT]](https://finance.yahoo.com/quote/LMT?p=LMT&amp;.tsrc=fin-srch)) to conduct 38 launches  \n* Arianespace SAS (French) to conduct 18\n  \n[Amazon [$AMZN]](https://finance.yahoo.com/quote/AMZN?p=AMZN&amp;.tsrc=fin-srch) doesn't seem to be up on the news, priced in?  \n  \nThis seems pretty massive and expensive, but few companies besides Amazon could pull this off. I find it interesting that Blue Origin is performing fewer launches than some competitors. I wonder if this is to avoid FCC or other agency criticism for self-dealing or anticompetitive behavior (though this is probably legal). It may just be that Bezos hasn't developed Blue Origin as much as I'd thought.", "upvote_ratio": 0.76, "id": "t3_twuqe6", "created_utc": 1649164615.0}
{"sub": "investing", "title": "[Twitter CEO Parag Agrawal] I\u2019m excited to share that we\u2019re appointing @elonmusk to our board!", "selftext": "Full tweet is \"I\u2019m excited to share that we\u2019re appointing @elonmusk to our board! Through conversations with Elon in recent weeks, it became clear to us that he would bring great value to our Board.\"                      \nHe goes on to say.                       \n\"He\u2019s both a passionate believer and intense critic of the service which is exactly what we need on @Twitter, and in the boardroom, to make us stronger in the long-term. Welcome Elon!\"              \n[Here is a link to the tweet.](https://twitter.com/paraga/status/1511320953598357505?s=21&amp;t=5xpJeKF-FsakkYSPvLeaNg)\n                  \nMusk tweets, \"Looking forward to working with Parag &amp; Twitter board to make significant improvements to Twitter in coming months!\"        \n\n[If you missed it.](https://reddit.com/r/stocks/comments/tvyoxd/elon_musk_takes_92_passive_stake_in_twitter/) Yesterday It was announced of Musk taking a large stake in Twitter.\n              \nHow does everyone feel about this?", "upvote_ratio": 0.88, "id": "t3_twudj4", "created_utc": 1649163558.0}
{"sub": "investing", "title": "Historical accuracy of the big name banks - is there a resource", "selftext": "I keep seeing all the big banks commenting one way or another about a market crash, or rally...  and there's always another contradicting comment somewhere...  so I was wondering if anyone knows of a resource that tracks this sort of thing...  so you can look back at previous volatile times and see which got it right more or less of the time...  \n\n\nRight now for example:  \nMorgan Stanley - Bear market rally is setting stage for a correction  \nJPMorgan - S&amp;P 500 Will Stall, Then Rise to New Rally Highs", "upvote_ratio": 0.67, "id": "t3_twuarq", "created_utc": 1649163315.0}
{"sub": "investing", "title": "Why is borrowing to invest frowned upon?", "selftext": "I get it, but I don\u2019t. People take out $50k loans to purchase a vehicle like it\u2019s no big deal. If you took out that same amount to put in the stock market people would think you\u2019re insane. Like a vehicle will most certainly lose value, while a stock has at least a chance to go up in value while also potentially paying you a dividend. What am I missing?\n\nEDITS - \n1. Thank you all for the good discussion. For the record, I do not use margin in my personal investing adventures. \n\n2. I agree, taking out a loan of $50k for a care seems insane.", "upvote_ratio": 0.72, "id": "t3_twtlxv", "created_utc": 1649161048.0}
{"sub": "investing", "title": "Taking out a personal loan to invest into S&amp;P 500", "selftext": "&amp;#x200B;\n\nHello everyone,\n\nI have a serious question, someone from my bank contacted me about personal loans and they're giving them out at 6.5% Interest, for the sake of the question lets assume I take out a 20k loan at 6.5% for 5 years, the repayment amount works up to 23,514 including bank commissions. Is it logical to take this 20k, invest it into S&amp;P 500 for the 5 years? Assuming at 10% growth rate i'll have around 32k by the end of the 5 years with a profit of around 10k.\n\nDo people actually do this? What are the downsides of a strategy like this? Other than the collapse of the stock market.", "upvote_ratio": 0.24, "id": "t3_twrc49", "created_utc": 1649152539.0}
{"sub": "investing", "title": "Investing tool with some requirements", "selftext": "Hello dear community,\n\nmaybe someone could recommend me a investing tool, that list stocks and especially Investment fonds, which shows:\n- all investing fonds which are allowed in Europe/ Germany\n- rank them and give a ranking at least for the last 10 years\n- compare/plot two or more charts\n\n\nOpen source is always welcomed, but I doubt there won\u2019t be any. Anyway the price isn\u2019t that important. A nice gui would be good as well. \nHope someone can help me.\nThanks in advance!", "upvote_ratio": 0.57, "id": "t3_twqtc2", "created_utc": 1649150242.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - April 05, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.76, "id": "t3_twqlqo", "created_utc": 1649149268.0}
{"sub": "investing", "title": "Discrepancies between YTD performances", "selftext": " Is YTD performance from Jan 3rd? Or from Dec 31st?  \n\n \n\n Google stock chart for TSLA says YTD performance is -4.53% \n\nSchwab says TSLA has a positive YTD performance of 8.39%. The only way to get that metric it seems if you go from Dec 31st. Why the discrepancy?", "upvote_ratio": 0.57, "id": "t3_twmjhj", "created_utc": 1649132751.0}
{"sub": "investing", "title": "is this a bear case for oil.", "selftext": "Russia is offering India Ural grade oil at $35 per barrel and it's unlikely that the US will be able to sanction India for buying Russian oil if it can't prevent Europe from buy Russian gas.\n\nIndia is one of the world's largest importers of oil. It produces less than 20% of the oil it consumes and last year imported just 2% from Russia.  In 2021, the US was among it's biggest suppliers. With Brent at $100+ in 2022, it seems like a no brianer to me that India will increase it's consumption of $30 Ural oil while reducing it's purchases of Brent.\n\nAt 4.5 million barrels a day, if India substitutes just 10% of their consumption of Brent with Ural that would add half a million barrels of oil back in the western markets.\n\nExpanding the case to China which imports 7.5 million barrels of oil mainly from The Saudis and Russia. Again at $35 it would be unlikely that China won't substitute Arab Light with Ural. Again as a base case China taking in additional 10% would put 750k barrels of OPEC oil back on the market.\n\nOn a high level,   I see one clear difference between the gas crisis of the 70s and vs today. There isn't actually a lack of oil.  The supply chain issue combined with sanctions has created an inefficiently in the market but this will eventually even out as state actors with leverage take advantage of the arb. \n\nTell me why I'm wrong", "upvote_ratio": 0.74, "id": "t3_twl8fa", "created_utc": 1649128334.0}
{"sub": "investing", "title": "My business is thriving and I need to start investing. How should I diversify?", "selftext": "My business is doing very well. Last year we grossed $760k and after expenses kept $500k. Deducting taxes and contributing to a SEP IRA ($53k) I will be left with around $350k. We have recently hired a financial advisor who will manage the SEP and other money we will give him. \n\nWe are already involved with real estate investments and would like to know how else could we diversify outside of the money market with the financial advisor. \n\nWhat do you suggest? I am 33 and my risk tolerance is pretty high. Our overall monthly expenses equal $3500. Thank you in advance.", "upvote_ratio": 0.33, "id": "t3_twl525", "created_utc": 1649128027.0}
{"sub": "investing", "title": "After seeing Dark Waters would you invest in DuPont?", "selftext": "\u201cDuPont started conducting cancer studies in 1988. The company\u2019s own studies showed that exposure to C8 killed rats, dogs and monkeys, by causing testicular cancer, liver disease and pancreatic disease. Not only did DuPont continue to manufacture Teflon, but it also continued to dump the chemical into waterways.\u201d\n\nThey still make this shit to this day and it\u2019s despicable.\n\nFirst paragraph Info is from: https://www.organicconsumers.org/blog/devil-we-know-how-dupont-poisoned-world-teflon", "upvote_ratio": 0.85, "id": "t3_twk20t", "created_utc": 1649124636.0}
{"sub": "investing", "title": "Are school stock competitions good for teaching kids and teenagers about stocks?", "selftext": " \n\nAfter seeing another post about a school stock competition, it reminded me of the \"feud\" on whether YouTuber boxing is good for the sport.\n\nOn one hand, introducing kids to finance and stocks is good because it (hopefully) destigmatize fears and misconceptions about the stock market. This could lead to better financial literacy and independence in the future.\n\nOn the other hand, however, it may encourage risky plays over slow and steady in order to \"win\" the competition (I.E most money after a year/semester = win). If a random kid throws the fake money into a random penny stock that 10x it must be super easy in real life to make money right?\n\nWhat do you guys think, is this all in my head?", "upvote_ratio": 0.77, "id": "t3_twi7wy", "created_utc": 1649119241.0}
{"sub": "investing", "title": "Hertz to buy up to 65,000 electric vehicles from Polestar", "selftext": "WASHINGTON (Reuters) -Rental car firm Hertz Global Holdings said on Monday it would buy up to 65,000 electric vehicles over five years from Swedish EV maker Polestar, the latest move by the rental car firm to add zero-emission models.\n\nHertz said Polestar cars would be available beginning this spring in Europe and later in 2022 in North America and Australia.\n\nThe Florida-based rental car company said that it would initially order the Polestar 2 sedan. Hertz shares were up 1.75% in premarket trading Monday.\n\nHertz in October announced its order to purchase 100,000 electric cars from Tesla Inc, primarily the EV maker's Model 3.\n\nIn March, Hertz added Tesla's mid-size SUV Model Y to its electric vehicle fleet, according to the car rental firm's website.\n\nPolestar, which was founded by China's Geely and Volvo Cars, is set to merge with special purpose acquisition company: Gores Guggenheim Inc, this year.\n\nThe Hertz partnership \"will bring the amazing experience of driving an electric car to a wider audience, satisfying a broad variety of our mutual customers' short- and longer-term mobility requirements,\" Polestar CEO Thomas Ingenlath said in a statement.\n\n[Source](https://finance.yahoo.com/news/hertz-add-65-000-polestar-115358883.html)\n\n-Hertz is up +10,70% on this news today, and Polestar($GGPI) is up +11,87% today.", "upvote_ratio": 0.94, "id": "t3_twhu4v", "created_utc": 1649118195.0}
{"sub": "investing", "title": "If Roth contributions can be accessed anytime, why should I as a 20-year-old invest in taxable account?", "selftext": "Hello, so lately, I've been thinking of liquidating my regular taxable account and putting all my money from now in a roth account. My reasonings are:\n\n1. dividend and profit are not taxed at all, so I don't have to worry about selling or accruing dividends (without withdrawing, of course)\n2. contributions can be accessed anytime, so if I need emergency funding, I will still have penalty free access.\n\nAm I thinking this, right? I treat my investment money as a retirement or future income anyway and don't have any intention in accessing them as I keep a separate emergency fund savings account. Is there any benefit I'm not thinking regarding a taxable account aside from I can liquidate and withdraw my whole portfolio anytime?", "upvote_ratio": 0.88, "id": "t3_twhh3d", "created_utc": 1649117259.0}
{"sub": "investing", "title": "publicly available info on Elon Musk's stock purchases", "selftext": "What publicly available info is out there to see when Musk purchase significant stakes in public companies? I know there are the SC 13G / 13D forms that report when someone takes a 5% or greater ownership % in a company. Are there other forms or publicly available info people look at? Basically, trying to see if there is something I can track to capitalize next time Musk invests big in a company. Right now my thought is a simple bot that alerts me next time there is a SC 13G form filed for Musk, but wondering if there is a better source for that info.", "upvote_ratio": 0.65, "id": "t3_twgnbv", "created_utc": 1649115147.0}
{"sub": "investing", "title": "$GT Goodyear Tire needs to turn on its dividend again\u2026", "selftext": "With the addition of Cooper Tire and it\u2019s robust EPS, Goodyear needs to turn on the dividend again, even in a small way. It seems as if Goodyear\u2019s management is too hyper focused on paying down its debt, which is good in the long run, but isn\u2019t that great in the short term. A balanced approach of both short and long term, would better maximize shareholder value. Either way, look for a robust revenue and earnings beat later this month. $GT should be $20, not $14.", "upvote_ratio": 0.42, "id": "t3_twdmf5", "created_utc": 1649107727.0}
{"sub": "investing", "title": "Risk involved with investing in CFD.", "selftext": "Im really interested in investing, currently I'm looking for brokers in Poland, and I have  \nsome questions about CFD which are hard to find on web. So I know a little about it but  \nI don't really know what risk is there, because it's a contract, you don't own any shares   \nSo my question is if CFD works like:  \n\n\nU buy 100$ long position contract (if it goes up you earn if it goes down you lose.) then what happens when it goes down to - number for example - 200$ is your balance on minus, and you can continue waiting or is your balance on minus and you automaticly lose the money?\n\nwhat happens", "upvote_ratio": 0.57, "id": "t3_twb1ak", "created_utc": 1649101406.0}
{"sub": "investing", "title": "vanEck analysis: Gold &amp; BTC potentially undervalued dramatically", "selftext": "Came accross this article of vanEck, performing analysis on Gold and BTC as reserve assets. \n\n[https://www.vaneck.com/us/en/blogs/emerging-markets-bonds/how-one-bond-manager-values-gold-and-bitcoin/](https://www.vaneck.com/us/en/blogs/emerging-markets-bonds/how-one-bond-manager-values-gold-and-bitcoin/)\n\nThe analysis suggests that gold is heavily undervalued if it were to become the main reserve asset. Central banks hold relatively little gold, which means they would have to buy massive amounts of gold to back/stabilize their currency in case of a financial crisis. They made a similar analysis for BTC. However, I think the analysis for gold is more realistic as I am not so sure countries will massively start adopting BTC right now. \n\nEven if the price of gold and BTC reach 10% of the mentioned targets, they would turn a nice profit. \n\nDo you invest in gold/BTC or you stick with fiat money for your holdings?", "upvote_ratio": 0.21, "id": "t3_twa5wj", "created_utc": 1649099233.0}
{"sub": "investing", "title": "What's a good (slim) intro book or website on corporate law?", "selftext": " What's a good (slim) intro book or website on corporate law?\n\nMost stock owners don't really seem to have any idea of what rights they have as a stock owner, and what rules the corporation is governed by. I find this troublesome since if you don't understand your rights as a shareholders, you're likely just looking at the market as a casino. On the other hand I don't want to read some voluminous book on corporate law. Is there a nice intro on the subject?", "upvote_ratio": 0.67, "id": "t3_tw7tnz", "created_utc": 1649093525.0}
{"sub": "investing", "title": "Should I rebalance my account and sell Energy ETFs", "selftext": "About a year ago I sold ARKK (at a sizeable profit) and diversified into an energy ETF (FENY -  FIDELITY MSCI ENERGY INDEX ETF ) and some commodities. The commodities have been a mixed bag but  FENY did really well - about 51% increase including dividend reinvestments.  \n\nWith value stocks starting to have a rebirth, and energy now reaching levels not seen for 3 years, is this a good time to take some profit and re-invest in growth stocks, or just stay put and reap the dividend reinvestments?  Just wondering what the general thought is on energy stocks/ETFs right now.", "upvote_ratio": 0.43, "id": "t3_tw6c5i", "created_utc": 1649089901.0}
{"sub": "investing", "title": "If you are up %50-90% on all your investments, what is the next move??", "selftext": "I am currently up %50 to %90 on all my investments. I wait during the week and buy the dip on ZSP and XEQT on the tsx \n\n( I buy 1-2 a week of each no matter what ) but what is the next move? Since alot of stocks are near all time high and I am up on all of my stocks and don't want to buy more as they are near a all time high also, and it will ruin my cost average. I have cash sitting in my portfolio doing nothing.\n\nwhat is the next move? what would you guys do?\n\n&amp;#x200B;\n\nAQN  + 66%\n\nBMO  + 94%\n\nENB  + 77%\n\nFTS  + 66%\n\nMFC  + 52%\n\nREI   +70%\n\nT  + 55%\n\nApple  + 34%\n\nZSP and XEQT only up about 15%  but i buy these no matter what during the week, they account for 30% of my total portfolio", "upvote_ratio": 0.42, "id": "t3_tw52ai", "created_utc": 1649086785.0}
{"sub": "investing", "title": "Is there a way to indirectly invest in Airbnb properties through a fund?", "selftext": "Would like to allocate a portion of my portfolio to short term rentals/airbnbs but wondering if there may be a fund to do this indirectly vs directly buying and managing the properties - like a REIT. Have done some googling but haven\u2019t found anything. Appreciate any insights. Thanks!", "upvote_ratio": 0.6, "id": "t3_tw4knt", "created_utc": 1649085560.0}
{"sub": "investing", "title": "Am I too Conservative with my portfolio?", "selftext": "Hi everyone, I'm 21 just started a new job and finished my emergency fund and maxed out my Roth IRA for this year. My newest long term goal is to buy a house in about 8-10 years. My current investment portfolio is QQQ (50%) VOO(25%) BERK.B (25%) in my brokerage fund/ house fund. For some reference I basically only hold VOO in my ROTH IRA, and have maxed it out both this year and last year. I feel that the returns are good but not and good as QQQ. Not really looking for dividends just growth. How does this portfolio look for the the next 8-10 years.", "upvote_ratio": 0.61, "id": "t3_tw4379", "created_utc": 1649084332.0}
{"sub": "investing", "title": "Business model of Motley Fool premium and similar services?", "selftext": "I do not get Motley Fool's premium service. \n\nWhat keeps people from posting Fool's stock recommendations on their blogs, podcasts or here on Reddit? Obviously, customers cannot literally copy/paste the recommendations, but they could simply post many of the recommendations here without stating it is from Fool. Why does Fool have 1 million paying customers?", "upvote_ratio": 0.5, "id": "t3_tw41xe", "created_utc": 1649084238.0}
{"sub": "investing", "title": "Does having a LISA impact buying property through a Ltd?", "selftext": "\n\nOkay, so a little context - I recently set-up a LISA which provided a government surplus so long as the savings are used exclusively for the purchase of a first home under a residential mortgage.\n\nWhat I am wondering however, is whether this would become void in any way if I chose to buy property through a limited company. My feeling is it wouldn\u2019t, since they are both distinct legal entities, but I just wanted to get some feedback.\n\nKind Regards,\n\nDaisy", "upvote_ratio": 0.5, "id": "t3_tw3cnf", "created_utc": 1649082434.0}
{"sub": "investing", "title": "CAGR calculation basic question", "selftext": "Hi all,\n\nSilly question for a lot of you I'm sure but for a long time I've been calculating CAGR, on a period from say March 2013 to March 2022 (10-years worth of data), with 10 being the number of years in the calculation.\n\nHowever, recently I picked up a copy of Phil Town's Rule 1 and Payback Time and it wasn't quite adding up. Then I read where he says that he subtracts 1 from the 10 years, so instead of the numbers of years in the calculation being 10 it would be 9. I also found it in his PDF here: [https://www.ruleoneinvesting.com/ExcelFormulas.pdf](https://www.ruleoneinvesting.com/ExcelFormulas.pdf)\n\n&amp;#x200B;\n\n&gt;*\"If we have a 10-year average growth rate, then you add up the number of years worth of data you have (10) then subtract 1. This means you would put a 9 in the nper section...\"*\n\n&amp;#x200B;\n\nDoes that sound right to you all? Have I been doing it wrong all this time?", "upvote_ratio": 0.25, "id": "t3_tw38wl", "created_utc": 1649082183.0}
{"sub": "investing", "title": "Cash Alternatives in a Cash heavy portfolio", "selftext": "I've created a chart that (I think) shows the relative purchasing power of US Dollars, Gold, and shares of STIP. I have a large cash allocation because much of my portfolio is leveraged. I'm trying to decide if some or all of my cash position should be in STIP shares. Obviously there's interest rate risk, but since that mostly seems linked to the Fed fighting higher than expected inflation, I'm having trouble deciding how much of an issue that is.\n\n[https://www.tradingview.com/x/p39KkXkX/](https://www.tradingview.com/x/p39KkXkX/)", "upvote_ratio": 0.33, "id": "t3_tw1i2i", "created_utc": 1649077407.0}
{"sub": "investing", "title": "FNGS - up 5% pm. FNGU - up only 3.5%. FNGU is literally just FNGS but triple leveraged. What's going on here?", "selftext": "It's not like the price hasn't had time to catch up or anything. It's been like this for like an hour or 2 and I can't figure out why. It should be around 15%, shouldn't it??   \n\n\nI bought this as a x5 leverage CFD on Friday, so I'm hitting my head against the wall here wondering what on earth I got wrong about it.", "upvote_ratio": 0.67, "id": "t3_tw11fe", "created_utc": 1649076039.0}
{"sub": "investing", "title": "Market Out-Performers in the 1970's", "selftext": "As per the title, wondering if anyone can list some companies/industries that out-performed during the 1970's? Many people suggest we're seeing a parallel in today's economy to that of the 1970's stagflation period. In this case, industries that performed well during this time may surely perform well today.\n\nAppreciate any answers.\n\nThanks,", "upvote_ratio": 0.5, "id": "t3_tw0m41", "created_utc": 1649074702.0}
{"sub": "investing", "title": "Twitter is up 26% pre-market as Elon Musk reveals a 9.2% stake in the company (now the largest shareholder of Twitter)", "selftext": "Per Bloomberg data, Musk\u2019s 9.2 per cent Twitter stake would make him the largest shareholder in the company. Notably it\u2019s more than quadruple the 2.25 per cent position of founder Jack Dorsey. At pixel time, Twitter\u2019s stock is up 25 per cent in pre-market trading to $49.09. \n\n[https://www.ft.com/content/29b9c884-02d7-4d1c-a4ab-c862242fa76e](https://www.ft.com/content/29b9c884-02d7-4d1c-a4ab-c862242fa76e)", "upvote_ratio": 0.94, "id": "t3_tvyx7l", "created_utc": 1649068916.0}
{"sub": "investing", "title": "Advice for Euro invester, what to do with cash?", "selftext": "So after a long time I have no idea what to do, here is a bit summary of past months on my side:\n\nAfter extraordinary military buildup around Ukraine borders, I was kind of sure Russia will invade it unlike doubters, so I invested accordingly (oil and grain, basically commodities that Russia and Ukraine produce most), then when invasion started I cashed in and started to collect Euro shares that lost quite a bit value. Now I am cashing them in gradually since peace talks in Turkey they are gaining quite a bit value.\n\nSo now I ended up with with some cash in my hand and I don't want to keep it as Euro, because ECB looks like very slow reacting to inflation, so every cash you hold will lose value surely and believe me I know Europe, they will think 10 times before moving a finger against CPI then they will be late as usual. I don't want to re-invest in stocks, they are likely perform similar to inflation at this point and I don't have too much trust in myself about finding an outperforming individual stock, so what to do, is there really a good alternative, may be I should buy some real estates (joking)? I have access to US and Euro markets due to some work restrictions, so my opportunities are limited with those. So what's the best idea, when you don't know what to do with cash?", "upvote_ratio": 0.5, "id": "t3_tvy3hb", "created_utc": 1649065729.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - April 04, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.89, "id": "t3_tvxek1", "created_utc": 1649062869.0}
{"sub": "investing", "title": "Everyone talks about the \u201cstrong job growth numbers\u201d but hasn\u2019t gig-work made those numbers impossible to calculate?", "selftext": "I\u2019m talking about people that do Uber, Lyft, Doordash, Postmates, Ebay, Amazon, Etsy, etc. Let\u2019s say someone gets a full time job at Walmart and also starts doordash on the weekends and sells random things on eBay. Does the government calculate that 3 jobs were \u201ccreated\u201d that month? Is there a minimum someone has to make at a gig-job for it to be calculated as a job?", "upvote_ratio": 0.82, "id": "t3_tvude8", "created_utc": 1649050409.0}
{"sub": "investing", "title": "Invest in real estate in form of funds on the exchange", "selftext": "Hi, \n\nI am looking to invest in real estate in the form of funds on the exchange. (Not buy physical property)\n\nI have heard REITs are a good way to invest in real estate. Are there any good ones out there?\n\nOr any funds which tracks these together?\n\nAny help is appreciated.  \n\nThanks,", "upvote_ratio": 0.44, "id": "t3_tvu3g4", "created_utc": 1649049459.0}
{"sub": "investing", "title": "Pimco MINT etf alternatives", "selftext": "Mint pimco etf has been loosing a lot. Inflation is probably the reason. I have been holding it for a while, but not sure if I should continue holding it given the current trend. What are the alternatives, do you think it is going to go down further\n\nhttps://finance.yahoo.com/quote/mint/", "upvote_ratio": 0.38, "id": "t3_tvt1oq", "created_utc": 1649045995.0}
{"sub": "investing", "title": "Buy high yield Emerging Market country bond and hedge back to USD?? Is it possible?", "selftext": " In today crazy high Market. It is better to buy fixed income rather than Overvalued stock.  \nI have Idea. Just buy any high yield Emerging Market country bond and Hedge back using USD.  \n\n\nLike Brazil Government Bond yield 11.0200%. If I buy one and Hedge back to USD. By doing this you get 11.0200% minus whatever the hedging fee/commission .Keep in mind that you get this Return on USD. Is it possible? Or Am I missing something?.  \nHow to do this?  \nIs any bank or financial Institute provide this type of service?  \nor Even Is it good Idea??? \n\n&amp;#x200B;\n\n As A bond investor do you try this type of setup?  \nCan you put some light on this Idea? Possibility ? which Banks???  \n\n\nThanks", "upvote_ratio": 0.44, "id": "t3_tvrarl", "created_utc": 1649040494.0}
{"sub": "investing", "title": "Changing company\u2019s and need info !", "selftext": "I started a Roth IRA when I was 18 and I\u2019m 20 now and I\u2019m looking to move from the company vanguard to M1 Finace because I don\u2019t like Vanguards interface and find it hard to actually keep track of performance and looking for something more user friendly like M1 (that I use already) and looking for advice on how I can do that or if it\u2019s even possible thank you in advance !", "upvote_ratio": 0.36, "id": "t3_tvqqwn", "created_utc": 1649038862.0}
{"sub": "investing", "title": "Would you invest money earmarked for a rental property into ETF's (VTI, VXUS, BND \u2014 50/35/15 allocation) right now?", "selftext": "I have some capital saved up and can just feel my savings evaporate in the midst of inflation and waiting for a cash flowing property. I can see myself buying in 2-5 years time, perhaps sooner. With that timeframe in mind, would you put that money into ETF's right now? Curious to get folks' thoughts on whether there's a 'min' investment horizon before it generally starts turning into a bad idea to invest downpayment money into stocks. My biggest fear is needing to sell in a downturn to finance the house and getting less than what I've put in. Thank you all for the input. Feel free to trash on my allocations as well!", "upvote_ratio": 0.66, "id": "t3_tvprpq", "created_utc": 1649036022.0}
{"sub": "investing", "title": "The 2Y/30Y Yield curve is Inverted", "selftext": "At the time of this writing, the 2y/30y yield is inverted (1-2 basis points).  \n\nSource: [https://www.cnbc.com/quotes/30Y2YS](https://www.cnbc.com/quotes/30Y2YS)\n\nIt's historically rare and usually happens near the end of a rate hike cycle by the Fed.  I know it's preceded some significant contractions in the economy, but the inversion near the start of the rate hike cycle makes it a little more concerning.  Just feels like the next 12-18 months is going to be volatile.", "upvote_ratio": 0.86, "id": "t3_tvpf7l", "created_utc": 1649035004.0}
{"sub": "investing", "title": "Investing account under my mom's name or brothers?", "selftext": "I've been having trouble deciding whether or not to open a long-term investing account for my younger brother (minor) under his name or my mom's name? \n\nThe reason I bring my mom into this is that I don't want my younger brother's FAFSA to be affected when it comes time for him to go to college. Which will then lead to whatever college he would like to attend to fork over his invested money/pull out loans.\n\nMy mom does not work and the money would be used later for my brother to do what he would like when he becomes of legal age. (Hopefully continues to invest, buys a property, etc)\n\nAre there any drawbacks to worry about? Any information that I should be aware of?", "upvote_ratio": 0.6, "id": "t3_tvnoy3", "created_utc": 1649029929.0}
{"sub": "investing", "title": "Who\u2019s adding to their Tesla portfolio before the split?", "selftext": "(Reuters) - Tesla Inc on Saturday reported record electric vehicle deliveries for the first quarter, driven by a ramp up in production at its Shanghai factory.\nTesla delivered 310,048 vehicles in the quarter, while Wall Street had expected deliveries of 308,836 cars, according to Refinitiv data.\nTesla said it sold a total of 295,324 Model 3 sedans and Model Y sport utility vehicles.\nIt delivered 14,724 Model S luxury sedans and Model X premium SUVs, up from 11,750 in the fourth quarter of 2021.\nAnalysts expected Tesla to deliver 15,066 Model S and Model X cars and 280,000 Model 3 and Model Y vehicles.", "upvote_ratio": 0.38, "id": "t3_tvnl5b", "created_utc": 1649029616.0}
{"sub": "investing", "title": "Retirement investing for a parent", "selftext": "I have opened my mom\u2019s eyes to the predatory fees and tactics of Edward Jones and she will be transferring to Fidelity. I feel comfortable making my own investment decisions for myself, but I\u2019m not a financial professional and don\u2019t know much about asset allocation for a retired person. \n\nHere\u2019s her portfolio: 500k AAPL, 75k KO and 500k variety of expensive EJ proprietary mutual funds. \n\nI don\u2019t think she should touch the apple or coke due to high cap gains and I think they\u2019re great stocks\u2026but the mutual funds will all need to be sold before transferring. What to do with 500k to produce most income and limit overlap with her other 2 biggest positions?\n\nThank you for any insights", "upvote_ratio": 0.7, "id": "t3_tvmxdv", "created_utc": 1649027726.0}
{"sub": "investing", "title": "Shares of American Funds target date for simple Ira being bought at higher than market price", "selftext": "Why is the guy who manages my simple Ira buying shares of American funds target date fund at a higher price than its currently being traded at? Is there a reason/strategy for this? Every statement shows shares being bought at a higher price than the market price", "upvote_ratio": 0.5, "id": "t3_tvm7fp", "created_utc": 1649025745.0}
{"sub": "investing", "title": "Leave my Funds in Traditional EJ Advisor Account vs Move to Robo Advisor?", "selftext": "Greetings Friends.\n\nI am in need of advice on what my best course of action is here. I have an Edward Jones account that had been created by a relative when I was a child. She invested in it regularly as Christmas and birthday gifts for years while I was growing up. She passed away a few years ago, and I am now graduating from college and beginning my career. I have not done anything with the account since she passed, and to be frank, neither has the advisor managing it.\n\nI have between 5-10k in the account, so I understand that it is not a large priority for the advisor. I am wondering if I am better off liquidating that account and reinvesting that money into either a Vanguard Digital Advisor or Schwab Robo Advisor that will have lower fees, a more diverse portfolio, and be more actively managed. I am looking for long term growth with a moderate to high risk.\n\nThanks", "upvote_ratio": 0.56, "id": "t3_tvm4dc", "created_utc": 1649025526.0}
{"sub": "investing", "title": "Best method to invest in gold?", "selftext": "Hi, \n\nIs there a safe and diversified way to invest in gold? \nLike an index fund tracking the s&amp;p500 is a nice way to invest in the entire market, is there something comparable for gold?\n\nI have heard of golf ETF like GLD, is there a different and or better way than that?\n\nThanks.", "upvote_ratio": 0.62, "id": "t3_tvlwat", "created_utc": 1649024920.0}
{"sub": "investing", "title": "Regular Account to Retirement Account Wash Sale", "selftext": "Hi, \n\nAs far as I understand, when you sell a security at a loss and buy it back within 30 days you can not count the loss on taxes for that year, but have to wait until next year. A way that brokers 'balance the books' (I use Merrill Edge) is by increasing the cost basis of the repurchased share by the loss amount. \n\nIf I were to sell my shares at a loss and repurchase them in my Roth IRA account I will also incur a wash sale penalty, but how will the loss be accounted for if the shares are in my retirement account, and thus their cost basis does not matter for taxing purposes?", "upvote_ratio": 0.56, "id": "t3_tvhoot", "created_utc": 1649014477.0}
{"sub": "investing", "title": "ETF's long term investment for early retirement", "selftext": "Hi!  \nWe're both 30 and are looking at what to invest in to start an early retirement account and a safe 15-20 year investment for a child.  \nWe already max out 401k and IRA so now looking at taxable brokerage for retirement at 3k monthly contribution.  \nWe have a child due this year that we'd like to start at 500/monthly contribution as an idea they can use this for a down-payment. We will also be starting a college fund.  \n\n\nWe have some riskier investments from college that have done well and we haven't touched so I'm looking for both of these to be more stable.\n\nWe have about 70k in variable tech like AMD, AAPL, NVDA, OLED etc. Some people have recommended slowly moving this into other funds- we have held for more than a year. But I'm not sure that'd be worth the capital gains to do vs just sell when we want the money out?\n\nFor long term we have about 20k in SCHB but my plan is to do the following for early retirement:\n\n70% SCHB  \n20% SCHF  \n5% SCHC  \n5% SCHE\n\nFor child I have no idea so I'd appreciate some help in that one.\n\nThank you!  \n\n\nEDITED To add: Brokerage is through Schwab. I think I can also buy vanguard there with no fees but they seemed to preform similarly so I was planning to stick with just schwab based. If i am wrong on this please let me know!", "upvote_ratio": 0.75, "id": "t3_tvhk5d", "created_utc": 1649014186.0}
{"sub": "investing", "title": "Deferred Loss and Adjusted Cost Basis - an example. Can someone help me understand", "selftext": "from https://www.thebalance.com/wash-sale-rule-3192972\n\n&gt;The amount of an investor's loss is added to the cost basis of the replacement investment when the wash sale rule is triggered. This defers the loss until a later date when the replacement investment is eventually sold off.\n\n&gt;You sold 50 shares of XYZ stock for $5 per share for $250 total on July 31, incurring a $250 loss, then you purchased 50 shares of XYZ stock on August 15 for $6 per share, or $300 total. August 15 is within the 61-day wash sale period, so your $250 loss on July 31 was a wash sale, and your loss is added to the cost basis of your new investment.\n\nQuestion 1: Why does it say 61-day wash sale period? I thought it was 31-day. \n\n&gt;Your adjusted basis in the replacement shares is now $550\u2014$300 from your August 15 purchase combined with your $250 loss from the July 31 sale. Your loss is a \"wash\" in this scenario, just as though you had held your original shares without selling.\n\nQuestion 2: $550 is the new cost basis. Is this $550 figure derived by adding your wash disallow amount ($250 in losses) + $300 (purchase price) = $550 cost basis?\n\n&gt;The tax benefit of your capital loss isn't gone forever, but it's deferred. The loss on the original investment will be taken into account when you sell your replacement shares by applying the losses to your adjusted cost basis.\n\nIf you go to sell your 50 shares at $600 total, then you made a capital gain of $50, correct? ($600 - $550 (adjusted cost)) = $50. Seems fine so far.\n\nAfter you sell the shares, suppose you buy it back within 31 days at $500 again, hoping to sell it at $600. This repurchase won't trigger a wash sale because there were no capital losses at the time of purchase at $500 again. Is this correct?", "upvote_ratio": 0.5, "id": "t3_tvhjxf", "created_utc": 1649014173.0}
{"sub": "investing", "title": "Confusion Determining Book Value", "selftext": "Good Sunday. I'm finishing this lecture demonstrated by Professor James Webb while taking notes on it. And right up to the very end I understood for the most part before being caught up in confusion.\n\n[Breakdown of Ratios](https://youtu.be/Jkse-Wafe9U)\n\nAround 53:00 into that link. Starts a minute prior but this question is covered regardless around 53 minutes. Professor Webb and a student of his agree that the Book Value = **Total Equity** divided by shares outstanding. (His specific words are \"by the number of shares\" which I'm assuming means shares outstanding given the complete lecture) The issue is when I fact check (I check what I'm learning as I go along) through web search for Book Value. I get Book Value = Total Assets divided by shares. Two different equations. I've even seen someone answer **CURRENT** Assets divided by shares.\n\nI'm hoping the more experienced here can provide insight on this one.\n\n&amp;#x200B;\n\nEDIT: Also for about 4 ratios prior to the aforementioned they begin using BOTH current year &amp; previous year to determine them. It just seems so off and out of the blue for me. Could yall take a spin at this one for me too.", "upvote_ratio": 0.72, "id": "t3_tvepbd", "created_utc": 1649007137.0}
{"sub": "investing", "title": "how do ETFs choose their allocation percentage?", "selftext": "My biggest problem with ETFs is that they tend to be too diverse. I want exposure to a sector... But, i don't want 25+ companies. I just to mimic a passive ETF's to 5 or 10 holdings.\n\nThat got me to thinking: how do they determine 4% to this company, 2.8% to that company, etc? I'm not interested in any \"you aren't as smart as them\" comments. I'm still trying to figure out an investing style that works for me, so even if you think its a dumb idea, I'd be interested in understanding how passive ETF indexing works.", "upvote_ratio": 0.25, "id": "t3_tvelt4", "created_utc": 1649006893.0}
{"sub": "investing", "title": "Why is Gerdau S.A. (GGB) trading so close to it's intrinsic value?", "selftext": "I calculated GGB's intrinsic value in two ways.   \n\n\n1: Total assets - Total liabilities / Total shares outstanding = $5.37 per share, currently trading at $6.51 per share.   \n\n\n2: Current assets + Net income - Current liabilities / Total shares outstanding = $6.24 per share", "upvote_ratio": 0.57, "id": "t3_tvelsq", "created_utc": 1649006892.0}
{"sub": "investing", "title": "Inflation and its effect on the stock market", "selftext": "While reading a bit on the ideas to protect money from inflation, I noticed many people have the idea to move their money into the  general stockmarket (e.g. indexes, managed-funds, S&amp;P, NASDAQ). The idea behind this is that the money is \"save\" in the market due to companies in general profiting from increasing prices and therefor increasing in value. \n\nI do not comprehend how this would work, for the following reasons:\n\n1. As long as the stockmarket value does not increase as much as inflation, you still lose value. \n2. As long as salaries do not incease as much as inflation (which currently does not seem the case), this would cause consumers to lose buying power. Anything consumer related thus would start making less profits and drop in value. Any money invested in related stocks would thus lose to inflation and to the stock dropping in value.\n\nPutting your money in indexes/funds might thus work countereffective as you are hit by inflation and the stock market dropping/being stagnant due to consequences of inflation. Looking at the inflationary period from the 1965-83, the S&amp;P was more or less stagnant over this timeframe. You would thus mainly have lost money to &gt;5% y/y inflation in that period in the market (I am not sure how much stocks have paid dividends in those times, that might have compensated for this). Market growth only continued after 1983 (after the insane rate hikes put trough by the FED, putting inflation down). Based on this it, seems that buying the general market is not a good strategy if inflation rages. \n\nI could not really find individual stock prices of companies that might have performed well in 70/80s (if anyone has a source, I am interested). However, judging from commoditiy prices in the timeframe of 1965 to 1985, oil, gold, iron etc (so basically commodities) would have been great investments over that timeframe. \n\nDo you guys think picking investments (e.g. commodities/related stocks) might allow you to outrun inflation, are you sticking with a general market DCA strategy or do you have another strategy?", "upvote_ratio": 0.86, "id": "t3_tvc9yg", "created_utc": 1649000921.0}
{"sub": "investing", "title": "Is there a lot of risk holding on to energy investments 20+ years into the future? (Oil/Gas)", "selftext": "I\u2019m a newer investor. I had a huge chunk of money in my savings that was sitting around doing nothing during the pandemic. I saw that oil prices were so low in Nov 2020 I knew that if we got out of Covid, that the world would be moving again. \n\nLong story short I\u2019m invested in about 5 different oil companies and an energy ETF through vanguard. I am up 120%. \n\n(I do have and hold many other stocks and ETFs Tsla, apple, etc) \n\nI have family that has held energy stocks for 40+ years. And I worry about them and myself if we hold them too long. I will be inheriting them. \n\nMy question is, in 20 years, do you think energy(oil/gas) will be a dead investment? Or do you think even though the energy transition, these investments should be safe? I wonder as these majors are planning other ideas (such as wind farms, etc).", "upvote_ratio": 0.91, "id": "t3_tvbt9t", "created_utc": 1648999672.0}
{"sub": "investing", "title": "Anyone like Whirlpool Stock (WHR) ?", "selftext": "I\u2019ve been watching this since it in the low 200\u2019s. It just hit a 52 week low on Friday, and tempted to buy in if it wasn\u2019t for that pending recession. Pros and cons I see are;\n\nPros: \n- It\u2019s cheap at around ~8 forward P/E and ~19% cashflow to market cap. \n- It has a buyback program with enough in it to buy ~20% of the current shares outstanding. \n- It\u2019s dividend just got big bump from $1.45/qrt to $1.75/qrt, is above its 5 year average at over 4% yield, and very well covered based on earnings. \n\nCons: \n- The revenue growth has been very slow over the years. Earnings have grown mainly via efficiencies &amp; share buybacks. \n- They have several competitors, and no one I\u2019ve seen swears by the product. Seems like appliance purchasers don\u2019t have much brand loyalty so consumer sensitivity to price is probably high. \n- While I like the manufacturing capabilities their supply chain has been hurt by the pandemic like many others. \n\nThoughts?", "upvote_ratio": 0.73, "id": "t3_tvb28k", "created_utc": 1648997673.0}
{"sub": "investing", "title": "What happens to Roth IRA if bank closes in crash", "selftext": "Recently started investing in Roth IRA and I\u2019m wondering if this is something I should have at multiple banks or just one.  Specifically, I\u2019m wondering what happens to my humble assets in case of bank closure from market crash. Is there a bank known for survivability  through such events? \n\n*Yes, I know the total contribution limit per year is $6,000 no matter how many Roth IRA accounts I have open.*", "upvote_ratio": 0.33, "id": "t3_tvaxqo", "created_utc": 1648997337.0}
{"sub": "investing", "title": "ETFs to invest in the world", "selftext": "Hi guys, I'm learning about finance and would like to ask if given 10 choices, which ETFs would you invest in to have the most wide-reaching coverage across different asset classes and countries? (Given that the returns don't matter)   \n\n\nI'm taking a theoretical standpoint in a sense that I want maximum diversification so that I have minimum risk", "upvote_ratio": 0.82, "id": "t3_tvavdl", "created_utc": 1648997155.0}
{"sub": "investing", "title": "How many people actually read the investor reports?", "selftext": "Once I found my winning stock I like to research comparable stocks in the sector and dive into the latest financial report or two including the M and D. How often do people actually read through everything verse not doing any research? A couple friends suggested they pick up stocks based on recommends and the general vision\n\nEdit: There seems to be a greater divide then I anticipated", "upvote_ratio": 0.85, "id": "t3_tv99zz", "created_utc": 1648992518.0}
{"sub": "investing", "title": "Swap &amp; leverage taxes. help needed", "selftext": " Hello guys,\n\nI    use eToro more than 6 months, all good so far. I'm trying to learn   more  things every day, so when possible I attend to online courses   about  investing. Some of them mentioned about diversification in   regards of  broker diversification - don't store all of your eggs in   same place ;-)\n\nThat's  why I   stared searching for a broker which offer the same services as  eToro.   Then I came to XTB - Polish broker, which operates in EU. I  played a   bit with their system, and it looks good to me.\n\nLuckily    they seems to be very transparent, which is good, but I still cannot    understand few things in there. For instance, they have tax called:    adjustment fee and swap fee, but this doesn't make sense to me. Even I    went to their website I cannot understand why this is needed.  \nBecause I haven't seen this anywhere else, I'm wondering if this is acceptable/normal or not.\n\n[swap](https://www.xtb.com/en/swaps-and-charges-kb)\n\nA    yeah, forgot to mention - they do all of your positions with  leverage,   which is also strange for me. In eToro I can choose to go  with  leverage  or not.\n\n[leverage](https://www.xtb.com/en/leveraged-trading-kb)\n\nPlease bare with me, I'm very young in this journey.", "upvote_ratio": 0.5, "id": "t3_tv7m4g", "created_utc": 1648986978.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - April 03, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.79, "id": "t3_tv527k", "created_utc": 1648976468.0}
{"sub": "investing", "title": "Making sense of $GT Goodyear Tire valuation", "selftext": "I\u2019m wondering if someone can better explain to me what happened to $GT Goodyear Tire stock back in mid February. I get they announced free cash flow would drop to basically zero for a little while while they invest in EV tire R&amp;D, but this is a well run company with increasing revenue, and profit, investing in the future. I get that Wall Street can be fickle, but it went from a $7B company to a $4B company overnight? Quite the over-reaction. Shouldn\u2019t this be a $20 stock?", "upvote_ratio": 0.63, "id": "t3_tv0t0z", "created_utc": 1648959803.0}
{"sub": "investing", "title": "VTI and VOO should I invest in both or just one? If just one, which would you recommend for a leave it fund?", "selftext": "I\u2019ve recently started automatically investing about $250 each week split between VTI and VOO. I thought the diversity would be beneficial but im reading that they are composed of  essentially the same stocks. Should i just pick one and invest $500 in it each week? And which one would you recommend? Thanks", "upvote_ratio": 0.91, "id": "t3_tuyr0s", "created_utc": 1648952980.0}
{"sub": "investing", "title": "Non-Tech Growth Stock Ideas?", "selftext": "I will admit I'm more of a dividend investor, but I'm young enough (early 30s) I don't want to fully focus on that in my portfolio. I currently hold Google and Microsoft, and am looking at adding Amazon. \n\nThat very heavily weights me towards tech growth, so I was wondering if you guys had recommendations to look into for other sectors of growth. Whether or not they pay a Div is not super important to me. If you are not comfortable naming specific stocks, info on where you guys get your information you use to research other sectors/companies would be appreciated.", "upvote_ratio": 0.76, "id": "t3_turzir", "created_utc": 1648930215.0}
{"sub": "investing", "title": "Question on commonly recommended funds/ETFs", "selftext": "Hello, I am looking to start moving more money into long term investments outside of my 401k - I have read about VOO, VT, VTI, etc and am still forming my ideas around what ratios per fund I choose. My employer's 401k plan with Vanguard doesn't have a very broad range of selection - I can use VIGIX, VTPSX, and have the perk of BRKB as well aside from regular target date funds. \n\nMy 2 questions are - does it cost me more in fees to invest in VOO, VTI etc outside of Vanguard in a brokerage like Fidelity? I have a separate brokerage account with Vanguard but prefer Fidelity's service and interface.\n\n2nd question would be, do you all recommend maxing my 401k BEFORE redirecting any money to my own picks above post-tax? I'm not sure if it would be worth putting more in 401 given the fund selection, or if I should go ahead and start buying my own outside of it. \n\n\nThank you all for your time.\n\nEdit: I am 29 and have about 90k in my 401k, outside of it I only hold individual stocks right now. Adding this if it helps determine answers. \n\nThanks", "upvote_ratio": 0.73, "id": "t3_tuqr40", "created_utc": 1648926955.0}
{"sub": "investing", "title": "Transfering \\ Trading IRA and Roth when living abroad", "selftext": "I am an American living in Germany and intend to stay here for the future and might even go for German citizenship (giving up US citizenship etc...)  But for now I am wondering what I can do with my IRAs which I funded back in the States.  I cannot fund them with foreign income.\n\nThe IRAs were with T Rowe Price and I transferred them to a Charles Schwab One (international) account because I wanted to trade them out of the expensive T Rowe Price SP500 mutual fund into something a Schwab ETF SP500 with a lower fee.  The transfer went fine but Schwab will not let me make any trades.  So now I have two T Rowe Price mutual funds with all my retirement savings sitting in a Schwab account and the only thing Schwab will allow me to do is sell them!  Super annoying.\n\nThis is some kind of FACTA rule which I do not fully understand.  According the Schwab advisor I neither have to move back to the States or become a German citizen to trade the damn things. \n\nWhat I was thinking of doing is moving them back to T. Rowe Price as my account there is still open and registered using my mother's American address as my legal residence.  T. Rowe Price will be none the wiser and then I can at least trade into another T. Rowe Price fund, although those are not super great in my opinion.  But I am going to guess that lying on an IRA transfer form some major financial crime that the IRS would love to punish my small-time ass for, while Panama Papers folks buy a third yacht.  \n\n\nAny advice or thoughts here?  Thanks!", "upvote_ratio": 0.43, "id": "t3_tup3xe", "created_utc": 1648922645.0}
{"sub": "investing", "title": "Sustainable Materials &amp; Technology in Clothing: The Allbirds Case", "selftext": "Path to profitability for sustainable materials and technology in the next 10-20 years looks good - specifically in clothing.\n\nTake Allbirds for example. One of the only publicly traded sustainable shoe brands out, and advertising themselves as much more - recently releasing a clothing line. Their shoes are just the beginning.\n\n$BIRD is essentially one of the biggest opportunities in the sustainable materials technology and clothing market right now. Their angle has virtually no competition, they're loved by big money, and show every sign they plan on being around for a very long time - and not primarily as a shoe company, but as a sustainable materials and technology company.\n\nAlso, it's been a while since a decent clothing line came to market. $BIRD sits around $6, with early investors having the opportunity to sell off their shares in the coming month. More or less an opportunity to buy at the bottom soon, if it isn't already. ...Can easily see a selloff to the $2 to $3 mark out of panic that the company isn't going about profit in a traditional shoe/clothing company way.\n\nAllbirds looks like a solid long term buy, add, and hold. Especially with their positioning to wool and material supplies in New Zealand. Also, their status in the tech space has virtually no competition - a whole market for IT clothing and apparel to be tapped. Plus, the CEO is dedicated and passionate about life in general - and also very smart to partner with Adidas - who could more or less buy them with pocket change if they wanted.\n\nWondering now who else is on a similar path. Definitely others competing, but nowhere near the same level of exposure.\n\nRidiculous as this may sound, Allbirds is more or less the Tesla of clothing companies right now. And sitting pretty at about 6 bucks.", "upvote_ratio": 0.66, "id": "t3_tun7nd", "created_utc": 1648917879.0}
{"sub": "investing", "title": "Different types of investments", "selftext": "Here is a list of investments I wrote up:\n\n1. Yourself:\n       A. Sleep/Diet/Fitness \u2022gives energy&amp;focus\n\n       B. education/books/podcast/seminars/lectures \u2022gives knowledge&amp;ideation\n\n       C. Experience \u2022gives skills\n\n\n2. Grows Net-worth\n       A. Business \u2022 gives cashflow&amp;creates wealth\n\n3. Preserves/Grows created wealth\n       A. Securities/currencies/crypto \u2022 possibly grow / or provide dividends/interest\n\n       B. Real Estate \u2022 possibly preserve wealth&amp;provide cashflow/Growth\n\n       C. Art \u2022 Preserve wealth \n\n       D. Commodities \u2022 Preserve wealth \n\nIf any, What would you add or change about my list and why?", "upvote_ratio": 0.33, "id": "t3_tuku2b", "created_utc": 1648911662.0}
{"sub": "investing", "title": "Brokerage platform that auto invests ETFs AND Indexes?", "selftext": "Fidelity won't auto-invest ETFs, and M1Finance doesn't offer mutual funds or index funds. Is there a platform that offers both that's still a good interface and offers good insights/analytics?\n\nI'd prefer to stay away from Robinhood because of that whole game stop thing. I've tried Public.com for stocks, but I've found myself buying things because other people are buying and that whole social aspect that I'd like to step away from.\n\n&amp;#x200B;\n\nedit - I won't be able to respond to everyone, but thanks for all the clarifications and suggestions!", "upvote_ratio": 0.65, "id": "t3_tuk8nn", "created_utc": 1648910049.0}
{"sub": "investing", "title": "Filed taxes as MFS, what are my investing options?", "selftext": "Hello!\n\nMy wife and I live in a HCOL area and our household gross income is around $242k. She works in the public sector on a G4 visa so she\u2019s tax exempt. My friend has been doing my taxes for years and suggested we file as married but separately, which apparently limits the amount of contributions I can make towards our Roth IRA if our gross income exceed $10k. \n\nI\u2019m already contributing around 15% towards my 401k and maximized our I-bond limits. Aside from a taxable account, what are some other investing options I should consider?", "upvote_ratio": 0.6, "id": "t3_tuk63q", "created_utc": 1648909850.0}
{"sub": "investing", "title": "Safe 10-15 year investments", "selftext": "My parents recently sold their home and downsized. At Christmas they gave each of us kids $1,000 for each grandkid (our kids) to be given to them on their 18th birthday. I\u2019d like to put this somewhere for each of my kids that will make it worth close to $1,000 when they take it out (keep up with inflation essentially). My oldest will get it in 10 years. \n\nWe already have a mutual fund for each kid that we plan to give them control of after college to help them start out/or get a head start on retirement but I want this to be separate of that. \n\nI kicked around the idea of each kid picking a business (stock) they like/know but my youngest two aren\u2019t really old enough to understand even that yet. \n\nAppreciate any ideas you all have!", "upvote_ratio": 0.88, "id": "t3_tujxgn", "created_utc": 1648909186.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - April 02, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.76, "id": "t3_tuej4u", "created_utc": 1648890068.0}
{"sub": "investing", "title": "Charles schwab target 2065 index vs vanguard target 2065 index", "selftext": "im finally opening my own path towards investing into my retirement, but im not sure which of these is  better  shwab  SWYOX  ( 2065 index ) vs vanguard  VLXVX ( 2065 index ). The schwab one came out in 2021 with annal yield of 8.4% but the vanguard one came out in 2017 with a 5 year yield totaling 47.7%. Does it really matter which one i go for since pretty much all brokerage 2065 target index out there are kind of the same with 8-10% yearly yields?\n\nthanks", "upvote_ratio": 0.75, "id": "t3_tucnpz", "created_utc": 1648882259.0}
{"sub": "investing", "title": "Annuity vs. Non-Annuitized Bucket strategy", "selftext": "Hi all, I have a dilemma that I 'm curious to hear your thoughts. I am 59 and retiring this June. I currently have $1.1M in 401(k), $70K in a money market (emergency fund). I'll have yearly costs of 70K in retirement which include year 13 of a $162,000 mortgage that will be paid off in 2039. Added income is $12K yearly pension starting in Jan. 2025 for life and SS at $25K starting at the same time. My financial advisor is recommending using the 401(K) to buy a $600K fixed annuity with a guaranteed rider that will pay out $32K yearly for life floor starting in Jan. 2025. The rest would be invested and used as discretionary $$. I am leaning toward a 5 bucket investment strategy with no annuity using various risk levels and drawing that to cover all expenses. Break even point is age 82 where the annuity supposably surpasses the non-annuitized sttrategy. Which strategy would you pick? Thanks!!", "upvote_ratio": 0.68, "id": "t3_tub6i4", "created_utc": 1648876428.0}
{"sub": "investing", "title": "Dollar Based Net Retention Rate Filter", "selftext": "Is there a website or a way to filter stocks based off of Dollar Based Net Retention Rate?  With growth stocks lower, I was hoping there was a way to search for a company's Dollar-Based Net Retention Rate without going into the quarterly reports or filter stocks with a Dollar-Based Net Retention Rate of over 120%.", "upvote_ratio": 0.8, "id": "t3_tu8khc", "created_utc": 1648867622.0}
{"sub": "investing", "title": "Mega cap market-weighted idea", "selftext": "I\u2019m just looking for advice, questions, recommendations, etc\n\nI did some back tests which revealed that the top 5-10 S&amp;P stocks tend to outperform the market over the long term.\n\nWhat if you made a portfolio 50% vti (or vt) and the rest the top 5-10 stocks weighted by market cap?", "upvote_ratio": 0.85, "id": "t3_tu8hsw", "created_utc": 1648867380.0}
{"sub": "investing", "title": "Investing in a potential recession", "selftext": "So the yield curve inverted which in the past has predicted 6 out of 7 recessions within 6 or 18 months of the invert happening. \nSo we might expect a recession around this October or next October \nWhat is y\u2019all\u2019s theory on the performance of stocks if we soon see a recession? Do y\u2019all see any industries that could possible still do good or even better in a recession?", "upvote_ratio": 0.57, "id": "t3_tu79e1", "created_utc": 1648863528.0}
{"sub": "investing", "title": "Helping young children invest", "selftext": "My wife and I are looking to start investments for our two young children (7 year old and 6 month old). The idea is to save enough money for their college and wedding as well as to set them up financially for when they are grown. \n\nMy first thought is to open a brokerage accounts for each of them and invest with equal monthly payments until they are 18. My rationale is that the SPY generates an average annual return of 10% and we could conceivably generate significant returns. Obviously there is risk here, but it\u2019s all I really know. \n\nMy question is whether or not this is the best approach or if there is a better way to go about this?", "upvote_ratio": 0.7, "id": "t3_tu21sx", "created_utc": 1648848497.0}
{"sub": "investing", "title": "IBond purchase for myself and wife over limit", "selftext": "I purchased $10k of Ibonds for myself and $10k of Ibonds for my wife using the same treasury direct login with different sub-accounts under each of our names. I got an email saying I was over the $10k limit and the amount would be refunded, but it's days later and my balance still shows the bonds, and nothing in my bank account. The email said it will take 8-10 weeks to get the refund, which seems kinda ridiculous, but whatever.\n\nI can buy $10k for each of us right? Does my wife have to create a completely separate login to purchase her limit or is there a way to buy them from the account I already created?", "upvote_ratio": 0.67, "id": "t3_tu1m06", "created_utc": 1648847351.0}
{"sub": "investing", "title": "Can anyone EILI5 what happened/is going on with Citigroup (C)?", "selftext": "So basically like the title says. I have a small position of Citigroup (only 5.xx shares but I\u2019m young and just starting) and have been watching it just go down and down and down. What\u2019s happening? I thought rising rates was good for banks? Its P/B value is getting smaller and smaller as each day goes by and I\u2019m honestly at a lost for why. I can\u2019t find anything online from my research either.", "upvote_ratio": 0.69, "id": "t3_tu01d1", "created_utc": 1648843304.0}
{"sub": "investing", "title": "Financial advisor advises not to invest cash currently?", "selftext": "My girlfriend let me look at her finances recently, and I saw she had a lot of cash set aside in her account managed by her financial advisor. She is a medical student currently and won't be touching that money for the next 3.5 years due to her being in school; she was planning to use part of that money for a down payment for a house when she finishes school.\n\nHer account consists of 70k, with 60k being in cash, I urged her to talk to her financial advisor to figure out why 60k is being left in cash as with rising inflation rates, as it will just lose value over the years. She talked to her financial advisors, and they stated that the market is volatile currently and isn't a smart move to invest all the cash, as she is very risk-averse.  They even stated that putting that money into bonds isn't the correct decision either.\n\nI'm not a financial advisor, but I'm confused, isn't it better to put your money into VOO/VTI and Bonds if you aren't going to be touching your cash for the next 3.5 years? It seems like just sitting on 60k to devalue over the upcoming years isn't the smartest financial decision? The only way I can see that being correct is if there is a recession coming, but no one can predict that.\n\nAny advice would be appreciated.\n\nedit: Again, I am just seeking advice and an understanding of the decisions made. I am not advising her to do anything with her money or will be managing her financial decisions, but apparently even asking for advice has to be labeled as wrong or right.", "upvote_ratio": 0.79, "id": "t3_ttuqer", "created_utc": 1648830035.0}
{"sub": "investing", "title": "Why do most investors/news treat stock buy backs announcements so different than dividends.", "selftext": "So personally I would much rather prefer a company to do stock buy backs vs dividends as it gives me the choice if I want to sell or take the tax hit. \n\nThis sort of got me thinking why do dividends seem so much more news worthy and structured vs stock buy backs? It seems like with stock buy backs the BOD will say \"We authorize $X to be allocated to buy backs\" and this rarely makes much news, hell they may not even need to spend the full amount. Next year/quarter they may opt not to do buy backs and this seems not to really make news\n\nWith dividends on the other hand it seems it generates much more news, if a company cuts its dividend its BIG news. With some companies if they do not increase their dividend it is also big news. \n\nJust wondering why this is the case? For example there is a dividend aristocrat index, would it make sense to have a buy back aristocrat index to track companies that allocate more money every year to buy backs?\n\nDrinking my coffee this morning and just wondering why a large group of investors seem very obsessed over dividends but buy backs are usually just a foot note.", "upvote_ratio": 0.69, "id": "t3_ttta0y", "created_utc": 1648826418.0}
{"sub": "investing", "title": "Is investing in art using masterworks worth it?", "selftext": "I've heard a lot about Masterworks and how you can invest in a shares of art pieces for a better returns than ETFs and other passive stock investing. In theory it sounds brilliant but all of these were paid promotions by youtubers. Also, the amount you need to invest is pretty large, so I don't wanna take any risk and wanna do proper research on the topic first. I don't trust most internet articles on this topic cus they could be paid for by masterwork. So, is it worth it?", "upvote_ratio": 0.39, "id": "t3_tts20f", "created_utc": 1648823384.0}
{"sub": "investing", "title": "Best news source for stocks and crypto???", "selftext": "anyone knows which platform provides the quickest and most up to date status of any stocks? For example ive seen so many investors buying/selling stocks almost always at the right timing, sometimes at 3am. How do they know so quickely and precisely? What news platform do they use???? Literally just use google news tab but i dont think its as effective as a dedicated one. When gme and amc stocks skyrocketed in the recent days so where did those investors got the news to buy and sell? So many made over 50-100%.\n\n&amp;#x200B;\n\nthanks", "upvote_ratio": 0.18, "id": "t3_ttrwi0", "created_utc": 1648822958.0}
{"sub": "investing", "title": "Interactive Brokers - Why My ETF Is On LSE and not On Euronext Paris?", "selftext": "according to [https://www.amundietf.fr/particuliers/product/view/LU1681049018](https://www.amundietf.fr/particuliers/product/view/LU1681049018) Nyse Euronext Paris\n\nticker is 500U. I'm using interactive brokers and I'm a french resident. When I search for 500U I get instead the 500U on LSE. Don't understand", "upvote_ratio": 0.67, "id": "t3_ttr2ft", "created_utc": 1648820788.0}
{"sub": "investing", "title": "Which UK based investing platform would you recommend", "selftext": "I\u2019m new to this investing lark but it\u2019s time I start putting a little a side for the future, is there a platform that I can use from the UK that is pretty reliable and safe to use.\n\nI\u2019ve heard of trading212 because well they\u2019re everywhere advertisement wise but was wondering if those in the know could recommend instead, word of mouth is better imo", "upvote_ratio": 0.6, "id": "t3_ttqosm", "created_utc": 1648819740.0}
{"sub": "investing", "title": "Best Investment Options Under Pre-Clearance", "selftext": "Hello all! I've recently saved up enough to begin investing in a serious way. However, due to my job, I am under a trade pre-clearance requirement, keeping me from being an active investor that can respond quickly to market changes. As such, I'm looking for advice for options with a strong yield that don't require rapid inputs and reactions from the investor. Any advice?", "upvote_ratio": 0.5, "id": "t3_ttpyvk", "created_utc": 1648817673.0}
{"sub": "investing", "title": "U.S. job growth solid in March; unemployment rate falls to 3.6%", "selftext": "From the article:\n\n&gt; U.S. job growth continued at a brisk clip in March, with the unemployment rate falling to a new two-year low of 3.6% and wages re-accelerating, positioning the Federal Reserve to raise interest rates by a hefty 50 basis points in May.\n\n&gt; The Labor Department\u2019s closely watched employment report\u2019s survey of establishments showed that nonfarm payrolls increased by 431,000 jobs last month.\n\n&gt; Data for February was revised higher to show 750,000 jobs added instead of the previously reported 678,000. Economists polled by Reuters had forecast payrolls increasing 490,000. Estimates ranged from as low as 200,000 to as high as 700,000.\n\nhttps://financialpost.com/pmn/business-pmn/u-s-job-growth-solid-in-march-unemployment-rate-falls-to-3-6", "upvote_ratio": 0.92, "id": "t3_ttpttj", "created_utc": 1648817216.0}
{"sub": "investing", "title": "China considers giving US FULL audit to Alibaba", "selftext": "Welp. There it goes again.\n\nThis was my post 10 days ago: [https://www.reddit.com/r/investing/comments/tk3mvm/alibaba\\_stock\\_the\\_steal\\_of\\_a\\_lifetime/](https://www.reddit.com/r/investing/comments/tk3mvm/alibaba_stock_the_steal_of_a_lifetime/)\n\n&amp;#x200B;\n\nToday, China has announced it is considering giving the US **FULL** audit to Alibaba\n\n[https://www.bloomberg.com/news/articles/2022-04-01/china-weighs-giving-u-s-full-access-to-audits-of-most-firms?srnd=markets-vp](https://www.bloomberg.com/news/articles/2022-04-01/china-weighs-giving-u-s-full-access-to-audits-of-most-firms?srnd=markets-vp)\n\nThis is huge. This is a game changer. Biggest opportunity of a lifetime.\n\n&amp;#x200B;\n\nDelisting is now most likely just fear. China is willing to accept all audits from USA with only a CERTAIN SUBSET of stocks. One of them being Alibaba. China isn't guaranteeing for all its listed stocks but only some. It also means that Alibaba's financials are legitimate enough for China to accept US to audit.", "upvote_ratio": 0.74, "id": "t3_ttpour", "created_utc": 1648816787.0}
{"sub": "investing", "title": "Thoughts on purchasing land and/or building a house in current housing market.", "selftext": "Hey y'all. I am 25 years old with a solid amount of capital from working 3 years after college as an engineer and not having much to any expenses due to living and working from home. \n\nRecently, I began looking at buying a plot of land in a neighborhood to build a house. The location will be very close to a new train station that is going to be built which more easily provides access to Chicago from Northwest Indiana. \n\nSo my question for you guys is.... Do you think now is the right time to invest either in the plot of land itself, or buy the plot of land to build a house?\n\nThe neighborhood is a \"bring your own\" builder, so the houses that are there are very unique looking, everything is different. In my mind, this also makes it a solid value investment due to the location of the neighborhood (good area + new train station for ease of access to Chicago being built by 2024). \n\nAs a young investor, I have been hesitant to jump too much into the market because fears of a bubble and \"crash\" that apparently has been looming for a few years. As a young investor, knowing the 2008 situation, but not KNOWING how a similar situation could affect me, is leaving me a little hesitant to move forward with buying the land / building the house. \n\nIs jumping in during this time wrong? Or should I wait for housing bubble to relax and construction costs to level out? \n\nI am open to any advice / discussion relating to the stock market / housing market!", "upvote_ratio": 0.75, "id": "t3_ttmrf9", "created_utc": 1648805914.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - April 01, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.87, "id": "t3_ttm8ia", "created_utc": 1648803668.0}
{"sub": "investing", "title": "Is this normal? Late dividends payment?", "selftext": "Hi \n\nI bought VUSA etf back in early march using a AJ Bell ISA account and the dividend payout was meant to be on the 30th but I still haven\u2019t received anything in my account. I sent them a email and this is what they said - \n\n\u201cOur dividedness team have stated we have still not been paid the dividends to ourselves to distribute. They will be chasing the registrar today in an attempt to obtain the dividends as quick as possible\u201d\n\nIs this normal or should I be worried? \n\nThanks", "upvote_ratio": 0.4, "id": "t3_ttlym9", "created_utc": 1648802454.0}
{"sub": "investing", "title": "Roku,Tdoc and Veeva : Deeply discounted or correctly valued?", "selftext": "Both Roku and Veeva are growing sales considerably YoY and have strong gross margins. However, their prices have been cut down in the last few months (granted Veeva had some recovery).\n\nTeladoc currently has a book value per share higher than current market price?? \n\nAny thoughts on these stocks?", "upvote_ratio": 0.33, "id": "t3_ttljbb", "created_utc": 1648800485.0}
{"sub": "investing", "title": "Should my brokerage account look like my ROTH IRA?", "selftext": "I have maxed out 2021 and 2022 contributions for my Roth IRA but still have a decent amount of money to invest in a brokerage account. Does it make sense to invest in the same index funds/stocks in both accounts to maximize growth? Or maybe buy more shares of top companies my index funds hold for my brokerage?", "upvote_ratio": 0.5, "id": "t3_ttl1p8", "created_utc": 1648798335.0}
{"sub": "investing", "title": "What are the best trading platforms for German investors?", "selftext": " I have a German friend who has recently moved back home after working  abroad for some time. My friend would like to start investing some of  their savings but they don't know which trading platform they should  use. Since they are new to the idea of investing they only want to start  small and invest maybe around 1k euros. So i figured the best place to  ask for advice and recommendations would be here.", "upvote_ratio": 0.75, "id": "t3_ttknpq", "created_utc": 1648796649.0}
{"sub": "investing", "title": "[Genuinely Curious] is there any way GME stock splitting could go wrong?", "selftext": "Following news today where GameStop is planning to split its shares, my understanding being it is reasonable to expect that shorters are 'obligated' to give X shares for each share they borrowed to short, i.e. price can only go up either tomorrow or after their annual meeting...\n\nI might be in the wrong subreddit, but just wondering if we were to view this critically, what are the possible factors this could go wrong - any view is much appreciated, thanks!", "upvote_ratio": 0.42, "id": "t3_ttjskc", "created_utc": 1648793112.0}
{"sub": "investing", "title": "What is the likely impact, if any, of yield inversion and inflation on US Dollar value within the next two years?", "selftext": "If the interest rates go up, I imagine there will be a demand for USD and the value may go up. Is that generally true?\n\nHow do the yield inversion and inflation affect the value of USD in the international forex market? \n\nIf yield inversion predicts a recession, how will that affect the value of USD during the uncertain period of the next 12-18 months?  If the recession actually does occur or start in 12-18 months, what will be its effect on USD?", "upvote_ratio": 0.5, "id": "t3_ttilal", "created_utc": 1648788620.0}
{"sub": "investing", "title": "The One ETF to Watch for Signs That the Fed Will Change Its Mind on Multiple Hikes", "selftext": "Via [Bloomberg](https://www.bloomberg.com/news/articles/2022-03-31/the-one-etf-to-watch-for-signs-that-the-fed-will-change-its-mind) (non-paywall link at [https://archive.ph/LXJGs](https://archive.ph/LXJGs))\n\n* Credit risk will veto the Fed,\u2019 Larry McDonald says\n* ETF tracking CMBS mired in second-worst drawdown ever\n\n&gt;If  you\u2019re wondering what the Federal Reserve will actually end up doing  with interest rates this year, Larry McDonald advises ignoring what  economists say.\u00a0  \n&gt;  \n&gt;Instead,  it\u2019s best to focus on credit risk, according to the founder of the Bear  Traps Report and former head of macro strategy at Societe Generale.  \n&gt;  \n&gt;Even  after just one increase to the target for the Federal funds rate,  credit markets are signaling that it\u2019s \u201chighly suspect\u201d that the central  bank will lift rates by the equivalent of seven 25-basis point hikes  this year, let alone the eight or nine increases that some economists  are predicting, McDonald said in a recent Bear Traps Report [video](https://www.youtube.com/watch?v=nmoC_3-B-rA).\u00a0  \n&gt;  \n&gt;McDonald pointed to the [iShares CMBS](https://finance.yahoo.com/quote/CMBS)  exchange-traded fund to make his case. The $753 million ETF tracks  commercial mortgage-backed securities, a critical lending tool for  banks. The 10-year-old ETF is experiencing the second-largest ever  drawdown, behind only the 2020 Covid-19 market crash. It remains mired  in weakness even as benchmark equity indexes have recovered much of  their year-to-date losses.  \n&gt;  \n&gt;\u201cEconomists  are looking at economic data, they\u2019re not looking at credit risk,\u201d  McDonald said. \u201cAnd at the end of the day, as we\u2019ve seen over the last  10 years, credit risk will veto the Fed, will veto the economists.  That\u2019s why you have to spend more time looking at credit risk and not  listening to economists.\u201d  \n&gt;  \n&gt;When  it comes to what areas of commercial real estate are riskiest, Bear  Traps suspects that it\u2019s the CMBS tranches that are most-exposed to  major U.S. cities --- risk that is also being flagged by the municipal  bond market, McDonald added in an email Thursday.  \n&gt;  \n&gt;\u201cWe  are seeing a corresponding surge in muni credit risk in some of the  large U.S. cities,\u201d he said. \u201cNYC spread vs. Austin and Miami widening a  lot.\u201d", "upvote_ratio": 0.8, "id": "t3_tthscq", "created_utc": 1648785725.0}
{"sub": "investing", "title": "Let's peel apart the February 2022 7.9% CPI measurement to figure out if the data shows \"it's because the fed printed money\"", "selftext": "Stuff in 2021 cost 7.9% less than it did in 2022. Let's investigate:\n\n&gt; The CPI-U is a more general index and seeks to track retail prices as they affect all urban consumers. It encompasses about 87 percent of the United States' population. The CPI-W is a more specialized index and seeks to track retail prices as they affect urban hourly wage earners and clerical workers.\n\n&gt; The three largest components of the CPI are housing, transportation, and food/beverages in that order.\n\n&gt; They are weighted this way: housing 41.4%; food and beverages 17.4%; transport 17.0%; medical care 6.9%; apparel 6.0%; recreation 4.4%; other 6.9%. https://en.wikipedia.org/wiki/Consumer_price_index\n\nHow much is each category up?\n\n    Housing (CPIHOSNS): 275.13 -&gt; 291.50 (5.95%)\n    Food and beverages (CPIUFDSL): 271.42 -&gt; 292.91 (7.9%)\n    Transport (CPITRNSL): 211.47 -&gt; 256.02 (21.06%)\n    Medical care (CPIMEDSL): 523.03 -&gt; 535.73 (2.43%)\n    Apparel (CPIAPPSL): 118.53 -&gt; 126.34 (6.59%)\n    Recreation (CPIRECSL): 122.89 -&gt; 128.97 (4.97%)\n    Other (CPIOGSSL): 469.88 -&gt; 496.04 (5.57%)\n\n2021: (41.4\u00d7275.13+17.4\u00d7271.42+17\u00d7211.47+6.9\u00d7523.03+6\u00d7118.53+4.4\u00d7122.89+6.9\u00d7469.88) / (41.4+17.4+17+6.9+6+4.4+6.9) = 278.11055\n\n2022: (41.4\u00d7291.50+17.4\u00d7292.91+17\u00d7256.02+6.9\u00d7535.73+6\u00d7126.34+4.4\u00d7128.97+6.9\u00d7496.04) / (41.4+17.4+17+6.9+6+4.4+6.9) = 299.61795\n\nMy weights are a little outdated/off but the math checks out: about 7.7% increase 2021 -&gt; 2022\n\nNow let's dive into some of the categories. A lot of people say CPI is a shitty metric that doesn't capture the \"true\" inflation Americans are experiencing. I've read countless times on Reddit how the entire formula is set up in a way to hide \"real inflation\" from the American people.\n\nHere are the same categories, sorted by how much they are up year over year:\n\n    (21.06%) Transport (CPITRNSL): 211.47 -&gt; 256.02 \n    (7.9%) Food and beverages (CPIUFDSL): 271.42 -&gt; 292.91  \n    (6.59%) Apparel (CPIAPPSL): 118.53 -&gt; 126.34  \n    (5.95%) Housing (CPIHOSNS): 275.13 -&gt; 291.50  \n    (5.57%) Other (CPIOGSSL): 469.88 -&gt; 496.04 \n    (4.97%) Recreation (CPIRECSL): 122.89 -&gt; 128.97  \n    (2.43%) Medical care (CPIMEDSL): 523.03 -&gt; 535.73  \n\nObviously, housing matters more to people than apparel. Keep in mind the transport category is what holds gasoline prices, and this was a reading before the Ukraine-Russia conflict if I remember correctly.\n\nMarch 2022 CPI will be released Apr. 12, 2022.\n\nDrilling in a bit more, here are some categories. The weights might be slightly out of date, since the page says it is from 2020. https://www.bls.gov/cpi/tables/relative-importance/2020.htm The categories are up to 8 tiers nested.\n\n    .sub1: Food and beverages 15.157\n    .sub1: Housing 42.385\n    .sub1: Apparel 2.663\n    .sub1: Transportation 15.160\n    .sub1: Medical care 8.870\n    .sub1: Recreation 5.797\n    .sub1: Education and communication 6.8\n    .sub1: Other goods and services 3.159\n    \n    .sub2: Food\n    .sub2: Shelter\n    .sub2: Fuels and utilities\n    .sub2: Household furnishings and operations\n    .sub2: Men's and boys' apparel\n    .sub2: Women's and girls' apparel\n    .sub2: Footwear\n    .sub2: Infants' and toddlers' apparel\n    .sub2: Jewelry and watches\n    .sub2: Private transportation\n    .sub2: Public transportation\n    .sub2: Medical care commodities\n    .sub2: Medical care services\n    .sub2: Video and audio\n    .sub2: Pets, pet products and services\n    .sub2: Sporting goods\n    .sub2: Photography\n    .sub2: Other recreational goods\n    .sub2: Other recreation services\n    .sub2: Recreational reading materials\n    .sub2: Education\n    .sub2: Communication\n    .sub2: Tobacco and smoking products\n    .sub2: Personal care\n    \n    .sub3: Food at home\n    .sub3: Food away from home\n    .sub3: Alcoholic beverages\n    .sub3: Rent of primary residence\n    .sub3: Lodging away from home\n    .sub3: Owners' equivalent rent of residences\n    .sub3: Tenants' and household insurance\n    .sub3: Household energy\n    .sub3: Water and sewer and trash collection services\n    .sub3: Window and floor coverings and other linens\n    .sub3: Furniture and bedding\n    .sub3: Appliances\n    .sub3: Other household equipment and furnishings\n    .sub3: Tools, hardware, outdoor equipment and supplies\n    .sub3: Housekeeping supplies\n    .sub3: Household operations\n    .sub3: Men's apparel\n    .sub3: Boys' apparel\n    .sub3: Women's apparel\n    .sub3: Girls' apparel\n    .sub3: Men's footwear\n    .sub3: Boys' and girls' footwear\n    .sub3: Women's footwear\n    .sub3: Watches\n    .sub3: Jewelry\n    .sub3: New and used motor vehicles\n    .sub3: Motor fuel\n    .sub3: Motor vehicle parts and equipment\n    .sub3: Motor vehicle maintenance and repair\n    .sub3: Motor vehicle insurance\n    .sub3: Motor vehicle fees\n    .sub3: Airline fares\n    .sub3: Other intercity transportation\n    .sub3: Intracity transportation\n    .sub3: Unsampled public transportation\n    .sub3: Medicinal drugs\n    .sub3: Medical equipment and supplies\n    .sub3: Professional services\n    .sub3: Hospital and related services\n    .sub3: Health insurance\n    .sub3: Televisions\n    .sub3: Cable and satellite television service\n    .sub3: Other video equipment\n    .sub3: Video discs and other media, including rental of video\n    .sub3: Audio equipment\n    .sub3: Recorded music and music subscriptions\n    .sub3: Unsampled video and audio\n    .sub3: Pets and pet products\n    .sub3: Pet services including veterinary\n    .sub3: Sports vehicles including bicycles\n    .sub3: Sports equipment\n    .sub3: Unsampled sporting goods\n    .sub3: Photographic equipment and supplies\n    .sub3: Photographers and photo processing\n    .sub3: Unsampled photography\n    .sub3: Toys\n    .sub3: Sewing machines, fabric and supplies\n    .sub3: Music instruments and accessories\n    .sub3: Unsampled recreation commodities\n    .sub3: Club membership for shopping clubs, fraternal, or other organizations, or participant sports fees\n    .sub3: Admissions\n    .sub3: Fees for lessons or instructions\n    .sub3: Unsampled recreation services\n    .sub3: Newspapers and magazines\n    .sub3: Recreational books\n    .sub3: Unsampled recreational reading materials\n    .sub3: Educational books and supplies\n    .sub3: Tuition, other school fees, and childcare\n    .sub3: Postage and delivery services\n    .sub3: Information and information processing\n    .sub3: Information technology, hardware and services\n    .sub3: Cigarettes\n    .sub3: Tobacco products other than cigarettes\n    .sub3: Unsampled tobacco and smoking products\n    .sub3: Personal care products\n    .sub3: Personal care services\n    .sub3: Miscellaneous personal services\n    .sub3: Miscellaneous personal goods\n    \n    .sub4: Cereals and bakery products\n    .sub4: Meats, poultry, fish, and eggs\n    .sub4: Dairy and related products\n    .sub4: Fruits and vegetables\n    .sub4: Nonalcoholic beverages and beverage materials\n    .sub4: Other food at home\n    .sub4: Full service meals and snacks\n    .sub4: Limited service meals and snacks\n    .sub4: Food at employee sites and schools\n    .sub4: Food from vending machines and mobile vendors\n    .sub4: Other food away from home\n    .sub4: Alcoholic beverages at home\n    .sub4: Alcoholic beverages away from home\n    .sub4: Housing at school, excluding board\n    .sub4: Other lodging away from home including hotels and motels\n    .sub4: Owners' equivalent rent of primary residence\n    .sub4: Unsampled owners' equivalent rent of secondary residences\n    .sub4: Fuel oil and other fuels\n    .sub4: Energy services\n    .sub4: Water and sewerage maintenance\n    .sub4: Garbage and trash collection\n    .sub4: Floor coverings\n    .sub4: Window coverings\n    .sub4: Other linens\n    .sub4: Bedroom furniture\n    .sub4: Living room, kitchen, and dining room furniture\n    .sub4: Other furniture\n    .sub4: Unsampled furniture\n    .sub4: Major appliances\n    .sub4: Other appliances\n    .sub4: Unsampled appliances\n    .sub4: Clocks, lamps, and decorator items\n    .sub4: Indoor plants and flowers\n    .sub4: Dishes and flatware\n    .sub4: Nonelectric cookware and tableware\n    .sub4: Tools, hardware and supplies\n    .sub4: Outdoor equipment and supplies\n    .sub4: Unsampled tools, hardware, outdoor equipment and supplies\n    .sub4: Household cleaning products\n    .sub4: Household paper products\n    .sub4: Miscellaneous household products\n    .sub4: Domestic services\n    .sub4: Gardening and lawncare services\n    .sub4: Moving, storage, freight expense\n    .sub4: Repair of household items\n    .sub4: Unsampled household operations\n    .sub4: Men's suits, sport coats, and outerwear\n    .sub4: Men's underwear, nightwear, swimwear and accessories\n    .sub4: Men's shirts and sweaters\n    .sub4: Men's pants and shorts\n    .sub4: Unsampled men's apparel\n    .sub4: Women's outerwear\n    .sub4: Women's dresses\n    .sub4: Women's suits and separates\n    .sub4: Women's underwear, nightwear, swimwear, and accessories\n    .sub4: Unsampled women's apparel\n    .sub4: New vehicles\n    .sub4: Used cars and trucks\n    .sub4: Leased cars and trucks\n    .sub4: Car and truck rental\n    .sub4: Unsampled new and used motor vehicles\n    .sub4: Gasoline (all types)\n    .sub4: Other motor fuels\n    .sub4: Tires\n    .sub4: Vehicle accessories other than tires\n    .sub4: Motor vehicle body work\n    .sub4: Motor vehicle maintenance and servicing\n    .sub4: Motor vehicle repair\n    .sub4: Unsampled service policies\n    .sub4: State motor vehicle registration and license fees\n    .sub4: Parking and other fees\n    .sub4: Unsampled motor vehicle fees\n    .sub4: Prescription drugs\n    .sub4: Nonprescription drugs\n    .sub4: Physicians' services\n    .sub4: Dental services\n    .sub4: Eyeglasses and eye care\n    .sub4: Services by other medical professionals\n    .sub4: Hospital services\n    .sub4: Nursing homes and adult day services\n    .sub4: Care of invalids and elderly at home\n    .sub4: College tuition and fees\n    .sub4: Elementary and high school tuition and fees\n    .sub4: Day care and preschool\n    .sub4: Technical and business school tuition and fees\n    .sub4: Unsampled tuition, other school fees, and childcare\n    .sub4: Postage\n    .sub4: Delivery services\n    .sub4: Telephone services\n    .sub4: Computers, peripherals, and smart home assistants\n    .sub4: Computer software and accessories\n    .sub4: Internet services and electronic information providers\n    .sub4: Telephone hardware, calculators, and other consumer information items\n    .sub4: Unsampled information and information processing\n    .sub4: Hair, dental, shaving, and miscellaneous personal care products\n    .sub4: Cosmetics, perfume, bath, nail preparations and implements\n    .sub4: Unsampled personal care products\n    .sub4: Haircuts and other personal care services\n    .sub4: Legal services\n    .sub4: Funeral expenses\n    .sub4: Laundry and dry cleaning services\n    .sub4: Apparel services other than laundry and dry cleaning\n    .sub4: Financial services\n    .sub4: Unsampled items\n    \n    .sub5: Cereals and cereal products\n    .sub5: Bakery products\n    .sub5: Meats, poultry, and fish\n    .sub5: Eggs\n    .sub5: Milk\n    .sub5: Cheese and related products\n    .sub5: Ice cream and related products\n    .sub5: Other dairy and related products\n    .sub5: Fresh fruits and vegetables\n    .sub5: Processed fruits and vegetables\n    .sub5: Juices and nonalcoholic drinks\n    .sub5: Beverage materials including coffee and tea\n    .sub5: Sugar and sweets\n    .sub5: Fats and oils\n    .sub5: Other foods\n    .sub5: Beer, ale, and other malt beverages at home\n    .sub5: Distilled spirits at home\n    .sub5: Wine at home\n    .sub5: Fuel oil\n    .sub5: Propane, kerosene, and firewood\n    .sub5: Electricity\n    .sub5: Utility (piped) gas service\n    .sub5: Wireless telephone services\n    .sub5: Land-line telephone services\n    \n    .sub6: Flour and prepared flour mixes\n    .sub6: Breakfast cereal\n    .sub6: Rice, pasta, cornmeal\n    .sub6: Bread\n    .sub6: Fresh biscuits, rolls, muffins\n    .sub6: Cakes, cupcakes, and cookies\n    .sub6: Other bakery products\n    .sub6: Meats\n    .sub6: Poultry\n    .sub6: Fish and seafood\n    .sub6: Fresh fruits\n    .sub6: Fresh vegetables\n    .sub6: Canned fruits and vegetables\n    .sub6: Frozen fruits and vegetables\n    .sub6: Other processed fruits and vegetables including dried\n    .sub6: Carbonated drinks\n    .sub6: Frozen noncarbonated juices and drinks\n    .sub6: Nonfrozen noncarbonated juices and drinks\n    .sub6: Coffee\n    .sub6: Other beverage materials including tea\n    .sub6: Sugar and sugar substitutes\n    .sub6: Candy and chewing gum\n    .sub6: Other sweets\n    .sub6: Butter and margarine\n    .sub6: Salad dressing\n    .sub6: Other fats and oils including peanut butter\n    .sub6: Soups\n    .sub6: Frozen and freeze dried prepared foods\n    .sub6: Snacks\n    .sub6: Spices, seasonings, condiments, sauces\n    .sub6: Baby food\n    .sub6: Other miscellaneous foods\n    \n    .sub7: Beef and veal\n    .sub7: Pork\n    .sub7: Other meats\n    .sub7: Chicken\n    .sub7: Other uncooked poultry including turkey\n    .sub7: Fresh fish and seafood\n    .sub7: Processed fish and seafood\n    .sub7: Apples\n    .sub7: Bananas\n    .sub7: Citrus fruits\n    .sub7: Other fresh fruits\n    .sub7: Potatoes\n    .sub7: Lettuce\n    .sub7: Tomatoes\n    .sub7: Other fresh vegetables\n    \n    .sub8: Uncooked ground beef\n    .sub8: Uncooked beef roasts\n    .sub8: Uncooked beef steaks\n    .sub8: Uncooked other beef and veal\n    .sub8: Bacon, breakfast sausage, and related products\n    .sub8: Ham\n    .sub8: Pork chops\n    .sub8: Other pork including roasts, steaks, and ribs", "upvote_ratio": 0.31, "id": "t3_tth665", "created_utc": 1648783660.0}
{"sub": "investing", "title": "Just finally closed my RE deal that I\u2019ve been working on for 15 months", "selftext": "Now I\u2019m trying to figure out what to reinvest in. I\u2019m only 19, not sure if that changes anything. But I\u2019ve got some capital now. \n\nIf you have any advice for me, I\u2019d like to find some videos/articles from people who really understand what they are doing and know how to portray their thoughts and explain why they do things the way they do. \n\nAlex hormozi is an example of someone I look up to and take advice from, is there anyone you guys like to watch / read?\n\nAlso what stocks are you keeping a close eye on right now?", "upvote_ratio": 0.29, "id": "t3_ttgld7", "created_utc": 1648781715.0}
{"sub": "investing", "title": "2022 Financial Goals 1st Quarter Update", "selftext": "My goal for 2022YE is $55,543 Net Worth. My NW as of 12/28/21 was $24,343. I track it every month, but can't post images so this is the most recent. I'll try to post here every quarter to keep myself accountable. What were some of the unique ways you were able to accelerate your net worth that I should be thinking about at this age? Thanks in advance.\n\n**22M 3/31/2022**\n\nCurrent Assets\n\n* Bank: $8,234\n* Standard Brokerage: $8,444\n* Roth IRA: $10,764\n* Roth 401k: $1,862\n* Credit Card #1 CashBack: $54\n* Credit Card #2 CashBack: $0\n* Inventory: $2,110\n\n**TOTAL ASSETS: $31,468**\n\nCurrent Liabilities\n\n* Credit Card #1 Balance: $230\n* Credit Card #2 Balance: $0\n\n**TOTAL LIABILITIES: $230**\n\n**NET WORTH: $31,238**", "upvote_ratio": 0.73, "id": "t3_ttg4lq", "created_utc": 1648780196.0}
{"sub": "investing", "title": "imagine having $50,000 cash, you need it accessible within a weeks notice but want to earn the best return, where do you put it and why?", "selftext": "Imagine having $50,000 cash, but you need it to be liquid in case an opportunity presented itself and you wanted to earn the best return on the money, where would you put it and why? \nGenerally checking isn't going to give a good return.\nSavings may pay a return but it's unlikely to be much, anyone have any good suggestions for high yield savings?\nMoney market? Which brokerage would you park it with and what would be the yield on such an account?\nHow about something like a blockfi account or Gemini account?\nThere's gotta be somewhere to put cash where it's fairly accessible and receives more than a fraction of a percent return, so where would you put it and why?", "upvote_ratio": 0.39, "id": "t3_ttg0rb", "created_utc": 1648779850.0}
{"sub": "investing", "title": "Investment Youtube Channels", "selftext": "Hi r/investing,\n\nI am looking for some good youtube channels that would explain some of the lingo and strategy around personal investing. I am currently invested in a few vanguard funds, mostly in mutual funds (about 85% of my portfolio is in VWMNX and VLCAX, with some in VTI and then around 10% in a few small ETFs). I've been investing over the past 3 or so years and seen fairly good returns, but I still feel as if I have a rudimentary understanding of how it all works, and don't truly understand which decisions I can make now to set myself up for long term growth.\n\nI'm in my mid-twenties and hold a 6-figure job, and have around 30K in this particular portfolio (all in vanguard). I travel and spend my money but also save and invest, with the latter becoming more prevalent in my life. Just seeing if anyone has any good suggestions for some basic investment and stock lessons, all the channels I've found so far are annoying and cheesy. Looking to gain a solid understanding of what to look for in these mutuals, ETFs, bonds, metals, and apply that knowledge to my personal goals. Thanks everyone.", "upvote_ratio": 0.5, "id": "t3_ttfen2", "created_utc": 1648777888.0}
{"sub": "investing", "title": "Bond ETFs look like a great opportunity.", "selftext": "Many people are talking about how bonds are a terrible investment right now, but looking at the NAV drops on bond etfs make it seem like these are great long-term opportunities. I'm still very new to this so I'm hoping someone smarter than me can explain why my thinking is wrong.\n\n&amp;nbsp;\n\n**Increasing rates causes decreasing bond prices**\n\nNew treasury bonds are giving better interest rates so it obviously makes sense, for right now, to park money here instead of the lower interest existing bonds. The value of the existing bonds drop until the existing coupon payments *roughly* translate to the same yield as the newer treasuries.\n\n&amp;nbsp;\n\n**Bond ETFs eventually catch up**\n\nSo with an etf like BND where the average duration is ~6 years, that means the older bonds are routinely getting removed while the newer (and higher yielding) bonds are coming in. Eventually, BND's dividend will start to increase (from higher coupons). This should also mean that BND's NAV price would start to increase alongside the increased dividends.\n\n&amp;nbsp;\n\nA super basic formula would be:\n\n    P(BND) * Div(BND) = Y(BND) = Y(NT)\n   \n\n&amp;nbsp;\n\n*where Y(NT) is yield of new treasuries, P(BND) is the BND etf NAV, Div(BND) is the current BND dividend/coupon, and Y(BND) is the current BND yield*\n\n&amp;nbsp;\n\n**Interest rate increases are already baked in**\n\nSo with BND, when the yield of new treasuries goes up (*Y(NT)*) then an efficient market will ensure BND's yield matches that. I could put $100 towards new treasuries and earn 2.5% yield or put $100 towards BND earning a 2.0% dividend. I'd obviously choose the former and therefore the market responds by lowering the BND price to $80. Now I can buy roughly 1.2 shares of the BND etf (with my $100) with a 2.0% dividend, which gives me a total 2.5% yield.\n\n\nKnowing what I just stated above, **I'd obviously buy the discounted BND price** because I know those higher coupon treasuries will make it into the etf and increase dividends. If the Fed keeps interest rates steady, then 6 years later the BND etf will hold all treasuries with that newer interest rate and return 2.5% dividends. This pushes the NAV back up to $100. When people say existing bonds and etfs like BNDs will drop further due to interest rate hikes, wouldn't the market already be taking that into account? I assume it would be and therefore the decrease in NAV will eventually reverse long term.\n\n&amp;nbsp;\n\n**Conclusion** \n\nBond etfs lag behind on their relative yield when interest rates increase, but for long term investing you can buy the discounted NAV price knowing that eventually the interest rates will stop increasing. You get a double benefit: increased dividends over time and increased NAV. \n\nI'm likely very misinformed or making bad assumptions. Sometimes when I see comments talk about how bonds are, there is this natural inclination to think *so this must mean it's a good time to buy*.", "upvote_ratio": 0.64, "id": "t3_ttfay9", "created_utc": 1648777539.0}
{"sub": "investing", "title": "Why the tanking Japanese yen should concern investors", "selftext": "[***This article***](https://finance.yahoo.com/news/why-the-tanking-japanese-yen-should-concern-stock-investors-morning-brief-090923267.html) ***first appeared in the Morning Brief.***\n\n&gt;Thursday, March 31, 2022  \n&gt;  \n&gt;The Bank of Japan (BOJ) was in a bind on Monday.  \n&gt;  \n&gt;Its currency, the yen, was crashing while yields on their government bonds were surging. The solution \u2014 [four days of unbridled bond buying](https://www.yahoo.com/video/forex-dollar-hits-6-high-194529413.html) by the BOJ to stem the hemorrhaging and contain interest rates. While the gambit worked (for now), Wall Street is waking up to this potential canary in the coal mine.  \n&gt;  \n&gt;Big moves in the yen are rare, but traders pay attention when the currency starts moving. It's the third most heavily-traded currency, and it's involved in trillions of dollars worth of highly levered trades. Hedge funds try to arbitrage differences in interest rates around the world by borrowing in \"cheap\" currencies (like the yen) and investing in bonds in higher-yielding countries \u2014 the so-called [carry trade](https://www.investopedia.com/carry-trade-definition-4682656).  \n&gt;  \n&gt;For instance, if 10-year Australian bonds yield 5% while similar Japanese bonds are paying close to nothing, investors can sell the yen, buy the Australian dollar, and use the proceeds to buy Australian bonds. There are lots of moving parts and wonky details, but that's the gist of it.  \n&gt;  \n&gt;But because traders are essentially picking up dimes in front of a bulldozer, these bets are highly levered to maximize returns \u2014 which means they can fall apart quickly and cause systemic risk if enough traders are effected.  \n&gt;  \n&gt;So when the yen starts making big upward or downward moves, traders face tough decisions. Hedge funds staring down the barrel of multiple margin calls will liquidate good bets \u2014 even safe haven assets like gold \u2014 to cover their bad bets. This is how contagion works.  \n&gt;  \n&gt;For now, the BOJ's bond buying \u2014 effectively printing more money, in this case yen \u2014 is supporting easy financial conditions. But if the bank's hand is forced and it abandons the buying, a massive unwinding will likely follow. And no one is currently pricing in this risk.  \n&gt;  \n&gt;Since the yen is being used as a cheap source of funds to leverage the carry trade, it's a risky bet, points out [Bloomberg's John Authers](https://www.bloomberg.com/opinion/articles/2022-03-28/japan-and-the-yen-find-a-new-way-to-market-relevancy-in-era-of-war-and-inflation?sref=eeq6exxF). Everyone is piling on the same side of the trade such that it becomes self-fulfilling. But the yen has also historically functioned as a flight-to-safety haven during times of stress. If that relationship reasserts and the yen strengthens materially, it's game over for those playing the carry.  \n&gt;  \n&gt;\"Far from offering sanctuary from the world\u2019s strife, Japan is being treated once more as an ATM to fund risk-taking elsewhere,\" Authers wrote.  \n&gt;  \n&gt;While the yen and Japanese bond market have cooled for now, the BOJ will have a big decision to make. Further pressure could lead Japanese authorities to intervene in the yen. Japan has a long and storied history of weakening the yen to favor their exports. But this would be the first time since 1998 that the bank would [intervene to strengthen the currency](https://www.reuters.com/markets/currencies/japan-weaker-yen-may-not-be-blessing-it-once-was-2021-11-26/).  \n&gt;  \n&gt;Surging commodity costs is currently the biggest factor. Japan is a huge energy importer, which depresses its currency as the yen is sold to buy oil and gas (and food and everything else) at higher prices. This outweighs the benefit of boosting their exports as their goods become cheaper abroad \u2014 especially as Japan has offshored a lot of its manufacturing over the last decade.  \n&gt;  \n&gt;**This content is not available due to your privacy preferences.**[**Update your settings here to see it.**](https://yahoo.mydashboard.oath.com/guc-redirect?app=thirdPartyContentEmbed&amp;bucket=pd_2&amp;lang=en-US)  \n&gt;  \n&gt;A plummeting yen also puts upward pressure on interest rates, which is at odds with the BOJ's policy of controlling the entire yield curve. (By way of reference, the Federal Reserve only seeks to influence short-term U.S. rates.) If the BOJ is forced to abandon its yield curve control strategy, that brings the yen devaluation option to the forefront.  \n&gt;  \n&gt;The other dynamic at play is the strengthening Chinese yuan, or renminbi, which is dangerously close to approaching the very level versus the yen that caused authorities in China to [devalue its currency by 3% in 2015](https://www.investopedia.com/trading/chinese-devaluation-yuan/). That surprise move upended global risk markets and sent many stock markets around the world plunging into bear territory.  \n&gt;  \n&gt;Today, add a pandemic and a war in Europe to the mix \u2014 not to mention a Federal Reserve that's the most hawkish in at least two decades \u2014 and markets may not bounce back so quickly as they eventually did in early 2016.  \n&gt;  \n&gt;Jens Nordvig, founder and CEO at Exante Data, recently remarked how different the current situation is from prior times of global gyrations in the currency markets, [tweeting](https://twitter.com/jnordvig/status/1507006087643508736?s=20&amp;t=S23lXKLsAAa4XBrkGev6Sw), \"\\[Y\\]ou can only reach the conclusion that the regime is now totally different. This cycle is different, very different, and all asset classes are gradually waking up to this new reality, with \\[foreign exchange markets\\] showing it forcefully lately.\"\n\n*By* [*Jared Blikre*](https://www.yahoo.com/author/jared-blikre/?.tsrc=fin-srch)*, a reporter focused on the markets on Yahoo Finance. Follow him* [*@SPYJared*](https://twitter.com/SPYJared)", "upvote_ratio": 0.56, "id": "t3_ttdm10", "created_utc": 1648772139.0}
{"sub": "investing", "title": "Do hedge funds Pay there investors with the leverage they use", "selftext": "For instance say a hedge fund borrows 100 Dollars, for investing with the standard 2 and 20. Let's us also say they raise 100 dollars in leverage, with your investment as collateral, unofficially. Lets say they invest all 200 dollars and make 100 dollars so the leverage made 50 and so did the original money. Does the hedge fund clear the leverage for themselves and only count the original investment in your return. So the hedge fund has held your investment as collateral in a leveraged investment.", "upvote_ratio": 0.25, "id": "t3_ttdet9", "created_utc": 1648771498.0}
{"sub": "investing", "title": "Sega and TMS Entertainment", "selftext": "Hello. Help? ...I am about to buy a lot of $SGAMY  \n\nTMS Entertainment is a wholly owned subsidiary of Sega Sammy Holdings. \n\nThe Sega part is nice, but TMS Entertainment's work and history also looks incredible and vast. \n\nThinking about buying and holding for a long time - a very long time. \n\nAny fair reason not to pick up 100 shares or so and hold for the next 15 years? Is there something wrong with the stock?? \n\nDespite the low share price, it's difficult to imagine Sega going away anytime soon - especially with regards to their TMS Entertainment deal. \n\nMainly confused over the price being so low for so long but also growing very steadily. The chart looks strong but seems to want to be around the $3-4 mark almost no matter what. A few decent rises, but mostly even. And recently climbing nicely.\n\nAnyone own this stock or staying away because such and such?", "upvote_ratio": 0.5, "id": "t3_ttd4jm", "created_utc": 1648770637.0}
{"sub": "investing", "title": "Good ETFs to hedge this inflation or potential recession?", "selftext": "Hey all, hope everyone is well. I\u2019ve been an inactive investor for about 2 years now. By which I mean I just basically set it and forget after I buy. I\u2019ve never done any trading as I am only 20 and I am looking for long term returns. My portfolio is majority VTI as well as 2 other ETFs. Lately though will all the noise in the market around inflation and potential \u201cmini-recession\u201d, I have been researching and looking at some good ETFs to pick up to hedge some of the inflation/poor economy over the next 5 to 8+ years. Today I picked up XLE (Energy sector fund), VNQ (real estate fund) and XLF (financials sector fund). I know Reddit is not going to be my main source of advice but I did want to hear any advice anyone has on here? What do you think some good ETFs would be for the next 3 years? Any thoughts on the ETFs I picked up today. I\u2019ve been researching for the past few weeks and have made a list but any new picks or ideas I would greatly appreciate. Also I should mention that I only buy ETFs, I plan on eventually buying individual stocks once I accumulate more capital and am able to focus a bit more time into the market. And lastly, I know the economy could easily recover and inflation may be slowed down significantly, as nobody can predict the market but I am just looking to trade right now based off what I have noticed may happen in the coming 3 years. \n\n*Note: I know I said good ETFs for the next 3 years, I know that\u2019s a really short time frame. I\u2019m not looking to cash out or make huge gains, I mainly just want a fund that hopefully won\u2019t decrease too much in value. Any money I make on investments in those next few years I would literally just reinvest back into my portfolio if I even decide to sell them.", "upvote_ratio": 0.6, "id": "t3_ttctak", "created_utc": 1648769632.0}
{"sub": "investing", "title": "Warner Media Discovery: Would the sale of WB Games require disclosure to the market/gov prior to the deal being approved?", "selftext": "In May 2021, it was announced that Warner Bros. would be merging with Discovery to form Warner Media Discovery.  The company received FTC approval back in February 2022, and shareholder approval a week or two ago.  The deal is set to formally close in mid-late April 2022.  \n\nOver the past couple years it's been rumored that WB Games would be sold off to a Microsoft, Sony, etc.  After the WMD merger deal was announced, WB came out and said that WB Games would not be sold off.  I guess my question is for how long does this statement have to be true such that they could not be successfully sued for securities fraud, etc.?  \n\nIn other words:\n\n* Once the deal closes are they free to sell off WB Games?  \n* If the sell-off of a WB Games studio were in the works prior to the merger closing, would disclosure of this potential transaction have been required?  \n* If WB Games were sold off some time in the future, could shareholders sue on the basis that they approved the deal with the understanding that WB Games would not be sold off? \n\nJust curious how this all works from a regulatory/legal standpoint.  Many thanks for any insight.", "upvote_ratio": 0.5, "id": "t3_ttcnqe", "created_utc": 1648769146.0}
{"sub": "investing", "title": "Long term DCA into my fav stocks vs index fund", "selftext": "I'm sure this is asked but I can't find it in the search. \n\nI plan on buying weekly, and right now half of my investment goes into index funds and the other half I use to just pick stocks. I pick them kind of at a whim, but I do a basic sanity check of the PE ratio and ESP. \n\nI don't plan on selling for a long time, I have been buying tech stocks near their ATH and still no plans to sell. \n\nI am wondering if there is a significant different to a buy &amp; hold vs index funds. I see that you can't time the market, you can't predict it, etc... but I'm not trying to as much as just investing in long term holds.", "upvote_ratio": 0.75, "id": "t3_ttc49p", "created_utc": 1648767518.0}
{"sub": "investing", "title": "The Yield Curve just officially inverted", "selftext": "[https://www.cnbc.com/quotes/10Y2YS](https://www.cnbc.com/quotes/10Y2YS)\n\nHistorically, the yield on the 2-year treasury topping the yield on the 10-year has preceded every recession.\n\nThis may be a signal that investors are fleeing to the safety represented by US treasuries, or that investors expect rates to fall in the short to medium term.", "upvote_ratio": 0.89, "id": "t3_ttbi9q", "created_utc": 1648765651.0}
{"sub": "investing", "title": "New Investment Portfolio: Hold off on entering or buy in gradually?", "selftext": "I\u2019ve just started working my first real job and want to put some money towards an investment portfolio; however, I will only slowly be getting the capital for it, as I get paid on a week to week basis. Should I just save my money for a couple of months and go start buying the shares I want then, or is it smarter to buy in weekly until I get to the amount I want to be managing (approximately $2000 USD).\n\nThe goal is this portfolio is to be held for 5+ years", "upvote_ratio": 0.76, "id": "t3_tt8m8l", "created_utc": 1648757724.0}
{"sub": "investing", "title": "Does Government's Forced Rothifying of Catch Up Contributions Change The Conventional Wisdom About Roth's In Your 20's?", "selftext": "As I'm sure you've heard, government seems poised to force Gen X into \"rothifying\" their 401k catch up contributions. This will raise 36 billion in revenue by not allowing people during their highest earning years to reduce their MAGI with a pre tax 401k catch up contribution. \n\nSimultaneously, Boomers are being allowed to punt their tax bill down the road by delaying RMD's and are getting extra catch up contributions at 62.   \n\n\nSince catch up contributions are going to be corralled into a Roth in your 50's, do you think the advice about contributing to a roth in your 20's should change? Maybe it should be a more even split? Ideas around this?", "upvote_ratio": 0.67, "id": "t3_tt8hye", "created_utc": 1648757398.0}
{"sub": "investing", "title": "Crypto exposure via self-directed IRA -- good idea/bad idea?", "selftext": "I recently left an employer and will rollover approx. 50K to a Traditional IRA (i.e. pre-tax money). I am considering rolling to self-directed IRA, specifically a Crypto IRA account offered by Directed IRA ([https://directedira.com/cryptocurrency/](https://directedira.com/cryptocurrency/)) and simply buy 1.xx BTC to hold long-term.\n\nContext: age 35,  this allocation would comprise roughly 6% of total assets -- 10% of retirement assets. \n\nPros: Crypto exposure when currently have none, diversify \n\nCons: Fees - for optional cold storage in particular\n\nIs anyone out there getting exposure to crypto this way? Good idea? Bad idea? Who knows?", "upvote_ratio": 0.25, "id": "t3_tt77ub", "created_utc": 1648753869.0}
{"sub": "investing", "title": "Forecasting Fuel Prices: Best Methods?", "selftext": " I\u2018m looking to forecast fuel prices for a long term play in the downstream energy sector. I want to forecast pricing 5 years out.\n\nThe approach I plan on taking is to look at historical refined product cracks (the spread between crude and gasoline/diesel) over the last 4 years and average them out, I will then layer on demand destruction on gasoline using a 1 to 1 application of demand destruction to pricing (i.e. if demand is forecasted to drop by 1%, refined product margin will drop 1%) and the same for diesel (where demand likely increases). After that I can take that and apply those margins to an aggregate of crude forecasts that exist out there (I don\u2019t want to get into trying to predict oil prices) which should then give me a total product price which I can then apply inflation to. Once I have product prices, I might adjust the demand destruction assumptions if prices are forecasted to test all time highs.\n\nDoes that sound logical to the experts out there? Seems pretty simplistic but it\u2019s how I used to do it in my former life. I\u2019m curious if there are ways to improve this.", "upvote_ratio": 0.25, "id": "t3_tt5bis", "created_utc": 1648748738.0}
{"sub": "investing", "title": "Anyone know how to invest in Zapier?", "selftext": "I\u2019ve been a fan of Zapier for years as a customer and I think that they are really at the forefront of no code. I heard that some investors bought some shares. Anyone know of a way to buy into some shares while they are still somewhat small abs undervalued?", "upvote_ratio": 0.5, "id": "t3_tt59ny", "created_utc": 1648748601.0}
{"sub": "investing", "title": "What percent of your overall portfolio do you earmark for \"active/risky investing\"", "selftext": "Hi,\n\nI couldn't find anything related to this but as the title says what do you allocate for investing in say options, individual stocks, crypto/alternative, etc? Are there any rules of thumb for this?\n\nFor a back context of this: This would assume you fully take care of your 401k, IRA, emergency fund, have a stable job and already invest heavily into target date/index funds.\n\nThank you!\n\nEdit: Thank you everyone who has lent their insights. General consensus looks to be 5-20% (with 10% for most) is the sweetsop depending on risk tolerance, individual situation, age, etc.", "upvote_ratio": 0.8, "id": "t3_tt3mqo", "created_utc": 1648744154.0}
{"sub": "investing", "title": "There\u2019s no reason to invest in VXUS / international etfs anymore given how tight the world market is to the US", "selftext": "I don\u2019t understand the argument for having international exposure anymore. The United States rules the world, financially speaking. If the US goes belly up, so does Europe, Australia, and East Asia. China and Russia aren\u2019t reliable. Africa, the Middle East, and South America are consistently unstable, and Europe is over regulated to death.\n\nThe only place at the current time that is stable enough and innovative enough to be worthy of investing in is the US. I don\u2019t see the point of VXUS at all.", "upvote_ratio": 0.26, "id": "t3_tt2zw2", "created_utc": 1648742440.0}
{"sub": "investing", "title": "Reluctant to invest in real estate because it may be immoral.", "selftext": "Hear me out. It\u2019s not any one person\u2019s fault. But housing has become relatively unaffordable for the working class. Real estate has become a store of value in a way it shouldn\u2019t partially due to the incentives in place resulting from our monetary policy. In the 80s, you could make double digit returns in a savings account. Now, savings accounts are negative real yield and significantly so. Again, this is not the fault of real estate investors. They\u2019re just playing the game with the rules presented. \n\nI\u2019m a small business owner with ~30 employees, and I see their struggles. My workers are not minimum wage, either. As I\u2019ve been investing my money I\u2019m frequently tempted to buy a rental as part of a diversified strategy, and I\u2019ve done it before. \n\nBut if I become a landlord, I \u201chave\u201d to directly participate in the inflationary market that makes housing less and less affordable for the working class. I don\u2019t like that. \n\nEquities are less directly a problem because it\u2019s less of a zero sum. Everyone can participate. But everyone \u201cneeds\u201d housing. \n\nSo, my chosen investments have been equities and Bitcoin. I\u2019m not here to \u201corange pill\u201d anyone. I just see these as open systems that don\u2019t necessarily promote wealth inequality like real estate seems to.\n\nCan someone show me how I\u2019m wrong from a moral standpoint assuming I care about the wealth of the working class? I\u2019m still tempted to add rentals to my strategy. It just feels wrongish atm. \n\nOpen to contrary perspectives.\n\n\nEDIT: Ok. So, for the first time, I understand what \"RIP my Inbox\" means. One small clarification. I am using the context of investment. So, the idea of charging less than cost is more like charity to me. That's fine! But, that's not what I'm talking about. I'll show you the napkin math. Median per capita income in my area is about 38k. Small (sub 1000 sqft) SFHs or condo in the same area start at $350k. With a 20% down payment, Cost comes in right about 1600 per month. This is right about 50% of GROSS income for a person here without any profit built in. Just pretty ugly. Now, a person can get help, they can buy with a partner or a family member, etc. I understand. But, the starting place is pretty ugly. So, it feels wrong to participate in that. If you're a median family of two it works okish until you add a child.", "upvote_ratio": 0.72, "id": "t3_tt1lyx", "created_utc": 1648738657.0}
{"sub": "investing", "title": "Understanding the effect of current macro economic conditions on Apple and Microsoft", "selftext": "All,\n\nI'm trying to understand why the market currently favors Apple over Microsoft.\n\nRight now, Apple (176) is only slightly below its all time high (182.9). While Microsoft (312) is quite below its ath (349.67).\n\nConsidering all current supply chain issues, inflation and major economies experiencing stagnation in terms of growth, wouldn't Microsoft have a much better future than apple?\n\nMicrosoft mainly addresses the B2B market and a chunk of its revenues come from subscriptions. It's hard to imagine that companies would simply stop using Microsoft products and switch to something else.\n\nOn the other hand, Apple is more susceptible to inflation and supply chain issues and there is no real need to buy apple products during economic downturns.\n\nIs there anything that explains why Microsoft doesn't seem to in the good books of the market right now?", "upvote_ratio": 0.71, "id": "t3_tt08vc", "created_utc": 1648734766.0}
{"sub": "investing", "title": "Is Chaikin Analytics legitimate?", "selftext": "Got an email invite to hear from this respected wizard who has been performing magic. Scheduled presentation for last night and sat through it.\n\nI should know better. Sounded like they were pitching time shares in Florida. Very hard sell. For only $5000 you too can get $18000 worth of stuff and for 30 days you can get a refund in  Chaikin Credits to spend on their stuff. No cash refunds because people have been signing up, looking at the materials and asking for their money back.\n\nThoughts?\n\nEdit: Has anyone here actually signed up for this?", "upvote_ratio": 0.5, "id": "t3_tszb2n", "created_utc": 1648731871.0}
{"sub": "investing", "title": "Dividend Questions?? Is it worth it?", "selftext": "Hi everyone, would someone please be able to help me understand something about dividend stocks?\nSo I have been looking a if it would be worth saving some money into a dividend stock instead of the bank since I\u2019m only getting .5% interest there.\nI look a look at coke stock to see how much I could save. But I can\u2019t see the point, looking at the payout rate it would take you something daft like 250 years to get the money back you put into your share\u2026 what is the point in investing in a dividend stock unless your already rolling in cash?", "upvote_ratio": 0.69, "id": "t3_tsxx09", "created_utc": 1648727128.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - March 31, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.95, "id": "t3_tsvlwn", "created_utc": 1648717269.0}
{"sub": "investing", "title": "Investment ideas related to microplastic pollution", "selftext": "Microplastics are showing up in the news more and more, and the number of problems people are discovering may be linked to microplastics is increasing fast. Since microplastics do not break down on any reasonable timeline and have basically blanketed the earth, two things are true:\n\n- The problems they cause are only going to get worse as plastic continues to build up in the environment and\n- Solutions that can eliminate microplastics will be infinitely valuable\n\nAll that said, does anyone have thoughts on ways to invest in this space, whether via solutions to problems hypothesized to be caused by microplastics (infertility is a huge one) or solutions to the microplastic problem itself?", "upvote_ratio": 0.5, "id": "t3_tspw5a", "created_utc": 1648694554.0}
{"sub": "investing", "title": "$TLRY controversy and your opinion", "selftext": "i recently bought 63.5 shares of trly at 7.95 (about 500) and i\u2019ve heard a bunch of controversies about the MORE act not passing and ive done my bit of research but what do you guys think will happen. Do you think you see it going up in the future? Maybe even by this friday after the MORE act is voted on or what? Let me know i wanna get a broad idea of what everyones thinking!!", "upvote_ratio": 0.57, "id": "t3_tsoahp", "created_utc": 1648689374.0}
{"sub": "investing", "title": "Foresters s&amp;s isa investment options", "selftext": "I'm pretty new to isas, I opened a stocks and shares isa thinking it was just a bankacount that drops of a lot of tax for investing but the foresters s&amp;s isa wants me to invest through it into a fund if its own choice. I thought I would link my isa to a brokerage account to input and receive money from investments, is it just the foresters s&amp;s isa that's like this or is this the case for all isas?", "upvote_ratio": 0.38, "id": "t3_tsn179", "created_utc": 1648685303.0}
{"sub": "investing", "title": "What are my odds of getting out of this mess unscathed?", "selftext": "So, I am learning how to trade options while using the Fidelity ATP platform. Today, right before the close of business, my intentions were to purchase 1 contract of SPY 458 call expiring on April 14th. Instead I bought 89 of these contracts! I realized something was terribly wrong when I  was instantly down $1300 and falling. I was able to get an order in to sell 5 of the contracts  for a profit of $800 (not sure why only 5 of the 89 contracts sold).\n\nSo here I sit, A long way until market open wondering and waiting!\n\nThe only reason I had 60k in cash in the account was because I just transferred a retirement account from ED Jones to Fidelity and it was transferred in cash. Any advice on how to escape this carnage unscathed is greatly appreciated!", "upvote_ratio": 0.9, "id": "t3_tsljzu", "created_utc": 1648680819.0}
{"sub": "investing", "title": "Am I making a mistake by selling off my mutual funds?", "selftext": "The wife and I started investing in some Vanguard Life Strategy funds (VASIX &amp; VASGX) last year with cash we had sitting around in our Ally accounts. I realize now these might not have been the best investment vessels, but it's too late to undo that. \n\n \u2022 We're planning on buying a new house in about two years and decided to switch our strategy to go in cash-rich for the purchase.\n\n \u2022 We're trying to hit a target dollar value by that point, so I'm planning to sell shares at their 1 year investment marks.\n\n \u2022 Selling our &gt;1 year VASIX shares right now will realize about a 7k loss, and the VASGX will be basically a wash. \n\n \u2022 We have no other capital gains or losses yet for the year, but plan to continue to liquidate the funds at their 1 year mark and putting them into an ALLY account. \n\nMy understanding is that I can only carry 3k of the capital losses into following years, so the extra losses are just that, losses. The market may change a bit before I sell off the other shares, cancelling out some capital gains, but who knows. \n\nDoes anyone see something I should be doing differently? I'm by no means an investing expert, but I'm reading enough market pessimism that's making me scared for the money that I plan on using relatively soon.", "upvote_ratio": 0.71, "id": "t3_tskuwa", "created_utc": 1648678783.0}
{"sub": "investing", "title": "Bond funds during period of rising interest rates", "selftext": "I'm sure I'm overlooking something big here, and I'm hoping someone can help clear this up for me. But to me, being in a bond fund right now is worse than holding cash. \n\nIt seems very likely that interest rates will be rising over the next year or two--not guaranteed but highly likely. As I understand it, the value of bond funds (the NAV) will decrease as rates increase, and that has certainly been happening since last year. For example, VBTLX is down about 6% just this year. \n\nEventually the higher interest rates will make bond funds more attractive, but for now you're losing more principal than you are getting in dividends (which is only 2% or so for VBTLX). And the losses should continue as rates rise. \n\nAt some point the higher interest rate will offset the loss in NAV, but we seem to be a ways from that happening. \n\nSo why bond funds and not cash during this period of rising rates? \n\n(And I'm not really interested in the issue of why one should have some bonds and not 100% equities--I get that. I'm more interested in comparing bond funds to cash as a ballast.)", "upvote_ratio": 0.73, "id": "t3_tsk6q0", "created_utc": 1648676917.0}
{"sub": "investing", "title": "PUBM looks interesting to me", "selftext": " \n\nTL/DR Pubmatic is a buy IMO\n\nPubmatic has seen its revenue rise consistently for the past four years. Even so, PubMatic shares are trading around $26 at the time of this writing, far below their 52-week high of $76.96. A key factor impacting the stock price is the massive change happening in the digital advertising industry.\n\nWith Apple and Google changing the way consumers are tracked, there is a lot of FUD around the online advertising business but IMO this creates a buying opportunity. I\u2019m not going to try to convince you guys that I have a firm grasp on this business. Instead I\u2019m going to regurgitate a bunch of stuff that I read on Motley Fool and Creating Alpha.\n\nPubMatic's cloud-based advertising platform helps content creators, called publishers, efficiently sell the ad space appearing alongside their website and app content. In return, publishers pay fees to PubMatic. By catering to publishers, PubMatic obtains first-party data about the audiences viewing this content for use in its platform. So while Google's impending ad targeting changes, set to happen next year, may hurt ad tech firms reliant on Google's data, PubMatic will largely maintain the targeting insights needed to deliver results to clients. In the first full quarter after Apple changed their privacy policy PUBM recorded record revenue with a 54% increase yoy.\n\nRevenue grew 15% in 2019, 31% in 2020 and 54% in 2021. Now if you\u2019ve read this far you get to hear the kicker. This is a high growth company but get this: THEY MAKE MONEY! And the amount that they make has been steadily growing. In 2021 their fully diluted EPS was $1.00 so they are trading at a p/e of about 26 and a price/sales of 8.5. These numbers are unheard of for a growth company like this. Compare them to TTD (not really a competitor but in the same business) with p/e of 249 and price to sales of 38.18 and I think this is a steal.\n\nPosition: short 10 x $40 PUBM puts 01/23 and long $12.50 PUBM calls 01/23 also, because I like to buy an industry, short: 10 x $22.50 MGNI puts 01/23 and long $7.50 MGNI calls 01/23.", "upvote_ratio": 0.5, "id": "t3_tsk4zf", "created_utc": 1648676781.0}
{"sub": "investing", "title": "What's a semi-accurate best guess on what parties are responsible (and at what % of volume) for the roughly 1-2b shares of SPY traded monthly on average?", "selftext": "Obviously we will never know EXACTLY who/what traded every share.\n\nBut I'm curious if we have any data available to us that can help us understand the markets better.\n\nBlackRock has IVV, Vanguard has VOO. From what I can tell, SPY is managed by State Street Global Advisors.\n\nI know the options chain is obviously part of a certain % of volume (probably a small one) because puts/calls getting exercised will always trigger shares trading hands... right? I can only imagine the complexity of the SPY options chain and won't pretend to understand even a fraction of it. The only question I have is... is it responsible for 90m shares trading a day alone? Probably a not.\n\nI don't think a lot of 401ks or pensions or any other retirement accounts where average Americans dollar-cost-average a portion of their paycheck income/savings end up in SPY. That takes out \"institutional investing\" into SPY, right? If not... what institutions are buying SPY? What product are they offering on top of it, and to who? How large are they? Why wouldn't their customers just buy SPY themselves?\n\nI don't think retail traders/retail day traders/retail swing traders/anything retail are a huge % of the market when speaking about a daily volume perspective. I don't know how big the retail trading market is (on a daily or weekly or monthly scale) but I'd love to learn more. I'm going to guess it's 5-20% of daily volume, being extremely generous. I agree that it's asinine to even think retail is responsible for 20% of daily volume but... I literally can't even estimate/guess/come up with a hypothesis on where the other 95% of the daily volume from SPY comes from. \n\nDo other asset management companies (who aren't big enough to have their own S&amp;P ETF offerings) charge their clients a fee to put their money in SPY for them behind the scenes, and they are daily rebalancing? Is that where some of the volume comes from?\n\nI know a lot of algorithms are fighting each other for fractions of a penny all day at rapid speed, shaving nanoseconds off and frontrunning buy/sell orders from Robinhood (and others) to make money at scale. But... they can't just trade in between each other all day, can they? Like... there are only so many Robinhood buyers, and all of their volume isn't going to SPY... If \"market makers\" are behind the majority of volume because they are providing liquidity, who are the buyers and sellers on each side? We already determined the majority is not retail, not institutions since they would be targeting mutual funds instead of the ETF.\n\nThe market is open from 9:30am -&gt; 4:00pm (6.5 hours, 23.4k seconds). If you ignore pre-market and after-hours (not sure what % of daily volume these two trading windows are responsible for, I'm going to guess less than 10%), you're talking about roughly 2b shares traded in a 21 day window, roughly 95.2m shares per day, just over 4,070 shares a second. What parties are buying and selling 4k shares a second every second of every trading day on average for the entire year on average?\n\n&gt; According to a 2017 JPMorgan analysis, passive investors like ETFs and quantitative investment accounts, which utilize high-frequency algorithmic trading, were responsible for about 60 percent of overall trading volumes while \"fundamental discretionary traders\" (or traders who evaluate the fundamental factors affecting a stock before making an investment) comprised only 10 percent of the overall figures. [1]\n\n[1] https://www.investopedia.com/terms/v/volume.asp\n\nSPY is traded on NYSEARCA.\n\n&gt; NYSE Arca is an electronic securities exchange in the U.S. on which exchange-traded products (ETPs) and equities are listed. NYSE Arca is the world\u2019s leading ETF exchange in terms of volume and listings. On Nov. 7, 2021, the exchange had a commanding 17.3% of the ETF market share in the United States. It claimed to have 2,683 individual ETFs listed with $6.67 trillion in total assets under management (AUM). NYSE Arca also offers the narrowest bid-ask spreads and quotes the most time at the best prices across all U.S. ECNs. [2]\n\n[2] https://www.investopedia.com/terms/n/nyse-arca.asp\n\n&gt; NYSE Arca Equities is a fully electronic stock exchange offering trading in more than 8,000 exchange-listed equity securities, including listings on Nasdaq. The trading platform connects traders to multiple US market centers, providing customers with fast electronic execution and open, direct and anonymous market access. The market structure of the stock exchange provides advantages of displayed and dark liquidity, transparency and efficiency. Trades are processed on a price-time priority basis. Generally, fees are assessed for removing liquidity and rebates may be provided for adding liquidity. Complete details regarding NYSE Arca's pricing structure are available on the NYSE Euronext website. Upon execution, trade details are transmitted to the National Securities Clearing Corporation (NSCC) for clearance and settlement via the Regional Interface Organization (RIO). [3]\n\n[3] https://www.interactivebrokers.com/en/index.php?f=2589", "upvote_ratio": 0.86, "id": "t3_tsirle", "created_utc": 1648673100.0}
{"sub": "investing", "title": "SCHD/QQQM for the long term?", "selftext": "I am thinking of ways to restructure my long term portfolio going forward. I've had QQQ as a core holding for the past few years. Large cap tech growth has done phenomenally obviously. But, given the current financial climate, how much positive EV is left here right now?\n\nI feel like I might need to go for a bit of a barbell approach and introduce SCHD into the mix to diversify out of large cap growth into large cap value/dividend aristocrats. I am also toying with the idea of adding AVUV/AVDV/AVEM to get small cap value and international markets exposure.Some think they are poised to outperform in the next years.\n\nSo, something like:\n\n40% SCHD 25% QQQM 15% AVUV 10% AVDV 10% AVEM \n\nI kind of like this because all have minimal to no overlap between each other. Didn't want to go with VOO as it already has significant overlap with QQQ in terms of large cap growth.And if we do go into a recession in the next few quarters growth plays will get hit the hardest. \n\nPlus, SCHD provides some dividend income that can be used to buy potential dips. \n\nI might switch the allocation between SCHD/QQQM around again in the next few years if I see that large cap growth sufficiently bottomed.", "upvote_ratio": 0.72, "id": "t3_tsgjln", "created_utc": 1648669700.0}
{"sub": "investing", "title": "Is there a loan service that can assist with exercising expiring options to hold?", "selftext": "Hello all,\n\nI am new to options trading and have a contract that expires in 15 days so I want to wage my options. Instead of having the option expire I want to exercise it if I'm unable to sell the contract and just hold onto the stock. Would this be a good idea and is there a service that would loan out the capital to cover that cost?\n\nI've read some company's do that but like EquityBee they prefer extensive investment portfolio's or being a startup employee.\n\nAny suggestions would be helpful.", "upvote_ratio": 0.5, "id": "t3_tsd0rr", "created_utc": 1648664987.0}
{"sub": "investing", "title": "Voo DRIP charged at a premium?", "selftext": "So naturally I set all my investments to DRIP but today I noticed my dividend from VOO was purchased at a 9.2% premium to the current price (459.66 vs the 424.29 acutal ending 3/29 price). Now ive heard of companies discounting DRIPs but never charging a premium. Is this information I\u2019d find on the prospectus?", "upvote_ratio": 0.57, "id": "t3_tsbftu", "created_utc": 1648660727.0}
{"sub": "investing", "title": "Investing In China; Risk/Reward Li Lu thinks it is the market for value/sophisticated investors.", "selftext": "With investing in **Chinese stocks** there are **three type of risks** that investors are mainly concerned with and that you should think about, besides the business itself. I would like to note that there are always risks with investing, and there is always something to worry about so don\u2019t let it trouble your view. I think **Mr. Market is very drunk** with investing in China, with really big swings on news. \n\nYou should always look at the risks/reward and weigh them.\n\nFull article risk, potential rewards, and videos about what Li Lu and Charlie Munger say can be found here:\n\n[https://www.financialstockdata.com/article\\_chinese\\_stock\\_market](https://www.financialstockdata.com/article_chinese_stock_market)\n\nHere you can also find the snippet of Li Lu's interview where he says that he thinks it is the market for value/sophisticated investors. \n\n**The Risks:**\n\n**VIE structure risk**  \nThe shares you hold when you buy Chinese stock on are Variable Interest Entities, these are shell companies of the company. The shell companies have contracts with which they own the right to the dividends and profits of these companies. In this sense you don\u2019t own the company directly, so you do not own their assets. People are afraid the government will just remove them, I don't see why this would be a very logical thing for the Chinese government to do. \n\nThis should be taken into account. If you are interested and have great investors' thoughts on it you can view **Charlie Mungers thoughts on it down below.** \n\n[**https://www.youtube.com/watch?v=zdqlH2XcnwM**](https://www.youtube.com/watch?v=zdqlH2XcnwM)\n\n**Government risk**  \nThe government has been tightening rules on big tech and been handing out big fines. But it has already stated that they will ease a bit again, so not really big risk in my opinion.\n\n**Auditing and Delisting risk**  \nThe financial numbers and figures of these companies do not get checked by any United State regulators, with all other foreign companies this needs to be done on the American exchanges. For this reason, the SEC will delist stocks if an agreement is not reached. However, if can transfer your shares to Honk Kong shares with certain brokers if this happens and if you buy your shares in Honk Kong there is nothing going on.\n\n**Moreover, both parties have stated that they want to work this out, there is news about this** [**here**](https://cnevpost.com/2022/03/11/china-us-expected-to-reach-consensus-on-audit-regulatory-cooperation-soon-report-says/amp/) **. A report stated that they expect to reach a consensus.** \n\nI do have to note that expecting to reach consensus is not the same as reaching consensus and the sec as stated that they just need certain things from these companies. \n\nMoreover, Beijing has released support for foreign listing, which can be viewed in the article [here](https://finance.yahoo.com/news/china-tech-stocks-rebound-dip-012706093.html).\n\n**Potential rewards of investing in China**\n\nThese are real risks, but with the stocks dropping very hard in 2021, 2022 certain Chinese stocks could have become worth the risk/reward. **Crashes** are great places for investors to investigate and look.\n\nIn terms of the strength of the business and the price of the stocks, you get a lot of value. Tencent and Alibaba are great businesses with a competitive advantage trading at low prices in relation to their growth. Investors like Charlie Munger, Monish Pabrai started investing in Alibaba and Tencent in 2021.\n\n[https://www.youtube.com/watch?v=37yQNu2VNdM&amp;t=8s](https://www.youtube.com/watch?v=37yQNu2VNdM&amp;t=8s)\n\n**Economically** if you like looking at that China has a growing middle class and a strong growing economy ( with its cycles of course) which Chinese companies could benefit from.\n\nPeople are very strongly opinioned on investing in China. I would like to see arguments if you disagree great tell me what you think. Don't hate, explain why you think someone is wrong and what you think.", "upvote_ratio": 0.54, "id": "t3_ts5e5k", "created_utc": 1648643569.0}
{"sub": "investing", "title": "How to prepare for a recession", "selftext": "I'm not trying to start a panic discussion so please take that elsewhere. This is a serious question that I'm curious about. When it comes to my investments (and all of our investments), how can we best prepare for the *potentially* coming recession?\n\nFor context: I have a Roth, 401K through work, and some crypto (real crypto, not meme-coins), all of which are contributed to regularly, plus a decent savings between my partner and I.", "upvote_ratio": 0.48, "id": "t3_ts3en7", "created_utc": 1648636330.0}
{"sub": "investing", "title": "Law regarding exemption from CG (bad formatting, from phone)", "selftext": "I'm writing this to ask the community if anyone knows if what I recall has any merit or if it's just plain wrong.\n\nSome time ago I read a paper about funds that avoided some form of taxation (CG I believe is the most likely) by never holding stocks overnight, hence only capturing daily movements (which is itself strange as for the last 20 years, SPY daily is almost flat while most of the gains happen overnight).\n\nIf this strategy permits the exemption from a particular type of taxation, then it could have some merit.\n\nThis strategy could have to do with the fact that in the US, the redemption of appreciated stocks to a partner, such as a broker, from a mutual fund is treated by the IRS as a tax-exempt event.\n\nI also can't recall if it was about the US or European market.\n\nThis memory is from a long time ago, I apologize if it's not very clear.\n\nAny help is appreciated!\n\nTL;DR: Some funds MAY avoid taxation by not holding positions overnight due to a specific law / loophole.", "upvote_ratio": 0.25, "id": "t3_ts3487", "created_utc": 1648635107.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - March 30, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.66, "id": "t3_ts26jb", "created_utc": 1648630869.0}
{"sub": "investing", "title": "50k life savings, put where?", "selftext": "I've had my savings (50k) sitting in a M1 Plus Spend account generating 1% APY. I figured this was a good temporary option until my free trial for a plus account was up. I am curious to where i should put this money now, and how much of it I should invest. I have about $500 in a '2045 aggressive' grouping of various vanguard mutual funds, how much of my savings should i add to this (or something else?)", "upvote_ratio": 0.35, "id": "t3_ts0pb4", "created_utc": 1648624077.0}
{"sub": "investing", "title": "AT&amp;T Announces Details for Completion of WM Spin-Off", "selftext": "How do you all think this will affect the price of AT&amp;T stock after the spin off? I've read some predictions of a $3-4 drop after the spin-off.\n\nhttps://about.att.com/story/2022/details-for-completion-of-warnermedia-spinoff.html\n\nDALLAS, March 25, 2022 \u2014 Today\u00a0AT&amp;T Inc.* (NYSE:T) announced that it has declared a stock dividend to effect the spin-off of 100% of AT&amp;T\u2019s interest in WarnerMedia to AT&amp;T\u2019s shareholders. The record date for the stock dividend is the close of business on April 5, 2022. This stock dividend is in connection with the previously announced transaction to combine AT&amp;T\u2019s WarnerMedia business with Discovery, Inc. (NASDAQ: DISCA, DISCB, DISCK). On the closing date of the transaction, anticipated to be in April, AT&amp;T shareholders will receive, on a tax-free basis, an estimated 0.24 shares of stock in Warner Bros. Discovery, Inc. (WBD) for each share of AT&amp;T common stock.\u00a0\n\nAT&amp;T shareholders as of the stock dividend record date will be entitled to receive shares of WarnerMedia Spinco common stock, representing 100% of AT&amp;T\u2019s interest in WarnerMedia. Immediately following this spin-off, the WarnerMedia Spinco shares will be exchanged for stock representing approximately 71% of the new WBD on a fully diluted basis. The exact number of shares of WBD common stock to be received by AT&amp;T shareholders for each AT&amp;T common share will be determined immediately before the closing based on the number of shares of AT&amp;T common stock outstanding and the number of shares of Discovery common stock outstanding on an as\u2011converted and as\u2011exercised basis.\n\nAT&amp;T shareholders do not need to take any action. Their WarnerMedia Spinco shares will automatically be exchanged for WBD common stock in the merger, which will occur on the closing date of the transaction. Following close of the transaction, AT&amp;T shareholders will continue to hold, along with their new shares of WBD common stock, the same number of shares of AT&amp;T common stock they held immediately prior to close. After close, investors should expect that AT&amp;T\u2019s share price will adjust to reflect the transfer of the WarnerMedia business to the newly formed Warner Bros. Discovery entity.\n\nFactors that May Affect the Timing of the Spin-Off\n\nThe timing of the spin-off is subject to the satisfaction or waiver of the closing conditions for the transaction. If certain closing conditions are not satisfied or waived in advance of April 5, AT&amp;T may elect to change the stock dividend record date to a later date.\n\nAT&amp;T Declares Dividends on Common and Preferred Stock\n\nWith the close of the pending WarnerMedia transaction expected in April, the AT&amp;T board of directors today also declared a second quarter dividend of $0.2775 per share on the company\u2019s common stock. While future dividends remain subject to board approval, this amount is consistent with AT&amp;T\u2019s previous announcement that the board had approved an expected post-close annual common dividend of $1.11 per share. At the updated rate, AT&amp;T\u2019s stock remains among the best dividend-yielding stocks in the United States and in the Fortune 500.\n\nAdditionally, the board of directors declared quarterly dividends on the company\u2019s 5.000% Perpetual Preferred Stock, Series A, and the company\u2019s 4.750% Perpetual Preferred Stock, Series C. The Series A dividend is $312.50 per preferred share, or $0.3125 per depositary share. The Series C dividend is $296.875 per preferred share, or $0.296875 per depositary share.\n\nDividends on the common stock and Series A and Series C preferred stock are payable on May 2, 2022, to shareholders of record of the respective shares at the close of business on April 14, 2022.\n\nThe board of directors also declared an annual dividend on the company\u2019s Fixed Rate Reset Perpetual Preferred Stock, Series B, of \u20ac2,875.00 per preferred share. Dividends on the Series B preferred stock are payable on May 3, 2022, to shareholders of record as of the close of business on April 14, 2022.\n\nTwo-Way Trading for AT&amp;T Stock Expected to Begin on April 4\n\nAT&amp;T has been advised by the New York Stock Exchange (the NYSE) that beginning on the trading day immediately prior to the April 5 record date for the spin-off distribution (currently April 4) and continuing through the close of trading on the business day before the closing date of the merger,1\u00a0there will be two markets in AT&amp;T common stock on the NYSE: a \"regular way\" market and an \"ex-distribution\" market. During this period of two-way trading in AT&amp;T common stock, there will also be a market on the Nasdaq for WBD common stock on a \u201cwhen issued\u201d (\u201cWI\u201d) basis.\n\nThe trading options that will be available during the two-way trading period are:\n\nAT&amp;T Regular Way Trading\nIf, during the period of two-way trading, an AT&amp;T shareholder sells a share of AT&amp;T common stock in the regular way market under AT&amp;T's NYSE symbol, \"T,\" the shareholder will be selling both the share of AT&amp;T common stock and the right to receive shares of WBD common stock in the transaction.\n\nAT&amp;T Ex-distribution Trading\nIf, during the period of two-way trading, an AT&amp;T shareholder sells a share of AT&amp;T common stock in the ex-distribution market under the temporary NYSE symbol \"T WI,\" the AT&amp;T shareholder will be selling only a share of AT&amp;T common stock and will retain the right to receive shares of WBD common stock in the transaction.\n\nWBDWV Trading\nDuring the two-way trading period, an AT&amp;T shareholder also has the option of selling the right to receive shares of WBD common stock while retaining shares of AT&amp;T common stock. This option will be available under the temporary Nasdaq symbol \"WBDWV\".\n\nTrades under the symbols \"T WI\" and \"WBDWV\" will settle after the closing date of the WarnerMedia-Discovery transaction. If the transaction is not completed, all trades made under these temporary symbols will be cancelled.\n\nIn all cases, investors should consult with their financial and tax advisors regarding the specific implications of selling shares of their AT&amp;T common stock or the right to receive shares of WBD common stock on or before the closing date of the WarnerMedia-Discovery transaction.", "upvote_ratio": 0.92, "id": "t3_trzp4j", "created_utc": 1648619975.0}
{"sub": "investing", "title": "Wash sale across 2 different brokerages, will it confuse the IRS and thus be a red flag?", "selftext": "I sold shares at a loss at one brokerage. Now it's much lower than what I sold it for and I want to buy it back. But the brokerage I sold it at is charging the equivalent of a 1% commission. My other brokerage doesn't have that fee. So I'm thinking about buying it back using my other brokerage. This will be a wash sale. But this won't be coordinated between the two brokerages. So my 1099 from the first broker will note the loss and the second broker won't have any idea. I would report the wash sale when I file but my return won't match the 1099s from the brokers. The 1099 from the first broker will say I had a big loss but my return will not since it's a wash sale applied to the shares bought from the second broker. In future, after I sell at the second broker their 1099 will show a gain but on my return it might still be a loss because of the wash.\n\nWill the IRS be confused by all this and thus cause a problem? It's all on the up and up and I would be able to show them at an audit. But I don't want to go asking for an audit.", "upvote_ratio": 0.73, "id": "t3_trx80a", "created_utc": 1648610829.0}
{"sub": "investing", "title": "Doordash has the best app. It will continue to take share", "selftext": "Doordash is on a tear. It is crushing Eats/Postmates share. Their apps simply aren't good. If they can't hold onto demand, supply will falter.\n\nThis space is more differentiated than rideshare, where the service is a larger part, and the app matters less. For rideshare, you need to select 2 locations. For food, there is order and variant complexity. No one can handle this like Doordash.\n\nI experienced the same thing with iFood and Rappi in Brazil. Would be very worried about iFood losing all of it's share to a superior app. Rappi is executing better at everything\u2014 15 minute delivery, market, pharmacy, alcohol. As Doordash takes core share, it can expand into more verticals in more categories, as other platforms struggle to get off the ground.\n\nLong $DASH.\n\n[https://secondmeasure.com/datapoints/food-delivery-services-grubhub-uber-eats-doordash-postmates/](https://secondmeasure.com/datapoints/food-delivery-services-grubhub-uber-eats-doordash-postmates/)\n\n&amp;#x200B;", "upvote_ratio": 0.19, "id": "t3_tru1xi", "created_utc": 1648600411.0}
{"sub": "investing", "title": "Home made screener using EDGAR", "selftext": "I know there are tools and service that do this better but I wanted to learn some programming and how to use the SEC EDGAR API  to fetch all the financial statements for some 12k+ traded entities\u00a0\n\n[SEC.gov | About EDGAR](https://www.sec.gov/edgar/about)\n\nNext, I had to parse through electronic financial statement filing (still trying to determine the many variations of what seemingly is the same thing  [Taxonomy Viewer (fasb.org)](https://xbrlview.fasb.org/yeti/resources/yeti-gwt/Yeti.jsp)\n\nSettled on these terms \u00a0\n\n* Revenues \\[Revenue\\]\n* IncomeLossFromContinuingOperations \\[Income\\]\n* CashAndCashEquivalentsAtCarryingValue \\[Cash\\]\n* LongTermDebt \\[Debt\\]\n* PaymentsOfDividends \\[Dividends\\]\n* AssetsCurrent \\[Assests\\]\n* LiabilitiesCurrent \\[Liabilities\\]\n* CommonStockSharesOutstanding \\[Shares\\]\n\nand then developed a few pseudo ratios as follows:\u00a0\n\nLeverage = \\[Liabilities\\]/\\[Assests\\]\n\nLiquidity = \\[Cash\\]/\\[Income\\]\n\nDebtRatio = \\[Debt\\]/\\[Revenue\\]\n\nProfitability = \\[Income\\]/\\[Revenue\\]\n\nPayout = \\[Dividends\\]/\\[Income\\]\n\nEPS = \\[Income\\] / \\[Shares\\]\n\nApplied some filters, sorted and here is what I got. (table is trimmed to fit the post size limit). There is some bad data so I think there will be some invalid entries but does this in general pass the smell test?  \n\n&amp;#x200B;\n\n|Entity|Yahoo|Profitability|Liquidity|Payout|Leverage|DebtRatio|\n|:-|:-|:-|:-|:-|:-|:-|\n|EnerSys|[https://finance.yahoo.com/quote/ENS](https://finance.yahoo.com/quote/ENS)|3.58|0.94|0.05|0.39|0.41|\n|CLEARONEINC|[https://finance.yahoo.com/quote/CLRO](https://finance.yahoo.com/quote/CLRO)|1.25|4.45|0.15|0.24|0.03|\n|ADVANCEDENERGYINDUSTRIESINC|[https://finance.yahoo.com/quote/AEIS](https://finance.yahoo.com/quote/AEIS)|1.00|1.64|0.00|0.26|0.38|\n|BEDBATHBEYONDINC|[https://finance.yahoo.com/quote/BBBY](https://finance.yahoo.com/quote/BBBY)|0.95|0.53|0.01|0.47|0.17|\n|HELMERICH\u00c2\u00a0PAYNE\u00c2\u00a0INC|[https://finance.yahoo.com/quote/HP](https://finance.yahoo.com/quote/HP)|0.81|0.71|0.00|0.35|0.25|\n|MSCIINC|[https://finance.yahoo.com/quote/MSCI](https://finance.yahoo.com/quote/MSCI)|0.80|0.79|0.07|0.57|0.29|\n|PROGRESSSOFTWARECORPMA|[https://finance.yahoo.com/quote/PRGS](https://finance.yahoo.com/quote/PRGS)|0.75|2.53|0.10|0.63|0.28|\n|ApolloMedicalHoldingsInc|[https://finance.yahoo.com/quote/AMEH](https://finance.yahoo.com/quote/AMEH)|0.69|2.15|0.19|0.41|0.21|\n|GILEADSCIENCESINC|[https://finance.yahoo.com/quote/GILD](https://finance.yahoo.com/quote/GILD)|0.67|0.84|0.15|0.41|0.35|\n|LASVEGASSANDSCORP|[https://finance.yahoo.com/quote/LVS](https://finance.yahoo.com/quote/LVS)|0.67|0.84|0.27|0.61||\n|HALLIBURTONCOMPANY|[https://finance.yahoo.com/quote/HAL](https://finance.yahoo.com/quote/HAL)|0.65|2.39|0.03|0.37|0.47|\n|SANDERSONFARMSINC|[https://finance.yahoo.com/quote/SAFM](https://finance.yahoo.com/quote/SAFM)|0.63|1.01|0.15|0.31|0.01|\n|INTUITINC|[https://finance.yahoo.com/quote/INTU](https://finance.yahoo.com/quote/INTU)|0.63|0.66|0.12|0.65|0.07|\n|KAISERALUMINUMCORP|[https://finance.yahoo.com/quote/KALU](https://finance.yahoo.com/quote/KALU)|0.60|1.63|0.06|0.28|0.00|\n|PARKAEROSPACECORP|[https://finance.yahoo.com/quote/PKE](https://finance.yahoo.com/quote/PKE)|0.57|7.23|1.65|0.07|0.38|\n|HENNESSYADVISORSINC|[https://finance.yahoo.com/quote/HNNA](https://finance.yahoo.com/quote/HNNA)|0.57|1.02|0.12|0.52|0.10|\n|UBIQUITIINC|[https://finance.yahoo.com/quote/UI](https://finance.yahoo.com/quote/UI)|0.57|0.88|0.06|0.19|0.14|\n|APPLIEDMATERIALSINCDE|[https://finance.yahoo.com/quote/AMAT](https://finance.yahoo.com/quote/AMAT)|0.56|1.70|0.22|0.38|0.39|\n|EBIXINC|[https://finance.yahoo.com/quote/EBIX](https://finance.yahoo.com/quote/EBIX)|0.55|0.41|0.04|0.61|0.17|\n|CISCOSYSTEMSINC|[https://finance.yahoo.com/quote/CSCO](https://finance.yahoo.com/quote/CSCO)|0.53|0.66|0.25|0.38|0.48|\n|NetAppInc|[https://finance.yahoo.com/quote/NTAP](https://finance.yahoo.com/quote/NTAP)|0.52|3.61|0.23|0.54|0.46|\n|VERISIGNINCCA|[https://finance.yahoo.com/quote/VRSN](https://finance.yahoo.com/quote/VRSN)|0.52|1.52|0.07|0.69|0.01|\n|VALVOLINEINC|[https://finance.yahoo.com/quote/VVV](https://finance.yahoo.com/quote/VVV)|0.47|0.33|0.08|0.48||\n|GENPACTLIMITED|[https://finance.yahoo.com/quote/G](https://finance.yahoo.com/quote/G)|0.45|1.08|0.01|0.56|0.37|\n|MERIDIANBIOSCIENCEINC|[https://finance.yahoo.com/quote/VIVO](https://finance.yahoo.com/quote/VIVO)|0.43|1.18|0.05|0.18||\n|MORNINGSTARINC|[https://finance.yahoo.com/quote/MORN](https://finance.yahoo.com/quote/MORN)|0.43|1.07|0.11|0.66|0.07|\n|FACTSETRESEARCHSYSTEMSINC|[https://finance.yahoo.com/quote/FDS](https://finance.yahoo.com/quote/FDS)|0.42|0.54|0.14|0.41|0.15|\n|SIGNETJEWELERSLIMITED|[https://finance.yahoo.com/quote/SIG](https://finance.yahoo.com/quote/SIG)|0.42|1.03|0.00|0.34|0.04|\n|WD40CO|[https://finance.yahoo.com/quote/WDFC](https://finance.yahoo.com/quote/WDFC)|0.41|1.04|0.36|0.44|0.15|\n|InterDigitalInc|[https://finance.yahoo.com/quote/IDCC](https://finance.yahoo.com/quote/IDCC)|0.37|2.42|0.17|0.28|0.08|\n|ResMedInc|[https://finance.yahoo.com/quote/RMD](https://finance.yahoo.com/quote/RMD)|0.35|1.92|0.29|0.32|0.23|\n|GRACOINC|[https://finance.yahoo.com/quote/GGG](https://finance.yahoo.com/quote/GGG)|0.35|0.42|0.26|0.31||\n|COLUMBIASPORTSWEARCOMPANY|[https://finance.yahoo.com/quote/COLM](https://finance.yahoo.com/quote/COLM)|0.34|2.24|0.02|0.32||\n|HEMISPHEREMEDIAGROUPINC|[https://finance.yahoo.com/quote/HMTV](https://finance.yahoo.com/quote/HMTV)|0.32|2.37|0.04|0.22||\n|INSIGNIASYSTEMSINCMN|[https://finance.yahoo.com/quote/ISIG](https://finance.yahoo.com/quote/ISIG)|0.32|2.69|0.54|0.28||\n|WEYCOGROUPINC|[https://finance.yahoo.com/quote/WEYS](https://finance.yahoo.com/quote/WEYS)|0.32|0.89|0.26|0.28||\n|VIRTUSINVESTMENTPARTNERSINC|[https://finance.yahoo.com/quote/VRTS](https://finance.yahoo.com/quote/VRTS)|0.32|0.86|0.02|0.44|0.04|\n|FreeportMcMoRanInc|[https://finance.yahoo.com/quote/FCX](https://finance.yahoo.com/quote/FCX)|0.31|0.92|0.14|0.41|0.16|\n|NATIONALPRESTOINDUSTRIESINC|[https://finance.yahoo.com/quote/NPK](https://finance.yahoo.com/quote/NPK)|0.30|0.93|0.18|0.17||\n|CRANECO|[https://finance.yahoo.com/quote/CR](https://finance.yahoo.com/quote/CR)|0.29|1.03|0.16|0.53|0.09|\n|WisdomTreeInvestmentsInc|[https://finance.yahoo.com/quote/WETF](https://finance.yahoo.com/quote/WETF)|0.28|3.43|0.66|0.33|0.32|\n|INTERPARFUMSINC|[https://finance.yahoo.com/quote/IPAR](https://finance.yahoo.com/quote/IPAR)|0.28|2.11|0.17|0.32|0.04|\n|CHASECORP|[https://finance.yahoo.com/quote/CCF](https://finance.yahoo.com/quote/CCF)|0.27|1.23|0.14|0.27|0.09|\n|LOUISIANAPACIFICCORPORATION|[https://finance.yahoo.com/quote/LPX](https://finance.yahoo.com/quote/LPX)|0.27|2.73|0.09|0.23|0.46|\n|STRYKERCORP|[https://finance.yahoo.com/quote/SYK](https://finance.yahoo.com/quote/SYK)|0.26|1.27|0.25|0.38|0.01|\n|NVIDIACORP|[https://finance.yahoo.com/quote/NVDA](https://finance.yahoo.com/quote/NVDA)|0.26|0.96|0.11|0.19|0.19|\n|STARBUCKSCORP|[https://finance.yahoo.com/quote/SBUX](https://finance.yahoo.com/quote/SBUX)|0.26|0.83|0.24|0.77|0.21|\n|BRileyFinancialInc|[https://finance.yahoo.com/quote/RILY](https://finance.yahoo.com/quote/RILY)|0.26|0.62|0.02|0.49|0.02|\n|TRACTORSUPPLYCODE|[https://finance.yahoo.com/quote/TSCO](https://finance.yahoo.com/quote/TSCO)|0.25|0.42|0.16|0.52|0.01|\n|KORNFERRY|[https://finance.yahoo.com/quote/KFY](https://finance.yahoo.com/quote/KFY)|0.25|2.54|0.00|0.53|0.00|\n|MASIMOCORP|[https://finance.yahoo.com/quote/MASI](https://finance.yahoo.com/quote/MASI)|0.25|1.06|0.07|0.27||\n|QUALCOMMINCDE|[https://finance.yahoo.com/quote/QCOM](https://finance.yahoo.com/quote/QCOM)|0.25|1.14|0.06|0.36|0.14|\n|CALMAINEFOODSINC|[https://finance.yahoo.com/quote/CALM](https://finance.yahoo.com/quote/CALM)|0.24|0.49|0.28|0.21|0.04|\n|FORTUNEBRANDSHOMESECURITYINC|[https://finance.yahoo.com/quote/FBHS](https://finance.yahoo.com/quote/FBHS)|0.23|0.35|0.10|0.62|0.27|\n|RBCBEARINGSINCORPORATED|[https://finance.yahoo.com/quote/ROLL](https://finance.yahoo.com/quote/ROLL)|0.23|1.00|0.03|0.16|0.01|\n|ETHANALLENINTERIORSINC|[https://finance.yahoo.com/quote/ETD](https://finance.yahoo.com/quote/ETD)|0.23|1.40|0.35|0.54|0.16|\n|BROADRIDGEFINANCIALSOLUTIONSINC|[https://finance.yahoo.com/quote/BR](https://finance.yahoo.com/quote/BR)|0.23|0.70|0.22|0.73|0.18|\n|ULTABEAUTYINC|[https://finance.yahoo.com/quote/ULTA](https://finance.yahoo.com/quote/ULTA)|0.22|0.43|0.01|0.42||\n|AdtalemGlobalEducationInc|[https://finance.yahoo.com/quote/ATGE](https://finance.yahoo.com/quote/ATGE)|0.22|1.78|0.00|0.57|0.14|\n|SEIINVESTMENTSCOMPANY|[https://finance.yahoo.com/quote/SEIC](https://finance.yahoo.com/quote/SEIC)|0.22|1.92|0.22|0.23||\n|GAPINC|[https://finance.yahoo.com/quote/GPS](https://finance.yahoo.com/quote/GPS)|0.22|1.37|0.20|0.57|0.07|\n|LantheusHoldingsInc|[https://finance.yahoo.com/quote/LNTH](https://finance.yahoo.com/quote/LNTH)|0.22|1.80|0.00|0.37|0.16|\n|HUNTINGTONINGALLSINDUSTRIESINC|[https://finance.yahoo.com/quote/HII](https://finance.yahoo.com/quote/HII)|0.21|0.86|0.22|0.72|0.21|\n|CVRENERGYINC|[https://finance.yahoo.com/quote/CVI](https://finance.yahoo.com/quote/CVI)|0.21|1.08|0.43|0.41||\n|MarcusMillichapInc|[https://finance.yahoo.com/quote/MMI](https://finance.yahoo.com/quote/MMI)|0.21|1.06|0.05|0.32||\n|BASSETTFURNITUREINDUSTRIESINC|[https://finance.yahoo.com/quote/BSET](https://finance.yahoo.com/quote/BSET)|0.21|2.71|0.44|0.53|0.03|\n|SimulationsPlusInc|[https://finance.yahoo.com/quote/SLP](https://finance.yahoo.com/quote/SLP)|0.21|2.15|0.47|0.10|0.02|\n|UNIVERSALDISPLAYCORPORATION|[https://finance.yahoo.com/quote/OLED](https://finance.yahoo.com/quote/OLED)|0.20|2.01|0.09|0.15||\n|WABASHNATIONALCorp|[https://finance.yahoo.com/quote/WNC](https://finance.yahoo.com/quote/WNC)|0.20|1.40|0.07|0.46|0.16|\n|TERADYNEINC|[https://finance.yahoo.com/quote/TER](https://finance.yahoo.com/quote/TER)|0.20|1.10|0.07|0.28|0.01|\n|PingtanMarineEnterpriseLtd|[https://finance.yahoo.com/quote/PME](https://finance.yahoo.com/quote/PME)|0.20|1.22|0.06|0.78|0.49|\n|NOBILITYHOMESINC|[https://finance.yahoo.com/quote/NOBH](https://finance.yahoo.com/quote/NOBH)|0.20|8.79|0.31|0.15||\n|LAZBOYINC|[https://finance.yahoo.com/quote/LZB](https://finance.yahoo.com/quote/LZB)|0.20|1.55|0.12|0.40|0.00|\n|ROCKYBRANDSINC|[https://finance.yahoo.com/quote/RCKY](https://finance.yahoo.com/quote/RCKY)|0.19|0.62|0.10|0.19|0.19|\n|HECLAMININGCODE|[https://finance.yahoo.com/quote/HL](https://finance.yahoo.com/quote/HL)|0.19|7.31|0.09|0.44||\n|PowerIntegrationsInc|[https://finance.yahoo.com/quote/POWI](https://finance.yahoo.com/quote/POWI)|0.19|2.31|0.23|0.15||\n|ZiffDavisInc|[https://finance.yahoo.com/quote/ZD](https://finance.yahoo.com/quote/ZD)|0.18|1.51|0.22|0.62|0.41|\n|TENNANTCO|[https://finance.yahoo.com/quote/TNC](https://finance.yahoo.com/quote/TNC)|0.18|0.96|0.25|0.50|0.05|\n|GameStopCorp|[https://finance.yahoo.com/quote/GME](https://finance.yahoo.com/quote/GME)|0.17|2.23|0.21|0.79|0.08|\n|FutureFuelCorp|[https://finance.yahoo.com/quote/FF](https://finance.yahoo.com/quote/FF)|0.17|3.28|0.86|0.16||\n|COHERENTINC|[https://finance.yahoo.com/quote/COHR](https://finance.yahoo.com/quote/COHR)|0.16|2.58|0.01|0.25|0.04|\n|TRIMASCORP|[https://finance.yahoo.com/quote/TRS](https://finance.yahoo.com/quote/TRS)|0.16|0.81|0.00|0.56|0.26|\n|HEARTLANDEXPRESSINC|[https://finance.yahoo.com/quote/HTLD](https://finance.yahoo.com/quote/HTLD)|0.16|0.68|0.13|0.35|0.01|\n|KEWAUNEESCIENTIFICCORPDE|[https://finance.yahoo.com/quote/KEQU](https://finance.yahoo.com/quote/KEQU)|0.16|1.66|0.25|0.46|0.06|\n|HACKETTGROUPINC|[https://finance.yahoo.com/quote/HCKT](https://finance.yahoo.com/quote/HCKT)|0.15|0.70|0.13|0.61|0.02|\n|CSGSYSTEMSINTERNATIONALINC|[https://finance.yahoo.com/quote/CSGS](https://finance.yahoo.com/quote/CSGS)|0.15|0.96|0.11|0.56|0.18|\n|EnovaInternationalInc|[https://finance.yahoo.com/quote/ENVA](https://finance.yahoo.com/quote/ENVA)|0.15|0.43|0.08|0.14|0.38|\n|COLUMBUSMCKINNONCORP|[https://finance.yahoo.com/quote/CMCO](https://finance.yahoo.com/quote/CMCO)|0.14|1.29|0.02|0.43|0.27|\n|NovaLifestyleInc|[https://finance.yahoo.com/quote/NVFY](https://finance.yahoo.com/quote/NVFY)|0.14|1.64|0.05|0.19||\n|BRUKERCORP|[https://finance.yahoo.com/quote/BRKR](https://finance.yahoo.com/quote/BRKR)|0.14|1.31|0.01|0.45|0.11|\n|JJSNACKFOODSCORP|[https://finance.yahoo.com/quote/JJSF](https://finance.yahoo.com/quote/JJSF)|0.14|1.68|0.24|0.29||\n|JerashHoldingsUSInc|[https://finance.yahoo.com/quote/JRSH](https://finance.yahoo.com/quote/JRSH)|0.13|2.23|0.21|0.20||\n|FLANIGANSENTERPRISESINC|[https://finance.yahoo.com/quote/BDL](https://finance.yahoo.com/quote/BDL)|0.13|1.51|0.00|0.70|0.08|\n|HURCOCOMPANIESINC|[https://finance.yahoo.com/quote/HURC](https://finance.yahoo.com/quote/HURC)|0.13|2.32|0.06|0.28||\n|BalchemCorporation|[https://finance.yahoo.com/quote/BCPC](https://finance.yahoo.com/quote/BCPC)|0.12|1.25|0.11|0.35|0.04|\n|STEELCASEINC|[https://finance.yahoo.com/quote/SCS](https://finance.yahoo.com/quote/SCS)|0.12|0.99|0.22|0.63|0.11|\n|PHIBROANIMALHEALTHCORP|[https://finance.yahoo.com/quote/PAHC](https://finance.yahoo.com/quote/PAHC)|0.12|0.53|0.17|0.34|0.03|\n|MATCHGROUPINC|[https://finance.yahoo.com/quote/MTCH](https://finance.yahoo.com/quote/MTCH)|0.12|3.14|0.01|0.37|0.19|\n|GENERALDYNAMICSCORPORATION|[https://finance.yahoo.com/quote/GD](https://finance.yahoo.com/quote/GD)|0.11|0.88|0.22|0.77|0.06|\n|SONOCOPRODUCTSCOMPANY|[https://finance.yahoo.com/quote/SON](https://finance.yahoo.com/quote/SON)|0.11|0.66|0.25|0.72|0.16|\n|OwensCorning|[https://finance.yahoo.com/quote/OC](https://finance.yahoo.com/quote/OC)|0.11|0.60|0.15|0.60|0.39|\n|SCHLUMBERGERLIMITEDNV|[https://finance.yahoo.com/quote/SLB](https://finance.yahoo.com/quote/SLB)|0.11|0.33|0.69|0.63||\n|UFPINDUSTRIESINC|[https://finance.yahoo.com/quote/UFPI](https://finance.yahoo.com/quote/UFPI)|0.11|0.50|0.09|0.32|0.04|\n|MEDIFASTINC|[https://finance.yahoo.com/quote/MED](https://finance.yahoo.com/quote/MED)|0.11|1.04|0.25|0.39||", "upvote_ratio": 0.43, "id": "t3_trtxpz", "created_utc": 1648600036.0}
{"sub": "investing", "title": "\"The market is punishing US companies that rely on international supply chains or have a high international manufacturing footprint.\" - Goldman", "selftext": "See https://twitter.com/jessefelder/status/1508852329633644544 for a chart and the original article (paywalled).\n\nWith an overweight to companies like Cleveland Cliffs and Alcoa, I am outperforming the indices so far this year. I think this trend can continue, although not in a straight line.", "upvote_ratio": 0.94, "id": "t3_trt2d4", "created_utc": 1648597267.0}
{"sub": "investing", "title": "Does paying AMT reset cost basis?", "selftext": "I have some ISO stocks which I exercised and recently paid AMT for. They have now cleared the period of time required to be taxed as long-term capital gains. However thanks to the market they are now losses vs the FMV at time of exercise.\n\nGiven that I paid AMT on that FMV, does that now count as the cost basis for these stocks? If I sell them, can I harvest tax losses? Or will they still count as gains vs the original strike price?", "upvote_ratio": 0.5, "id": "t3_trlc75", "created_utc": 1648587722.0}
{"sub": "investing", "title": "Anyone have experience with EquityBee on the investor side?", "selftext": "On paper EquityBee looks appealing, but they can't share historical performance  and I can't find much of anything online about from the investor point of view. Does anyone here have experience investing in an EquityBee deal? Any thoughts on the experience?", "upvote_ratio": 0.67, "id": "t3_trea40", "created_utc": 1648579348.0}
{"sub": "investing", "title": "Ford (F) P/E ratio is around 4 !", "selftext": "[https://www.macrotrends.net/stocks/charts/F/ford-motor/pe-ratio](https://www.macrotrends.net/stocks/charts/F/ford-motor/pe-ratio)\n\nCurrent and historical p/e  ratio for Ford Motor (F) from 2010 to 2021.  The price to earnings ratio  is calculated by taking the latest closing price and dividing it by the  most recent earnings per share (EPS) number.  The PE ratio is a simple  way to assess whether a stock is over or under valued and is the most  widely used valuation measure.", "upvote_ratio": 0.81, "id": "t3_tre80o", "created_utc": 1648579194.0}
{"sub": "investing", "title": "Take stock advice with a grain of salt", "selftext": "Just a reminder. I know this is entirely anecdotal but I see people recommending a lot of stocks on here like a herd. I decided to invest in some of the popular reddit stocks about ~yr ago and guess what, they're mostly down. In short, no one knows what they're talkin about, your best bet is sp500 unless you like to gamble.", "upvote_ratio": 0.75, "id": "t3_tr9swk", "created_utc": 1648573634.0}
{"sub": "investing", "title": "Dividends - Monthly or Quarterly ?", "selftext": "I understand the basics of dividends and compound interest so I know that, for an identical dividend amount it's always better to pick something that pays a monthly dividend. What has got me thinking is how to figure out which is better when one is monthly and the other is quarterly. I'll make an abstract example to illustrate my point:\n\nImagine an asset that pays a monthly dividend of $0.46, an amount roughly equal to 0.75% the cost of one share. \n\nImagine a second asset that pays a quarterly dividend of $0.47, an amount roughly equal to 2.78% the cost of one share. \n\nWhich would be the better investment? Would it be better to purchase one of the first asset, or three of the second asset at roughly the same price? \n\nFor the curious, my examples are loosely based upon JEPI and PMT.", "upvote_ratio": 0.25, "id": "t3_tr50xw", "created_utc": 1648568882.0}
{"sub": "investing", "title": "When will Psychodelic market take off", "selftext": "I have a significant position in the emerging psychodelic market and it is downby greater than 50%.  I bought under the advice of Teeka Twarri at Palm Beach Research group.  Their psychodelic connection is some douchbag named \u201cZappy\u201d  note:  never take stock advice from someone named Zappy.\n\nI am not going to sell at a loss but am wondering how far behind is their medical psychodelic movement from standard practice, if that ever happens.  I think I might be waiting ten years on this one", "upvote_ratio": 0.09, "id": "t3_tr4jmz", "created_utc": 1648567541.0}
{"sub": "investing", "title": "The spread between the 2yr and 10yr is narrowing", "selftext": "As of writing this, the yield on the 2 yr Treasury Note is sitting at 2.407% while the yield on the 10 yr is sitting at 2.440%. This leaves the spread at .033%, while it closed yesterday, 03/28, at .11%. \n\nThis is the lowest spread we've seen since August of 2019, just before the spread inverted. \n\n\"An inverted yield curve in U.S. Treasuries has predicted **every recession**  since 1955, with only one false signal during that time.13 It even  \"predicted\" the economic downturn that followed the COVID-19 pandemic  (although most economists attribute this to luck, and not the fact that  it can predict natural disasters).\" -Investopedia", "upvote_ratio": 0.92, "id": "t3_tr3ecc", "created_utc": 1648566467.0}
{"sub": "investing", "title": "Panasonic is supposedly an undervalued stock that could grow massively the next 5 years, what do you think?", "selftext": "I just read an article in a local danish investment magazine that predicted Panasonic to grow massively the next 5 years. Reason is:\n\n1. Panasonic is massively undervalued as is. While most other tech stocks are growing Panasonic has been forgotten and hasn't really grown at all. This is despite them being one of the biggest EV battery manufactures. They make the batteries for Tesla. This is largely due to them being seen as another Japanese tech company that operates in a ton of boring markets and make largely unexciting products. While this has been true in the past it might not be the case in the future:\n\n2. The company has just started a full restructuring. Instead of being in around 50 different markets (try going to their website), they plan to reduce to being in 15-20 markets that have growth potential (such as EV batteries). They also have new senior management and are planning to restructure the company into a \"western\" style holding company with different sub-companies (such as Google and Facebook has done with their Alphabet and Meta restructuring and rebranding). This gives them more options for starting new ventures and selling/spinning off other branches that do not benefit the group. This also allows them to separate their consumer and industry products more. They make everything from electric razors, to batteries, and to industrial automation under the same brand currently. With the restructuring these different products will be more clearly managed under different companies and brands. \n\nWhat do you think? Is Panasonic a good value stock?", "upvote_ratio": 0.81, "id": "t3_tr0uh1", "created_utc": 1648563650.0}
{"sub": "investing", "title": "How long till we see market prices reflect lumber futures?", "selftext": "I\u2019ve seen that lumber futures prices have fallen about 30% since all recent highs. Saw a high of $1357 on March 4th but fell to about $1000 as of today on the May 2022 contract. But I haven\u2019t really seen any price changes in the general market. Like for instance at Lowe\u2019s or Home Depot. How long till we actually see a decrease in general market price?", "upvote_ratio": 0.53, "id": "t3_tr0rms", "created_utc": 1648563435.0}
{"sub": "investing", "title": "What do you use for investment research?", "selftext": "I'm a college student looking to find the best way to keep up with my investments.  I know of many different outlets and channels, but I wanted to see what everyone uses.  Twitter, YFinance, youtube, brokerage sites, Marketwatch, seeking alpha, cnbc etc?  I really appreciate your help!", "upvote_ratio": 0.6, "id": "t3_tr0jhd", "created_utc": 1648562792.0}
{"sub": "investing", "title": "Can I max out my 403b and Roth IRA in the same year?", "selftext": "Hello, can I max out both my 403b and Roth IRA in the same year? \nMy accountant said no because I have maximized my contribution via my employer\u2019s retirement plan. My employer also contributed $8500 to my 403b through matching. \nI am under the income limit for Roth IRA so I thought I am allowed to contribute and also max out my Roth IRA?\n\nUpdate:\nHe responded and said he thought I was asking about buying traditional IRA for deduction. Thanks everyone!", "upvote_ratio": 0.9, "id": "t3_tqzgyk", "created_utc": 1648559624.0}
{"sub": "investing", "title": "Recommendations of bonds ETFs", "selftext": "Basically the title. I\u2019m an international investor who\u2019s already exposed to the US stock market but I would love to diversify my portfolio abroad to include bonds. What would you recommend me? Is there anything in particular I should look for in those ETFs? Thanks.", "upvote_ratio": 0.81, "id": "t3_tqzgbk", "created_utc": 1648559566.0}
{"sub": "investing", "title": "How could my average cost basis be higher than the daily high?", "selftext": "I have an average cost basis of $65 on TLRY, but looking into the past, i see that on my exchange, the highest price on that day was $25. Ive never touched options. How is this possible? Im seriously concerned that this could be some kind of mistake, as unlikely as that sounds.", "upvote_ratio": 0.29, "id": "t3_tqvz56", "created_utc": 1648546418.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - March 29, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.94, "id": "t3_tqvjyc", "created_utc": 1648544468.0}
{"sub": "investing", "title": "Is better to own stocks directly?", "selftext": "Hello investors. I hope my question doesn't sound silly. I am new in investing, using etoro with little money and just getting started. I want to ask you, is there any difference between owning the real stocks or just the contracts? Can i theoreticaly impact the price, when i do a trade with huge money? Or is owning stocks directly any safer? If there isn't any difference, why do biggest investors owns directly? \nThank you for answer", "upvote_ratio": 0.8, "id": "t3_tqviqj", "created_utc": 1648544343.0}
{"sub": "investing", "title": "Advice on Trading App \u2018Pies\u2019", "selftext": "As an absolute novice when it come to investing I\u2019m looking for some experienced takes on Trading App \u2018Pies\u2019. \n\nThere are multitude of different \u2018Pies\u2019 from Growth to Dividend and I\u2019m wondering if any what would you look for? \n\nAny help is greatly appreciated.", "upvote_ratio": 0.56, "id": "t3_tquoi5", "created_utc": 1648540491.0}
{"sub": "investing", "title": "What else can I do? Trying to achieve more that 7% growth per year", "selftext": "Hi all. I\u2019m 36 and my wife and I have real estate and $$$ in investments like low index funds, Tesla, Apple, etc.\n\nI make over 300k a year and max out 401k, IRA, and put $$$ in my brokerage. \n\nI\u2019m trying to find other investment opportunities and not sure what to do to achieve more growth. I feel a bit stagnant in low index and real estate all the time, and looking into commercial real estate, land, and other assets or opportunities to make bigger gains. \n\n What have y\u2019all found to be the best return? What delivers over 7% per year?", "upvote_ratio": 0.38, "id": "t3_tqs492", "created_utc": 1648529723.0}
{"sub": "investing", "title": "What happens when PUT/Shorts is expired worthless? Does the stock go up?", "selftext": "Based on  [AMC Short Interest Ownership - AMC Entertainment Holdings Inc (fintel.io)](https://fintel.io/sosh/us/amc) \n\n&amp;#x200B;\n\nOne PUT example is, Simplex Trading on 2/23, has a PUT position of 4mil shares, and are currently down -85%, if they close their position or it expire worthless, will this bump AMC price up? \n\n&amp;#x200B;\n\n&amp;#x200B;\n\nOne Short position example is JPMORG 2/25 has a short position of 157,000 around 17 dollars, and are down -36% (but i believe they are down 80% as of right now) so they are paying daily margin interest, short borrowing fee correct? Also if they close their position, would the stock price go up a lot or little?", "upvote_ratio": 0.73, "id": "t3_tqqkia", "created_utc": 1648524118.0}
{"sub": "investing", "title": "CPI Announcement Scheduled for April 12th", "selftext": "What strategy do you think is best with regard to the CPI for March which will be announced on April 12th before the market opens? \n\nWith oil prices spiking during March would you be surprised if the CPI was below 10%?\n\nIf your life depended on maximizing your return on capital through April 14th, and you could only take a position in three investments where would you employ capital?", "upvote_ratio": 0.75, "id": "t3_tqplp4", "created_utc": 1648520843.0}
{"sub": "investing", "title": "Sentiment folks? I'm 80% cash looking to come back in.", "selftext": "So i was wondering what the sentiment is with everyone right now considering inflation and war talks. 2.5 weeks before Russian invasion i turned 80% of my 401k into cash and now I'm looking to buy back in contingent on advice from people I know and the SPY chart breaking over $450 area and holding.\n\nWhat does everyone think? Wait for better confirmation on the charts? Wait for correction to pass? Buy in NOW? Go in bond heavy? \n\nI realize this might go into lump sum / percentage buy talk. I'll take any thoughts/advice i can get.\n\nBtw it would be a waste of a reply if all i get is people saying why did you go into cash you missed out on the great recovery blah blah blah. If  i was an idiot i was, and still might be, just want some thoughts. I mean that respectfully.", "upvote_ratio": 0.49, "id": "t3_tqnibp", "created_utc": 1648513977.0}
{"sub": "investing", "title": "Buy and hold the S&amp;P 500 stocks!", "selftext": "The S&amp;P 500 showed a 28.7% return last year, marking a stronger return than 85% of U.S. large-cap stock-picking mutual funds.\n\nThis come as no surprise to investors, as it's the 12th year in a row the S&amp;P 500 outperformed most actively managed large-cap funds.\n\nI was surprised that the S&amp;P 500 did just as well last year as my real estate portfolio. \n\nEdit - [Why Investors Should Care More About the Fortune 500 Than the S&amp;P 500](https://fortune.com/2019/05/17/fortune-500-s-and-p-500/)", "upvote_ratio": 0.72, "id": "t3_tqn6bz", "created_utc": 1648512912.0}
{"sub": "investing", "title": "Regulators are worried that inexperienced retail traders are getting in over their heads", "selftext": " \n\nDo you like to trade options? How about leveraged and inverse ETFs? Want to buy structured notes?\n\nThese kinds of \"complex\" investment products have exploded in popularity in the last several years, particularly as self-directed investors have been trading at home during Covid.\n\nHere's the bad news: Regulators are getting worried that you may be getting in over your head, and they seem to be looking to erect more \"guardrails\" to protect you against making stupid investments. They may even want you to take a test to prove you know what you're doing.\n\nThe recent market volatility is not helping, and likely making regulators even more nervous.\n\n### FINRA is sending a warning signal\n\nThe Financial Industry Regulatory Authority(FINRA) , which is the regulator for all the brokerage firms and exchanges in the U.S., recently released [a regulatory notice](https://www.finra.org/rules-guidance/notices/22-08) to its members (brokerage firms), reminding them of the risks of these \"complex\" products and the legal obligations they have of making sure their investors are in products that are suitable for them.\n\n\"The number of accounts trading in complex products and options has increased significantly in recent years,\" FINRA wrote in the note. \"However, important regulatory concerns arise when investors trade complex products without understanding their unique characteristics and risks.\"\n\nFINRA reminded its members that Regulation Best Interest (Reg BI), which was adopted in 2020, requires brokers to act in the \"best interest\" of the customer when making such recommendations. That means brokers need to be able to explain to clients the nature of the product they are recommending and the potential risks and rewards, and to determine if these investments are \"suitable\" for the client.\n\n### FINRA wants broader rules on these products\n\nFINRA said it was seeking comments on whether the current regulatory framework is adequate to protect investors. It noted that the old rules were adopted when most financial products were bought through financial professionals, whereas today many of these products are bought and sold through self-direct trading platforms like Robinhood or other online brokers.\n\n\"This is clearly a stalking horse for a new piece of rulemaking,\" Dave Nadig, financial futurist at ETF Trends, told me.\n\n\"The SEC is concerned there is a new raft of retail investors who are under-educated about what they are doing,\" Nadig said.\n\n\"It is partly a reaction to retail investing in options, but it is even broader than that. They are proposing to create a class of new products called 'complex' that is basically everything besides plain vanilla stocks and bonds.\"\n\n### 'Complex' products could include your crypto ETF fund\n\nIn its note, FINRA describes a complex product as \"a product with features that may make it difficult for a retail investor to understand the essential characteristics of the product and its risks (including the payout structure and how the product may perform in different market and economic conditions).\"\n\nThese can include leveraged and inverse exchange-traded products, volatility-linked ETPs, structured products, and defined outcome ETFs, which offer exposure to the performance of a market index but with downside protection and an upside cap on potential gains over a specified period.\n\nAlso on the list: \"Mutual funds and ETFs that offer strategies employing cryptocurrency futures.\"\n\n\"We continue to believe that the features of these products are such that they may be difficult for a retail investor to understand the essential characteristics of the products and their risks and, are, therefore complex,\" FINRA said.\n\n\"These concerns may be heightened when a retail customer is accessing these products through a self-directed platform and without the assistance of a financial professional, who may be in a position to explain the key features and risks of the product to the retail investor.\"\n\nFINRA is doing more than warning about these products. It is asking if additional requirements are necessary to protect investors.\n\nIt is particularly concerned about the growth of \"self-directed platforms\" and asks, \"are additional guardrails needed for these types of platforms?\"\n\n### Should retail traders be required to take tests?\n\nFINRA also seems to want the retail investor to demonstrate a lot more knowledge of the products they are buying.\n\nFor example, it is asking whether retail customers should be required \"to demonstrate their understanding of those common characteristics and risks of complex products by completing a knowledge check and, if the customer fails to show the requisite knowledge, requiring the completion of a learning course and additional assessment?\"\n\nEssentially, FINRA is asking if retail traders should be required to pass a test before they can trade these products.\n\n### FINRA is also worried about plain-vanilla options trading\n\nEven the trading of options has become a source of concern. FINRA notes that listed options trading volume has grown to over 38.6 million contracts a day on average, 30% higher than 2020 and almost 100% higher than 2019.\n\n\"Similar to transactions in complex products, buying or selling options can be risky for retail investors who trade options without understanding their vocabulary, strategies and risks. Members should consider whether investors understand the various risks of trading options...\" FINRA said.\n\n[Source:](https://www.msn.com/en-us/money/markets/regulators-are-worried-that-retail-traders-are-getting-in-over-their-heads/ar-AAVzUgz?ocid=msedgdhp&amp;pc=U531&amp;cvid=f6d5481f037a49ba81c4924232afc0c0)", "upvote_ratio": 0.86, "id": "t3_tqdrxf", "created_utc": 1648486507.0}
{"sub": "investing", "title": "Investing principles - illiquid and profit", "selftext": "Not so much directed towards stocks, but more general investment theory. \n\nHow do you decide to liquidate your position? Particularly when it comes to illiquid asset classes such as real estate. If you have the opportunity to sell at a 400% profit do you sell? You can sell a little bit of stock at a time, but a house can only be sold once. \n\nI know it \u201cdepends on your own goals, appetite for risk, if you like eggs that are sunny side up, etc.\u201d \n\nMore just asking how you approach this topic.", "upvote_ratio": 0.7, "id": "t3_tq359l", "created_utc": 1648449480.0}
{"sub": "investing", "title": "Tesla News:stock split, factory shut down. Guess which one has a bigger impact on stock price?", "selftext": "Tesla on tweeter announced a plan to split stock. It is seeking investor approval in the next general meeting. However, no further details was given, no mention on the meeting date, or the split ratio.\n\nAlso today, due to COVID restrictions in China, Tesla gigafactory in Shanghai will be shut for 4 days. It has already shut for 2 days in mid March. Current estimate is the factory could produce just over 2,000 vehicles per day ( output in Dec 2021 was 70,000 for the month). So that is over 12,000 less cars to be shipped in March.\n\n\nIf you think a stock split has a bigger impact on the market,  then congrats, you are right. Although I'm considering throwing all my financial books down the toilet at this point.", "upvote_ratio": 0.9, "id": "t3_tqbxml", "created_utc": 1648481526.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - March 28, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.72, "id": "t3_tq52qa", "created_utc": 1648458070.0}
{"sub": "investing", "title": "Is CoinBase gonna screw me over?", "selftext": "So I've bought some Shibu recently and while I don't expect it to reach a penny anytime soon, I know that if it does I'll have at more than $3,000,000. \n\nNow, based on what I've heard from other people, one thing I'm worried about is my whole account being deleted or being forcibly logged out and unable to have access to my account before I can withdraw anything.\n\n\nIs this likely to happen despite there already being a class action lawsuit against them, and is there anywhere safe you would recommend me to transfer my currency instead?", "upvote_ratio": 0.11, "id": "t3_tpzevx", "created_utc": 1648435399.0}
{"sub": "investing", "title": "A question for the experienced.", "selftext": "Hello investors of Reddit, please be aware that I have been doing rigorous amounts of research into all kinds of short term ways of saving and things like that but I just wanted to run it up with some down to earth people rather than you tubers.\n\nSo in my situation 17 and male, I\u2019m currently a college student and am going to be for the next year to come, I\u2019m currently earning about \u00a3800 a month and don\u2019t pay taxes, around \u00a3300/\u00a3350 of that is completely disposable income, in terms of saving over the course of a few years, until I\u2019m 20 what do you think my best bets are?\n\nBuying ETFs and holding them short term and try and squeeze as much of that compound interest as I can out of the 3 years?\n\nBuying crypto in large amounts keep buying Bitcoin for instance every month etc or eth.\n\nJust play save and keep it in a savings account?\n\nThere are a few others, but you guys are the experts so please whatever you guys might think the best choices are for me please do feel free to make a suggestion I do appreciate everyone\u2019s efforts!\n\nMany thanks :) - X840.", "upvote_ratio": 0.61, "id": "t3_tpxo5q", "created_utc": 1648429422.0}
{"sub": "investing", "title": "To invest in stocks or do options", "selftext": "Hey guy, so I have a bit of experience in option trading. More particularly in selling puts. Right now, I\u2019m investing into apple and just wanting to hold the shares while the compound interest build it up over the years. However, I\u2019m wanting to sell puts and I\u2019m wondering which is better or should I do both? I guess my main question is, what\u2019s the smartest?", "upvote_ratio": 0.24, "id": "t3_tpxk6i", "created_utc": 1648429043.0}
{"sub": "investing", "title": "How does the valuation of Sibanye Stillwater (SBSW) make sense?", "selftext": "They own the Stillwater and East Boulder mines in the United States, which have 25 million ounces of platinum and palladium reserves. Cost of production is about $1200 per ounce, and they sell for about $2000 per oz. The net present value of this is about $9B USD. Add in their net cash of $2B and you are pretty much at the current market cap of $12B.\n\nThey also have huge PGM+gold assets in South Africa and return all of the profit to shareholders, giving them a P/E of 5 and dividend yield of 10%. You basically get that for free. Even if you assume everything in SA gets zeroed due to confiscation, their US assets alone justify the valuation, and you get paid 10% every year they stay in operation.", "upvote_ratio": 0.76, "id": "t3_tpx7cm", "created_utc": 1648427773.0}
{"sub": "investing", "title": "Looking for possible short term investment ideas. Possibly under two years.", "selftext": "We just sold off an investment property. We will have about $100k street paying off the final of our debt. The original plan was to use that money for a forever home as a fund for rehab or paying down the mortgage. Currently interest rates have pretty much priced us out of the local market. We are thinking that we could wait this out for up to two years. We are looking into short term investments for that money. Possibly something that could go liquid easily (bonds?) In case the housing market comes around in our area to something resembling sanity.\nOur current situation:\nMarried with two young kids. The oldest is in kindergarten in Catholic school with a tuition of $4000/year. (Local school system is awful, which is why we are looking for a home).\nWe live in our second investment property. (Starting to outgrow it). With this house sale we paid off all debt except the mortgage. We make enough in rent to convert all expenses and same a little extra. We have an emergency fund for the house we live in, an extra from the house we just sold, and a third for the future house (we used a rough estimate and think it's pretty solid).\nI have a job that will pay my a healthy pension in 15 years when I retire. We both contribute max to our 403b&amp;457b. Cars are paid off. We live well within our means.\nIf anyone has suggestions, please send them my way. I'm also open to answering any questions presented to me.\nI'm very basic with this stuff and while I creep this and the personal finance subs I don't have an extensive knowledge. So I'm open to criticism as well.\nThank you!", "upvote_ratio": 0.62, "id": "t3_tprtdf", "created_utc": 1648411095.0}
{"sub": "investing", "title": "Warren Buffet has previously called EBITDA \"utter nonsense\". Can someone please ELI5 why he (and others) believe this?", "selftext": "I recently watched [this video](https://www.youtube.com/watch?v=tvnKylAyLbQ) where Warren Buffet calles EBITDA \"utter nonsense\", but I don't fully understand why he thinks this is the case.\n\nCan someone please ELI5 why some people take this view?\n\nWhat are the arguments and counterarguments for paying attention to EBITDA when it comes to valuing a company?", "upvote_ratio": 0.95, "id": "t3_tpqhlf", "created_utc": 1648407361.0}
{"sub": "investing", "title": "ETF Discussion: European and Chinese equivalents to S&amp;P 500", "selftext": "I want to bet on Europe, China, and the US by investing in ETFs. For the US, I have chosen the S&amp;P 500. What would the best equivalents be for China and Europe? These will be incremental investments held for years.\n\nAlso, what is a way to access a low-cost and secure means to these ETF's as a Brit? Thank you", "upvote_ratio": 0.44, "id": "t3_tpiq44", "created_utc": 1648383327.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - March 27, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.71, "id": "t3_tpg3dp", "created_utc": 1648371670.0}
{"sub": "investing", "title": "Paying off student loans vs investing", "selftext": "I and my partner are in our late 30\u2019s, completely debt free, not looking to buy a house (it\u2019s typically covered by our employers) and we have a nest egg of about $300,000 in various investments and retirement accounts. I\u2019m completing a masters degree next year that should cost around $30,000. My question is, if my loans are capped at rates below 6%, does it make sense to pay them off immediately? \n\nI paid off my undergraduate loans about 8 years early and when I think of the money those payments could have generated in this past decade, ugh I want to  kick myself. So this time around, I feel like my choices are to pay off my loans early again or stretch them out and let what I would pay them off with go into safe ETFs and possibly this https://www.treasurydirect.gov/indiv/research/indepth/ibonds/res_ibonds_ifaq.htm\n\nAny advice is welcome.", "upvote_ratio": 0.66, "id": "t3_tpekns", "created_utc": 1648364516.0}
{"sub": "investing", "title": "Why invest in the S&amp;P instead of buying real bussinesses?", "selftext": "So obviously with the S&amp;P you should expect a 6-8% annual return. With investing in real estate something similar, probably even less. However, when you look online at businesses for sale, they are typically valued at 1x revenues and sometimes even cash flows.\n\nObviously, they are more riskier and timelier. They require paying employees and maintaining operations. In general, private businesses for sale seem to produce way better returns than investing in a public company. I presume this is what Buffett meant when he said that if he was investing small sums of money that he would produce way higher returns.\n\nWhy aren't more people doing this? Are these businesses too good to be true?\n\nedit: i misspelled title... rip", "upvote_ratio": 0.41, "id": "t3_tpbwf1", "created_utc": 1648353197.0}
{"sub": "investing", "title": "Success with Online Published Resource advice", "selftext": "So 75% of my portfolio is allocated in S&amp;P Indexes, I\u2019m new to the world of investing and that\u2019s my foundation. I have a lot of money in low cost ones too like IVV Etc. The other 25% is what I call my risk/learning fund where I essentially do research on the internet for Motley Fool, U.S News Bloomberg basically whatever they say is a buy. What are some of you more pro investors take to this concept.", "upvote_ratio": 0.13, "id": "t3_tpb67p", "created_utc": 1648350424.0}
{"sub": "investing", "title": "Charles Schwab PCRA Guidance", "selftext": "Hello,\n\nI just opened a PCRA and I am in the process of transferring my 401K amount into that account. Currently, the company I work for is giving me a 15% direct contribution on top of the 10% I already put away. Where would you start for long term growth? I am 24 years old and have some time to play with risk (not a lot of risk). \n\nThank you for your help!", "upvote_ratio": 0.5, "id": "t3_tpasnb", "created_utc": 1648349056.0}
{"sub": "investing", "title": "Is a Class action lawsuit \"free money?\"", "selftext": "Short and sweet, I received a notification of a class action lawsuit for a stock I own. I have no qualms with the company or their management and plan to hold the stock for several more years.\n\nIf I submit my information to participate in the lawsuit, are there any drawbacks to consider (i.e. do I have to forfeit my holding, does it restrict be buying more, etc) or is it an opportunity for \"free money\" from the resulting settlement? On the surface, this SEEMS like a \"no downside, slight chance of upside,\" scenario.\n\n*NOTE: I have no intentions of filling an individual lawsuit, against the company, so my choices are 1) do nothing or 2) submit info to class action for possible check in the mail down the road.", "upvote_ratio": 0.71, "id": "t3_tparki", "created_utc": 1648348946.0}
{"sub": "investing", "title": "Rising Interest Rates And The Coming DeFi Implosion", "selftext": "It's quite well known that crypto has been propped up by things like stable coins and DeFi; both things meant to keep people HODL-ing so that outflows are kept to a minimum. This helps keep coin values artificially high by keeping fund flows stable.\n\nThis is a lot easier to do when credit is loose and the cost of debt is very cheap. However, we have now entered a time where interest rates are expected to rise and central banks are looking to tighten the money supply.\n\nA big part of the DeFi \"boom\" has been speculators trading in their real currency (Dollars, Euros, Yen, RMB, etc.) into coins (BTC, ETH, DOGE, etc.). Then when they want to exit one of those coin speculations, they trade their coin into something that is supposed to represent a store of value; stable coins. The money stays in the system because it is converted from one crypto to another; rather than a real currency.\n\nRegardless of whether a crypto speculation is a stable coin or not stable coin (e.g. - BTC), people can \"stake\" it. The idea of \"staking\" is that you lend your coin to another party for a period of time for a set return. This return is oftentimes a multiple of the underlying coin (BTC, ETH, Tether, etc.) but is sometimes converted back into a real currency (Dollars, Euros, etc.) before paying off the \"interest\" on that contract. \n\nWe could get into more depth about specifics and intricacies of these agreements and speculations, but that isn't really the premise of this post. Nor is it material to the issue at hand. Rates are about to rise, liquidity is going to be removed, and there will be significant pressure on DeFi / crypto returns.\n\nEveryone saw that mortgage interest rates rose about a half a percent last week after the Fed recently announced that they would be increasing rates. This is a substantial move in rates and foreshadows the coming risks for crypto.\n\nThis is because as rates rise, you'll see more and more people (institutions) move from risky cryptos to safer investment vehicles like treasuries and other bond type investments. From their perspective the incremental return vs the risk becomes less and less compelling as interest rates rise.\n\nAs they move their money from DeFi contracts to safer investments, the price of cryptos will fall as money leaves the system. As the price of cryptos fall, the demand for margin debt from DeFi will fall as well. This decreased demand will cause the \"interest rates\" that they pay on these DeFi contracts to fall as well. So a person who used to get \"6% annualized\" on their BTC, will now get something closer to 5%. And combined with the fact that cryptos will fall in price while treasuries are paying more in interest, we'll see this momentum only grow as rates continue to rise. (One thing to think about is whether you believe governments will bail out coins that want to operate outside the control of governments. I say they won't, but crazier things have happened.)\n\nObviously, there could be more to say about this and I'm sure I'll get a lot of posts debating these observations. Some will say that I don't understand everything there is to know about the crypto world; which is true. Some will disagree with my assumptions; and to me that is probably more reasonable. \n\nBut I will say that not describing everything about what is going on in crypto doesn't make the observation incorrect. Because, in the final analysis, there are only a couple of things that matter with these Ponzi's. Liquidity, leverage, the cost of debt, and a Peter Pan level belief.\n\nPlease share your thoughts. \n\nI would love to hear what others think.", "upvote_ratio": 0.61, "id": "t3_tpa8wv", "created_utc": 1648347074.0}
{"sub": "investing", "title": "Are there better investment options than a 401k without a company match?", "selftext": "Hello everyone, I recently started a new career journey as a government contractor. The company I work for offers a 401k plan, but they do not match my contributions. I have a pension from my previous job, but it will need to be supplemented by additional income once I retire, so I\u2019m trying to find the best option. I\u2019m in my 40s and plan to continue working for at least 20 years. Would a person in this type of situation be better suited to find other investment avenues, or stick with the 401k plan being offered?", "upvote_ratio": 0.69, "id": "t3_tp7tgy", "created_utc": 1648338741.0}
{"sub": "investing", "title": "ROTH IRA and YLDs, Long Term Idea", "selftext": "So this is my current ROTH IRA and im very proud of my current position, im 24 years old, make about 65k a year before taxes, i have maxed out my IRA since i was 18, i put in $125 a week automatically, currently have 65k assets, 32k debts, 780 credit score\n\nso my Question here is im thinking about redoing my holdings in my roth ira, i wanna sell pretty much everything i own, liquidate 30k and move it into a more simplified strategy for long term investing and also long term cash flow, i understand i cant touch the cash flow until retirement age and all that\n\nAlso there is a very good chance that if the company i work for goes the way it should i can be well over the threshold of being able to contribute to my ROTH anymore, **Side Question, if in 3-5 years i make more then the threshold for contributing will it still be able to grow tax free with appreciation and dividends?**\n\nso here are my thoughts, liquidate 30k and put into,\n\n5k spy or spy alternative aka ITOT\n\n5k XYLD\n\n5k IJR or small cap fund\n\n5k RYLD\n\n5K QQQ or alternative, i dont currently have a fund\n\n5k QYLD\n\nmy thinking is i can catch the upside appreciation with the underlying asset and DRIP with the YLDs so in 35 years i can be in a position of basically having all tax free income\n\nThis will only make sense for me based on the answers to my **side question,** because i plan to make more than 150k a year in the next 5 years and will not be able to contribute to a ROTH IRA anymore but wanna take full advantage of tax free income in the future\n\nI know in a normal account it would make more sense to do all index funds and switch to YLDs once i actually need the income but my thinking is, this way i can snowball the YLDs and **grow the tax free income even after i can not contribute anymore** and also let the underlying assets appreciate for the next 35 years, leaving me in 35 years close to 100k or more of **TAX FREE** income every year plus the value of the index funds\n\nI will continue to invest as much as i can split evenly between the holdings for as long as i can\n\n**PLEASE leave any honest review and anything i missed that would make this thinking flawed would be much appreciated, can also tell me to just stick with what im doing**\n\n**CURRENT ROTH** [**https://imgur.com/a/buD6b7L**](https://imgur.com/a/buD6b7L)", "upvote_ratio": 0.56, "id": "t3_tp2dbg", "created_utc": 1648324747.0}
{"sub": "investing", "title": "Advice for Investing in Private Equity", "selftext": "I am subscribed to an investment advisory service called Palm Beach research group.  I have invested in a lot of their positions and they all have been lackluster at best and terrible for a few making me 70% unrealized losses because I don\u2019t want to sell at a loss.\n\nWell they are offering new services for investing in private equity.  It made me think that I would like to get into private equity but probably not with them.\nI think some of the money I have invested in REIT\u2019s and ineffective portfolio\u2019s such as with Manward Press could better be used for invested in private equity\n\nBut I don\u2019t know anything about private equity other than cursory info I have read on the internet.  Does anyone have any advice for me?", "upvote_ratio": 0.47, "id": "t3_tp12ou", "created_utc": 1648323445.0}
{"sub": "investing", "title": "I am unimpressed w Manward Press", "selftext": "I have been following their MAP (Modern Asset Portfolio) for a year now.  So far it has made about 8 % which is commiserate with inflation over the same period of time. what\u2019s the good of having a portfolio that doesn\u2019t outperform the market much less inflation.\nI could have just bought diamonds and spiders with a lot less work.", "upvote_ratio": 0.35, "id": "t3_tp0wt9", "created_utc": 1648322977.0}
{"sub": "investing", "title": "10k in savings set aside but looking for a higher rate of return", "selftext": "Hello! We have 10k set aside in a regular savings account for a modest home. We were going to buy but things fell thru, and thought we\u2019d make another offer so needed quick access to the capital. However, we\u2019ve decided to hold off for another few years as my fianc\u00e9 goes into her graduate program and not sure where we\u2019ll land. \n\nHaving 10k in a regular yield credit union savings account seems like a waste. We\u2019re looking for suggestions for where to hold the money with the absolute lowest risk, yet with a higher rate of return than traditional savings. Would it be an ETF maybe?\n\nThanks in advance for any suggestions!", "upvote_ratio": 0.73, "id": "t3_tp0uka", "created_utc": 1648322786.0}
{"sub": "investing", "title": "Looking for the company that produces Elfbar Disposable Vape pens", "selftext": "These things have blown up where I live, with many young people completely hooked on them. They are extremely cheap as well. I want to know who controls the production of them, in what country are they registered, and whether they are publicly traded or not.", "upvote_ratio": 0.18, "id": "t3_tp0avf", "created_utc": 1648321173.0}
{"sub": "investing", "title": "Bad time to put money into mutual funds?", "selftext": "Hello all,\n\nJust looking for some advice about mutual funds. Do you think it\u2019s a bad time to invest in one right now? This is my first time doing any sort of investing or putting my money away. I have finally saved 15,000. I am wondering if the war in Ukraine makes it a bad time for me to put my money into a medium risk mutual fund. My bank tells me that it is not a bad time. However I don\u2019t know a lot about investing or mutual funds. I\u2019m located in Canada B.C if that makes any difference. The gas prices are insane out here right now which is scary.", "upvote_ratio": 0.74, "id": "t3_tozy3h", "created_utc": 1648320107.0}
{"sub": "investing", "title": "How do investors hold their investable cash?", "selftext": "Say a billionaire wants to invest in a multimillion dollar start up. Does that billionaire have cash in an account that gets paid to the start up, or does the investor usually have to liquidate his or her assets/ cash equivalents in order to be able to buy a share of the company?", "upvote_ratio": 0.9, "id": "t3_toyqz1", "created_utc": 1648316585.0}
{"sub": "investing", "title": "How would one hedge against triple digit inflation?", "selftext": "I currently reside in Turkey, which has seen triple-digit inflation, massive currency depreciation and double-digit decreases in the real wage over the past year. Given future prospects of globally increasing food &amp; oil prices as well as terrible monetary and fiscal policy on my governments part, I don't expect this to change substantially in the foreseeable future with double digit inflation and even more currency depreciation expected over the next couple of years. How would I protect my savings against such insane amounts of reduction in value? Obviously, I can, and do, hold dollars and US equities, but with the state of the US economy as well as the rate of depreciation of the TRY against the USD vs. the rate of inflation, my real return is still negative. Even though I have a long-term investment horizon (10+ yrs), I still want some more liquid assets that keep their, or appreciate in, value to finance trips abroad, my hobbies, etc. should I need the funds. What kind of portfolio would you suggest I hold, given these prospects? Thank you.\n\np.s I'm a senior college student graduating in a couple months (albeit with a full-time job lined up), so I do not currently have a lot of capital to invest into anything, i.e, I'm broke! :^)", "upvote_ratio": 0.84, "id": "t3_toxhyy", "created_utc": 1648312937.0}
{"sub": "investing", "title": "custodial accounts for my children", "selftext": "I was wondering if anyone on here has any experience with setting up custodial accounts for their children. Is there a brokerage thats better for custodial accounts? Is there something I could do differently in regards to investing into my children's future? Thank you!", "upvote_ratio": 0.76, "id": "t3_tosq9q", "created_utc": 1648308128.0}
{"sub": "investing", "title": "A way to invest in Europe's transition away from fossil fuels and towards renewable energy: ITM Power", "selftext": "For years European governments have been making a push to decarbonize, and this transition has only become more urgent with the rift that is growing between Europe and Russia due to the invasion of Ukraine. I first heard of ITM Power (symbol ITMPF) in late 2020 in a [Scientific American article entitled \"Top 10 Emerging Technologies of 2020\".](https://www.scientificamerican.com/article/top-10-emerging-technologies-of-20201/)\n\nFrom the article: \"These and the other emerging technologies have been singled out by an international steering group of experts. The group, convened by Scientific American and the World Economic Forum, sifted through more than 75 nominations. To win the nod, the technologies must have the potential to spur progress in societies and economies by outperforming established ways of doing things. They also need to be novel (that is, not currently in wide use) yet likely to have a major impact within the next three to five years.\"\n\nOne of the biggest criticisms of renewable energy like wind and solar is \"What do you do when it isn't sunny or windy?\" One of the answers is energy storage. When you have excess sun or wind, you store that energy for later, when it isn't sunny or windy. One way of doing this is by using excess energy to electrolyze water, splitting the water molecules into separate hydrogen and oxygen. Then, the hydrogen gas is stored until later. It is then piped into power plants and burned to make electricity, or it is piped into existing gas lines to heat homes, or it is sent to hydrogen fuel stations where it can refill hydrogen fuel cell cars. [These things are already happening in Europe.](https://www.scientificamerican.com/article/solar-and-wind-power-could-ignite-a-hydrogen-energy-comeback/)\n\nSo what does ITM Power do? They make and develop state of the art electrolyzers used to electrolyze water to make hydrogen gas. [They are already involved in several large hydrogen projects in Europe.](https://itm-power.com/projects) I expect that the demand for their products will only grow in coming years. For transparency's sake, know that I currently own 94 shares in ITMPF", "upvote_ratio": 0.62, "id": "t3_torgms", "created_utc": 1648305176.0}
{"sub": "investing", "title": "I work in the alternative investment space in the private markets. AMA", "selftext": "I source new investment opportunities for my family office clients. Average check is $2-15MM. I\u2019m sector agnostic and evaluate 30-40 GPs/fund managers a week. \n\nI\u2019m going to The Met until 3 then have time to kill until I see a broadway show with my family at 8. Happy to answer questions but not to give any investment advice - only educational inquiries. If this violates the rules, then I apologize and kindly remove this. \n\nAMA", "upvote_ratio": 0.43, "id": "t3_tor3r7", "created_utc": 1648304892.0}
{"sub": "investing", "title": "Investing podcast suggestions", "selftext": "I have a long drive and looking for some good content to listen too. Does anyone know of any good podcasts that have been recently released that focus on the current market environment, trends, stock picks, fundamentals and/genuinely good interviews from experts. I listened to a few the other with week with el erian and dalio but can\u2019t remember what show it was on. Thanks!", "upvote_ratio": 0.9, "id": "t3_tomt10", "created_utc": 1648295144.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - March 26, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.72, "id": "t3_tokkmr", "created_utc": 1648285268.0}
{"sub": "investing", "title": "How to estimate when a company will go bankrupt?", "selftext": "There's this stock RespireRX that's literally going for about a penny. But, they have some high value medications in development. The one that makes the company recover should be out in 2 to 3 years. But, they might go bankrupt prior. Currently, they're going through restructuring and seeking to raise additional capital to stay in business.   \n\n\nHow can I estimate when they'll go under? At one point this was a $1k stock, and if they recover it's going from a penny to $1k a share in ten years. Would turn $1k into $100 million.", "upvote_ratio": 0.29, "id": "t3_toiss9", "created_utc": 1648277118.0}
{"sub": "investing", "title": "Has anyone had some success using guidance from Better Investing?", "selftext": "I joined Better Investing a while back. It seems like they use a good value-based (proprietary) software system to help their customers make decisions. Has anyone out there found it useful or found some quirks or downsides?\n Their philosophy seems reasonable, though it requires making a best guess for future sales and earnings but they provide a lot of links to good information (morningstar , value line, Zacks etc) to help you make your own forecasts. I can't say it's lead me to any righteous 10 baggers (yet) or anything but more evaluations could lead to that diamond in the rough. \nJust wanted to get other takes on their system.", "upvote_ratio": 0.6, "id": "t3_togg0j", "created_utc": 1648267320.0}
{"sub": "investing", "title": "Looking at Doordash's 10k filing. how can i find their definition?", "selftext": "Looking at Doordash's 10k filing, their section on Revenue shows this\n\nhttps://imgur.com/a/csRMxRs\n\nbasically saying all their revenue is from their core business. which is \"primarily comprised of Marketplace, which includes Pickup and DoorDash for Work, and Drive.\"\n\nbut wat does that mean?\n\ni dont know how to find out how they define that.", "upvote_ratio": 0.62, "id": "t3_tof4oh", "created_utc": 1648262589.0}
{"sub": "investing", "title": "Question about sold Cash Secured Puts", "selftext": "If you sell a CSP that expires at end of week and the stock price never dropped to  or went below the strike that you set, is there anything else that you need to do with that contract at end of the week market close?  Or will the entry at your brokerage automatically just fall off?", "upvote_ratio": 0.5, "id": "t3_toa2qt", "created_utc": 1648256344.0}
{"sub": "investing", "title": "Will Shaw shares vanish on deal completion?", "selftext": "Hello, \n\nWill someone please kindly advise what will happen to my 125 class B Shaw shares after the Rogers deal closes?  Purchased on TSX \n\nDo I simply get paid out and the stocks are removed? \n\nI imagine shaw stocks will be absorbed into Rogers once the deal is completed. \n\nOr is there more to this? \n\nThanks!", "upvote_ratio": 0.73, "id": "t3_to4w37", "created_utc": 1648249404.0}
{"sub": "investing", "title": "Vanguard offering three new actively managed funds for their VPAS clients", "selftext": "https://investor.vanguard.com/investor-resources-education/campaign/the-advised-advantage-new-active-equity-offer\n\nSince this sub generally prefers the lowest ER funds, which aren't usually actively managed, I'm interested in feedback on Vanguard's new offering.  The ERs are of course higher than the usual recommended Vanguard funds. I'm curious if you think the potential returns of these funds could make the higher ER worth it.", "upvote_ratio": 0.75, "id": "t3_tnxjdv", "created_utc": 1648237948.0}
{"sub": "investing", "title": "Investing in the Sports Industry", "selftext": "It is possible to invest in sports, although it\u2019s different from investing in other industries like technology, retail, materials, financial institutions, and others.\n\nThe main reason?\n\nOnly a few big sports organizations are trading in public markets,\u00a0and unless you have deep pockets and know the right people, it\u2019s unlikely you\u2019ll have any other choice.\n\nFor example, you can\u2019t directly invest in the Dallas Cowboys or Tottenham\u00a0\u2013 they\u2019re private companies.\n\nYou could buy a team if they get put up for sale, but you\u2019d need a LOT of money.\n\nSo, where/how could you invest? What are investments in sports available for the public investor?\n\n# 1. Stocks\n\nThe easiest way to invest in sports is by buying shares of publicly traded companies available to the general public through different stock exchanges.\n\nSome examples are:\n\n* **Teams**: Manchester United, New York Knicks, Atlanta Braves, Juventus, Toronto Blue Jays, Borrusia Dortmund, Ferrari\n* **Sports Apparel**: Nike, Puma, Lululemon, Adidas\n* **Sports Retail &amp; Accessories**: Dicks Sporting Goods, Footlocker\n* **Technology:** Peloton, Catapult Sports\n* **Media:** Formula 1 (through Liberty Media), The Madison Square Garden Company, ESPN (through Disney), CBS (through Paramount), Fox, Comcast\n* **Betting &amp; Entertainment** \u2013\u00a0Draftkings, the WWE, MGM Resorts, Penn National Gaming, EA Sports\n\nSome advantages of investing in publicly traded companies are high liquidity, relatively low risk, ease of access, and availability of information.\n\nThe disadvantages are the limited alternatives and return potential.\n\n# 2. Venture Capital / Early Stage investments\n\nThe idea behind venture investing is to find startups developing technology or solutions for the sports industry and invest in them early to help them grow.\n\nSome of the most popular funds that invest in early-stage companies in sports are:\n\n* Elysian Park\n* leAD Sports\n* Raine Group\n* KB Partners\n* SeventySix Capital\n* Stadia Ventures\n* Courtside Ventures\n\nThe advantages of investing in VC are high asymmetric return potential and a broad set of alternatives/opportunities in diverse sports sectors.\n\nThe disadvantages are the high barrier of entry (i.e., you need more money), low liquidity, high risk, and there isn\u2019t much information on the companies either.\n\n# 3. Private Equity\n\nPrivate equity firms invest huge amounts of money and have access to diverse investment opportunities related to sports.\n\nInvestments here are more distinctive \u2013 from funding a specific sporting event, acquiring a stake in clubs and leagues, or even getting a share in media rights deals.\n\nSome PE firms in the space are Arctos Sports, Dyal Capital Partners, RedBird Capital, and Sixth Street.\n\nThese investments are even harder to access than VC, but they offer some exciting opportunities in sports to those who can afford them.\n\n# TLDR;\n\nThere are a few ways to invest in sports (stocks, VC, PE) that depend on your budget and interest; however \u2013 as you probably realized, there are many gaps in the market.\n\nWhat if you wanted to invest in the future of up-and-coming athletes? What about investments in youth sports teams or leagues? Or sports facilities? Cities hosting Olympics/World Cups?\n\nBlockchain technology is helping with that, but expect new technologies to enable more and different investments in sports to the general public.\n\n*Edit: Source \u2013 Originally posted on the Sports-Tech Biz Magazine*", "upvote_ratio": 0.8, "id": "t3_tntuw7", "created_utc": 1648231155.0}
{"sub": "investing", "title": "Stock splits inside an ETF", "selftext": "Pardon what may be a silly question! I can\u2019t find the answer anywhere that I\u2019ve looked online\u2026\n\nWhen a stock that\u2019s held in an ETF splits, does the ETF adjust the amount held at all? I know the value of the stock remains the same at the time of the split, and therefore does not change the value of the ETF, but would the ETF be bound by any sort of allocation number to sell the newly split shares?", "upvote_ratio": 0.61, "id": "t3_tnptvx", "created_utc": 1648225702.0}
{"sub": "investing", "title": "is investing in autocracies too risky now?", "selftext": "So we all know what's going on with Russia and what happened to everyone who had investments in Russia. I am guessing it was still a small portion of most portfolios.\nLately I've been looking at my MSCI Core EM IMI ETF and wondering about the risk China introduces in my portfolio. Should China get adventurous and USA decides to delist Chinese stocks, there goes a big chunk of my portfolio. Is this a concern for others too? If so I imagine they will be limiting further investments in China which already translates to poor returns.", "upvote_ratio": 0.66, "id": "t3_tnpkfr", "created_utc": 1648225327.0}
{"sub": "investing", "title": "Why are other companies unable or unwilling to improve their trading UIs?", "selftext": "So it's certainly not an unpopular opinion to say that Robinhood is a flaming garbage barge of a company, but I know there are many people out there, myself included, that still use it simply because the UI is so seamless and intuitive compared to the major brokerage firms. I have a Fidelity account that I opened early in 2021 with the intention of transferring my portfolio to, but after putting \\~20% of my funds in to test it out, I couldn't convince myself to make the switch. Their mobile app and website are clunky and off putting to use and their desktop app, Active Trader Pro, while extremely powerful is archaic, unintuitive, and looks like it was built for windows 95 and hasn't been updated since.\n\nA big part of investing/trading for me is the ability to quickly ascertain information and then execute on it. In Robinhood that is astoundingly easy. Information about your portfolio is immediately present as soon as you open the app, you can check the status of all of your holdings on the same homepage at a glance, watchlists are presented alongside this information in a way that makes sense, and it all just flows. If at any point I want to buy stock, I can tap the ticker and execute an order within 20 seconds.\n\nFidelity on the other hand (using their \\*new and improved\\* UI), the homepage also shows you a portfolio line graph, however you can only view performance over the last month or year, no 1 day, 1 week, or 1 quarter options, and for some bizarre reason it measures total change in your account rather than ROC/ROI so any deposit or withdrawal makes this information useless. The price charts for stocks are harder to read and update less frequently (5min rather than every few seconds). Buying stock requires twice as many steps/clicks at a minimum. And viewing options chains and trading options in general is a nightmare.\n\nI am not a web/app developer so I cannot speak to the difficulty of improving these systems, but I can't imagine that with the budget these major financial institutions have they wouldn't be able to create a similar user experience that is offered by Robinhood but with a much better company backing it. And I used Fidelity as my comparison just because that is the only other brokerage account I have actively used, but I have seen the UI of TD Ameritrade and Vanguard and was not impressed with them either.", "upvote_ratio": 0.91, "id": "t3_tnoli6", "created_utc": 1648222628.0}
{"sub": "investing", "title": "Hi! I am knew! So can somebody help?", "selftext": "So\u2026.I read or listened someone talk about of 5%divided low risk no volatility\u2026ecc,so the jest I get is if I give 1000\u20ac in that I get 25-30 \u20ac every months?right? So now the kicker where can I do that?online?or at whatever bank?,and more importantly I am eu(italy) can I do it or is it not for me?,(my dream is to make a good deposit like 10.000 to 25.000\u20ac,that would take me years of working and more\u2026.but of I can get every month a significant amount like 600-900 I can get a anticipated pension I can get my real dream of living in my countryside real,can I?)", "upvote_ratio": 0.07, "id": "t3_tnjei4", "created_utc": 1648206123.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - March 25, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.82, "id": "t3_tnhrmu", "created_utc": 1648198869.0}
{"sub": "investing", "title": "Amazon vs. Tesla - what are your thoughts?", "selftext": " Over the past 5 quarters, Amazon\u2019s operating income has been getting decimated by inflation. The share price has been range bound for the past two years. Meanwhile, Tesla\u2019s operating income has exploded. They\u2019ve been range bound while their PE has compressed from over 1000 down to 180. In other words, through their impeccable execution, they\u2019ve delivered unbelievable margins while continuing to scale. Amazon and Tesla are two companies that have been range bound for roughly the same duration. That sometimes means they\u2019re about to make a swing in either direction. I\u2019d put a spread trade on the two (Tesla up and amazon down) for the following reason: Tesla\u2019s operating income has exploded sequentially for the past 5 quarters while amazons has done the exact opposite. I also think amazon is going to take a -$9 eps hit in Q1 driven by rivian and continued operating expense inflation. In Q1 \u201821 amazon hit $16 eps. That means their PE ratio is set to sky rocket with limited upside growth on top line revenue. Tesla on the other hand is just starting this journey. They built the infrastructure during a period pre-inflation and are flooded with cash. In an inflationary environment, you want/need improving margins. Tesla is delivering on that while Amazon is not.", "upvote_ratio": 0.38, "id": "t3_tndlzz", "created_utc": 1648181047.0}
{"sub": "investing", "title": "Buying land or property to install and lease digital billboard", "selftext": "I have a few residential rentals and will probably be selling one soon. I'm researching different investments and came across the digital billboard market. My plan would be to buy the land and the digital billboard but having a management company (probably an outdoor or advertisement agency) sell the ads and operate it. Ideally I would get a property on a high traffic road and, if a building exists, be able to lease the building separately. I have the impression that it could be very profitable.\n\nHas anyone here done something similar? \n\nI'm specifically trying to find hard cost (digital billboard with installation), operational cost, and revenue/ROI.\n\nAny feedback is much appreciated. Thanks.", "upvote_ratio": 0.17, "id": "t3_tna1br", "created_utc": 1648169085.0}
{"sub": "investing", "title": "Is there a housing price bubble in Turkey?", "selftext": "Hello everyone, i have a question. Is there a housing price bubble in Turkey atm? I have a lot of euros in cash right now and as you know there is an ongoing housing price increases in Turkey. I am currently considering buying a house at the moment with euros. We have low interest rates but a new system that covers the fall of Turkish lira compared to US dollars and pays the difference to you in full. But the payment is done by printing money so i think we are on the verge of a huge hyperinflation. Should i wait for black market cash euro sell or hyperinflation, instead of buying a house? Is there a housing price bubble in Turkey? Will the prices of houses drop? Will i be able to buy more with the euros that i have in a crisis situation? Thanks in advance.", "upvote_ratio": 0.38, "id": "t3_tn6j5x", "created_utc": 1648158735.0}
{"sub": "investing", "title": "Alternative data - what to look at?", "selftext": "If you had access to literally hundreds of various datasets, what are some of the things you'd be interested in analyzing when it comes to the relationships with the data and stock prices across equities/sectors/and so on?\n\nThink (in alphabetical order): Amazon AWS, Bloomberg, Refinitiv, Snowflake.\n\nI am a dev (at a company, which does use this type of data) and would love to test a few things out. The challenge - overload of information, so your ideas would be greatly appreciated.", "upvote_ratio": 0.22, "id": "t3_tn4pij", "created_utc": 1648155879.0}
{"sub": "investing", "title": "Lithium in American and $LBNK IPO", "selftext": "LG Energy Solution is planning to build a battery factory in Arizona which will be the first in the U.S. to make cylindrical cells. Cylindrical cell batteries are used in Tesla and Lucid vehicles.\n\nsource: https://www.reuters.com/technology/lg-plans-build-battery-factory-arizona-supply-tesla-others-sources-2022-03-23/\n\nWith this development and others like point towards investing in North American sourced lithium IMO. \n\nOne such company Lithium bank (which has lithium assets in Canada) is expected to IPO on the TSXV under the symbol $LBNK.V.\n\n$LBNK's flagship project Sturgeon Lake is located near Edmonton making it very accessible. \n\nThis project has an estimated 5.97M tonnes LCE, an average LI concentration of 67.1 mg/l and a total elemental Li recourse of 1.12M tonnes. \n\nConsidering that Canada does not have many domestically-sourced lithium companies that are public, I think $LBNK's IPO poses a unique investment opportunity\n\nDefinitely something to look into before they go public IMO: https://www.lithiumbank.ca/", "upvote_ratio": 0.53, "id": "t3_tn22s2", "created_utc": 1648153433.0}
{"sub": "investing", "title": "The elephant in the sub when it comes to discussing investing in countries like China or Russia.", "selftext": "Can we talk about how anytime politics become part of the discussion, the comments get locked.\n\nThe mods need to understand that investing has a lot to do with politics and political systems. We need to let people know about risks involved in investing in non democratic countries.\n\nPeople always seem to forget the number one rule of capitalism. Only invest where your capital ownership is protected.\n\nIn otherwords, never loan money to the king of Spain... since he is the judge that will ultimately determine if has to pay you back.\n\nPlease let people discuss issues steming from investing in non democratic political systems.\n\nThanks.", "upvote_ratio": 0.87, "id": "t3_tmwiy3", "created_utc": 1648148754.0}
{"sub": "investing", "title": "How Do Taxes Work On Cash App Investing?", "selftext": "I'm a 17 year old, so my mother's name is on my cash app account. If I were to buy and sell stocks, would I have to pay the taxes on her tax filing next year or does cash app automatically tax it?  I've done m a good bit of research into investing but I dont fully understand how its taxed.", "upvote_ratio": 0.73, "id": "t3_tmrgia", "created_utc": 1648144474.0}
{"sub": "investing", "title": "Alternatives to TradingView that have Bar Patterns and More Broker Feed Features", "selftext": "Hi Everyone,\n\nWas wondering if you all could point me to alternatives other than Trading View that have features such as Bar Patterns, more Broker Feeds (Other than just ARCA and NYSE), etc.\n\nI like Trading view but the lack of alternative brokers makes real time feed not too accurate...Thanks for the help!", "upvote_ratio": 0.44, "id": "t3_tmq1va", "created_utc": 1648143263.0}
{"sub": "investing", "title": "New Investment Strategy - Brainstorm Party", "selftext": "Hey everyone, was hoping to see if I could get some opinions on a potential new (at least new for me) strategy. I\u2019ve been very modestly investing in individual companies with the super majority of my funds going straight into either the SP500 (FXAIX \u2013 Fidelity\u2019s index fund) and Nasdaq (QQQ).\n\nI\u2019ve been reading how diversification can help lower overall risk but can also impact portfolio growth. My goal is NOT to buy $2000 Tesla calls or $250 Amazon leaps for a potential post-stock-split-surge or anything like that (and quite honestly, I barely have touched contracts so just bear with me if that verbiage is off lol). It also seems like doing the \u201c*classic*\u201d \u201c100 minus your age\u201d formula for stock to bonds ratio seems extreme, as I\u2019m 29 years old having my portfolio and 29% bonds seems ridiculous \u2013 ***to me*** (debating this is not so much my hope for in this post lol). I don\u2019t know if I\u2019ll ever be able to get over the fact that the [Vanguard Total Bond Market Index Fund (VBTLX)](https://www.google.com/finance/quote/VBTLX:MUTF?window=MAX) has had a 2.54% increase since November 16, 2001 granting about a 0.12% increase per year, on average, over the last 21 years \u2013 not inflation adjusted.\n\n# Goal\n\nTo find some type of happy medium between spending all day researching stocks (which I enjoy \u2013 but not sure how sustainable that is long term as I do data analysis full time and someday, in the next 3-5 years, will hopefully have kiddos to look after) and over-diversification which *could* prevent portfolio growth (....*and is a little boring*).\n\n# Disclaimers before continuing:\n\n**1.**  I know that adding bonds to your portfolio hedges against risk but as I\u2019m still (very slightly) part Ape..I just really don\u2019t care about taking a big hit \u2013 AS LONG AS I KNOW I\u2019m invested in good companies that will (also understand this isn't guaranteed) bounce back over the next 30 years.  \n**2.** I know that SP500 is roughly 500 companies and that QQQ is even less at about 100. Which you could easily argue is pretty concentrated. My argument against this is that there are a handful of companies that I personally don\u2019t like as much (some from moral standpoints, some just purely uninterested and would rather have my money invested elsewhere) and that with the top 10 companies (really the top 9 since Google is always split into Class A and Class C for some reason in these holdings) they generally make up 30% of the entire allocation (at least in the SP500). So if I\u2019m okay with a *little* more risk (hoping for a little more growth) why not purely invest in those?\n\n# Alright \u2013 to the potential strategy part..\n\nMy thought is that I could pull the holdings of SP500 ([FXAIX](https://fundresearch.fidelity.com/mutual-funds/composition/315911750) for this example \u2013 [here is the full list of holdings](https://www.actionsxchangerepository.fidelity.com/ShowDocument/ComplianceEnvelope.htm?_fax=-18%2342%23-61%23-110%23114%2378%23117%2320%23-1%2396%2339%23-62%23-21%2386%23-100%2337%2316%2335%23-68%2391%23-66%2354%23103%23-16%2369%23-30%2358%23-20%2376%23-84%23-11%23-87%230%23-50%23-20%23-92%23-98%23-116%23-28%2358%23-38%23-43%23-39%23-42%23-96%23-88%2388%23-45%23-32%23-112%23-4%23-65%23-3%2375%23102%23-104%23-74%235%23-89%23-105%23-67%23126%2377%23-126%23100%2345%23-44%23-73%23-15%238%23-21%23-37%23-17%23-14%23-98%23123%23-18%2345%23-59%23-82%2367%2383%23112%2317%2370%23-78%2378%23-50%2336%23-86%23-90%2381%23-21%23-119%23-30%23120%2349%2328%23-98%2333%2351%23-78%23-119%23-16%2350%23-58%2350%23102%2348%23-17%2352%23-99%23), you may need to click \u201cMonthly Holdings Report\u201d tab on that link if you\u2019re not directed there). Then get the top 10 companies (you could change this to top 5, top 20, 30, etc. \u2013 whatever you\u2019d feel comfortable with, obviously the more you add, the more diversification, the less risk, and the closer to SP500 performance you\u2019d be).\n\nIn this example, we\u2019ll take the top 11 holdings (since Google is split into two \u2013 we\u2019ll combine that leaving us with top 10 companies). Grab the allocation percentage for each, add those up, then divide the allocation percent by the sum to get it based on 100%. Calculation walkthrough:\n\n**1.** Apple is currently #1 at 7.098%  \n**2.** If you add the top 11 holdings you get 30.028%  \n**3.** 7.098/30.028 = 23.638%. This would be the allocation in our portfolio for Apple\n\n**Test Portfolio Allocation**:\n\n|Company|Our Test Portfolio Allocation|SP500(FXAIX) Allocation 3/2022|\n|:-|:-|:-|\n|Apple (AAPL)|23.64%|7.10%|\n|Microsoft (MSFT)|20.26%|6.08%|\n|Google (GOOG + GOOGL)|13.65%|4.10%|\n|Amazon (AMZN)|11.32%|3.40%|\n|Tesla (TSLA)|6.61%|1.99%|\n|Meta Platforms (FB)|6.43%|1.93%|\n|NVIDIA (NVDA)|5.31%|1.60%|\n|Berkshire Hathaway Class B (BRK.B)|4.98%|1.49%|\n|Johnson &amp; Johnson (JNJ)|3.94%|1.18%|\n|UnitedHealth Group (UNH)|3.86%|1.16%|\n|**Total**|100.0%|30.03%|\n\nHere is a [screenshot of the portfolio allocations and calculations](https://imgur.com/a/Iimxz2q).\n\n# Backtesting\n\nBefore I\u2019d want to implement this strategy, I\u2019d want to try to back test it and compare it to SP500 performance. This is just difficult because the *holdings change over time*. I can\u2019t take the holdings of the SP500 today and compare it over the last 20 or 30 years because they were not allocated similarly back in 2000 or 1990. Similarly, it\u2019s surprisingly difficult trying to find something along the lines of historical, monthly holdings (or even historical, annual holdings) for these index funds/ETFs.\n\nIn an attempt to use the current holdings, I created a portfolio test comparing the new strategy to FXAIX from *1/1/2021 to today* and it looks like we have pretty good results. I know this is an extremely short time range but at least it\u2019s something.\n\nHere's a [screenshot to the portfolio test](https://imgur.com/a/Y1K00z8). The highlighted portion is for argument #1 below.\n\n# The two main arguments I could see going against this strategy is:\n\n**1.** If the top 10 companies underperform the rest of the SP500 then, because you\u2019re not as diversified you could have a larger loss. And yes, while I\u2019d agree with that, I'd also add if the top 10 are going down, the whole thing is more than likely going down \u2013 while FXAIX may drop less than our test portfolio (like we see highlighted in 9/2021 to 10/2021 in the [portfolio test screenshot](https://imgur.com/a/Y1K00z8)), having a more concentrated portfolio in these top companies, I believe, long term, may give better results while also giving you a steady strategy to keep your eyes on for the future.  \n**2.** You can have companies go bankrupt (like General Electric, Sears, etc.), how do you account for this? Those companies don\u2019t just fall off the face of the earth, we saw a decline in their holdings percentage as they lost market cap - to be honest, I don't know exactly how quickly they fell from their places in the top 10 since finding that historical information is a little bit of a pain and this was before I really followed the markets. However, this loss in market cap would be reflected in our portfolio as well. Because of taxes, I don\u2019t know if you\u2019d want to rebalance monthly or what the best cadence would be \u2013 but I think rebalancing either quarterly or semi-annually would be okay. Also note, if Apple (or any one of those top 10 companies) were to...say double their debt while losing like 50% of cash flow or something, I\u2019d bet just about anything we\u2019d see headlines all over the place \u2013 so yes, it may be beneficial to keep some type of finger on the pulse of the market but it\u2019d be much more hands-off than reading 10-Ks/doing fundamental research and technical analysis.\n\n&amp;#x200B;\n\nThis strategy seems like it lets you invest in the greatest companies in the world and be more concentrated in those companies, while also taking the emotional aspect out of the game. Wanted to know everyone\u2019s thoughts on this and see if we can poke any holes! Thanks all!", "upvote_ratio": 0.5, "id": "t3_tmp710", "created_utc": 1648142527.0}
{"sub": "investing", "title": "How to defer capital gain taxes, forever (Europe)", "selftext": "Let's cut to the chase. I like trading options, even for things that I'd normally intend to buy-and-hold forever. Eg think the most boomer ETF, but with covered calls.\n\nThis is fine, but there's a big difference between a but-and-hold strategy, wrt taxation. Buy-and-hold doesn't realize capital gains. Selling LEAPS calls and letting them expire worthless, *does*.\n\nIs there any trick that one can use to get the taxation benefits of buy-and-hold while realizing (necessarily, since they expire) gains from options? I'm talking about continental Europe, there are no \"wash sale\" rules here, nor \"short term\" vs \"long term\" rules.\n\nI've seen the trick where one opens a short and a long position on essentially the same underlying, hoping that one of the sides will be deep in the negative (while the other similarly in the green, of course), then that one can be used to realize a loss, say on Dec 31st, while the other can be closed on Jan 1st (with some overnight risk there), hence one harvests a realized loss in year N for the cost of higher realized gain in year N+1. But maybe the same trick can be used year-over-year? If someone manages to do this, which market/positions you use a instruments and why?\n\nAny other ideas?\n\n**EDIT**: the more I contemplate this, the more it seems to work. The key details (where the devil lies) is what instruments would be used to create large enough unrealized gains &amp; losses, and cancel each other out. Whether such instruments would be two tickers, or whether they could be built via options. And what would be the overnight risk between Dec 31 to Jan 1, when an unhedged position would need to remain open. \n\nFutures don't seem to work, since (I think) they can't be used to create long-term unrealized gains/losses; they are marked-to-market frequently (daily). Which is a pity, because futures usually trade almost 24/7.\n\nCryptos would be another interesting possibility (24/7 trading), but it's hard to create a long AND short position, via the same broker (and it would be better to have everything in the same account, for margin reasons).\n\nSo, I can't think of a good solution that trades 24/7. My next thought is whether it can be built on more \"boring\" ETFs plus options, lower volatility stuff, so the overnight risk would not be much.", "upvote_ratio": 0.45, "id": "t3_tmo3gf", "created_utc": 1648141487.0}
{"sub": "investing", "title": "Tracking Capital Loss Carryover deduction from year to year?", "selftext": "For Capital Loss Carryover, how do you guys keep track of it from year to year to maximize your income deduction? The max deduction is 3k/year, but I will likely need several years of carryover to fully use up my capital loss. What is the best way to keep track of this capital loss and carryover from year to year?\n\n1) keep track of it on my own personal worksheet\n\n2) Tax software (TurboTax) that I use to file my taxes\n\n3) Annual 1099-B from broker?", "upvote_ratio": 0.86, "id": "t3_tmndb8", "created_utc": 1648140905.0}
{"sub": "investing", "title": "Put more money into the stock market or a mortgage at 4%", "selftext": "I am purchasing a new primary residence house and getting a large mortgage at 4%. Meantime I am selling my current house and not sure if I should put 100% of sale proceeds into the stock market (I would put all into qqq) or into the new mortgage (mortgage at 4%) or maybe split it 50/50. I know the stock market on average returns 7% per year vs 4% mortgage and that makes it seem like putting it into the stock market is better idea. But there are other considerations like taxation that makes it quite complicated. I am wondering if anybody has good insight into what is best thing to do here financially.\nEdited to add: I am 34.", "upvote_ratio": 0.91, "id": "t3_tmikc6", "created_utc": 1648136927.0}
{"sub": "investing", "title": "Do you guys watch the Bond Yields?", "selftext": "Before you invest, are you making sure your watching bond yields ?\n\nThe first  thing I  learned when working on wall street was to watch bond yields. As yields go up, stocks go down, as yields go down stocks go up.\n\nIn 2020, 2021 Yields have for the most part gone down. In 2022 however they will likely only go up (given the fed doesn't back down).\n\nCompared to **JUST 6 MONTHS AGO**, Bonds are up:\n\nUS 2Yr up 690%\n\nUS 5yr up 204%\n\nUS 10yr up 64%\n\nUS 30 yr up 31%\n\nThis rate of increase is unprecedented. (because rates are moving from the lowest lows, to higher) For reference, the debt crisis that caused 2008 was, among other things, caused by a 30% increase in the US 10yr (from 3.9 to 5.1). Also want to mention that this yield increase happened over the course of 1.5 years, not a few months like we're seeing today.\n\nExample, the monthly payment for a $300,000.00 mortgage loan in the past 6 months has gone from $1,292 (figured 3.1%) to $1,626 (figured 5%).\n\nAKA our economy is grinding to a halt.\n\nI'm hoping to start a discussion, what do you think? What are you doing with your money? When does the system crack, is it within the next few months since were rising so fast, or will this take another year or two to play out?\n\n&amp;amp;#x200B;\n\n\\-I've been shorting bonds, TTT &amp;amp; TYO since October of last year.\n\n&amp;amp;#x200B;\n\nEDIT: Lots of questionable responses on this thread. We're in a bubble. \n\nLast edit: Maybe not grinding to a halt. But more a turtles pace. Steep decline though considering we were at cheetah speed.", "upvote_ratio": 0.73, "id": "t3_tmczj8", "created_utc": 1648131447.0}
{"sub": "investing", "title": "Find out who makes your favorite private label products", "selftext": "Yesterday I went to Mercadona, the largest supermarket chain in Spain, and all the products in the toilet paper section (baby and adult diapers, wipes, all different types of toilet paper...) were made by the same company: Essity. \n\nEssity is a Swedish company, perhaps for this reason unknown, and is second only to Kimberly Clark in its sector. Not only does it produce for other brands, but it owns some well-known own brands.\n\nThe bottom line, be curious. In a world where private labels are gaining weight, seeing who produces them can make you find hidden gems stocks.\n\nP.S. Mercadona is famous in my country for the high quality of its private label products. This is always a good sign, make sure the a private label is not only cheap, but also good value for money.\n\nPlease let me know if you find other examples! \n\nHave a great day :)", "upvote_ratio": 0.75, "id": "t3_tmcqce", "created_utc": 1648131243.0}
{"sub": "investing", "title": "Can anyone explain to me the capital flows into the market relative to the actively traded AUM?", "selftext": "I\u2019m attempting to understand how actively trading AUM can cause a dip/correction in the stock market when capital flows into passively managed funds are at an all time high.\n\nIt would seem every two weeks, the passive fund flow with retirements would be so impossible to fight from a selling perspective, there would be no way to ever see a lasting correction.\n\nCan anyone provide color to this relationship or somewhere I can read about it?", "upvote_ratio": 0.72, "id": "t3_tm9kn3", "created_utc": 1648128062.0}
{"sub": "investing", "title": "Investing in Southeast Asia?", "selftext": "I\u2019ve been thinking about supply chain shifts, and Larry Fink mentioned it yesterday as well. I expect some will be shifting away from China to other southeast Asian countries such as Vietnam, Cambodia etc. Does anybody think it\u2019s worth investing in any Southeast Asia ETF\u2019s? If so, which ones are you looking at?", "upvote_ratio": 0.78, "id": "t3_tm7wdb", "created_utc": 1648126730.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - March 24, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.8, "id": "t3_tm1lkr", "created_utc": 1648112469.0}
{"sub": "investing", "title": "My option and analysis on JD and DADA", "selftext": "updated the whole analysis and tables [here](https://www.westmoney.com/share/stocknewsdetail?id=956495782389878784&amp;lang=en&amp;color=1&amp;wm=reddit):\n\nJD is the leaders among other O2O platform in supermarket segment with a market shares of 27%. They also have now established partnership with 85 out of the top 100 supermarket chains in China. The gap between JDDJ and the second and third players remained wide at 11 and 18 % points respectfully. Thus, I believe that Dada will benefits from this overall trends as they are the leaders in the space.\n\nDada also posted a very good Q4 results with net revenues from Dada Now achieved a RMB718 million. This represent a pro forma revenue growth rate of 80.5% yearover-year, mainly driven by increases in order volume of intra-city delivery services to chain merchants. Net revenues from JDDJ also increased by 80.1% to RMB1.3 billion, mainly due to the increase in GMV, which was driven by increases in the number of active consumers and average order size. The increase in online marketing services revenues as a result of the increasing promotional activities launched also contributed to the revenue growth of JD.\n\nIn terms of the outlook, For the first quarter of 2022, Dada expect a total revenue to be between RMB2 billion and RMB2.05 billion, representing a pro forma growth rate of 72% to 76% The reason why the company used a pro forma growth rate to evaluate the company is because of effective from April 2021, the cost of riders for last-mile delivery services has been directly paid through third-party companies instead of through the Company. The Company no longer recognizes rider-related revenue and rider-related costs in the income statement for the last-mile delivery services.\n\n**Valuation**\n\nAccording to analyst estimates, even though the company is heavily burning money, but it was suprising that analyst is expected the company started to generate profit in 2023 and currently Dada has a forward 2023 PE ratio of 29.67. If the company were able to achieve those results and become profitable in 2023, I would believe that the company would be a good buy because of the big total addressable market and the high growth rate of the industry.\n\nHowever, I remain sceptical about the company achieving profitability soon, as on the latest quarter results of the company, they are no sign of the company slowing its cash burning activities. The sector is very challenging and with a low switching cost if there are no offers or coupons made or given by the platform. This results in consistent cash burning in order to retain the consumer on the platform, thus I will wait until the company shows that it has the road to achieve profitability before I invest in the company.", "upvote_ratio": 0.54, "id": "t3_tlzq1s", "created_utc": 1648104348.0}
{"sub": "investing", "title": "Military cybersecurity firms to invest in?", "selftext": "I've been hearing a lot about modern warfare shifting more toward cyberspace and seems to be looming over the war in Ukraine.\n\nAny military cybersecurity firms worth investing in? Especially ones that have an emphasis on defense.\n\nDon't really know where I'd start going about researching. I've got some industry insight in general IT cybersecurity but quite unsure where the overlap is when it comes to military capabilities.", "upvote_ratio": 0.6, "id": "t3_tlyi4t", "created_utc": 1648099347.0}
{"sub": "investing", "title": "Question: How can I find out how the macroeconomic environment affected the stock market historically?", "selftext": "For example, in the past -\n\n* when inflation was high, how did the stock market perform?\n* when the interest rate increased, how did the stock market perform?\n* When there was a war, how did the stock market perform?\n\nWhat kind of websites and apps should I use?", "upvote_ratio": 0.56, "id": "t3_tlxqwv", "created_utc": 1648096504.0}
{"sub": "investing", "title": "Trouble choosing a broker and having trouble switching.", "selftext": " Hello,  I have a tough time figuring out what broker to use and I  don\u2019t know  why since the obvious answer is Fidelity or TDAmeritrade. I  am  currently with Charles Schwab for my broker using their   StreetSmartEdge.  My uncle referred me to them and I\u2019ve been using the  ever since I started investing and day trading back in September of   2020. They are also my only bank, but starting to move funds over to   Fidelity since I have some retirement accounts with them and to see how  they are. I like Schwab\u2019s platform but it has issues. I\u2019ve ran into some  problems with them, such as placing orders above the ask price and it  not filling when it should have like today it should have filled for the  few seconds my order was up, but they didn\u2019t go through. Or orders  being filled but taking a few seconds to even show up and I have to  close the desktop application on windows and log in again to see if the  order shows I have a position in the stock and it shows it since I  re-started the program. I don\u2019t use Mac OS because StreetSmartEdge is  cloud-based for Mac and it\u2019s slow. Or like a couple of times so far when  I  logged on and the chart, level 2 info, and time and sales being  behind  by a couple to a few minutes of what Fidelity is showing even  though the chart on Schwab and time and sales are showing the present  time just the chart values are off, so I restart the program and the  information is all right all of a sudden. Almost every time this happens  I get mad and say I\u2019m going to go to Fidelity, but it never happens.\n\nToday   I sort of had enough of Schwab\u2019s platform and think I\u2019m going to  really move to Fidelity. Today I submitted about 7 buy orders before the  opening bell well above the ask price by a few cents and they didn\u2019t  fill, I got really mad. The Schwab broker said something like the only  way the order didn\u2019t fill is the exchange didn\u2019t have any sellers below  my limit price which is total BS because there weren\u2019t any other sellers  above my limit price and I doubt there weren\u2019t any NASDAQ  sellers at  the time and then goes on to say they had issues with orders/  routing  them this morning before the bell for like 10-15 minutes.  Schwab has,  again and again, had issues.  Apparently, Schwab and  TDAmeritrade will  be \u201cbecome one\u201d the representative said like these two companies will  finally merge in about the summertime of 2023 and then 12  months later  Schwab and TDA clients will be able to trade using either   StreetSmartEdge or TOS with their same account. So I can use TOS one day  and SSE the next which is cool, but yeah the problems they have. I\u2019m  not really wanting to wait for that just to use TOS.\n\nI  day trade Triple Leverage ETFs on margin, almost completely on margin  so some cash but barely any. Usually, the max I do is 56% margin against  my securities, with Schwab the margin requirement amount for triple  leveraged ETFs are 70% but that\u2019s overnight. Last week I accidentally  went with a little over 100% margin with these triple leveraged ETFs for  a second. I love the thrill of always having a somewhat predictable  security that makes big swings constantly throughout the day. Sometimes I  place a directed trade so not an auto routed smart trade to buy or sell  and immediately as I place the trade the price goes in the direction I  want it, but my order isn\u2019t filled and I need to place another order  with a higher limit price, so either I am really good at knowing when to  get it and I can see I am good at that, but feels like there\u2019s a PFOF  going on? Idk.\n\nI  think I\u2019ll move  over the Fidelity, just Schwab seems to be great at mostly everything  except for their platform has issues and I have a hard  time changing  things in my life sometimes.\n\nI  don\u2019t like having to wait 31 days while switching brokers and not being  able to trade these triple leveraged ones to have to avoid cost basis  information and all that mess it\u2019ll cause, but I  might just have to  wait.\n\nHas anyone had the same problems with Schwab that I am having? What broker do you prefer?", "upvote_ratio": 0.44, "id": "t3_tlujjq", "created_utc": 1648087893.0}
{"sub": "investing", "title": "V2G Tech Bridging The Gap Between EV and Diesel Cost In an Untapped Market", "selftext": "Full disclosure: I retained a small position in $PTRA in my Roth IRA. Disclaimer: I'm not a financial advisor. Do your own DD as always.\n\nI want to start a discussion on **V2G** as I think it has reached the turning point in commercialization of V2G tech and its unique but perfect marriage with electric school bus.\n\n&amp;#x200B;\n\n**What is V2G:**\n\nV2G or Vehicle-to-Grid is using electric vehicle specially equipped with bidirectional charging to take energy from the grid when the cost is low such as overnight and selling the energy back to the grid during peak hours when the electric cost is highest. It is important that the EV has a lot of energy left, ie: short routes and mostly unused hours/days. School buses have short routes, spend most of the day not in use and have many months without service particularly in the Summer assuming 180 days in a typical school year. Most electric vehicles can't do V2G. For example, not only my Tesla can't do bidirectional charging due to limitation, even if it could there just isn't enough 'juice' left to sell back to the grid at the end of the day because of high mileage.\n\n&amp;#x200B;\n\n**Successful use of V2G in commercial settings:**\n\nThomas Built Buses just announced a [long term agreement](https://electrek.co/2022/03/18/electric-school-buses-are-reaching-cost-parity-with-diesel-and-a-california-district-will-deploy-one-of-the-largest-e-bus-fleets-in-the-state/) to 2025 with Highland Electric to sell a fleet of the electric bus Saf-T-Liner C2 Jouley powered by Proterra's 226kWh battery and drivetrain. The 226kWh battery is the largest in the industry for school buses and has the capacity that is more than 16 times that of a Tesla Powerwall battery. This agreement is expanding on their previous fleet order of 326 electric bus in February of 2021 after Highland spent the past year successfully confirming that it is profitable using V2G on the Jouley electric school bus. [Highland estimated that a single bus generated roughly $10,000 for the company over the course of the summer](https://environmentnorthcarolinacenter.org/sites/environment/files/reports/NC_V2G%202022%20scrn.pdf#NC_V2G%202022.indd%3A11654) performing V2G. Highland will finance the purchase of the electric bus fleet, charge the Jouley bus overnight when cost is cheapest, [have the school districts pay on subscriptions for just the miles driven](https://cleantechnica.com/2022/03/21/sbaas-model-brings-electric-school-buses-to-more-districts/), then sell the leftover energy back to the grid during peak hours when the cost is highest. Highland would take care of all maintenance and charging infrastructure. Highland Electric is a company that was created for the sole purpose of commercializing V2G in electric school buses. It's doing this for profit. The result came back so promising that it resulted in them opening their order book for more.\n\n&amp;#x200B;\n\n**Why is this important:**\n\nHaving a working business model where it is profitable to buy an electric school bus vs diesel is a game changer. An electric school bus is no longer viewed as just a novelty transport but as a stack of 16 Tesla Powerwalls that just happen to also earn transport income. Cost has always been the biggest hurdle in transitioning from 480,000 diesel school buses to electric. Electric school bus is often priced $120,000-$200,000 higher than its comparable diesel school bus. I'm fully aware of the few billions in funding for new electric school buses in a use-it-or-lose-it mandate and the touted $10,000 or so per year savings on fuel and maintenance. But unless the upfront cost comes down to the same level as diesel, I don't see many of those diesel getting phased out to electric. Having companies like Highland Electric take on the upfront cost and the associated long term risk would get transition to electric sped up and on a massive scale. The fact that Highland found long term, V2G not only make up the difference in $120,000-$200,000 cost difference but actually turning a profit from it is extremely intriguing. Now, for the same amount of money, school districts can get 3 electric school buses instead of 1 by having Highland maximizing the full potential of these powerful batteries instead of having them waste away in the parking lot in down time.\n\n&amp;#x200B;\n\n**More companies want in on V2G:**\n\nAside from Thomas Built Buses long-term partnership using Proterra's 226kWH battery and drivetrain with Highland Electric, a number of other companies are taking advantage of the working V2G &amp; electric school bus model.\n\n\\- Dominion Energy selected Thomas Built's Jouley electric bus with its 226 kWh Proterra battery for V2G ([source](https://www.schoolbusfleet.com/10133648/bus-electrification-partnership-to-drive-cost-savings-cleaner-air)). Dominion plans to add 200 buses per years for the next 5 years.\n\n\\- On a smaller scale, Alaskan Energy and DTE Energy both selected the Jouley electric bus to study and obtain their own data on V2G.\n\n\\- Blue Bird will provide Nuvve with electric school bus equipped with 155 kWh battery ([source](https://cleantechnica.com/2020/09/20/nuvve-and-blue-bird-combine-to-create-electric-school-buses-that-are-v2g-enabled/))\n\n\\- BYD just released a small electric school bus with 150 kWh battery with V2G tech but no partners yet([source](https://www.autoweek.com/news/green-cars/a38952770/byd-electric-school-bus-v2g-charging/)). BYD might have a difficult time getting partners as BYD is being blacklisted by US government from using tax money.\n\nMost updated list of Utility V2G pilot programs (3/14/22): [link](https://www.greenbiz.com/article/3-design-considerations-electric-school-bus-vehicle-grid-programs)\n\n&amp;#x200B;\n\n**Minimal battery degradation from V2G over 10 years:**\n\n\"In the case of frequency regulation and peak load shaving V2G grid services offered 2 hours each day, battery wear remains minimal even if this grid service is offered every day over the vehicle lifetime.\"\n\n\"Frequency regulation and peak load shaving at power rates typical for vehicle charging and discharging will not significantly accelerate battery degradation in comparison to the degradation incurred from driving and calendar aging. Even in the \u201dextreme\u201d cases in which we assume all EVs provide grid services from 7:00pm-9:00pm every day for ten years, the capacity losses from frequency regulation and peak load shaving only increase by 3.62% and 5.6%, respectively.\"\n\nSource: [Quantifying electric vehicle battery degradation from driving vs. vehicle-to-grid services](http://manuscript.elsevier.com/S0378775316313052/pdf/S0378775316313052.pdf)\n\n&amp;#x200B;\n\n**FYI for potential investors:**\n\n\\- Proterra (PTRA) produces battery and drivetrain for Thomas Built Buses as well as other heavy-duty commercial companies. Proterra will have its 3rd and massive 327,000 square foot EV battery factory up and running in the 2nd half of 2022. Proterra also builds electric transit buses.\n\n\\- Thomas Built Buses is the oldest American bus manufacturer and is owned by Daimler. Daimler Truck is an investor in Proterra.\n\n\\- Lion Electric (LEV) is a Canadian company and it builds electric school buses among other large commercial vehicles.  Lion Electric will have its new $70 million US manufacturing plant completed late 2022.\n\n\\- Nuvve (NVVE) specializes in V2G platform and is more of an EV charging company.\n\nThe size of the pie(480,000) is so large that no single company can eat it all. Even if everyone gets just 1% of that pie or 4,800 school buses, at [$500/kWh](https://mdpi-res.com/d_attachment/sustainability/sustainability-12-03977/article_deploy/sustainability-12-03977.pdf) average price for battery and a 226 kWH battery, a company like Proterra would be looking at revenue of $542M for the battery alone. Proterra already [locked up](https://www.proterra.com/press-release/proterra-lg-partner-on-ev-battery-cells/) a stable supply of high-grade Nickel battery cells with LG Energy Solution through 2028.\n\n&amp;#x200B;\n\nTo conclude, I believe the fact that our electricity infrastructure being far from perfect is what makes V2G profitable. I'm excited at the potential of V2G opening the door to a massive and untapped market of electrification of 480,000 school buses.", "upvote_ratio": 0.67, "id": "t3_tklbaw", "created_utc": 1648007482.0}
{"sub": "investing", "title": "What\u2019s your investment plan for investing in post-Russian Europe?", "selftext": "Now that Putin\u2019s successfully set in motion the separation of Russia from the rest of the European continent, and food prices are set to rise even more plus all the fluctuations that\u2019ll be happening to energy prices, are you taking a pause or changing your investments?", "upvote_ratio": 0.56, "id": "t3_tld5az", "created_utc": 1648067521.0}
{"sub": "investing", "title": "Any good sources of downloadable forward dividend data?", "selftext": "I trade on IBKR and Thinkorswim (less often) and IBKR has forward dividends, with a big warning as to accuracy. So figured I'd find another source to compare it to before trades.\n\nYahoo has Forward dividends in screener, but no obvious way to download results. I want a long list to look at daily for trade, and not have to go in with each symbol. Also weirdly the Yahoo results don't include a column with forward dividends, so you just have to trust the screener worked, and even if I figured out how to download (an API?) I'm not sure there would be a column for it.\n\nAny other choices that I can screen for all US equities and dowlnoad a results list into excel, of forward divs? I would be willing to be a modest monthly amount. Thanks.", "upvote_ratio": 0.6, "id": "t3_tld0wb", "created_utc": 1648067191.0}
{"sub": "investing", "title": "Intel INTC Trading P/E $10", "selftext": "Intel is spending its own money to build its own factories in both the United States and Europe \n\nThey have beat on earnings pretty handily over the last three years \n\nThe have 9+ billion in cash \n\nThey have a .4 debt to equity ratio \n\n\nWhat am I missing here?", "upvote_ratio": 0.63, "id": "t3_tl7y9x", "created_utc": 1648061413.0}
{"sub": "investing", "title": "Stop limit orders for a call to mitigate risk?", "selftext": "Hi all. I was wondering if it would be a good idea to put a stop limit order on calls that I purchase? I'm looking to get into buying calls and I have heared that it can be very high risk. To help mitigate the risk, would it be a possibility to set up a stop limit order so that if the price falls below a certain price that I don't make too many losses? I've been doing some research on calls but I haven't seen this addressed anywhere.", "upvote_ratio": 0.57, "id": "t3_tl61pb", "created_utc": 1648059896.0}
{"sub": "investing", "title": "Investing advice in Metaverse companies", "selftext": "I'm creating a list of metaverse-focused companies to consider. Do you think they are worth investing in? You will see GME because today they announced that they are entering the NFT business. Any advice?\n\n[https://i.imgur.com/HkSmFU4.jpg](https://i.imgur.com/HkSmFU4.jpg)\n\n[https://medium.loopring.io/gamestop-nft-marketplace-powered-by-loopring-l2-6cdb9289d937](https://medium.loopring.io/gamestop-nft-marketplace-powered-by-loopring-l2-6cdb9289d937)", "upvote_ratio": 0.46, "id": "t3_tl442d", "created_utc": 1648058321.0}
{"sub": "investing", "title": "What is Ray Dalio trying to say in his new 43 minute video \"Principles for Dealing with the Changing World Order\"?", "selftext": "1. He believes what is currently happening in America is not new. We were a previously untested global economic superpower but we are slowly losing our dominance to China, just how countries in the past did.\n\n2. We are losing our dominance to China because we over-exerted our status at the world reserve currency because we printed too much money, just like other countries in the past did, giving China an opportunity to rise.\n\nWhat do others here think about his short film?", "upvote_ratio": 0.9, "id": "t3_tl2ert", "created_utc": 1648055797.0}
{"sub": "investing", "title": "Which of these accounts should I get rid of and which should I keep?", "selftext": "I signed up for all these things thinking there could be some benefit to trying different platforms, but it's confusing, and I don't have serious money to invest anyway.\n\nHere's the list:\n\n* Acorns\n* Betterment\n* Robinhood\n* Stash\n* Stockpile\n* Webull\n* SoFi\n* Chase, but I have to Chase bank accounts, so it's included. \n\nI also have various accounts for crypto, but in that case each has it's own benefits like different coins. I have Coinbase, Kraken, and \"Crypto\" for that.", "upvote_ratio": 0.33, "id": "t3_tl05ve", "created_utc": 1648053606.0}
{"sub": "investing", "title": "Why wasn't the call I sold exercised?", "selftext": "So, a couple months ago I sold an XOM covered call at $85 for $131.00. \n\nXOM went all the way up to $91.50 and my call didn't get exercised. Was $419 not enough of a profit for it to get exercised? (sarcasm)\n\nSomewhat new to CC, have not had one exercised yet, but this is the one that surprised me. Was I just lucky? When I sold the call I would have been happy to sell it all at $85, as I got in at $54.", "upvote_ratio": 0.55, "id": "t3_tkxg68", "created_utc": 1648051065.0}
{"sub": "investing", "title": "Opinion on Wheat 3x Lev USD", "selftext": "Today I bought  wheat 3x lev USD stocks at 13,09 \u20ac on the basis that grain is gonna go up within the next 60-90 days due to the enduring Ukraine-Russia conflict and the expected bad harvests in many countries.\n\nI did invest on petrol, gas and uranium months ago expecting the conflict and made good profit, I know the grain prices already exploded earlier this month but I have a hunch becouse how couldn't they go up? What's gonna offset the grain shortage?\n\nBut I will be honest, I have my doubts, is anybody else investing on wheat right now?", "upvote_ratio": 0.41, "id": "t3_tkuyyv", "created_utc": 1648045126.0}
{"sub": "investing", "title": "Russian Stock Market to Partially Reopen on Thursday", "selftext": "https://www.wsj.com/articles/russian-stock-market-prepares-for-an-unusual-reopening-11648023739\n\n&gt; Russia\u2019s stock market is set to have a partial reopening Thursday, nearly a month after it shut down following the invasion of Ukraine.\u00a0\nThe challenge for Moscow is that the resumption of trading could simply send Russian stocks back into free fall. On Feb. 24, the day when President Vladimir Putin began the assault on Ukraine, the main Russian stock index tumbled 33%. While the index regained a fraction of those losses on Feb. 25\u2014its last day of trading\u2014that was before Western sanctions hammered the ruble and sent the country into an economic crisis.\n\n&gt;To limit the fallout, Moscow has turned to some heavy-handed policies. It blocked foreign investors from dumping local stocks\u2014a move that some market participants saw as retaliation for a Western freeze on Russian central bank assets since a big chunk of the Russian market is owned by foreigners. The Russian government ordered its main sovereign-wealth fund to buy billions of dollars worth of shares.\u00a0\n\n&gt;The Russian stock market could ultimately look very different than it did before, with a plan under discussion to split it into separate markets for foreign and local investors, according to a person familiar with the matter.\n\n&gt;Russia\u2019s central bank said Wednesday that it will allow trading of 33 shares out of 50 included in the benchmark stock index, the MOEX, on Thursday from 9:50 a.m. to 2 p.m. Moscow time. Among the companies to be traded are Gazprom PJSC and Lukoil PJSC. Bets on the fall of a stock, known as short-selling, will be banned.\n\n&gt;Under a policy announced by the central bank on Feb. 28, Russian brokerages aren\u2019t allowed to let foreign clients sell securities. This will prevent foreigners from bolting for the exits as soon as the market reopens, which could be ruinous because of their outsize role in Russian stocks. International institutional investors held about three-quarters of the Russian market\u2019s free float as of February 2020, according to Sberbank Investment Research.\n\n&gt;That has raised concerns that the market will be skewed by the absence of foreign investors, who accounted for nearly half of equities trading volume at the Moscow Exchange in the first half of last year.\n\u201cThere will be an illusion of a working, recovering Russian stock market, even though a huge class of players in the market\u2014foreigners\u2014won\u2019t have the opportunity to sell,\u201d said Vladimir Kreyndel, CEO of ETF Consulting, a Moscow firm that advises issuers of exchange-traded funds.\u00a0\n\n&gt;Among the Western investors that held Russian stocks before the freeze were asset-management giants Vanguard Group and Fidelity International. Both firms have said they are reducing exposure to Russia.\nDue to the freeze, foreign investors won\u2019t have much to do when the stock market reopens.\u00a0\n\n&gt;But the plan under consideration by Russian officials\u2014which is still in the discussion stages\u2014would effectively split the country\u2019s securities market in two, with one market for foreigners and another for local investors, the person familiar with the matter said. In this arrangement, foreign investors could sell their shares or bonds, but would face restrictions on moving the proceeds out of Russia because of capital controls that Moscow has imposed since February, the person said.\n\n&gt;Such a bifurcated market could result in oddities, such as the same stock having two different prices. That isn\u2019t completely unprecedented. In China, there have long been discrepancies between shares on mainland exchanges in Shanghai and Shenzhen and those listed in Hong Kong.\nIt could also prevent further erosion of the ruble\u2019s value. Russia\u2019s currency has stabilized in recent sessions to trade near 104 rubles to the dollar, though it remains 22% weaker than before Russia invaded Ukraine.\n\n&gt;\u201cThe biggest fear is that the central bank is under sanctions and they don\u2019t want foreign investors to sell their shares and take the ruble and buy hard currency,\u201d said Jacob Grapengiesser, head of Eastern Europe at emerging markets fund manager East Capital.\n\n&gt;The Moscow Exchange said Monday that it would allow for the settlement of trades that foreign investors had placed before Feb. 28 that were still being processed. Mr. Grapengiesser said his firm had trades still awaiting settlement from the start of the war that he expects to go through soon.\u00a0\n\n&gt;\u201cIt\u2019s a natural step before opening the market. You need to take care of those unsettled trades,\u201d he said. \u201cThings are slowly moving forward.\u201d\nShortly after the war began, Russia\u2019s prime minister ordered the country\u2019s National Wealth Fund to buy up to one trillion rubles, equivalent to $9.38 billion, worth of shares this year. Analysts also expect some Russian oil companies to prop up their share prices with buyback programs.\n\n&gt;Local investors may buy stocks too. When Russia invaded Crimea, the MOEX fell almost 18% between mid-February and mid-March of 2014. But by the end of that year, it had rebounded more than 12% from that March low. The broad index has posted gains in all but one year since 2014. Stocks in unstable countries can also serve as hedges against inflation because locals expect companies can offset rising costs by charging higher prices.\n\n&gt;The government\u2019s efforts have led some to be cautiously optimistic about the reopening. \u201cInitially, I think there will be a moderate correction,\u201d said Natalia Smirnova, a financial adviser in Moscow. \u201cBut I wouldn\u2019t rule out the possibility that the first day could end up with a modest increase.\u201d\n\n&gt;Russia is a minnow of a financial market by global terms. In December 2021, the total market capitalization of companies listed on the Moscow Exchange was about $842 billion, according to the World Federation of Exchanges, which is just under 90% of the current value of Tesla Inc. That made the Moscow Exchange the 20th largest bourse by market cap, just above Brazil\u2019s B3 exchange, in the WFE\u2019s ranking of global exchanges.\n\n&gt;Until the war, Russia mostly attracted attention from specialist emerging-market funds and hedge funds, though it made up only a fraction of holdings for most globally minded investors.\u00a0\n\n&gt;MSCI Inc. said it would drop Russian stocks from its influential indexes that track emerging markets. Before the war, MSCI\u2019s emerging market index had a 2.8% weighting for Russia. FTSE Russell has also announced plans to remove Russian stocks from its indexes. The moves will force investors whose holdings track the indexes to sell\u2014when they can.\u00a0\nWars have led to stock-market shutdowns before, although it is unusual. The New York Stock Exchange closed for about four months when World War I broke out in 1914, the longest closure in the NYSE\u2019s history. The Beirut Stock Exchange reopened in 1996 after a nearly 13-year shutdown caused by Lebanon\u2019s civil war.", "upvote_ratio": 0.96, "id": "t3_tkued5", "created_utc": 1648043543.0}
{"sub": "investing", "title": "what's the best way to save for my niece's education\u203d", "selftext": "To keep it short, I would like to setup a savings account for my neice without her mother knowing. \nMy niece is 3years old. I would like to be the account holder. We're in michigan. \n\nOf course 529 plans come to mind, but are there any other decent options? And how do I compare different 529 plans?", "upvote_ratio": 0.82, "id": "t3_tktcos", "created_utc": 1648040303.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - March 23, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 1.0, "id": "t3_tkppsg", "created_utc": 1648026069.0}
{"sub": "investing", "title": "Is there a market for green shipping boats?", "selftext": "Not an environmentalist, but it seems there are forces in the world that want the population to trend towards environmentally friendly means of transportation and shipping i.e. electric vehicles. After looking at some statistics for shipping vessels I was slightly shocked by the numbers: \n\n\u201cShipping is by far the biggest transport polluter in the world. There are 760 million cars in the world today emitting approx 78,599 tons of Sulphur Oxides (SOx) annually. The world's 90,000 vessels burn approx 370 million tons of fuel per year emitting 20 million tons of Sulphur Oxides. That equates to 260 times more Sulphur Oxides being emitted by ships than the worlds entire car fleet. One large ship alone can generate approx 5,200 tonnes of sulphur oxide pollution in a year, meaning that 15 of the largest ships now emit as much SOx as the worlds 760 million cars.\u201d\n\nAnyway, I\u2019ve only heard whispers about hybrid ships and obviously this would be a long term thing. Anyone have any idea on whether or not this will be a viable market in our lifetime?", "upvote_ratio": 0.67, "id": "t3_tkabgu", "created_utc": 1647975637.0}
{"sub": "investing", "title": "is it worth waiting for the splits? (specifically Amazon and Google, but in general)", "selftext": "With Amazon  and Alphabet both announcing 20-1 splits later this year, I was wondering what you guys think about taking the guaranteed price now vs waiting for the fluctuations after the split. I'll admit I'm one of the people who got into investing when the covid crash happened, so I'm rather inexperienced with a more \"traditional\" market. The only major split I've seen was Tesla and immediately after the 5-1 split you were able to get 5 shares at a pretty decent discount over the original share for a short time. Is that fairly standard?\n\nTl;dr: do splits normally cause large fluctuations, or is it a much better bet owning pre-split", "upvote_ratio": 0.87, "id": "t3_tkc8te", "created_utc": 1647980763.0}
{"sub": "investing", "title": "How will the next phase of the chip wars affect companies like: AMD, ASML, NVDA, INTC, Samsung, TSMC?", "selftext": "There's a new phase ahead in the semiconductor sector that will have major implications for the market dynamics in the semiconductor sector: the transition to next generation techniques involved in &gt;5nm nodes. Lets discuss the implications and our strategies!\n\nWhat companies do you think will benefit from the adoption of next-gen techniques in the decade to come, and why? I'll provide my theories in the comments.\n\n&amp;#x200B;\n\nBackground info:\n\nLithography: [https://semiengineering.com/multi-patterning-euv-vs-high-na-euv/](https://semiengineering.com/multi-patterning-euv-vs-high-na-euv/)\n\nHigh-NA challenges: [https://semiengineering.com/gearing-up-for-high-na-euv/](https://semiengineering.com/gearing-up-for-high-na-euv/)\n\nPackaging: [https://semianalysis.com/advanced-packaging-part-2-review-of-options-use-from-intel-tsmc-samsung-amd-ase-sony-micron-skhynix-ymtc-tesla-and-nvidia/](https://semianalysis.com/advanced-packaging-part-2-review-of-options-use-from-intel-tsmc-samsung-amd-ase-sony-micron-skhynix-ymtc-tesla-and-nvidia/)", "upvote_ratio": 0.86, "id": "t3_tkcp2r", "created_utc": 1647981984.0}
{"sub": "investing", "title": "Koch Industries, Built on Oil, Bets Big on U.S. Batteries", "selftext": "https://www.wsj.com/articles/koch-industries-built-on-oil-bets-big-on-u-s-batteries-11647946147?mod=hp_lead_pos7\n\nKoch Industries Inc., the energy-based conglomerate whose CEO long opposed environmental regulation and funded groups that questioned climate change, has emerged as one of the biggest financial backers of the battery industry.\n\nA Koch Industries unit has made at least 10 investments worth at least $750 million in the U.S. battery supply chain and electric vehicles in the past 18 months, regulatory filings, news releases and FactSet data show. Koch\u2019s battery investments are among the biggest from outside the auto industry, analysts say.\n\nFounded more than 80 years ago as an oil refiner, Koch Industries is now the most diversified U.S. battery investor, said Vivas Kumar, a former Tesla Inc. senior manager and industry analyst who last year launched a battery-parts startup. \u201cIt\u2019s stunning just how many different battery supply chain players they\u2019ve taken a stake in,\u201d he said.\n\nKoch Industries is now a top shareholder in startups such as Freyr Battery SA, FREY 4.41% Aspen Aerogels Inc. ASPN 7.29% and Standard Lithium Ltd. SLI 5.61% The money comes at a crucial time for many of these companies, which need to spend heavily to commercialize their products. Koch appears to be focused on building up the battery industry in the U.S.\n\nKoch Industries operates thousands of miles of pipelines that move oil and gas around the country and several large refineries. The company posts annual sales of about $120 billion through brands such as Brawny paper towels and Dixie cups, fertilizers and fabrics.", "upvote_ratio": 0.9, "id": "t3_tkjdhe", "created_utc": 1648001160.0}
{"sub": "investing", "title": "Risks of options trading?", "selftext": "I have a relative who is not risk averse whatsoever, in any aspect of their life. I long have been investing in index funds because it seems less risky to me. This relative on the other hand says \"that's for boomers\" (we're millenials).\n\nI am wondering, can someone explain the risks of options? Believe it or not, a cursory Google search was inconclusive, and as we all know, reddit's search function is garbage.\n\nThanks.", "upvote_ratio": 0.45, "id": "t3_tkhdaw", "created_utc": 1647994986.0}
{"sub": "investing", "title": "any angel investors- what is the pros and cons?", "selftext": "I recently started considering multiple vehicles for investment. My buddy is a VC and said he's interested in getting started as an angel investor and told me to check out the book Angel by Jason Calacanis. The book is awesome and makes the risks and benefits clear, but the message is that the reward outweighs the risk in a good volume of deals. \n\nI would love to hear from any angel investors on here of your experience. I want good and bad stories alike. Pros and cons. Risks, warnings, anything you can tell me about angel investing is greatly appreciated. \n\nAnother priority is any educational resources you'd recommend to learn more about the whole process. I have a biotech background and need much further understanding of the financial side of this game.", "upvote_ratio": 0.69, "id": "t3_tketwg", "created_utc": 1647987743.0}
{"sub": "investing", "title": "Global risk spreads - hear me out", "selftext": "Ok, so this is more of a theoretical question, but something I\u2019ve been pondering. Does the underperformance of the Russian military in Ukraine actually compress global risk spreads and send the market into a rally in 2023? Their performance is in sharp contrast to the fears/assessments by the global intel community. Is the world safer from a military point of view? Lower risk = more upside?", "upvote_ratio": 0.79, "id": "t3_tkd3p6", "created_utc": 1647983028.0}
{"sub": "investing", "title": "One of Wall Street\u2019s Most Vocal Bears Says Sell The Rally", "selftext": "So is this a Bear rally or the end of the Bear market we have seen over the past week with +20% in Tech stocks?\n\n&amp;#x200B;\n\n[https://www.bloomberg.com/news/articles/2022-03-21/morgan-stanley-s-wilson-says-this-is-a-bear-market-rally-to-sell](https://www.bloomberg.com/news/articles/2022-03-21/morgan-stanley-s-wilson-says-this-is-a-bear-market-rally-to-sell)\n\n\nEdit (20 days later): This dude was correct! \ud83d\udcaf", "upvote_ratio": 0.43, "id": "t3_tkcwee", "created_utc": 1647982519.0}
{"sub": "investing", "title": "What are the arguments for continued bullish behavior in the energy sector?", "selftext": "With WTI oil prices at essentially a ten year high, this seems like the logical point for an investor to step out of the energy sector. Additionally, volatility in the sector due to the invasion of Ukraine might make a more risk adverse investor want to go for the exit.\n\nStill, with energy sector ETF prices continuing to rise, what are the arguments behind continued demand?", "upvote_ratio": 0.83, "id": "t3_tk9hfj", "created_utc": 1647973415.0}
{"sub": "investing", "title": "Any good investments outside of the US?", "selftext": "We have plenty of ETFs that grab a broad cross-section of the US market, but I want to diversify outside of the US. I don't quite like the feeling of having all my eggs in one country's basket.\n\nThe trouble is I can't seem to find a single worthwhile market outside of America. I'm not talking 5% annual growth vs the US's 10+%, I'm talking everywhere else has been basically flat, long term. Some go up a couple percent, some go down a couple percent, but none of them even surpass average annual inflation (and inflation is extra high now, too!)\n\nNot China, not Europe, not Africa. Nowhere comes close. I even searched for the best countries to invest in. Same pattern. Am I missing something, here? Businesses in other countries must also make money, right? Why is everything so stagnant outside of the US? Is there a good way to invest in other countries and perform better than bonds?", "upvote_ratio": 0.55, "id": "t3_tk8br8", "created_utc": 1647970370.0}
{"sub": "investing", "title": "Do inverse bond ETFs reveal a bond market inefficiency?", "selftext": "The 10 year US Treasury yielded 1.52% on Dec 31 2021 and yields 2.14% now, for a rise of 0.62%.  This year the Fed is expected to raise interest rates 1.75%, which is very low compared to inflation currently over 7%.  Let's say inflation cools - it will still be 3-5%, so the Fed needs to keep acting in 2022-2023.\n\nIf the Fed simply kept on track, yields would need to push 1% higher.  As that happens, inverse bond ETFs make significant gains.  In my view, the chance of this happening is so high, the bond market should reflect it - but there's only 0.62% worth of higher yields in the 10 year treasuries.\n\nIn this scenario, I would expect I'm missing something rather than the market not being efficient.  What risks are there to inverse bond ETFs that I've missed?", "upvote_ratio": 0.82, "id": "t3_tk4oan", "created_utc": 1647960539.0}
{"sub": "investing", "title": "Which platform do you use for trading? How does it impact you psychologically?", "selftext": "Hello friends.\n\nAs I  know, making too frequent trades would negatively impact any investment. \n\nI'm just wondering if any of you have the experience of things like \"I make more money on the desktop because It's not easy to reach, therefore better planned for each trade?\" or vice versa.   \"Because of the ease of using mobile app makes me make swifter decisions, therefore making more money.\"\n\nWhich platform do you find most helpful to you? Have you found yourself make more successful trade/ investment on a particular platform?\n\nWould love to hear some thoughts. Both trader and investor insights are appreciated.", "upvote_ratio": 0.77, "id": "t3_tk5nbj", "created_utc": 1647963204.0}
{"sub": "investing", "title": "Good way to hedge against the SP500?", "selftext": " Buying 20 year treasuries (TLT) and selling the same amount of high yield bonds (JNK) seems to be a good hedge for a drop in the SP500.\n\nThe way I see it is:\n\n\\- If JNK moves up more than TLT most likely it means SPY has been going up as well, hopefully SPY has a bigger gain than JNK/TLT ratio.\n\n\\-If SPY starts tanking then JNK will lose more (or gain less) than TLT, hopefully offsetting the loses of the SPY.\n\nI wish I could add a chart on this post to show a comparison but anyone in tradingview can search for \"TLT/JNK\" and compare it to SPY.... every time SPY drops TLT/JNK ratio moves up.\n\nI know we are living through historical times with the money printing but what case scenarios could affect the TLT/JNK ratio to not move up when SPY drops?\n\nAny other ways to hedge SP500 drops?\n\nI am a fan of put spreads on SPY or even covered calls on VIX related etfs when SPY is in pain but the issue is the upside is limited and holding just Puts or even VIX related etfs seem to be a bit costly on the long run. With TLT/JNK ratio I feel that even if SPY doubles, the TLT/JNK ratio wouldn't go down as much lol", "upvote_ratio": 0.57, "id": "t3_tk5fi4", "created_utc": 1647962591.0}
{"sub": "investing", "title": "Time to GTFO China? Morningstar and Bill Browder...", "selftext": "**EDIT: I'm aware that this is a much discussed topic.  I believe Morningstar's commentary tackles the broader question of not just getting out of China but also dissects the investment sensibility of being involved at all in autocratically-controlled markets.  Thanks to** u/J0eBidensSunglasses **for pointing me to this article.**\n\nFrom [Morningstar.com](https://www.morningstar.com/articles/1083334/autocracy-is-a-bad-investment):\n\n&gt;Sure, investors might make money for a while, but in the end, all that matters are the rules set by the person running the country. And often that means they are setting the rules to maintain power, enrich themselves and their cronies, or both.  \n&gt;  \n&gt;It was one thing to miss the risks of investing in Russia. It's a country where most diversified investors have only a [small percentage](https://www.morningstar.com/articles/1082249) of their portfolio. It's a different story for a country like China. Many mutual funds and stocks have hefty direct or indirect exposure to the country, and observers who had warned about Russia are encouraging investors to ask similar questions about China.\n\nFurther, famed fund manager Bill Browder opines:\n\n&gt;Rule of law should be a primary consideration for investors, says Bill Browder, the famed hedge fund manager who made his fortune in Russia, only to be deported after run-ins with oligarchs and whose Russian lawyer was arrested in Moscow, mistreated by authorities, and died in a prison.  \n&gt;  \n&gt;\"You can do all the analysis you want on an industry, on the economics, on the management team, and then all of a sudden somebody comes along and rips you off, and you don't have any recourse in the courts, you don't have recourse in the media \u2026 and generally if they're not rule-of-law countries, you have no recourse with the regulators,\" he says. \"It's flying by the seat of your pants, hoping you're in the good graces of whoever is in charge.\"\n\nHaving come from that side of the world, I've always been wary of markets like China and Russia for exactly the reasons that Morningstar lays out.  When asked, my answer is always the same:  Do you understand the accounting rules of that country?  Do you understand the business dynamics and customs of that country\u2014i.e. if lying and bribery are commonplace, what does that mean for your fundamental analysis?  Still, as Browder points out, you can be an expert on these things and still get whacked by one man who makes decisions not on the basis of what is economically sound, but whatever strokes his own ego.", "upvote_ratio": 0.87, "id": "t3_tk4y60", "created_utc": 1647961274.0}
{"sub": "investing", "title": "Alibaba Stock the Steal of a Lifetime?", "selftext": "We all know Alibaba. The controversial Chinese company that has been beaten down all the way from $320 (Oct 16, 2020) to all the way up to $73.28 (March 15, 2022).\n\nFrom Ant Financial to Jack Ma's disappearance to delisting issues to Chinese tech crackdowns and so on. Many of which did actually permanently change the health of the company itself.\n\nHowever, it seems much of the more recent price drops have been due to fears that came out to be untrue.\n\n&amp;#x200B;\n\nThe biggest price drops in the recent year has been due to:\n\n1. Delisting fears\n2. Unending Chinese tech regulations\n3. Trust of the entire VIE structure\n\nIt seems all of those are coming to be just that: fears.\n\n**0. Fundamentals At A Glance**\n\nI have no idea why this stock traded at $73 at one point (\\~200 billion market cap) when the cash position alone is worth over a third of the market cap and its earnings are 22+ billion in a normal year. The company is still growing (albeit much less in % than it used to \\[makes sense given how big the company is\\]) and a money printing machine. Sure its margins are lowering due to investments in lower cost communities in China but that just means more profits long term. This company is a true cash machine.\n\n**1. Delisting fears**\n\nFirst of all, delisting does not mean the stock will go to zero. This might come off as a surprise to some people but with a reputable brokerage, you can exchange your 1 BABA shares to 8 XHKG:9988 shares. And you can vote with those 9988 shares.\n\nUnless you suddenly came into the belief that China will nationalize all its stock exchange AND un-employ millions of Chinese grads who majored in finance/econ/etc., this idea is just baffling. Especially when China has been opening more stock exchanges recently: [Beijing Stock Exchange](https://en.wikipedia.org/wiki/Beijing_Stock_Exchange) has opened just end of last year.\n\nChina has actually been taking steps the past year to support the concept of the stock market. And it seems Evergrande has been a life saving event for stock holders. The Evergrande fiasco has been one of the biggest bull cases for the Chinese govt to support the stock market as the place of investments for the younger generation over real estate. [Chinese stocks in surge after promise of state support](https://www.bloomberg.com/news/articles/2022-03-16/chinese-stocks-in-the-u-s-surge-after-promise-of-state-support)\n\nAs of actual delisting fears, all the commentaries from the Chinese side has been very bullish despite Bloomberg constantly claiming otherwise.\n\nThis year, the Chinese Govt has FINALLY made it public. Chinese tech stocks like Alibaba will BE allowed audits from the US. This is huge. Not only does this imply the accounting is credible for some of these big names, but it also means the entire delisting fear is just one big political play. [China plans audit concession](https://www.ft.com/content/0463c70d-4d24-4b9b-b7e5-ace22409d8e2)\n\nSo the entire 'delisting' issue should be off now. The very issue that has crashed majority of the value in Alibaba share price.\n\n&amp;#x200B;\n\n**2. Unending Chinese tech regulations**\n\nChina has been different from the US in the tech market in that China opened its tech market with basically no laws. US on the other hand has laws in place ahead of time and then lets the companies compete.\n\nAnd if are an East Asian, you would be well aware how toxic the private education industry has been to the people and how exploitive some of these tech giants have been becoming. Alibaba for instance with AlliPay was basically playing around with exploiting  the poor with loans to make profit. In the US, none of that would have happened. China however has been a developing nation and is finally starting to put rules in place for what tech should and shouldn't do.\n\nAll this led to fears of whether the Chinese Party hates the entire Chinese Tech industry or not. The crackdown seemed to be unending. Did China just hate all of big tech?\n\nWell, the answer is now clear. The big tech regulation is ending. \n\nIn October 2021, China hinted the crackdown for big tech was coming to an end: [news](https://www.bloomberg.com/news/articles/2021-10-22/china-s-top-financial-regulator-sees-progress-in-tech-crackdown)\n\nVery recently (last week), China officially declared tech crackdown was at closure: [news](https://www.cnbc.com/2022/03/16/china-says-it-will-support-chinese-ipos-abroad-calls-for-closure-on-tech-crackdown.html)\n\nIn fact, that news evidences China is supporting Chinese IPOs abroad on top too.\n\nIn other words, the entire big tech regulation fear is ending.\n\n&amp;#x200B;\n\n**3. Trust of the entire VIE structure**\n\nThe ambiguity of the entire VIE structure and whether the Chinese govt would support it has been a huge \"?\" to the foreign investors for some time now.\n\nAre VIE structure investing legit?\n\nWell on end of December of 2021 (during Christmas time), China finally announced the verdict. VIE structures are a LEGITIMATE form of investing in Chinese firms. [VIE support](https://realmoney.thestreet.com/investing/global-equity/china-tightens-loophole-for-companies-listing-abroad-15871357)\n\nSo that fear is entirely misplaced as now there's clarifications. All those decades of investors placing a discount on Chinese stocks BECAUSE of VIE structure should have been solved end of last year.\n\n&amp;#x200B;\n\n**Bull information to consider**\n\nThe Chinese government has been proactively BUYING Chinese stocks since end of last year. Chinese firms have been proactively BUYING its own shares since end of last year.\n\nLast time China endorsed its own companies to do buybacks proactively was the bottom of the financial crisis.\n\nAlibaba has upped its share repurchase program to [$25 billion](https://www.msn.com/en-us/money/topstocks/alibaba-rises-as-much-as-11-after-the-chinese-e-commerce-giant-lifts-its-share-buyback-program-to-25-billion/ar-AAVlVV4?ocid=BingNewsSearch) claiming the price was not justifying the value of the company.\n\nWhy is it all the foreign investors in US are running away from Chinese stocks while both the Chinese government and the Chinese firms are throwing tens of billions at the Chinese stocks? Maybe just maybe the entire fear has been misplaced and the Western Media has been grossly overexaggerating all this?\n\nAnd of course, it helps figureheads like Charlie Munger is also bullish on the stock and puts money where his mouth is. The financials of this company is phenomenal. It's a cash machine even without the growth story.\n\nWith the past 2 years of downturn, Alibaba shares have been more and more owned by Chinese investors. Add on with the buybacks, Alibaba shares will soon be traded by Chinese natives through Stock Connect. As of now (and in the past), Alibaba shares weren't traded by Chinese natives. That rule can soon change (in fact, the new buyback program alone is more than enough) as at least 55% of the company would now be traded in Hong Kong going forward. How is that not bullish? Alibaba stocks will soon be traded by BOTH foreigners AND domestic holders!", "upvote_ratio": 0.5, "id": "t3_tk3mvm", "created_utc": 1647957550.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - March 22, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.71, "id": "t3_tjyp07", "created_utc": 1647939668.0}
{"sub": "investing", "title": "Filing Taxes on Stocks Question", "selftext": "I started investing last year, and so this is the first time I will have to do taxes for them. When doing my taxes, so I have to manually input each and every sale and its date,or can I not just use the overall amount that I gained/lost for the year? I have my 1099 form, but it seems way too tedious and time-consuming to do it like this. How does everyone else do it? I appreciate any help :)", "upvote_ratio": 0.76, "id": "t3_tjpyzc", "created_utc": 1647907708.0}
{"sub": "investing", "title": "Why is the Federal Reserve raising interest rates going to lower inflation?", "selftext": "fed raising interest rates: forces businesses/corporations who are looking to continue financing growth through debt/loans spend more due to higher interest rates, aka less free cash flow (since it will cost more to service any new loans going forward) aka less money to hire people with, give bonuses/raises, reinvest into the company, etc.\n\nhow does this slow inflation, which is mainly driven by supply constraints + commodity prices?", "upvote_ratio": 0.32, "id": "t3_tjpdnx", "created_utc": 1647905977.0}
{"sub": "investing", "title": "Interest rates, real estate, and inflation hedging", "selftext": "There are a lot of articles (just google it) saying real estate is an investment hedge.\n\nI don't understand why real estate is an inflation hedge given the following:\n\n* During inflationary periods central banks raise interest rates to attempt to control inflation\n* Raising interest rates will very likely suppress house prices due to mortgages becoming more expensive\n\nSo if 1)interest rates rise during inflation and 2)rising rates suppress house prices then how is real estate supposed to be an inflation hedge?\n\n&amp;#x200B;\n\nEdit: \n\n [Inflation Hedge: Is Real Estate a Good Investment Hedge Against Inflation? - Bloomberg](https://www.bloomberg.com/news/articles/2022-01-24/is-real-estate-a-good-investment-hedge-against-inflation-what-the-experts-say) \n\n [Real estate as an inflation hedge - Canadian Business](https://archive.canadianbusiness.com/blogs-and-comment/real-estate-as-an-inflation-hedge/) \n\n [Is Real Estate A Hedge Against Inflation? (forbes.com)](https://www.forbes.com/sites/forbesrealestatecouncil/2021/09/28/is-real-estate-a-hedge-against-inflation/?sh=55d8474519da) ", "upvote_ratio": 0.75, "id": "t3_tjnoet", "created_utc": 1647901262.0}
{"sub": "investing", "title": "Evergrande Trading Suspended In Hong Kong; Ronshine Auditor Resigns: Evergrande Update", "selftext": "\"China Evergrande Group\u00a0and its other units were suspended in Hong Kong Monday pending an announcement containing \u201cinside information,\u201d according to\u00a0exchange filings\u00a0that didn\u2019t elaborate further. The embattled developer may hold a cain this week\u00a0to brief investors on its debt restructuring plan, REDD reported.\nElseHoldingRonshine China Holdings Ltd.\u00a0won\u2019t meet a March 31 deadline to\u00a0publish audited full-year results after it became the latest property firm to announce the resignation of its auditor. Ronshine\u2019s stock and bonds plunged.\nChinese high-yield dollar bonds traded flat to 2 cents higher with better quality names leading gains, according to credit traders. Property firms snapped a three-day stock rally, as the Bloomberg Intelligence gauge of developers fell 2.6%.\u00a0\n\n\n\nhttps://www.bloomberg.com/news/articles/2022-03-21/trading-suspended-ronshine-auditor-resigns-evergrande-update", "upvote_ratio": 0.96, "id": "t3_tjnk2s", "created_utc": 1647900927.0}
{"sub": "investing", "title": "Long term vs short term, stocks and options", "selftext": "Just thought of something, please correct me if I\u2019m wrong as I am a rookie. \n\nIt seems like based on demographic and general vibe, options are what shorter term investors trade and stocks (meaning individual company stocks, mutual funds and ETF\u2019s) are what longer term investors buy and hold. But I thought about it, and shouldn\u2019t it be the opposite?\n\nShort term options, due to their expiry dates, time decay, and volatility movements around earnings and &lt;30 DTE are very unpredictable and can lose all their value very quickly. especially if they are OTM plays. \n\nIf an investor is looking for a short term gain, wouldn\u2019t simply buying and selling a stock be more suitable? Stocks will always be at fair market price based on supply and demand even after dramatic moves and theres no way your position value will go down despite you being correct about the direction of the stock (no IV crush). Also, if you are WRONG about the direction and it DOESN\u2019T move the way you want to, you still (theoretically) and infinite amount of time to recover it with a stock purchase, but not with an options contract. The latter is likely to go to zero if its close to expiry\n\nMn the other hand, options (LEAPS) seem to be more suited for the \u201clong term\u201d (even though you can only go a maximum of 2 years+ out). having high leverage (delta &gt; 60) on a trusted stock like AAPL or SPY or BRK that is very very unlikely to have two bad years in a row seems like the best case scenario. \n\nIs this a correct way of thinking? Or have I over-thought myself into thinking totally backwards? LOL", "upvote_ratio": 0.5, "id": "t3_tjn945", "created_utc": 1647900104.0}
{"sub": "investing", "title": "Raising interest rates to actually fight inflation", "selftext": "This more a hypothetical question for those smarter than myself. Let\u2019s start with my understanding of how raising interest rates impact inflation:\n\nRight now, my money is sitting in my savings account doing nothing. Goods and services are getting more and more expensive everyday. Rather than letting inflation eat away at my money, I decide to spend as much as I reasonably can. For example, cars are getting more and more pricey. Might as well buy one now before they\u2019re even more expensive next year. Now let\u2019s say everyone in my area picks up that mindset. Everyone wants to buy a cat. Now the demand is much stronger than the supply, and prices of cars increase not only because of inflation, but because of demand. This is a sort of inflation spiral. \n\nNow let\u2019s translate this to the business owner. My restaurant buys wine every week to sell to patrons with their meals. Every week wine goes up 3%. Any extra cash I have is not going into the bank, it\u2019s going towards stocking up as much wine as I possibly can. All other restaurants do this and supply cannot match demand and we have runaway inflation. \n\nHow can the Fed fight this? Make it so that your money sitting in savings isn\u2019t getting destroyed by inflation. How would they do that? Raise interest rates. Here\u2019s my question\u2026 how is raising interest rates 0.25% going to do ANYTHING AT ALL?!! Wouldn\u2019t interest rates have to rise to the rate of inflation (or close enough)?? So anywhere from 7-15 percent depending on who you ask? I know things aren\u2019t this simple but I\u2019m just curious how anyone thinks small rate hikes will do anything. The house of cards is pretty easy to predict if/when rates explode. Most people believe we\u2019re in a bubble now. If you had the choice between 7% savings rates and rolling the dice on the SP500 index which would you choose? I feel like we are going to have to drink our inflation medicine (big rate hikes) to avoid major economic collapse, which will be the catalyst for a large market adjustment in stocks/real estate. Am I missing something?", "upvote_ratio": 0.64, "id": "t3_tjkhs7", "created_utc": 1647892677.0}
{"sub": "investing", "title": "Should I hold strong currency with weaker interest rate or a weaker currency with high interest rate?", "selftext": "I earn in USD but my home country has a different currency INR. \n\nPros of holding USD:\n\n1. Converting to INR cost me 0.5% of the amount which I save by not converting.\n\n2. USD/INR rate seems to be going up for decades.\n\n3. More stable currency.\n\nPros of holding INR:\n\n1. Higher interest rate by bank/govt but there is also taxation on interest.\n\nLet's assume the interest rate is consistent. Also, assume 0.5% is a fixed charge and will remain the same forever. Let's ignore geographical risk.\n\nNow, I want to calculate which is more attractive currency. Our variables are:\n\n1. 0.5% fees (straight-forward)\n\n2. Income taxation on interests on deposit (straight-forward too)\n\n3. USD/INR exchange rate improvement over the years (how do I find the statistical mathematical value?)\n\nAssuming I don't need the money anytime soon or want to invest in stocks, how do I put all these 3 variables in an equation to find more attractive currency?\n\n**Ultimately, I want to arrive at some conclusion like \"There is 90% statistical chance that I will earn x% higher interest if I invest in y currency\".**", "upvote_ratio": 0.63, "id": "t3_tjjswi", "created_utc": 1647890831.0}
{"sub": "investing", "title": "White House to meet with oil, bank, other companies about Russia invasion", "selftext": "[Biden administration officials will meet](https://thehill.com/homenews/administration/599053-white-house-to-meet-with-oil-bank-other-companies-about-russia) *with executives from various industries including oil companies, clean energy firms and major banks about the ongoing conflict between Russia and Ukraine.*\u00a0\n\n*It will be led by NEC Director Brian Deese, national security adviser Jake Sullivan, Treasury Secretary Janet Yellen and Commerce Secretary Gina Raimondo.*\u00a0\n\n*Simons said that the meeting will feature companies in the financial services space, including Visa, JPMorgan and Bank of America.*\u00a0\n\n*It will also include oil companies like ExxonMobil and ConocoPhillips, refiners like Marathon Petroleum, and those in the clean energy sector like Pattern and Invenergy*.\n\n&amp;#x200B;\n\n**This either means he will ask oil companies to lower rates, and/or commit to remove red tape, and/or (dare I say) funding for the them... perhaps  there  could be a bullish play for oil.**", "upvote_ratio": 0.92, "id": "t3_tjiynm", "created_utc": 1647888610.0}
{"sub": "investing", "title": "Practically, what is the point of having lots of money in the stock market?", "selftext": "I have a good chunk of my net worth in stocks, and I\u2019m trying to understand how I should feel about it lol, because it\u2019s not like the balance is accessible right now.\n\nShould we view stocks as simply appreciation vehicles where the point is that it\u2019s just a better form for your money to be in than cash, and there\u2019s no purpose other than sell it when you\u2019re old so you have more money?\n\nOr should we actually view them as ownership, and the point of having money invested is to increase our position in companies we like and have more of a vote?\n\nOr should we try to accumulate as many shares as we can so they can be collateralized and we live off the margin loan? Or also live off dividends?\n\nOr some other thing?\n\nWould love to hear peoples thoughts!", "upvote_ratio": 0.58, "id": "t3_tjeaun", "created_utc": 1647876236.0}
{"sub": "investing", "title": "If you exceed your day trade limit but the gains from your last DT put your account over $25K, do you still get marked as a PDT?", "selftext": "I have 3 day trades and account value was $23.75K this morning when I made a purchase. My account value has now exceeded $25K as a result of gains from that purchase that I\u2019m still holding. If I sell now (which would be my 4th day trade within a 5-day period), will I be restricted?", "upvote_ratio": 0.73, "id": "t3_tjgbc2", "created_utc": 1647881615.0}
{"sub": "investing", "title": "Powell: \"If we conclude that it is appropriate to move... by more than 25 basis points at a meeting or meetings, we will do so. And if we determine that we need to tighten beyond common measures of neutral and into a more restrictive stance, we will do that as well.\"", "selftext": "https://www.cnbc.com/2022/03/21/powell-says-inflation-is-much-too-high-and-the-fed-will-take-necessary-steps-to-address.html\n\n&gt;Federal Reserve Chairman Jerome Powell on Monday vowed tough action on inflation, which he said jeopardizes an otherwise strong economic recovery.\n\n&gt;\u201cThe labor market is very strong, and inflation is much too high,\u201d the central bank leader said in prepared remarks for the National Association for Business Economics.\n\n&gt;The speech comes less than a week after the Fed raised interest rates for the first time in more than three years in an attempt to battle inflation that is running at its highest level in 40 years.\n\n&gt;Reiterating a position the Federal Open Market Committee made in its post-meeting statement, Powell said interest rate hikes would continue until inflation is under control. He said the increases could be even higher if necessary than the quarter-percentage-point move approved last Wednesday.\n\n&gt;\u201cWe will take the necessary steps to ensure a return to price stability,\u201d he said. \u201cIn particular, if we conclude that it is appropriate to move more aggressively by raising the federal funds rate by more than 25 basis points at a meeting or meetings, we will do so. And if we determine that we need to tighten beyond common measures of neutral and into a more restrictive stance, we will do that as well.\u201d\n\n...\n\n&gt;As he has before, Powell ascribed much of the pressures coming from pandemic-specific factors, in particular escalated demand for goods over services that supply could not meet. He conceded that Fed officials and many economists \u201cwidely underestimated\u201d how long those pressured would last.\n\n&gt;While those aggravating factors have persisted, the Fed and Congress provided more than $10 trillion in fiscal and monetary stimulus. Powell said he continues to believe that inflation will drift back to the Fed\u2019s target, but it\u2019s time for the historically easy policies to end.\n\n&gt;\u201cIt continues to seem likely that hoped-for supply-side healing will come over time as the world ultimately settles into some new normal, but the timing and scope of that relief are highly uncertain,\u201d said Powell, whose official title now is chairman pro tempore as he waits Senate confirmation to a second term. \u201cIn the meantime, as we set policy, we will be looking to actual progress on these issues and not assuming significant near-term supply-side relief.\u201d\n\n&gt;Powell also addressed the Russian invasion of Ukraine, saying it is adding to supply chain and inflation pressures. Under normal circumstances, the Fed generally would look through those types of events and not alter policy. However, with the outcome unclear, he said policymakers have to be wary of the situation.\n\n&gt;\u201cIn normal times, when employment and inflation are close to our objectives, monetary policy would look through a brief burst of inflation associated with commodity price shocks,\u201d he said. \u201cHowever, the risk is rising that an extended period of high inflation could push longer-term expectations uncomfortably higher, which underscores the need for the Committee to move expeditiously as I have described.\u201d\n\n&gt;Powell had indicated last week that the FOMC also is prepared to begin running off some of the nearly $9 trillion in assets on its balance sheet. He noted that the process cold begin as soon as May, but no firm decision has been made.", "upvote_ratio": 0.97, "id": "t3_tjg116", "created_utc": 1647880843.0}
{"sub": "investing", "title": "Buying put options instead of holding bonds for long term investor", "selftext": "So, I've been following the old school way of holding bonds as a hedge against downwards trends in equities, which, as you can imagine, hasn't been so hot lately. I'm about 70% equities (SPLV), 30% long term bonds (TLT). So a fairly conservative portfolio. The equities part of my portfolio isn't so bad right now, but bonds are getting trashed. \n\nWith that trashing in mind, I'm thinking I could buy long term puts (1 year) on the underlying equities I hold, instead. Every six months, I would be selling them, and buy new 1 year puts, which seem to be the timing that has the lowest carrying costs. \n\nIt seems to me it would be cheaper to do that in a rising market than traditional bond holding, which reduce the amount invested in equities by the value held in bonds, and hence, the return. I could also invest in more aggressive ETFs than SPLV, since the downwards trend would be protected.\n\nBeing the inverse of the long equities position, it means it would also do well during a major correction. \n\nThe only issue would be during a flat market, but then, I see that as the cost of getting \"insurance\".  \n\nAnything stupid I'm overlooking before I run some numbers on this to see if it has any merit?", "upvote_ratio": 0.5, "id": "t3_tjec0z", "created_utc": 1647876321.0}
{"sub": "investing", "title": "Stock that was gifted to me question", "selftext": "I have some stock that was given to me many many years ago.  It's sitting at Vanguard and I don't know the average cost.  Basically just a dollar amount that I let grow or regress...\n\n&amp;#x200B;\n\nWell here's my question, if I were to cash it out to add into my down payment for a house how would I be taxed?  USA taxes\n\n&amp;#x200B;\n\nThank you and your feedback is appreciated. Have a good day.", "upvote_ratio": 0.73, "id": "t3_tjdnm1", "created_utc": 1647874533.0}
{"sub": "investing", "title": "Question concerning huge pop upward in price before market close", "selftext": "Just an actual moment before market closed on friday the 18th there were a few (or maybe many) stocks that saw a humongous pop in price just before the bell rang, and then a decrease back down to a level at or a little above the previous price in aftermarket hours.  Nvidia was among one of the stocks that did this. \n\nDoes anyone have some insight as to what was happening here?", "upvote_ratio": 0.72, "id": "t3_tja1xp", "created_utc": 1647863703.0}
{"sub": "investing", "title": "Berkshire acquires Alleghany in $11.6 billion deal", "selftext": "Looks like Buffett is finally starting to put Berkshires massive cash pile to use. Today, Berkshire announced a $11.6 billion acquisition of property and casualty insurance company Alleghany. Alleghany closed trading on Friday with roughly a $9 billion market cap. \n\nThis is one of Berkshires largest moves in recent years and could signal that the Oracle of Omaha finally deems the market to be undervalued enough to build positions. This comes after Berkshire had also recently announced that they have increased their stake in Occidental Petroleum.\n\nShares of Alleghany are up roughly 15% pre market as of this writing.", "upvote_ratio": 0.96, "id": "t3_tj8693", "created_utc": 1647856511.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - March 21, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.91, "id": "t3_tj7el8", "created_utc": 1647853269.0}
{"sub": "investing", "title": "What do you think is a worthy subscription service (not trading signals, please)", "selftext": "Background: I\u2019ve done options trading on the Greeks and a bit of fundamental analysis, did some technical analysis and generally don\u2019t believe in using it to trade now, but more of an affirmation of good entry timing.\n\nI would like to continue doing a little bit of options trading on the side, and invest using options to buy longer-term calls, so that begs the question. What services do a good job of analysing and picking stocks to invest or options to trade, specifically for short term options trading, or mid-long term investing?\n\nI\u2019m considering:\n\n- Seekingalpha\n\n- Motley fool\n\n- Benzinga (for options)\n\n- Stockearnings.com (for options)", "upvote_ratio": 0.41, "id": "t3_tj1iq9", "created_utc": 1647830335.0}
{"sub": "investing", "title": "Does the long-term capital gains rate apply to options too?", "selftext": "For example, if I purchase a call option (with an expiration date of at least a year out obviously), and I sell that contract (not exercise it) after 1 year for a profit, would that gain be taxed at the long-term capital gains rate like stock sales would?", "upvote_ratio": 0.73, "id": "t3_tj1bku", "created_utc": 1647829672.0}
{"sub": "investing", "title": "Brokers with Strong Visuals &amp; Charts", "selftext": "I am struggling to leave Robinhood because I love the simplicity and the clean visuals of the app. Is there a brokerage that shows a chart where it distinguishes your contributions and shows returns (similar to a Vanguard 401k graph)? Second, is there a brokerage that shows all holdings a percentage of a pie (similar M1, but not M1)? \n\nCurrently, using Schwab but I find it really confusing to track multiple stock investments over time. I am really surprised at the lack of charts available", "upvote_ratio": 0.68, "id": "t3_tiuotc", "created_utc": 1647809795.0}
{"sub": "investing", "title": "Hello guys, looking for clarofication between 3 fund and 2 fund portfolio", "selftext": "Hello my friends. I'm new here and I hope my post is not in breach of the sub's rules, it is not personal advice, just a general query on the Bogleheads 3 fund portfolio. I notice that this portfolio advocates for the equities to be split between US stocks and Intl ex-US stocks  and then bonds (with variable weighting to your taste)\n\nI also notice that Buffet is a big advocate of a 90/10 split for S&amp;P500/1-3yr Gov bonds. With both this and Bogleheads 3-fund performing similarly over 40 years (depending on weighting)\n\nSo my question is this: if you do prefer the benefit of global diversification over US concentration risk, why not simply go for an all world fund like VWRL (where US is approx 60% weighted anyway) Isn't putting your cash into fewer pots better for long term compounding... Why would you split into 2 funds where 1 can do the job of both? Is there a benefit to doing this?\n\nAll I can think of in terms of benefit of 3 fund is that when the time comes for retirement and you are liquidating your investment and (theoretically) at that period the US and Intl ex-US have performed differently, you can liquidate one and perhaps let the other stay a little longer?\n\nThere isn't a straight forward answer i've been able to find on this so far. Thanks for any help or insight.", "upvote_ratio": 0.74, "id": "t3_tiro8o", "created_utc": 1647801509.0}
{"sub": "investing", "title": "past performance doesn't mean future performance, so buy VTI??", "selftext": "I see this all the time. People always want to make it seem like you'll never be able to pick out a good stock or company no matter what. If you say, \"Hey, apple and Microsoft are great! they've been great companies, have tons of money flowing to them, and are one of the biggest things ever!\" people will yell at you and go \"NOOOO PAST PERFORMANCE DOESNT EQUAL FUTURE PERFORMANCE, NOOO\" \n\nbut then when you ask those same people what they invest in, they'll say \"I invest in VTI, since they've gone up consistently year over year over year, have tons of money flowing to it, and is one of the biggest things ever! It even goes up on average 7-10% every year!!\"\n\nLike. sure, I get the whole theory behind it but I'm pretty sure people wouldn't be so keen to invest in VTI if it wasn't going up an average of 7-10% every year. \n\nit seems like in order to really get a good investment on anything, you need to factor in past performance. You could have a billion dollar idea but if you haven't generated the cash flows and performance, what you have is just that, an idea that doesn't exist. It just seems odd that people dismiss investing in good solid companies as \"recency bias\" or \"past performance chasing\" even though they're doing that exact thing with VTI. Again, even if the theory was the exact same but VTI lost money year over year, I genuinely don't think people would be investing in it. *everybody Invests in broad market funds because past performance shows they do go up over time*", "upvote_ratio": 0.75, "id": "t3_tirnhk", "created_utc": 1647801450.0}
{"sub": "investing", "title": "The best companies to buy in this market", "selftext": "The best companies to buy in this market are companies that have high intrinsic value, are making good profit, and are at good value (on a p.e basis). Companies that have these traits will offer good long term returns and are much less susceptible to market sentiment. Because at the end of the day if you buy companies that are making good profit, and at good value on a p.e. basis, you'll get shareholder returns through dividends, potential acquisitions, and especially share buybacks. These companies offer the best risk/reward adjusted returns. Here are some examples:\n\n1. Intel. Chips are much needed right now, clearly high intrinsic value. They make 5b each quarter in profit, and are trading lower than 10 p.e.\n\n2. Facebook. Advertising is needed for companies to advertise their products and services. Clearly high intrinsic value. They make huge profit and are trading at a 14 pe.\n\n3. Moderna. Vacines and medical treatments are needed, clearly high intrinsic value. They made 5b in profit last quarter and are trading at 3 p.e. Market hugely overestimates revenue depreciation.\n\n4. Crescent Point Energy. Oil, natural gas, and energy are needed. Clearly high intrinsic value. High oil prices reflect this. They make good profit and are trading at 4 p.e.\n\n5. Google. Advertising is needed for companies to advertise their products and services. High intrinsic value. They make a lot of profit and are trading at like an 18 p.e.\n\nHere are examples of companies that are BAD buys in this market. Fubo, Square, Roblox, Skillz, PTON. Why take the risk and invest in speculative companies like this. Intel, facebook and the others are significantly less risky, and offer just as much upside. They are much better risk/reward options. Those are my thoughts.", "upvote_ratio": 0.38, "id": "t3_tioodh", "created_utc": 1647793368.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - March 20, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.77, "id": "t3_tigyox", "created_utc": 1647766867.0}
{"sub": "investing", "title": "Returns aren't what they should be...", "selftext": "Hello! I've been investing through Robinhood for the past year. I have held Apple stock (though small amounts in fractional shares) though the whole year. In the past year, Apple's stock increased by 37%, but on my returns for Apple, it says a I have gotten a total Apple return on 3%. How is this possible when I haven't sold any shares or fractional share, just bought more? I appreciate any help I can get.", "upvote_ratio": 0.27, "id": "t3_tid048", "created_utc": 1647749980.0}
{"sub": "investing", "title": "Why is Stellantis (STLA) so cheap?", "selftext": "I\u2019ve been looking for cheap stock and found Stellantis recently that I think fits the mold. Pros are:\n\n- They have several brands under management including american brands Dodge, Chrysler, Jeep and Ram. \n- Discounted compared to peers with a P/E of ~3.25, market cap of 45.7 billion with 1/3 of that being their net cash position, and cash flows of 18 billion annually. \n- Transitioning to EVs and realizing great synergies from a recent merger. \n\nCons are:\n- Paying a heavy dividend of over 7%. I\u2019d rather see them reinvest or buy back shares. \n- Based in Europe. Adds complexity to taxes. The company handles revenue in Euros and is more exposed to the russian-ukraine situation than non-european manufacturers. \n- If oil stays high for years (something I\u2019m hoping for with other positions) their truck and SUV sales will suffer. \n\nThoughts on STLA?", "upvote_ratio": 0.77, "id": "t3_tiauc1", "created_utc": 1647742313.0}
{"sub": "investing", "title": "QFIN - 360 Digitech - What do you think?", "selftext": "Has been coming up on the Stockosaur.com screener as one of the 4 most undervalued stocks on the market right now. Even after it's 43% surge in recent days, it's sector relative P/E, P/B is very appealing. It pays dividend, EPS is above $8, has a positive Free cash flow per share and there are 8 analysts who on average expect it will grow 116% from current price in the next 12 months. Also their EPS has been growing for 2 years. What do you think, is it worth to open a large position in it? If not, why not? What could be the catch here?", "upvote_ratio": 0.75, "id": "t3_ti78pw", "created_utc": 1647730812.0}
{"sub": "investing", "title": "Concerning results from Fidelity Survey", "selftext": "A few highlights\n\n*  21% Who Left Job During Great Resignation Cashed Out 401(k) After Leaving \n*  55% of next gen say they put their retirement planning on hold during the pandemic \n*   Almost half (45%) of next gen don\u2019t see a point in saving for retirement until things return to normal \n\n[https://www.businesswire.com/news/home/20220317005129/en/Fidelity%C2%AE-Study-Most-Americans-Hope-to-Put-Pandemic-Behind-Them-and-Reclaim-Strong-Financial-Grounding-but-Retirement-Concerns-Abound](https://www.businesswire.com/news/home/20220317005129/en/Fidelity%C2%AE-Study-Most-Americans-Hope-to-Put-Pandemic-Behind-Them-and-Reclaim-Strong-Financial-Grounding-but-Retirement-Concerns-Abound)\n\nPersonally, I find it deeply concerning that 1/5th of people who quit cashed out their 401ks. It's their money, but jeez.", "upvote_ratio": 0.93, "id": "t3_ti3aq3", "created_utc": 1647719564.0}
{"sub": "investing", "title": "The simplest investing strategy I use", "selftext": "We've all heard the phrase that time in the markets, beats timing the markets. But what if I told you that's it's possible to actually do both. Be in the markets 100% of the time while also being able to attempt to time big sell offs and rallies.\n\nThis strategy relies on keeping money in the simplest low cost index funds, vanguard is a very great option but not the only one. I'll explain the concept first then how I apply it to my portfolio.\n\nThe concept, is to use simple ratios to decide what major index is performing best. On trading view you can make a ratio by charting either an etf or the index, whatever you chose to use just he consistent. So you do up charts that look like Sp500/Nasdaq100 or sp500/dow30. I then add a 365 day sma to the chart.\n\nThis line simply tells us, given the past 1 year of price action what index is outperforming the other? If our candles are above the sma, it shows strength in the index kn the numerator. If it's below the sma it's showing strength in the index in the denominator. The sma acts as a switch so to speak. \n\nWe can go further with this. When you've analyzed these ratios you'll see the sp500 is dominant right now. So let's go further. Sp500/sp500 value. And sp500/sp500 growth. And sp500value/sp500 growth.\n\nIf you chart these ratios you'll see sp500 value is currently performing the very best, relative to all the other indices.\n\nSo what do we do with this information? I personally use it for my 401k. 10% bonds, 10% global, and 80% us stocks. So I take my calculation I've made and allocate that to my 80% us sp500 value stocks. Until a month ago, I was sp500 growth stocks. \n\nAs you can see with this strategy, I've never exited the market but I have picked certain times when I want to switch which index will give me better returns.\n\nCheers \ud83d\ude18 \n\nAnd I can provide the exact tickers I use if interested. But I picked my CRSP indexes because they are what vanguard uses exactly to balance their index funds, it could be different for you", "upvote_ratio": 0.36, "id": "t3_ti2ur2", "created_utc": 1647718310.0}
{"sub": "investing", "title": "Short term losses question", "selftext": "Let's say you have a short term loss of $10k. You can write off $3k against your income. Then next year lets say you have a loss of another 10k. You write off another $3k. You still have $14k in losses youve never had a benefit for. Now in year 3 you make $15k in short term capital gains. Would you be able to offset that with the past losses and only have $1k in short term gains this year?", "upvote_ratio": 0.67, "id": "t3_thz4ml", "created_utc": 1647708011.0}
{"sub": "investing", "title": "MLPs and Charitable Giving", "selftext": "I\u2019m trying to understand if there are any tax benefits to donating master limited partnership (MLP) units to charity. As MLPs distribute dividends in the form of return on capital, making these tax-deferred and lowering the cost basis, can\u2019t you just wait until your cost basis is $0 and then donate the units to charity to avoid paying LTCG on the entire unit price?\n\nI\u2019ve tried to find more info on this and it seems very sparse and somewhat unclear. The best I could find was a [Forbes article from 2010](https://www.forbes.com/sites/baldwin/2010/12/02/tax-guide-to-master-limited-partnerships/?sh=1a4b28b670d1) which stated:\n\n\u201cDon\u2019t donate depleted MLPs to charity. Since they are pregnant with ordinary income, your deduction would be limited to the stock market appreciation in the shares. (Buy at $50, let the basis run down to $0, donate when the share is worth $52: Your charitable deduction is $2).\n\nYou\u2019d be better off donating appreciated shares of Exxon; provided you\u2019ve held them more than a year, you will get a deduction for their current market value and the appreciation is never taxed.\u201d\n\nI admittingly don\u2019t understand the logic used in that article.", "upvote_ratio": 0.86, "id": "t3_thygah", "created_utc": 1647706149.0}
{"sub": "investing", "title": "Is the current market sustainable?", "selftext": "I\u2019m looking for all opinions out there!\n\nI might just be a doom and gloomer, but I\u2019m just waiting for a market crash to hit.\n\nI may be being biased in my caveman like research, not having a background in economics aside from a couple classes in university. \n\nThank you to any that take the time to respond.\n\nEdit: \nFor any clarification, I more or less left out my rationale in order to get the opinions of others, rather than simply having a debate over my own. \nI do appreciate the input of all who\u2019ve commented, and I hope you have a good weekend!\n\nWith regards to actually watching the market, this year I really have not been. Work has been nuts, and the stocks I do own have been quite stable and gaining whenever I check.", "upvote_ratio": 0.51, "id": "t3_thxym2", "created_utc": 1647704794.0}
{"sub": "investing", "title": "Receiving dividends in other currencies", "selftext": "Is anybody familiar with a broker that allows you to select what currency to receive dividends in?\n\nMy current broker (Fidelity) translates all received dividends into dollars in my account. Are there any brokers that allow you to select a different currency?", "upvote_ratio": 0.56, "id": "t3_thw88g", "created_utc": 1647699754.0}
{"sub": "investing", "title": "Famous TSLA Short Jim Chanos is Now Short Coinbase ($COIN)", "selftext": " Jim Chanos recently revealed that his fund is now short Coinbase. Coinbase provides exchange and platform services for digital assets, and they're growing rapidly into new verticals. They provide services for both retail and institutional investors, and are now even releasing a platform for digital art that will act as a competitor to Opensea. They also own a venture arm named Coinbase Ventures that is invested in many up and coming startups in the field, as well as individual tokens and projects in many cases.\n\nChanos is famously known for his great calls on Enron and Wirecard, but most recently also lost almost half of his fund's capital shorting Tesla. He has been a renowned TSLA bear for quite some time - a decision that has not worked out for him.", "upvote_ratio": 0.9, "id": "t3_thtzk7", "created_utc": 1647692205.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - March 19, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.74, "id": "t3_thr9km", "created_utc": 1647680467.0}
{"sub": "investing", "title": "Canadian Oil Sands: Buried Treasures", "selftext": "https://www.wsj.com/articles/canadian-oil-sands-buried-treasures-11647601381?mod=hp_minor_pos19\n\nDirty, expensive to extract and trapped by a lack of pipelines, Canadian oil sands can be a tough investment proposition. Yet a year of elevated oil prices has turned companies mining them into cash machines.\n\nSoaring energy prices are set to reward almost everyone producing hydrocarbons: Major oil companies and U.S. shale producers reported record free cash flows in 2021 and should do even better this year. Analysts polled by FactSet predict that a subindex of U.S. oil and gas exploration companies in the S&amp;P 500 will beat last year\u2019s bounty by an impressive 35%. Impressive, that is, until compared with Canadian oil sands producers: Suncor Energy, SU -0.16% Canadian Natural Resources, CNQ -0.93% Imperial Oil and Cenovus are set to increase their free cash flow by 60.5% this year, on average.\n\nLonger term, the bull case for carbon-heavy Canadian oil is shakier and will depend in part on a shift to a more nuanced view of environmental, social and governance concerns. Oil sands\u2019 carbon footprint is high, but Russia\u2019s invasion of Ukraine has brought social concerns to the forefront\u2014Western oil majors almost immediately pulled out of Russia\u2014as well as the perils of relying on autocratic regimes for vital commodities.\n\nEnergy investors today are laser-focused on two things these days: Immediate cash returns and ESG alignment. At the moment, Canadian oil companies are ticking the first box. A paradigm shift in ESG could really supercharge their shares.", "upvote_ratio": 0.62, "id": "t3_tho8nf", "created_utc": 1647666849.0}
{"sub": "investing", "title": "Need some help with an unknown stock and investment.", "selftext": "Years ago I invested in a stock. But I don't remember what stock I invested in. I don't remember what website or software I used Is there any possible way to find out that information and what info I need to cash it out. It would have been late 90s or early 2000s Thank you in advance for any information. If you got any questions just ask.", "upvote_ratio": 0.45, "id": "t3_thnzj7", "created_utc": 1647665842.0}
{"sub": "investing", "title": "Investing in Awesome Companies You Don't Like", "selftext": "Hi all, hopefully the title isn't misleading.  \ud83d\ude05\n\nLet me specific: I've never been an Apple product owner.  I don't like the walled garden and I don't like Apple's generally anti-consumer behavior.  For the most part, I can't do my work nor can I play video games on a Mac.  All that said, I absolutely respect Apple's driving force behind many industries.  I'm most impressed with their innovation in silicon, but they push every industry they touch forward.\n\nDoes it make sense for me to invest in Apple, even if I don't particular like them?  I respect them.  They do incredible stuff, but I really dislike the idea of driving the share price (even if by one-millionth of a percent) of a company that wouldn't respect me or consumers' collective dollars.\n\nThoughts on this predicament?  How do you invest in great companies that aren't good for their consumers or have other distasteful business practices, even if they're doing really compelling stuff elsewhere?\n\nMaybe this is the totally wrong perspective and a good investing opportunity, regardless of what goes on behind the scenes, is just that.  Is this a *it's the only game in town* situation?\n\nApple is just an example, but there are many more I can think of.  Silicon and CPUs are really my circle of competence, but maybe I can extend that a little more to technology generally.  Another obvious one is Nvidia, (though, full disclosure, I *do* use Nvidia GPUs both at work and at home.)", "upvote_ratio": 0.75, "id": "t3_thmg48", "created_utc": 1647660111.0}
{"sub": "investing", "title": "which do u think are the best startups", "selftext": "I'm looking to invest in eight different startups, but I'd like to know your thoughts about which companies will likely become successful in the long-run. Advance thanks to those who will provide their suggestions! :)\n\n**1.** **Startup #1**\n\nThis startup company develops an app that matches students who have free time to run little errands with students who don't. In-app cash will be assigned to each errand (similar to debits and credits).\n\n**2. Startup #2**\n\nThe project entails developing an application that gathers information about location, availability, and charges from of the websites of car-sharing businesses (such as Evo, Cars to Go, and Zipcar).\n\n**3.** **Startup #3**\n\nThe project entails developing an application that gathers fashion clothing goods in one place and allows users to compare what they want to what is available. The concept is based on a Spanish application of a similar nature.\n\n**4.** **Startup #4**\n\nThe plan is to open a restaurant where customers will be able to reserve their own booth and choose their own music, atmosphere, and food. There will be no servers, and the food will be delivered via a booth conveyer in the wall. The concept is to provide a one-of-a-kind integrated experience that is tailored to the needs of each individual guest.\n\n**5.** **Startup #5**\n\nThe project involves offering meals to students who do not know/do not want to cook/go to restaurant/have a takeout. The meals will be delivered on an agreed-upon weekly schedule.\n\n**6.** **Startup #6**\n\nThe goal of the project is to develop an app that will pair students with similar interests and lifestyles as housemates (in apartments and houses). The programme will provide extensive matching criteria as well as a roommate rating system.\n\n**7.** **Startup #7**\n\nThe goal of the project is to develop an all-in-one \"smart\" collar for pets (primarily dogs and cats). When warranted, the collar will combine GPS and important health information and warn the owners.\n\n**8.** **Startup #8**\n\nThe goal of the project is to create an information/travel gateway for newcomers/tourists to a certain new city/town/environment.", "upvote_ratio": 0.27, "id": "t3_thivj1", "created_utc": 1647648399.0}
{"sub": "investing", "title": "\"China is not a party to the [Ukraine] crisis, and doesn't want the sanctions to affect China\"", "selftext": "https://www.cnn.com/2022/03/17/business/china-russia-sanctions-friction-intl-hnk/index.html\n\nFears that Chinese companies could face US sanctions over ties with Russia had contributed to an epic sell-off in Chinese stocks recent days. That slump was reversed Wednesday when Beijing promised it would pursue policies to boost its sputtering economy and keep financial markets stable.\n\nAnalysts say that China is attempting to strike \"a delicate balance\" between supporting Russia rhetorically but without further antagonizing the United States.\n\nBeijing and Moscow share a strategic interest in challenging the West. However, Chinese banks cannot afford to lose access to US dollars, and many Chinese industries cannot afford to be deprived of US technology.\n\nWhile China is Russia's No. 1 trading partner, Beijing has other priorities. Trade between the two countries made up just 2% of China's total trade volume. The European Union and the United States have much larger shares, according to Chinese customs statistics from last year.\n\nChina's currency, the yuan, doesn't trade completely freely, moving instead within bands set by officials at the People's Bank of China (PBOC). Last week, they doubled the size of the ruble trading range, allowing the Russian currency to fall faster.\n\nIf China allowed Moscow to convert its yuan reserves into US dollars or euros, \"that would clearly help Russia's current impasse,\" Garc\u00eda-Herrero noted. However, \"the reputational risk of potentially breaching Western sanctions would be a huge step for the PBOC to take and therefore makes it highly unlikely,\" she said.\n\n\"The long-term gains of moving closer to Russia might not match the impact of Western investors suddenly losing interest in China,\" she added.\n\nIn response to CNN's request for comment, China's foreign ministry reiterated Beijing's opposition to sanctions adding that China and Russia will maintain \"normal economic and trade cooperation.\"", "upvote_ratio": 0.94, "id": "t3_thea35", "created_utc": 1647635109.0}
{"sub": "investing", "title": "What\u2019s your introduction to investing mistake? And how did it shape your current investing style? ie. active to passive?", "selftext": "I think we all have a story to be told. Mine\u2019s that I was told to invest but my family doesn\u2019t know anything about investing and use (or get used by) a financial advisor. My advice was to go to TIAA. I opened a lifestyle (basically like a target-date fund). Then I bought FB after the cambridge analytica scandal. I bought 4 more stocks MA, IQ, AMZN and XPO. I also bought a high-yield, junk bond fund. Eventually I sold everything and now go 3-fund portfolio 70/20/10, bogelhead model. I\u2019m happy I learned this lesson at age 35 before it was too late. Anyone else learn the hard way?", "upvote_ratio": 0.93, "id": "t3_thc9az", "created_utc": 1647629542.0}
{"sub": "investing", "title": "Can I/how do I ask my boss to invest in his company?", "selftext": "My boss started his own company a year ago. In January he hired me as his first employee. He plans to hire at least one more person soon, but he has big plans that I see a lot of potential in\n\nI want to offer $5k-$7k, but idk how to go about it, or if it's legal. Like, would working where I invest be a conflict of interest?\n\nIf not, how do I approach my boss? What questions do I ask?\n\nThanks!", "upvote_ratio": 0.63, "id": "t3_th9hgm", "created_utc": 1647626066.0}
{"sub": "investing", "title": "Series I bonds or s&amp;p ETF, which is the better investment right now?", "selftext": "I just recieved my bonus, With the current guaranteed rate of %7.12 in wondering if series I bonds are the better investment right now compared to say, an s&amp;p 500 ETF. I don't know much about series I bonds so I may be missing something that would make an S&amp;P investment better. Any input is appreciated.", "upvote_ratio": 0.91, "id": "t3_th6y0d", "created_utc": 1647623662.0}
{"sub": "investing", "title": "What to do with taxable mutual fund?", "selftext": "Have a high expense ratio edward jones mutual fund in a taxable account. It was started a long time ago so it's almost all gains. If I don't need the money, Do I sell and put it into a roth IRA? Will taxes fuck me? Do I keep holding? Right now I have dividends put into my bank account so I can at least put those into a roth.", "upvote_ratio": 0.82, "id": "t3_th3ok6", "created_utc": 1647616578.0}
{"sub": "investing", "title": "I am a college student who lost about $3k in investing", "selftext": "I need some advice. I had invested in a crypto and some in companies. I invested around 9k total divided in 7 companies and a crypto coin. I am losing some% in all of them. I need some advice on what should I do with the loss. should I go to my bank and ask for some advice if they even do such things. I have not sold anything just my portfolio is blood red. Any advice on how to get those back except just waiting for the day everything is green", "upvote_ratio": 0.29, "id": "t3_th3l27", "created_utc": 1647616312.0}
{"sub": "investing", "title": "I bonds what to expect???", "selftext": " Please forgive me as I am a amateur at investing.  I recently purchased  1k in Ibonds, before the rate changes in April. Any advice would help me  tremendously. What should I expect?  I know about the rules of at least  1 year, and you are not penalized after 5 years. Is the money that you  can make taxed? Is it worth it? Thanks in advance.", "upvote_ratio": 0.67, "id": "t3_tgtaps", "created_utc": 1647578137.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - March 18, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.68, "id": "t3_tgx1we", "created_utc": 1647594069.0}
{"sub": "investing", "title": "If gross profit is 'Revenue - COGS', then why does Yahoo Finance makes it confusing?", "selftext": "When looking at a company's Income Statement through Yahoo Finance, it clearly shows gross profit as revenue subtracted by cost of revenue. But COGS and cost of revenue are two different things. \n\nCost of Revenue includes distribution and marketing expense. And it's also weird how Yahoo Finance's 'Cost of Revenue' doesn't include these expenses, because they are put into the other 'Operating Expense' category.\n\nSo what's the deal? Did Yahoo Finance screw up and actually mean COGS? Or am I misinformed?", "upvote_ratio": 0.69, "id": "t3_tgtkds", "created_utc": 1647579160.0}
{"sub": "investing", "title": "Most indices are up over the past 12 months and past 6 months. Is now the time to \"buy the dip\" do you reckon?", "selftext": "Despite recent volatility, most major indices seem to be [**up over the past 12 months**](https://imgur.com/gallery/AP9UwNP)**.** \n\nThe heaviest losses seem to have been driven by tech / NASDAQ heavyweights, with some major indices [**down over the last 6 months**](https://imgur.com/gallery/R9qBUZz). \n\nThis is most obvious in the February drops - most all major indices [**are down since 2022 started**](https://imgur.com/gallery/HdqxKT0). \n\n&amp;#x200B;\n\nAssuming Russia does not nuke (or gas) Ukraine, do we think we've more or less corrected as much as we're going to?  \n\nIn other words, is there evidence we are near the bottom?", "upvote_ratio": 0.79, "id": "t3_tgtd2m", "created_utc": 1647578384.0}
{"sub": "investing", "title": "Oil Market\u2019s Big Winners: \u2018Little Guys\u2019 Who Are Eager to Drill", "selftext": "Autry Stephens and other small operators race to produce more crude with big players on sidelines; \u2018almost too good to be true\u2019\n\nhttps://www.wsj.com/articles/oil-price-surge-winners-drillers-endeavor-stephens-11647528657?mod=hp_lead_pos5\n\nWith oil prices today gyrating around $100 a barrel, Mr. Stephens\u2019s company, Endeavor Energy Resources, and a few other ***privately held*** U.S. drillers, have emerged as pivotal players in the global energy market. The war in Ukraine and sanctions against Russia have hit supplies, and these smaller operators are among the few racing to produce more crude.\n\n---\n\nJust wanted to also throw this out here...anyone have some names of smaller publicly traded outfits?  Looks like company risk will be minimal in this environment compared to this time last year.", "upvote_ratio": 0.79, "id": "t3_tgq0i3", "created_utc": 1647567462.0}
{"sub": "investing", "title": "Recharacterization and backdoor roth", "selftext": "My spouse and I put in 6000 each into Roth IRA accounts in 2021. However, it turns out our MAGI is above the limit, so we need to recharacterize.  I'm thinking we can recharacterize and then convert to Roth.  Is that correct?  One issue is that I have money in a rollover IRA, about $5000.  I can pay taxes for conversion out of cash.\n\nDoes this make sense to do? Would the following be the correct steps?:\n\n1. File recharacterization form to change the 2021 Roth -&gt; traditional IRA (do I also need to calculate how much what each of the 6000 is now worth since has been invested for the past 10 months or so?, we also have funds from earlier years)\n2. Move this money back into the Roth accounts\n3. For me, actually move ALL of my traditional IRA into the Roth (\\~12,000)\n4. Use the 8606 form to report this and figure out the tax on the \\~1\\`2000 for me and separate 8606 for the 6000(plus growth) for spouse.  Don't need to worry about pro-rata since I would be converting everything.\n\nAm I missing anything? Thank you!", "upvote_ratio": 0.62, "id": "t3_tgouaa", "created_utc": 1647563900.0}
{"sub": "investing", "title": "Russia avoids default. Bond paid in Dollars.", "selftext": "\nhttps://www.reuters.com/business/russian-sovereign-bond-payment-received-by-jpmorgan-processed-source-2022-03-17/\n\nRussian Federation dollar denominated bonds received thier coupon payment today. Referring to the 2023 &amp; 2046 bonds. \n\nDespite capital controls and a decree signed by Putin to temporary pay creditors of hostile nations in Russian currency.", "upvote_ratio": 0.97, "id": "t3_tgkqx0", "created_utc": 1647552327.0}
{"sub": "investing", "title": "Tax basis and Yahoo's adjusted price", "selftext": "I need to calculate a stock basis for some inherited stock, sold several years after date of death.  I know that the basis in this case is FMV calculated as of date of death, usually using the average of open and closing prices.  This is easy to get from Yahoo's historical data.\n\nBut what threw me is that Yahoo also has an adjusted close, factoring in dividends and splits.  I've read Yahoo's explanation about the adjusted basis, but I want confirmation that my analysis is correct. \n\nFrom a tax perspective, only a split affects the calculation.  A 2 for 1 split means halving the original basis.  A dividend doesn't, because dividend is taxed when received.  \n\nWhat this suggests to me is that Yahoo's adjusted close isn't useful for computing the tax basis.  You still need to look for splits (fortunately none in my case), but you can go ahead and use the actual closing price and not Yahoo's adjusted closing price for computing the tax basis?\n\nAgree?  If so, what is the value of the adjusted close?  I assume it has some meaning for investment analysis, but don't know what it is.  \n\nMany thanks.", "upvote_ratio": 0.82, "id": "t3_tgk50h", "created_utc": 1647550726.0}
{"sub": "investing", "title": "Is ammunition a decent investment?", "selftext": "I\u2019ve been into shooting guns since I was young and I\u2019m 24 now and realized my hobby has gotten much more expensive as ammo keeps going up in price. Does anyone buy ammo as a store of value or an investment? I would feel justified in buying thousands of rounds if I knew that whatever I was buying would at least outpace inflation. Other than PMs or ammo, what other tangible assets do you guys hold in your houses?", "upvote_ratio": 0.42, "id": "t3_tgf4i1", "created_utc": 1647537163.0}
{"sub": "investing", "title": "How to buy gazprom on foreign market from the U.S.?", "selftext": "\nI believe it\u2019ll blow back up after this is all over, and at 1 dollar it\u2019s a steal!\n\nBut our free country is not giving us the freedom to buy the stocks we want.\n\nAny way to buy it on a foreign market?\n\nIs it even legal to do this? I got a fidelity account so do they have a way of assisting with this?\n\nIt sounds complex lol\n\nMaybe there\u2019s a way to use crypto to buy stocks or something lol", "upvote_ratio": 0.25, "id": "t3_tged9p", "created_utc": 1647535171.0}
{"sub": "investing", "title": "Why are those with the highest Sharpe ratios most likely to cease trading?", "selftext": "I was reading [this](https://www.sciencedirect.com/science/article/pii/S0377221721007426) paper. Figure 1 suggests that those with the highest Sharpe ratios are most likely to crease trading. I have no background in finance/economics, but this result is unintuitive. Similarly, Tables 3, 4, and 5 suggest that those with profitable Sharpe ratios are more at risk of ceasing their trading, and those with unprofitable Sharpe ratios are most at risk of continuing their trading.", "upvote_ratio": 0.69, "id": "t3_tg7top", "created_utc": 1647515715.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - March 17, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.76, "id": "t3_tg5uzd", "created_utc": 1647507668.0}
{"sub": "investing", "title": "Chinese Stock Regulator Supports Investors w/ Friendly Policies", "selftext": "&gt;[China\u2019s tech rout: stocks regulator joins chorus of care and support for beleaguered investors with market-friendly policies](https://www.scmp.com/business/markets/article/3170791/chinas-tech-rout-stocks-regulator-joins-chorus-care-and-support?module=lead_hero_story&amp;pgtype=homepage)  \n&gt;  \n&gt;China will ***encourage publicly traded companies to buy back*** their shares and money managers to invest in their own funds, offering investor-friendly policies to bolster the world\u2019s second-largest capital market amid an unprecedented rout.  \n&gt;  \n&gt;Most importantly, the vice-premier asked that market-sensitive regulatory policies to be \u201ccoordinated with financial regulatory authorities in advance\u201d to manage market reactions and expectations, according to the [official readout of the meeting](http://www.xinhuanet.com/english/20220316/3bf0096ed0ad4bd98f84b77ca4c7f22a/c.html?module=inline&amp;pgtype=article) by Xinhua News Agency.  \n&gt;  \n&gt;n other areas, the CSRC said it will ***push institutional investors to focus on long-term and value investing***, and cooperate with other departments to defuse the risk in the property market, according to the statement. It will keep the door open for companies to list overseas, and engage its US counterparts on accounting issues, it said.\n\nGiven this, what are your picks for Chinese Companies that are in the best position for buy backs and what Chinese companies would you classify as long-term value?", "upvote_ratio": 0.56, "id": "t3_tg4g7r", "created_utc": 1647501524.0}
{"sub": "investing", "title": "Hey, Working stiff here.\ud83d\ude4b\u200d\u2642\ufe0f I have an important question for those who know more than me.", "selftext": "Is Acorns a real investment or a pyramid scheme? They said if I invite 3 friends they will add $500 to my account. That seems like a pyramid scheme\u2026 \n\nBut, they are SIPC insured\u2026 soooooo\u2026. That says to me they are legit\u2026. Any helpful insight would be appreciated.\n\nEdits for grammar and spelling*", "upvote_ratio": 0.37, "id": "t3_tg2sf1", "created_utc": 1647494620.0}
{"sub": "investing", "title": "Stock picks for Income generation and boosted returns in 2022", "selftext": "With everything going on in the world right now (Russian invasion, supply chain shortages, staggering inflation, interest rate hikes, etc) the markets can seem pretty uncertain. And you might be wondering *what am I gonna do? I'm down 15% YTD, that's basically half of last year's return figures.*\n\nWell I have a solution, or more technically, something that I conceived while studying backtests and options data by people much smarter than me.\n\nWhat's the solution? Selling puts.\n\nI see you rolling your eyes, *this guy thinks he's a genius for discovering CSPs, what a moron.*\n\nSomething you might not have heard before is that CSPs outperform in both bear and stagnant markets. So while in the last couple years CSPs have underperformed relative to growth stocks, *when not if* the market reverses the pendulum will swing back towards CSPs outperforming.\n\n[https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.458.4882&amp;rep=rep1&amp;type=pdf](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.458.4882&amp;rep=rep1&amp;type=pdf)\n\nIf you read the article, you can also note another important thing on page 50. CSPs also realize a much lower standard deviation, in fact, it happens to be **50% lower** when compared to the S&amp;P 500.\n\nSo what does it mean when you match the annualized returns (or sometimes slightly outperform) the overall index while also having a lower standard deviation? We lower the Sharpe ratio! Which makes our portfolio have better risk adjusted returns.\n\nSo now we get to my twist. If you sell CSPs on the index you get better risk adjusted returns, but what if we now want to raise our returns to a point where we have the same risk exposure as the overall market?\n\nWe concentrate our holdings. Rather than holding the entire index we hold less stocks, which increases our standard deviation, but at the same time should also raise our returns. Now I may have lost some of you at this point saying to shy away from indexes, but bear with me a little longer.\n\n*We don't need 500 stocks to be fully diversified.*\n\nAccording to this article where they randomly picked stocks within certain indexes and then calculated the standard deviation based on the number of holdings, *the ideal number of holdings for large caps is between 15 and 26.*\n\n[*https://blogs.cfainstitute.org/investor/2021/05/06/peak-diversification-how-many-stocks-best-diversify-an-equity-portfolio/*](https://blogs.cfainstitute.org/investor/2021/05/06/peak-diversification-how-many-stocks-best-diversify-an-equity-portfolio/)\n\nSo that's the sweet spot, we find high yielding CSPs on blue chip stocks inside already built indexes like the S&amp;P 500 and then pick between 15-26 of them to improve our returns.\n\nConveniently, I've built a handy tool in Python that screens for these options within the indexes (which you can find in my post history [here](https://www.reddit.com/r/options/comments/lzklj8/i_created_an_options_screener_for_csps_and_ccs/)) that uses Robinhood's unofficial API. I more recently rebuilt the whole project again using Python again, but this time with TDAmeritrade's API which is actually made for pulling options data.\n\nSo using that tool, I'm gonna lay out five CSP option plays that I would make for the next month.\n\n**Sell CCL $15P 04/14/2022 for $29 to capture a 1.93% ROI**\n\nThis company has had some bad earnings since the pandemic started, but I think that it still is a good company with a low PB ratio at 1.65. As people return to their normal lives in the coming years they will want to go on cruises again.\n\n**Sell GPS $13P 04/14/2022 for $22 to capture a 1.69% ROI**\n\nThis company also suffered from the pandemic, but is starting to make a recovery. Their PE ratio sits at 21.3 and PB ratio at 2.44.\n\n**Sell FANG $105P 04/14/2022 for $177 to capture a 1.69% ROI**\n\nOil and gas company, pretty self explanatory. War makes oil prices go BRRR. PE ratio: 11.2 PB ratio: 1.58\n\n**Sell KMX $80P 04/14/2022 for $120 to capture a 1.5% ROI**\n\nCarmax is a retailer of used cars. With new cars suffering awful supply chain shortages, most people will be buying from sites like this for awhile. PE ratio: 14.2 PB ratio: 4.47\n\n**Sell F $14P 04/14/2022 for $21 to capture a 1.5% ROI**\n\nFord is coming up with some great new vehicles. Their Mustang Mach-E gave Tesla a run for their money and they won't stop there. Their sales are also way up from where they were. PE ratio: 3.79 PB ratio: 1.76\n\nAnother thing to note, the prices at which I say to sell this options are subject to fluctuation, if you try to sell them on market open tomorrow the prices most likely will have changed. If you want to know if the option still offers a good ROI just take the option contract price and divide it by the strike prices times 100. Then just convert that number to a percentage.\n\n\\*\\*I am not a financial advisor, nor do I have any positions in the stocks that I recommended. If I did have the money I would definitely enter these positions, but sadly I am a broke 17 year old high school kid with an unhealthy obsession with the stock market. Take my recommendations with a grain of salt as you would any stock tips from a stranger on the internet.", "upvote_ratio": 0.45, "id": "t3_tg1n9l", "created_utc": 1647490432.0}
{"sub": "investing", "title": "what entity decides if a dividend is qualified or nonqualified?", "selftext": " I am a US investor and I have an investment in an Israeli corporation, MIND,C.T.I., which pays a handsome dividend. My broker's 1099 says the dividend is nonqualified. I talk to the company and they say the dividend is qualified. I read the IRS rules and it seems to me the dividend is qualified. So my question is after the company declares the dividend, how or who decides the nature of the dividend and how does that information get to the broker? The broker told me that the dividend information comes to them on their \"little green screens\" and they report what they see on the screen.", "upvote_ratio": 0.54, "id": "t3_tg0vyz", "created_utc": 1647487812.0}
{"sub": "investing", "title": "Russia could lose 30% of its oil output within weeks, IEA warns", "selftext": "https://www.cnn.com/2022/03/16/energy/russia-oil-output-opec/index.html\n\nRussia could soon be forced to curtail crude oil production by 30%, subjecting the global economy to the biggest supply crisis in decades \u2014 that is, unless Saudi Arabia and other major energy exporters start pumping more.\n\nThe world's second-largest crude oil exporter could be forced to limit output by 3 million barrels per day in April, the International Energy Agency warned on Wednesday, as major oil companies, trading houses and shipping companies shun its exports and demand in Russia slumps. Russia was pumping about 10 million barrels of crude per day, and exporting about half of that, before it invaded Ukraine.\n\n\"The implications of a potential loss of Russian oil exports to global markets cannot be understated,\" the IEA said in its monthly report. The crisis could bring lasting changes to energy markets, it added.\n\nThe IEA, which monitors energy market trends for the world's richest nations, said that refiners are now scrambling to find alternative supply sources. They could be forced to reduce their activity just as global consumers are hit with higher gasoline prices.\n\nSo far, there's little sign of relief. Saudi Arabia and the United Arab Emirates are the only producers with significant spare capacity. Both countries are part of the 23-member OPEC+ coalition, which also includes Russia. OPEC+ has been increasing its collective output by a modest 400,000 barrels per day in recent months, but often fails to meet its own targets.\n\n\"The long-running inability of the bloc to meet its agreed quotas, mostly due to technical issues and other capacity constraints, has already led to sharp draws in global inventories,\" the IEA said. If major producers do not change course and open the taps wider, global markets will be under supplied in the second and third quarters of 2022, the agency warned.", "upvote_ratio": 0.96, "id": "t3_tg0bxf", "created_utc": 1647485980.0}
{"sub": "investing", "title": "small scale SaaS shotgun approach", "selftext": "when covid hit, i saw smaller energy companies being decimated more than everyone else, maybe there was fear they couldn't make it. i wanted to take a small scale high risk position. i didn't want to research all of them, just take a position in the space. i didn't nothing, most of them have 10x - 20x in the last couple years while bigger players like XOM trippled.\n\ni think something similar may be happening right now with tech: FAANGs may be down 20-30% but a lot of SaaS companies are down more than 50%.\n\ni don't know which ones will do the best, i'm not sure that i have the drive to research them, so my plan:\n\ntake $10k, divide it evenly into names like SNOW, NET, CRM, OKTA, S, RBLX, CRWD, maybe some others.\n\ni'm less interested in ETFs because i'm sure these guys will be bought out over time.", "upvote_ratio": 0.33, "id": "t3_tfzo4o", "created_utc": 1647483834.0}
{"sub": "investing", "title": "Russia says it made a payment to avoid default", "selftext": "&gt;Russia says it has ordered the $117 million in interest payments it owes Wednesday to be sent to investors, attempting to avoid its first international default in more than a century. But it's not out of the woods yet.\n\n&gt;That's because the funds the country used to make the debt payments came from Russia's frozen foreign assets, sanctioned because of its attack on Ukraine \u2014 so it remains unclear whether investors will receive their money.\nAnton Siluanov, Russia's finance minister, told state media Russia Today that the country had made good on its obligations to creditors. But the \"possibility or impossibility of fulfilling our obligations in foreign currency does not depend on us,\" Siluanov said, according to RT, warning that the payment might not go through if the United States disallows it.\n\n&gt;\"We have the money, we made the payment, now the ball is in America's court,\" he said.\nA spokesman for the Treasury said the United States would allow the payments to go through.\n\nhttps://www.cnn.com/2022/03/16/investing/russian-debt-payments/index.html", "upvote_ratio": 0.94, "id": "t3_tfzgsh", "created_utc": 1647483187.0}
{"sub": "investing", "title": "Why is the Federal Reserve so concerned about engineering a \"soft landing?\" Have they ever successfully done so?", "selftext": "I've never read about a successful \"soft landing\" in the past; every recession I've read about seems to have had some sort of \"hard landing\" somewhere. Is such a \"soft landing\" achievable at all? If not, why does the Fed keep acting like a scared turtle with the interest rates?", "upvote_ratio": 0.69, "id": "t3_tfsppf", "created_utc": 1647463554.0}
{"sub": "investing", "title": "Question: OTC Pink foreign ordinaries", "selftext": "https://international.schwab.com/investment-products/adr-stock-and-otc-stock\n\ni still don't quite understand after reading the article. i'm interesting in buying some MediaTek stocks. if i buy it thur OTC PINK (MDTKF), do i actually own the stocks that is trade in Taiwan or no?", "upvote_ratio": 0.5, "id": "t3_tfpbsp", "created_utc": 1647456831.0}
{"sub": "investing", "title": "Federal Reserve approves first interest rate hike in more than three years, sees six more ahead", "selftext": "So its begun we are now lifting off from low interest  rates and it's good to see they have 6 interest rate hikes planned. It seems like Jpow is going of the measured slow approach which imo is the best approach as there is so much uncertainty these days (Russia, COVID) \n\nWill be interesting to see what they do with the balance sheet roll off as that will be needed to be addressed soon as it's at 9 trillion \n\n\nhttps://www.cnbc.com/2022/03/16/federal-reserve-meeting.html", "upvote_ratio": 0.98, "id": "t3_tfoau5", "created_utc": 1647454249.0}
{"sub": "investing", "title": "Does a single lot marked as a wash sale mean my entire position is considered a wash sale?", "selftext": "I have an ESPP, and my company has a dividend. I used to have my account set to automatically reinvest dividends.\n\nLast year, I sold an ESPP lot, then the dividend landed which was immediately reinvested, and the new lot was marked as a wash sale since it was within 30 days of when I sold the ESPP lot.\n\nI am now looking to sell my entire position, which is probably 40+ lots, in a few weeks. This will realize a loss of a few thousand dollars.\n\nDoes the single lot being marked as a wash sale mean the entire sale of all those lots is considered a wash sale? Or just that one single lot?", "upvote_ratio": 0.73, "id": "t3_tfo2b3", "created_utc": 1647453616.0}
{"sub": "investing", "title": "Stock puts and calls investment opportunity", "selftext": "Hi I have an opportunity to invest $100,000 with a friend who does puts and calls. He said that he guarantees a 14% return regardless of market conditions. I\u2019m not familiar with how this would work and I\u2019m asking for input as to whether this is a to good to be true situation.", "upvote_ratio": 0.28, "id": "t3_tfmfzg", "created_utc": 1647449312.0}
{"sub": "investing", "title": "Investors Sue Vanguard After Target Date Funds\u2019 Big Tax Bill", "selftext": "[https://www.barrons.com/advisor/articles/vanguard-target-date-retirement-funds-lawsuit-51647366034?siteid=yhoof2](https://www.barrons.com/advisor/articles/vanguard-target-date-retirement-funds-lawsuit-51647366034?siteid=yhoof2)\n\n&amp;#x200B;\n\n\\&gt; Three investors are suing Vanguard Group for alleged negligence and breach of fiduciary duty, saying that changes the company made to target date retirement funds resulted in \u201cmassive tax bills\u201d for individual investors. \n\n\\&gt; The asset manage ... had two tiers of target date funds, one for individual investors and retirement plans with fewer than $5 million and one for institutional investors with more than $100 million, according to the lawsuit. \n\n\\&gt; In December 2020, Vanguard lowered its minimum for institutional investors to $5 million.\u00a0\n\n\\&gt; That change sparked a sell-off in the retail target funds as smaller retirement plans sold assets in order to shift money into lower cost institutional funds, according to the lawsuit. Vanguard\u2019s retail funds sold as much as 15% of their assets to raise cash to redeem shares, and in doing so realized capital gains which were distributed to the funds\u2019 remaining investors as required by law, according to the lawsuit.\u00a0\n\n\\&gt; Vanguard had other options to avoid this outcome such as lowering the retail fund fees for plans that had at least $5 million invested or merging the two funds together, according to the lawsuit. \n\n\\&gt; Ultimately, Vanguard did the latter, merging the funds together in September 2021. This had no tax consequences for investors, according to the lawsuit. \u201cAt this point, however, the harm was done. Taxable investors had already incurred unnecessary capital gains distributions\u2014and corresponding taxes\u2014that could not be erased.\u201d", "upvote_ratio": 0.93, "id": "t3_tfked2", "created_utc": 1647445023.0}
{"sub": "investing", "title": "People that set up a daily or weekly reoccurring investments, how well has that worked out? And would you recommend it?", "selftext": "I\u2019m debating on setting up a daily reoccurring invest of a small amount into SPY, QQQ, and two more ETFs that I\u2019m still deciding on but I\u2019m wondering if this is the way to go about it or if I should just put in chunk amounts on red days/ when I have the extra money.", "upvote_ratio": 0.83, "id": "t3_tfjni2", "created_utc": 1647442968.0}
{"sub": "investing", "title": "Liquid Investments - Something Physical?", "selftext": "First, let me get this out of the way- I'm maxed out on retirement contributions.\n\nI want something physical to invest in that is liquid.  I can't explain why, but my brain likes something I can feel/touch.  I miss EE paper bonds.  There was something so satisfying about getting that envelope in the mail.  Something reassuring about flipping through them, counting them, admiring the designs. \n\nAny suggestions on something similar?  Something physical, fairly liquid, low risk, low return so I can scratch this itch?", "upvote_ratio": 0.56, "id": "t3_tf50c6", "created_utc": 1647391284.0}
{"sub": "investing", "title": "Do you think it\u2019s possible to use technical analysis on housing market?", "selftext": "As with stocks, we use TA for that. People use TA on SPY or ES to predict the price movements. Do you think it\u2019s possible to use TA on real estate market? Even if it\u2019s for a specific location? Maybe trying to apply MACD or RSI or Stochastic oscillator to do it", "upvote_ratio": 0.07, "id": "t3_tfg8om", "created_utc": 1647432734.0}
{"sub": "investing", "title": "Diversification of Portfolio", "selftext": "So I\u2019m just beginning investing and I was looking for ways to diversify the portfolio. Right now I\u2019m using Schwab and am in a few mutual funds, but I wanted to see what this forum had to offer. There\u2019s so many options out there that it can be very overwhelming to choose. I am young so I can take risk.", "upvote_ratio": 0.64, "id": "t3_tfesnv", "created_utc": 1647427542.0}
{"sub": "investing", "title": "Bloomberg: China vows to keep markets stable and support foreign listings (ADRs)", "selftext": "https://ca.finance.yahoo.com/news/china-vows-keep-markets-stable-053034834.html\n\nThe government should \u201cactively introduce policies that benefit markets,\u201d according to a meeting of **China\u2019s top financial policy committee led by Vice Premier Liu He**, the country\u2019s top economic official. That vow to take investors interests into account comes after a sell-off in domestic shares due to fears over growth risks and tough regulation of real estate and internet companies.\n\nThe meeting offered investors re-assurance that a sweeping crackdown on internet companies was nearing its end and that the government would prevent a disorderly collapse in the property market. China\u2019s banking regulator said after the meeting that it would support insurance companies to increase investment in stock markets.\n\nStocks surged after the announcements. **The Hang Seng China Enterprises Index jumped 13% at the close in Hong Kong, the most since 2008**, recouping nearly half of this year\u2019s losses. The CSI 300 Index of mainland shares climbed 4.3%.", "upvote_ratio": 0.93, "id": "t3_tfeh2q", "created_utc": 1647426279.0}
{"sub": "investing", "title": "Preferred Stock HCDIP, 14%+", "selftext": "Thinking of picking up some shares of this preferred with 14%+ yield and potential for capital gains of 30%. Financials seem to be on the uptick. \n\nAny thoughts?\n\n\n\nPensando en adquirir algunas acciones de esta preferencial con un rendimiento superior al 14 % y un potencial de ganancias de capital del 30 %. Las finanzas parecen estar en alza.\n\n\u00bfAlguna idea?", "upvote_ratio": 0.38, "id": "t3_tfedif", "created_utc": 1647425895.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - March 16, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.86, "id": "t3_tfd9bb", "created_utc": 1647421269.0}
{"sub": "investing", "title": "Wash sales and selling ITM Covered Calls?", "selftext": "If I buy 100 shares of a stock and immediately sell an ITM call with enough premium to To cover the share loss with a profit, does the lost value of the shares trigger a wash sale against the net gain of the premium I sold with the ITM Call?\n\nExample:\n\nBuy 100 shares of a stock $100 stock for $10,000\n\nSell ITM CC for $95 for the end of next week and take a premium of $750 (and allow it to exercise)\n\nIn this scenario you lose $500 to shares but net a $250 profit.\n\nDoes it incur a wash sale?\n\nThanks in advance.", "upvote_ratio": 0.71, "id": "t3_tfbeps", "created_utc": 1647413263.0}
{"sub": "investing", "title": "rates vs quantitative easing", "selftext": "There is a lot of talk about raising interest rates, but little talk of unwinding quantitative easing. Why wouldn't the fed fully unwind quantitative easing first before raising rates?\n\nRaising rates now but waiting until summer to even start unwinding quantitative easing is obviously intentional.\n\nHistory says that quantitative easing was implemented because rates were already close to zero, and yet the fed still wanted to do more, so why are they keeping quantitative easing while at the same time that they are raising rates?", "upvote_ratio": 0.86, "id": "t3_tfaznw", "created_utc": 1647411515.0}
{"sub": "investing", "title": "International or Emerging Market funds that aren't heavy in China?", "selftext": "When I look at a lot of these international funds, I see a bunch of Chinese stocks. We're talking like 30% in China.\n\nGiven how unpredictable the Chinese government is, that's a lot of risk. Are there any good international or Emerging Market funds that are diversified but also not heavy in China?", "upvote_ratio": 0.7, "id": "t3_tf976j", "created_utc": 1647404673.0}
{"sub": "investing", "title": "How long does it take for rollover money to appear in my Vanguard IRA now that my rollover checks are in the mail to them?", "selftext": "ADP mailed me the rollover checks. I did the pre setup rollover paperwork for Vanguard to know it's coming. I mailed the checks a day ago. It will probably take take 3 or 4 days to reach them in Texas. Once they receive the checks, how long before I see my money in my account?\n\n\nEdit: So I mailed them from NY at 3 pm. on Monday, and it's now Thursday morning, and all the funds are in there. That was quick.", "upvote_ratio": 0.65, "id": "t3_tf8y41", "created_utc": 1647403786.0}
{"sub": "investing", "title": "Seeing a lot of talk about an incoming recession (feels legit this time) best investment opportunities?", "selftext": "I know people preach the end of days perpetually, but given everything that\u2019s going on I think a recession is incoming. I\u2019ve never been an adult during a recession but early days of COVID had investing opportunities that I failed to see. Im not going to miss what\u2019s next. Recessions suck, but for an investor it\u2019s an opportunity.\n\nWith that being said I\u2019m on the lookout for good investment opportunities. Not buying a house while things are peaking. \n\nFor other investments im looking at electric freight like hyliion and some others.\n\nWhat are you gonna jump on in the case of a recession?", "upvote_ratio": 0.79, "id": "t3_tf76z7", "created_utc": 1647398025.0}
{"sub": "investing", "title": "6 unit NYC rent controlled deal structure. give me feedback please!", "selftext": "There's a 6 unit rent controlled property in Brooklyn I found for $1.6M. Here are my thoughts on a plan:\n\n1) equity loan on my home for 20% down (it would be less than the total value of my home) \n2) conventional loan for the remaining 80%\n3) pay both loans in tandem (unless there's a way to only pay 1 at a time?) through cash I have saved up for a year plus tenant rent until I can refinance into 1 loan. Ideally during that 1 year I will do a value add and cash out refi for BRRRR to repeat this again. \n\nRent control in NYC is strict and I think that's why it's only $1.6m for a 6 unit. You may say I'm better off taking that amount and using it outside the city, but I live in the nyc and it's a unit in an amazing neighborhood with a lot of long term growth potential. $1.6m for 6 units is not super common here at least from my searching over the last few years. \n\nPlease let me know if you think it's a bad idea to get involved with rent control tenants, having two loans in tandem, starting my first rental property at a 6 unit $1.5m spot, using home equity loan, or any other issues you may see with this plan. Looking to figure out all that may go wrong to find solutions up front. I do not have pre approval yet but my wife and I make a comfortable living between us with good credit scores that I am not so worried about the approval.", "upvote_ratio": 0.35, "id": "t3_tf653k", "created_utc": 1647394757.0}
{"sub": "investing", "title": "Thoughts on UK housebuilders ?", "selftext": "There's a shortage of houses in the UK, they're very expensive. Government is slow to act because of changes to planning laws and NIMBYism. But I believe that something has to give here and we will start building much more in the coming years.\n\nI'm looking at Barratt Developments (BDEV) and Persimmon (PSN)\n\nThey have sensible PE ratios, BDEV has a 6% dividend and PSN is close to 11%.\n\nAny thoughts ?", "upvote_ratio": 0.7, "id": "t3_tf28ab", "created_utc": 1647383392.0}
{"sub": "investing", "title": "Thoughts on EPAM (Epam Systems) stock?", "selftext": "(**tldr** \\- excellent tech company with excellent financials + lots of Eastern European offices crashed, is now picking up steam again. Is this a golden opportunity to buy them cheap, or should I stay away?)\n\nEPAM Systems is an American tech company that specializes in product development, digital platform engineering, and digital product design. Their financials look pretty solid:\n\n* Revenue and net income growing year over year\n* Total liabilities only \\~29% the value of their total assets\n* A good ROE\n\nThere's just one problem: they have (had) a lot of offices in Eastern Europe, and predictably, **their stock lost about half of its value when Russia invaded Ukraine**. [They have joined the sanctions against Russia,](https://seekingalpha.com/news/3809843-epam-systems-rallies-behind-employees-caught-in-the-deeply-personal-ukrainian-war) and it's not clear when (or if) they'll get their Ukrainian workforce back.\n\nBut in the past 5 days, their stock has regained a lot (+25%). Don't forget that they are still an American company, and they have also announced \"business continuity plans\" (i.e. hiring a bunch of people elsewhere in the world).\n\nDo they have a chance of continuing to rebound? Is this a great chance to snatch up an excellent company/stock at a low price? Or are global companies with their hands in Eastern Europe basically done (for the foreseeable future), and should be avoided at all costs for now?\n\nWould love people's input on this.", "upvote_ratio": 0.75, "id": "t3_teoxt4", "created_utc": 1647350688.0}
{"sub": "investing", "title": "Need help with brokerage reports", "selftext": "Any way to access high quality research reports by the likes of JP Morgan, CS , Goldman , Morgan Stanley, Jefferies? \n\nI\u2019m an Indian investor..Have an account with IBKR ..and Jpmorgan morgan stanley etc don\u2019t have services for non us residents.\n\nOr any other resources to read about companies and do deep dive?\nLike Substack or telegram channels or websites that offer for a price?\n\nThanks!", "upvote_ratio": 0.67, "id": "t3_texaxp", "created_utc": 1647371621.0}
{"sub": "investing", "title": "Is this a good time to change brokerage?", "selftext": "So as I understand it, when you switch brokerage, going from Fidelity to Vanguard, for example, all your shares get sold and the new brokerage buys new shares for the cash out amount. \n\nDoing this when the market is high is a detriment, correct? Meaning, selling high then buying back in high, equals less shares. So my thought is, switch now when the markets low to be able to buy more shares at the lower price due to current market rates. \n\nI'm referring to strictly index funds and remaining in the same funds after the switch. My current brokerage charges higher fees and doesn't have the index the new brokerage offers. \n\nIs my thinking completely flawed?", "upvote_ratio": 0.29, "id": "t3_tewxsw", "created_utc": 1647370811.0}
{"sub": "investing", "title": "What are the chances S&amp;P500 index sees 3800 this year?", "selftext": "Sorry if this post is stupid. I know macroeconomic predictions are stupid but I also feel that this sort of falls closer to \"long term investing\" than it does \"daytrading/swing trading/momentum trading/technical analysis/guessing when + where the market is going at a micro level\".\n\nWe opened the year 4796 and we're at 4242, a 554 point/11.5% drop\n\nTo get to 3800 would be another 442 points downwards, which is 10% from where we are now (would be 20.7% overall)\n\nWe all knew that with the fed raising interest rates, stock valuations were due to come down. More expensive debt (if the company is using leverage/debt to grow, which most are?) = more revenue/income/cash flow spent on servicing debt = less profits posted each quarter = lower EPS. According to https://www.cmegroup.com/trading/interest-rates/countdown-to-fomc.html, we will most likely see interest rates be within the range of 1.5% - 2.25% by December 2022. \n\nOil seems to have come back down almost to pre-Russia/Ukraine war numbers. Light crude oil futures (CL1!) were at $92/barrel on February 24th. Currently sitting around $98 (down from their highs of $123). About a 6.5% increase. Granted, $92 wasn't a great price either, with oil mainly trading between $65-$85 for the year prior. If oil stays high... that's obviously an issue since it is responsible for so much of how the global economy works. \n\nRising interest rates, high oil prices eating into consumer spending power, inflation as a whole eating into consumer spending power, geopolitical conflict in Ukraine/Russia, fed slowly tapering the amount of bond/MBS purchasing they have been doing to remove the extra liquidity + \"artificial propping up of value\" from those markets, residential real estate home supply crunch keeping most regular people away from being able to purchase a home, therefore increasing demand for rent, increasing rental prices as well.\n\nIf this subreddit allowed polls, I'd do a poll. What are the chances the markets hit 3800? A geopolitical war in Ukraine/Russia got us 11%... my thinking is... that's it? Is the worst of it behind us? Will the markets keep trending/ticking higher like they always have?", "upvote_ratio": 0.54, "id": "t3_teuhlf", "created_utc": 1647365438.0}
{"sub": "investing", "title": "I am starting to think the funds in my investment accounts is Monopoly Money", "selftext": "With all the things going on nowadays it is amazing the stock market has held up as well as it has but now I am resigned to a black swan to wipe out most of my stock market money in the next year.\n\nThe only way I stay sane about the potential for a HUGE stock market drop is to see it as  Monopoly Money.  I could lose it all or maybe see huge gains if the craziness improves.\n\nI can survive on my Social Security and cash on hand for the next ten years if there is a black swan.\n\nTo put all my eggs in one basket and expect a good chance that money in the stock market is going to pay my bills for the next thirty years because historically the stock market always recovered is a fool's game.", "upvote_ratio": 0.3, "id": "t3_terpqp", "created_utc": 1647358483.0}
{"sub": "investing", "title": "Hedge against potential recession / stagflation. DD.", "selftext": "Background: Well, we all know this but the recent economic data is mixed at best. Although it seems that the economy is booming, there's also a lot of risk factors. Moreover, stock market performance isn't always tightly correlated to the market i.e. USA economy grew quite a bit from 2000-2010 but looking at the stock market you wouldn't exactly guess that. Moreover the [schiller ratio is high](https://www.google.com/search?q=themotleyfool+schiller+ratio&amp;rlz=1C1GCEU_enAE883AE884&amp;oq=themotleyfool+schiller+ratio&amp;aqs=chrome..69i57.8144j0j7&amp;sourceid=chrome&amp;ie=UTF-8) even with the current bear market/correction. Volatile energy markets present a difficult challenge, and a chance to further shake up, already damaged due to the pandemic but slowly recovering, global supply chains. \n\nAssumptions: Although history doesn't repeat itself, it does rhyme. This recession won't be very different from the ones experienced in the past. So let's assume that this recession won't be different from the previous ones. Meaning that in the long term, DCA will be a good strategy. Buying the dip will be worth it. And that spare cash / cash flow at the lowest points of the market will be amazing long term.\n\nRisk factor of the assumptions: Just to play the devil's advocate, the rise of China and the shrinking economic pie of the USA is, at least in my opinion, why this recession might be different to the previous ones. This recession, if significant, can change the world order. Now, not to be dramatic, but world orders do change , previously mostly because of wars (like when USA became the hegemon after the disastrous world wars in Europe), but this time the Pandemic, the war in Ukraine (which can lead a lot of countries like Russia to start using Chinese swift or currency as the reserve currency) can be deciding factors. Is this certain? No, but it is a risk factor in the assumptions. That doesn't mean that the US stock market won't recover, but that the insane explosion of the stock market that USA experienced in the 20th century, in large parts funded by the USD being the reserve currency for the rest of the world as the result of Bretton-Woods agreement, might be enjoyed by China this time.\n\nStrategies:\n\nBelow are my 4 strategies. Main goal is DCA through DRIP and contributions. I generally aim to strike a balance between them all, but I still have a lot of growth stocks left, but I'll try to rebalance my portfolio into roughly equal parts with the new cash flow.\n\nA. [The famous quad-fecta covered call income portfolio](https://www.reddit.com/r/qyldgang/comments/ncp0bl/quadfecta_covered_call_income_portfolio_analysis/). I do not see the the point of writing too much about this because there's some amazing DD done on the original post. I'll just add this as there seems to be some confusion about this, in my understanding unlike dividend stocks that give you dividends per share, covered call ETFs give you dividends as the percentage of the fund, therefore during the recession - the lower the NAV of the fund, the lower is the dividend. However, having a steady stream of monthly cash from dividends will help you catch the bottom\n\nB. Dividend Stocks &amp; ETFs\n\nNOBL-  is the ETF tracking the so-called Dividend Aristocrats, basically the companies that have increased their dividends at least once per year for at least 25 consecutive years. Unfortunately dividends are at an all-time low right now simply because the P/E ratios are so high. NOBL yields about 2.5% per year. The point of this fund is to have the same income per share no matter what market conditions. Companies like Coca Cola (KO) have held their dividend steady for at least 60 years iirc, so if they could weather previous market downturns, they can weather this too. \n\nTROW - T. Rowe Price a large mutli-services financial services company with $1.5 trillion AUM. They're dividend aristocrat with a long and storied history. Their P/E is at about 10 right now, which is below the liked of Black Rock (18x) Charles Schwabb (30x) State Street (11.6x). They also yield the highest dividend of the ones listed above, about 3.5% right now. They're also down about 35% from the highs. I've searched far and wide to see if they have some sort of massive problems that are dragging down their shares, but the conclusion I've come is that they are quite invested into growth companies that have been in recession which is dragging them down, however their financials and their future is not in any jeopardy at this point and they look very stable and close to value territory.\n\nABBV - Abbvie, a health pharma company that specializes in branded drugs. Spun off from Abbot Labs in 2012. Some count them as a dividend aristocrat by counting their shared past with Abbot. They actually have an amazing and growing financial sheets. YTD while the market was in correction territory they grew 14%, and the best part? They still yield above 3% per year. Pharma is an inelastic good, although people might prefer to use generic drug during an economic downturn. Also important to note that they're expensive with a 23x P/E ratio. That being said, their 25% net profit margin that's still growing and the 3.5% yield is enough to sway me.\n\nBTI - British American Tobacco company. Currently yielding above 7% per year. Nicotine is quite an inelastic good that is generally not affected much by recessions, if at all. Of course, there are also moral aspects to consider, but this post is not about that. BTI is heavily involved into smokeless nicotine products like VUSE, which are rapidly growing, and they're a very profitable company (about 26.6% net profit margin)\n\nSCHD - A value fund from Charles-Schwabb. Pays monthly dividend, in total about 3.5% pa\n\nA few other ideas: Not invested yet, O (realty Income), Shell, SPHD, SBUX, HD\n\nC. Growth stocks:\n\nA lot so-called hype companies are more than just hype. Right now might not be the best to invest into them, but I think a lot of them have very high potential and be worth a lot more in the future, therefore I'm buying the dip. Here's the list of the ones I have:\n\nFUBO, NIO, PLTR, PSYK, SGHC\n\nDD on these companies are popular on a lot of reddit forums therefore I will not write much about them\n\nD. Global investments:\n\nAlthough not invested yet, this will be my next move.\n\nJGGI -  JpMorgan Global Growth &amp; income PLC. I like everything about this fund. Large name association, Global diversified portfolio, low P/E (5x), high dividend (3.5%), stable NAV Growth (44% in 5 years). \n\nACWU - MSCI iShares fund that invests mostly in EAFE stocks without USA. 2.9% yield + stable growth 46% in 5 years.\n\nI'll be researching more into Global Markets to diversify the risk, however would be happy to hear get some recommendations on Global Markets and anything else I mentioned in my post. Hope it's been useful.", "upvote_ratio": 0.71, "id": "t3_teql13", "created_utc": 1647355428.0}
{"sub": "investing", "title": "Question about the Efficient Market Hypothesis: You can't outperform the market, but... which market?", "selftext": "An investor who went to Wharton overheard me talking about finance at a diner the other day. He told me you can't outperform the market by stock-picking. My question is: Can you \"market-pick\"? The Japanese stock market sucks, for example, compared to the US stock market. The cryptocurrency market probably outperforms everything else by a long shot, too. Does he mean you can't crypto-currency pick and out-perform the crypto market or precious-metal pick and outperform the precious metals market? My thinking is that each market differs in volatility, and the reward goes up with the volatility, and there is a smooth balance between risk and volatility such that you can choose a market with more volatility and reliably get more reward if you invest for long enough. Does this make sense?", "upvote_ratio": 0.8, "id": "t3_teqgzf", "created_utc": 1647355114.0}
{"sub": "investing", "title": "Saudi arabia is apparently pricing some oil sales in yuan instead of dollar", "selftext": "[https://www.bloomberg.com/news/articles/2022-03-15/yuan-surges-after-report-on-saudis-accepting-currency-for-oil](https://www.bloomberg.com/news/articles/2022-03-15/yuan-surges-after-report-on-saudis-accepting-currency-for-oil)\n\n&amp;#x200B;\n\nThis could be the beginning of the end of the petro dollar", "upvote_ratio": 0.85, "id": "t3_teq7kt", "created_utc": 1647354373.0}
{"sub": "investing", "title": "My Strategy is Keep Buying", "selftext": "My sympathies are with anyone who is retiring this year or next, it must be a very stressful time for you. If you don't need to cash out though what's your strategy? Mine is just keeping buying. I think we are in an accumulation phase not to be missed and that new all time highs in the 20s are all but guaranteed. #BABA #QQQ", "upvote_ratio": 0.83, "id": "t3_tepy0n", "created_utc": 1647353622.0}
{"sub": "investing", "title": "What are your possible multibagger stocks to play the possible rebound in the next few years?", "selftext": "Out of adversity comes opportunity. While most active investors that weren't invested in Commodities or Real Estate are taking a beating, bears are laughing all the way to the bank, index investors and stock pickers are eyeing the stock market for opportunities. I am one of those, and while I somewhat disagree with the idea of index investing, I have something in common with them: the idea that the market, save for a few points in time, will always go up. Now, I'm not advocating for timing the market (well, I mean, kinda), nor am I calling the bottom. I'm suggesting that the current downward trend of the market will end at some point, and will be prime time to buy amazing companies at a discount. Some of the exact same type of companies that are being punished today will be the heroes of tomorrow. So for those of you who haven't relented to index investing and still dream of multibagger gains, what companies are on your radar that you think right now are multibagger opportunities (and will only become more attractive as markets keep going down).", "upvote_ratio": 0.46, "id": "t3_teps3s", "created_utc": 1647353168.0}
{"sub": "investing", "title": "Oil output expected to rise in April. Thoughts on the impact to the market?", "selftext": "Do you think this will be a pass through savings or still impact market pricing?\n\n&amp;#x200B;\n\n[Oil Price Article:](https://oilprice.com/Latest-Energy-News/World-News/EIA-US-Shale-Production-Set-For-Big-Jump-In-April.html)\n\n*For April, the EIA now sees U.S. crude in the Permian rising from 5.138 million bpd to 5.208 million bpd\u2014a 70,000 bpd rise. The Eagle Ford is expected to see the second largest increase with a 23,000 bpd rise to 1.146 million bpd. The Bakken is expected to increase by 16,000 bpd.*\n\n*The EIA has estimated that the number of Drilled but Uncompleted wells (DUCs) fell 156 to 4,372 for the month of February. The DUC count\u2014or fracklog\u2014is often seen as a bellwether for the state of the oil industry. Higher DUC counts often signal that oil companies are comfortable spending money on wells that are unfinished and no producing. The DUC count in the United States has been falling since mid-2020.*", "upvote_ratio": 0.66, "id": "t3_tepcyj", "created_utc": 1647351938.0}
{"sub": "investing", "title": "What do you think of this plunge of the Chinese giant?", "selftext": "BABA has lost $600 billion in market capitalization.The Chinese company has lost 70% of its stock market value since October 2020. A setback following the release of exceptionally bad financial results, including a more than 50% decline in profits.\n\n[https://imgur.com/tWiDB6j](https://imgur.com/tWiDB6j)\n\n[https://imgur.com/CcsMgxF](https://imgur.com/CcsMgxF)", "upvote_ratio": 0.94, "id": "t3_teopi0", "created_utc": 1647349990.0}
{"sub": "investing", "title": "People with bond heavy portfolios - are you buying more with every rebalance/paycheque?", "selftext": "I recently cashed out a large portion of my home equity to increase my pace of investing. I decided to hedge my tech heavy portfolio with bonds (US LTT and US LT TIPS). I also decided to add more bonds with every paycheque to slightly reduce the correlation of my portfolio with the Nasdaq. Unfortunately, that backfired spectacularly and I'm seeing near six figure losses YTD as bonds and stocks became heavily correlated, which I found strange considering that just about anyone on even Reddit, let alone professionals knew that the Fed was going to greatly increase interest rates in 2022. In addition, I figured that the US bond market would be a flight to safety that is sheltered heavily against the war in Ukraine. \n\nFor those of you heavy in bonds, are you still adding with each paycheque/rebalance, or are you sitting this one out?", "upvote_ratio": 0.67, "id": "t3_tenue1", "created_utc": 1647347317.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - March 15, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.78, "id": "t3_teko43", "created_utc": 1647334869.0}
{"sub": "investing", "title": "Best way to compare fund performance before 2020?", "selftext": "I am actively researching the best funds to invest in for the long term of 20+ years. But it is bad enough that we've been in a 10 year bull market and it is long overdue for a correction. Then came 2020 and everything in the market seems to have shot up only exacerbating the long term scenario. So I would rather compare funds that don't involve the massive run of the last 2 years. I would prefer to be able to compare using annualized return parameters such as between 2001 - 2008 or 2010 - 2019. Those were recovery periods after crashes that were more representative of what a fund might perform like in the next 20-30 years. Plenty of research sites give you 5 or 10 year returns, but those include the anomalous years like the last 2.\n\nAnd yes, I know that I can't discount crashes like in 2008 if planning for the long term future as they will inevitably happen again. I'm just wanting to see which funds are better performing on the positive side during more moderate years.\n\nIs this possible to get a list of funds to compare annualized performance during specific ranges? Or do I have to plug in annual returns into an excel spreadsheet and run the math myself? TIA", "upvote_ratio": 0.36, "id": "t3_teex03", "created_utc": 1647312694.0}
{"sub": "investing", "title": "Is it a good plan to fund your Roth IRA extra with dividends inside of it?", "selftext": "I've seen people say they have like 10k in Reits like O or other quality paying dividend funds in their Roth IRA so that they can use those dividends to buy other funds like VTI. They do this because they max out the 6k limit and want to be able to invest more. Does this work or are you better off just using the 10k to purchase VTI from the start?", "upvote_ratio": 0.83, "id": "t3_tecii8", "created_utc": 1647305439.0}
{"sub": "investing", "title": "My 13 year old daughter wants a job and I want her to help me analyze/track stocks", "selftext": "So, she's starting to be a full blown adolescent, going out with friends to movies and the mall, and wants to have more money than just an allowance. She's very smart, and is driven. She's been trying to make some odd jobs around the house for herself. Today, she asked me to buy a lawn mower so she can mow the lawn and make money. I was in the process of reading some analisys on a stock, and came up with the idea of having her do some basic data entry on a spreadsheet/track some info on stocks for me. She was thrilled! she immediately started asking questions. I told her we can work something out, and of course now I'm here for advice:  \n\n\nWhat would you do with a very basic clerk/intern to help you with your investing?  \n\n\nhere's some of the stuff I came up with:  \n\n\n\\- I want to give her tickers and have her record on a spreadsheet things like P/E, P/S, earnings growth, sector, etc. maybe update it once a week or once a month? what would you track? I have morningstar premium, and I can have her record their fair value too. Tipranks price targets?  \n\\- Use her as a basic alert system, like texting her \"set alert for MSFT at 250\" and have her notify me  \n\\- I wouldn't want her reading up news, or reddit posts, since that can be too much for a 13 year old.  \n\\- Some of the harder stuff to find online, maybe? like insider selling/buying? would that be easy enough for a 13 year old to find and record? maybe she can just look it up and screenshot it for me?  \n\n\nI bet I'm not the first one trying to do this, what are other's experiences with this?", "upvote_ratio": 0.67, "id": "t3_teaaik", "created_utc": 1647298878.0}
{"sub": "investing", "title": "The housing market crashed due to sub prime mortgage back in 2008. If the housing market is crashing again, what could the reasons be this time?", "selftext": "We could already be in a crash, and if the housing market is ever going to crash, what could the reasons be this time? A lot of people are still moving their money to a safer place, the real estate. It doesn't seem the demand has decreased. Are we ever going to see a crash in the housing market?", "upvote_ratio": 0.85, "id": "t3_te6g7g", "created_utc": 1647288323.0}
{"sub": "investing", "title": "Are IG accounts (digital assets) better than real estate?", "selftext": "Think about it. I could buy a home for 50k down probably rent it for 2k-3 a month. BUT I could also buy an active ig account meme/media page with 100k-1 million followers  for maybe 10-50k and make 10 grand a month from sponsors and brand deals on the account. What do you guys think?", "upvote_ratio": 0.19, "id": "t3_te69xy", "created_utc": 1647287889.0}
{"sub": "investing", "title": "Why not use Leverage ETFs over a long term horizon?", "selftext": "Dollar cost averaging SPY or QQQ or VTI is considered prudent investing over long term (eg 20 years). My question is, that if stock market is expected to grow over 20 year timeframes, does it not make sense to put money in a Leveraged SPY (SPUU?) or TQQQ like funds to make your money grow faster?\nSure crashes will be harder, but we are talking about long term investing over 15-20 years?\nAny long term leveraged index funds which are recommended?", "upvote_ratio": 0.71, "id": "t3_te5peg", "created_utc": 1647286369.0}
{"sub": "investing", "title": "10 year treasury yield up to 2.13% - yield curve now + 0.65 bps", "selftext": "Crazy move in bond yields today as stocks and bonds retreat lower.  Fed expected to raise rates this week to .25bps as the fed funds rate.  Nasdaq is now down -22% since its high and officially in a bear market.  Since 2018, it is pretty evident any tightening of monetary policy will send markets immediately into a correction when there are these valuations.  However some stuff out there is looking pretty great . \n\nI bet big boy Warren is deploying cash left and right.", "upvote_ratio": 0.95, "id": "t3_te4da8", "created_utc": 1647282888.0}
{"sub": "investing", "title": "Can anyone help me understand ibonds?", "selftext": "I've seen some talk of these ibonds lately and don't know if I quite understand them. Any information anyone can share on them would be helpful. \n\nMy main consideration is that I'm in the phase of my life where I'm saving for a house. I have a fair amount of money sitting in a savings account and I keep adding to it each week. Is it more worth it to keep majority of that money in a brokerage account, buying ibonds? I won't be buying a house for 2-3 years and am worried about my money losing its value during that time.\n\nIs this a safe bet? Or am I looking at this wrong?", "upvote_ratio": 0.75, "id": "t3_te0pus", "created_utc": 1647273456.0}
{"sub": "investing", "title": "Domestically produced sunflower oil", "selftext": "As I\u2019m sure many know by now Russia and Ukraine are the 2 largest producers of sunflower oil and the current conflict means we can\u2019t import sunflower oil from there.\n\nI was wondering if anyone is looking into investing into domestically produced sunflower oil companies or companies producing alternative oils, and what companies there are that do this.", "upvote_ratio": 0.68, "id": "t3_tdzqmr", "created_utc": 1647270792.0}
{"sub": "investing", "title": "Stagflation in the 1970s &amp; Stagflation in the in the 2020's.", "selftext": " When people think of the U.S. economy in the 1970s, many things come to mind:\n\n\\- High oil prices\n\n\\- Inflation\n\n\\- Unemployment\n\n\\- Recession\n\nA question for baby boomer investors and historians who have studied the US or European stock market during the 70's. \n\na) Which sectors of the economy performed well and why do you think this was so? \n\nb) Can you recall any specific companies that thrived in this environment, and if so what were the forces that enabled them to outperform?  \n\nc) Do you recall if there were any windfall taxes on companies that outperformed in the 1970's? \n\nd) If we have a stagflation and a recession in the later 2020's, which sectors of the economy do you think will out perform and why? For example, can you draw any parallels between what worked in the past and what might work in the future.", "upvote_ratio": 0.79, "id": "t3_tdvcb2", "created_utc": 1647257332.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - March 14, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.78, "id": "t3_tdt3km", "created_utc": 1647248470.0}
{"sub": "investing", "title": "Does Mid-Cap Value have a historical premium?", "selftext": "I've been doing quite a bit of reading about Value premium most prevelant on Small Caps. I decided to tilt my portfolio to small Value following more closely to Ben Felix's 5 factor Portfolio.\n\nIn my readings, I have seen mixed messages about the tilt on Mid Caps. Often times it is implied that Value overall historically performs better across all Caps. Sometimes it seems like Small Caps and Mid Caps are sort of lumped together. And of course, often times it seems like Small Cap Value is the sole focus.\n\n* Is it worth tilting towards Mid-Cap Value in a Value strategy?\n* Are there any noteworthy Mid-Cap Value ETFs to look out for?", "upvote_ratio": 0.65, "id": "t3_tds3r6", "created_utc": 1647244160.0}
{"sub": "investing", "title": "Oil Markets and Geopolitics", "selftext": "As the largest oil companies in the world are government-owned, oil is very political.  This war in Ukraine has obviously spooked energy markets, and that has had spillover effects into the stock market.  John Mearsheimer, who has been correct in most strategic matters involving use of force, [particularly pertaining to Ukraine](https://www.reddit.com/r/NeutralPolitics/comments/t54rlm/what_was_the_stated_reason_for_nato_expansion/hz3jd8z/), believes that Russia is serious about NATO posing an existential threat to itself, and thus will go to extreme lengths to ameliorate this threat, making comparisons to US firebombing campaigns over Japan which caused hundreds of thousands of civilian casualties in WWII.  [He believes Russia will not stop until Ukraine is reduced to rubble.](https://www.youtube.com/watch?v=ppD_bhWODDc&amp;t=1434s)\n\nIf the above prediction comes to pass, it will likely have consequences that will take several years if not decades to play out.  As Russia is a major hydrocarbon producer, this will result in a significant supply disruption to most NATO countries for the foreseeable future, which will likely lead to elevated oil prices in the foreseeable future.  \n\n[The WSJ also believes there may be spillover effects related to America's security guarantee around the world](https://www.wsj.com/articles/how-ukraine-was-betrayed-in-budapest-russia-vladimir-putin-us-uk-volodymyr-zelensky-nuclear-weapons-11645657263):\n\n&gt;The inability of the U.S. to enforce its Budapest commitments will also echo in allied capitals that rely on America\u2019s military assurances. Don\u2019t be surprised if Japan or South Korea seek their own nuclear deterrent. If Americans want to know why they should care about Ukraine, nuclear proliferation is one reason. Betrayal has consequences, as the world seems destined to learn again the hard way.\n\nThe last time a geopolitical event spooked oil markets was the US invasion of Iraq.  Oil spiked from $20 to $140  during the occupation, with the only thing causing oil to fall was the 2008 financial crisis.  Before that, there was the oil embargo during the 70s, which also resulted in a decade-long elevation in oil prices.  Both the 2000s and the 70s saw a severe, decade-long contraction in the stock market from previous highs.\n\nThis particular geopolitical event (according to video above) is the worst Europe has seen since WWII.  Caveat emptor, plan accordingly.", "upvote_ratio": 0.79, "id": "t3_tdq2bx", "created_utc": 1647235785.0}
{"sub": "investing", "title": "Is there a chance for deflation?", "selftext": "The economic situation is incredibly tough right now, even on people doing moderately well (engineers, docs, lawyers). \n\nMarkets are down 10-12 %. Crypto isnt doing so hot. International markets are also down\n\nBut the cost of everything is getting worse and worse. \n\nI know gas can go up and down pretty flexibly but wages, food, cars, homes, pretty much everything is shooting through the roof. So while essentials climb, it feels like income streams/investments are falling\n\nWhen I look at yearly inflation charts, it seems like there really isnt any deflation except for &lt;1% in 2008. \n\nIs this really the new normal for commodities?", "upvote_ratio": 0.9, "id": "t3_tdkwvp", "created_utc": 1647218343.0}
{"sub": "investing", "title": "Historical Stocks Dynamic Data", "selftext": "Would this product be of interest to anyone?\u00a0\n\n[https://fog-class-423.notion.site/Historical-Stocks-Dynamic-Data-8ef1933409d54a9cb845498f151b7127](https://fog-class-423.notion.site/Historical-Stocks-Dynamic-Data-8ef1933409d54a9cb845498f151b7127)\n\nAs a retail investor I have found myself creating an excel spreadsheet of EPS/PE ratio data of stocks I am interested in and have found it annoying to keep updated as it isn't dynamic. Would any value investors be interested in a dynamic interface of historical EPS data ?", "upvote_ratio": 0.69, "id": "t3_tdhxpd", "created_utc": 1647209465.0}
{"sub": "investing", "title": "Why I Love Active Investment: It's not about beating the market", "selftext": "**TLDR**: Market is made up of participants with different goals &amp; investment horizons. I reject the notion of an \"Efficient\" market...its simply a weighted average of what market participants are doing. My own goals are different from that of the average market participant.\n\nMuch adieu is made by Bogleheads over how difficult it is to \"beat the market\", with fewer than 80% of funds \"beating the market\" year-over-year after costs. The assumption here is that the investor seeks to earn the 'average' market return, minus fees. Efficient market hypothesis will state that every stock has the same risk-adjusted expected return given the information available out there. Let's entertain this idea, and assume that Johnson &amp; Johnson (JNJ), Tesla (TSLA), and some bio-tech small-cap have the same expected **risk-adjusted return**.\n\nNotice the key part highlight: **risk-adjusted**. Clearly the expected **dispersion** of outcomes for a blue-chip dividend grower like JNJ is going to be different from a higher-risk bio-tech small-cap. The small-cap has potential to be a 10-bagger, whereas it also has a much higher chance of going to zero. Tesla on the other hand is an entirely different beast from either, with option-activity off the charts compared to anything else in the S&amp;P 500.\n\nImagine if you will, that there is no benchmark &amp; no index. You only have three investors:\n\n* **The conservative**: They are seeking the 'surest bet' of a return that keeps up with inflation. They are okay under-performing other investors in exchange for limiting risk of a major draw-down or wipe out. \n* **The professional day trader**: They don't care if the stock goes up or down long-term, they are seeking the most intra-day volatility.\n* **The YOLO speculator**: Seeking the biggest max payout possible.\n\nIf they each are only allowed to pick a single stock, it's pretty clear who would pick JNJ, TSLA, or the bio-tech.\n\nLet's bring this to why I reject the absolutism over broad-index investing. The market is efficient, sure, but **efficient at what?** Pricing, valuation? Clearly not, as ARK bag-holders have learned the hard way. Instead it's efficient at providing a view over the $$$ flows of the **average-weighted market participant**. So what does it mean then to invest in a broad index? At a macro-level, it means  you are following the flows of what the average participant is doing, but what sense does that make if every participant has different investment horizons and goals?\n\nLook, the Boglehead system is certainly right for most folks. Costs matter, and most folks are better off index investing as many fortunes have been lost chasing the returns of hot funds, or doubling-down on losers. Active investing is hard, you either need to spend a lot of time or find a good - and cheap - manager whose portfolio-management is in alignment with your investment goals. I know only a handful of managers I'd trust in this regard, and they all fall under the 'Conservative' camp.\n\nAs for my own circumstance, I actively invest so that I can stick to securities and strategies that are in alignment with my goals and investment horizon. I am risk-averse, I don't hold \"bubble stocks\". This doesn't mean I'm afraid of market downturns, those are unavoidable. I'm afraid of things **much worse.** My biggest fear is a complete wipe-out, followed by a Japan-style scenario where markets have a dead double-decade. My goal isn't return-maximization, but risk-minimization. \n\nI'll share a glimpse of my current stock portfolio construction (not a recommendation, really... I've gotten very \"lucky\" if you will in energy and do not suggest chasing) :\n\n* Over 15% allocation (currently) to oil &amp; gas producers &amp; infrastructure to hedge against inflation. Most of this is in a Canadian energy producer as I seek to avoid the risk of US oil/gas depletion.\n* 5% allocation to gold \\[previously 10%\\] as a hedge against slowing growth &amp; geopolitical risk. \n* A small allocation to gold miners, which are all hand-picked so my exposure is only to Canadian/US gold mines. Note: there are literally gold miners in the index that have exposure to Russia - oof.\n* A significant over-weight to European/Japanese equities, simply because I don't want all my eggs in the US basket.\n* Most my US stocks are in industries that have minimal geopolitical or disruption risk. For example, I have a large holding in a funeral-services conglomerate.\n* An overweight to utilities as I am bullish on their long-term contributions to electrification, but instead of holding XLU I hand-pick as I want to avoid any utilities with wild-fire risk.\n\nHave I been \"beating the market\"? Actually - mostly thanks to the energy overweight- I have over the last few years...but I frankly don't care about beating the market and who knows if I will going forward. Investing for me is about meeting my goals. To the folks out there that want to deviate from the broad index, just be aware that the market is more efficient than you think. The only thing you can control is how much **risk** you are willing to take. If your goal is to \"beat the market\", you're thinking about it wrong. Instead, think about your **goals** and whether you want to take on more-or-less risk than what the market is offering.", "upvote_ratio": 0.48, "id": "t3_tdfol2", "created_utc": 1647203278.0}
{"sub": "investing", "title": "Prosus N.V. ($PRX.AS) $160 billion valuations, real market cap a lot lower than online can be found.", "selftext": "Prosus N.V. ($[PRX.AS](https://PRX.AS)) is a technology investment company, with it largest holding being Tencent. **Tencent** is one of the big tech companies of China. This is by far their largest position. **Different outlets have stated that Monish Pabrai bought Prosus**. Of course, the focus should not be on the fact that such an investor has bought the stock. \n\nThe very interesting thing is that Prosus is trading at a big discount of the value of the Tencent shares they hold. It is mainly interesting because it is a big difference, some difference is not weird because you can not decide what they sell and hold. The CEO has announced they will not reduce their position for three years.\n\nEven more interesting is that online the shares outstanding are wrong, with that the market cap. Yahoo Finance high overestimates the market cap of Prosus N.V. ($[PRX.AS](https://PRX.AS)), even google. This is caused by the fact that Prosus owns a lot of shares in Naspers and Naspers in Prosus, they also share a board. Because of this you have cross shares, Prosus owns shares of itself due to Naspers. I was referenced to the source of their shares outstanding here: [https://www.prosus.com/news/investors-shareholder-information/](https://www.prosus.com/news/investors-shareholder-information/), **after I emailed their investor relations.** Since with google searching, this page was not easily found. \n\nWith valuing Tencent, Prosus can be conservatively valued by just looking at the Tencent stake. They have $12 billion in liabilities. The value of their other assets widely covers this, these are in part ventures, food delivery, fintech, and classifieds. You could argue that these are highly valued in this environment, you could argue tech is in a bit of a bubble. This gets taken into account in the **$160 billion valuation**: [https://www.financialstockdata.com/prosus\\_write\\_up](https://www.financialstockdata.com/prosus_write_up) . The focus is on Tencent, CEO's share activity, and showing their other assets for your own judgment. \n\nWhat do you think about Prosus, and such types stock? I can vaguely remember Peter Lynch had an example of this in one up of Wallstreet, Does anyone know about this?", "upvote_ratio": 0.41, "id": "t3_tdbeaq", "created_utc": 1647191375.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - March 13, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.8, "id": "t3_td2xq8", "created_utc": 1647162069.0}
{"sub": "investing", "title": "What are your thoughts on GTE?", "selftext": "How do you guys feel about GTE? I just threw a couple dollars in there and it\u2019s since dipped 4.82%\nNew to this and was just curious if this is a hit or a miss and should I hold?.\nP/e is at 8.32 and p/b is at 1.9\nIt\u2019s been steady down for the last couple days until yesterday afternoon. It bounced up but that\u2019s about it. \nThank you.", "upvote_ratio": 0.38, "id": "t3_tczrxl", "created_utc": 1647148762.0}
{"sub": "investing", "title": "Actively Managed vs S&amp;P 500", "selftext": "The title pretty much says it all. Up until now I\u2019ve been \u201cmanaging\u201d my portfolio aside from my 401k that\u2019s offered at my job. I want to also have another account that would be ready to pull from in the next 10-15 years (I\u2019m 20). Should I just DCA into the S&amp;P for the next 15 or so years or should I get a professional to pick and choose for me? Pros and cons of either? I\u2019m not really sure what those financial firms do or if they are consistently capable of outperforming the market. Any advice is appreciated, thanks.\n\nEdit: If I do go the S&amp;P route, what is the next step to diversifying beyond real estate (physical and REITS).", "upvote_ratio": 0.85, "id": "t3_tcyvp8", "created_utc": 1647145405.0}
{"sub": "investing", "title": "Can I switch Brokerages for my Roth IRA account?", "selftext": "I opened up a Roth IRA account with Fidelity some time ago, despite trying to read up on it I\u2019m still confused about two things:\n\n1.) Do I need to reinvest the money in my roth ira into stocks/ETFs to see significant gains?\n2.) Can I switch brokerages for my Roth IRA? Say I wanted to switch from Fidelity to Vangaurd, is there any penalty if I do that?\n\nedit: thanks to everyone who replied!", "upvote_ratio": 0.69, "id": "t3_tctbu2", "created_utc": 1647126515.0}
{"sub": "investing", "title": "Rocket Company Cyber Security", "selftext": " Anyone have any info on what operating systems/security software the various rocket companies are using? Any software/computer hardware companies they're working with, and/or if any related partnerships have taken place/planned/ongoing?? ...Wondering about what tech stocks and other companies might grow as a result from the various rocket/space companies doing launches in the coming years. Building a cybersecurity/space portfolio.", "upvote_ratio": 0.43, "id": "t3_tcqkm8", "created_utc": 1647118322.0}
{"sub": "investing", "title": "Rate/Advice for my possible dividends portfolio?", "selftext": "So I have some sizeable funds coming my way in the near future. I want to grow my wealth and create a long hold dividends portfolio. This dividend portfolio won't be 100% of my net worth though. I've curated a list and I think it's pretty good, curious what you guys think and if I should add or remove anything, or if you have any other helpful suggestions. I want to add that on top of this I plan to set aside a portion for mutual funds and my tax free accounts (Canada). Currently in my TFSA I hold stocks that would be considered riskier, such as TSLA, RKLB, CHPT, LCID, PL, CRSP. I also want to add AAPL, GOOG, MSFT to that list. I am late twenties, a mature student with 3 years of schooling left, no dependants, not a homeowner. Considering that too, except markets here are fucked.\n\nFinance | Yield\n---|---\nBMO.TO | 3.63\nBNS.TO | 4.39\nCM.TO | 3.98\nNA.TO | 3.49\nRY.TO | 3.27\nTD.TO | 3.5\n\nConsumer | Yield\n---|---\nKO | 2.75\nMCD | 2.16\nMO | 7.15\nSBUX | 2.02\n\nEnergy | Yield\n---|---\nCNQ.TO | 3.56\nCWEN | 4.07\nENB.TO | 6.53\n\nInsurance | Yield\n---|---\nMFC.TO | 5.15\nSLF.TO | 3.7\n\nComputing | Yield\n---|---\nIBM | 4.99\nINTC | 2.67\n\nTelecom | Yield\n---|---\nBCE.TO | 5.28\nRGI | 3.04\nTU | 3.88", "upvote_ratio": 0.77, "id": "t3_tcpp6y", "created_utc": 1647115718.0}
{"sub": "investing", "title": "Things are going to get a lot worse before they get better", "selftext": "Ive been bullish for a long time despite even the pandemic. I was confident the FED would do the right thing and raise rates, and the government would chill the fuck out with the spending. (Monetary and fiscal policy).\n\nWell, after seeing inflation data and the governments response, I\u2019m bearish. Biden blames Russia 100% for inflation. There is zero talk of cooling spending, he even said \u201cI\u2019m sick of the American people saying government spending causes inflation\u201d. Fact, high government spending is one of the many parts of demand pull inflation which is what we are experiencing; unless he has discovered a new facet of Keynesian economics.\n\nNow there are [speculation] talks of an invasion of Taiwan, if that happens we are definitely fucked because thats where almost all of our silicon comes from. (This is the most speculatory part, it is a risk that needs to be considered though)\n\nInflation is going to get worse. The government is not going to do shit as they\u2019ve indicated. Since 2008 the government solution to everything is to throw money at it. The fed isnt going to do shit, theyve literally fucked themselves into a corner. They cant raise rates, we wont be able to pay for our debt. (I mean raise them significantly. Not fucking .25%)\n\nI think we are in for a horrible fucking quarter with where gas prices are. The spike in gas prices solidified my bearishness: when gas prices are up literally everything under the sun costs more.Companies will need to raise prices more, consumers will become priced out and more frugal, demand will fall, stagflation. \n\nThis being said I am holding still, but I will be waiting a couple months before buying more. Especially after Q2.\n\nAlso, why are NO politicians holding the FED accountable right now? Ive seen nothing in the news cycle about it.\n\nOh yeah lets not forget the housing market is absolutely fucked right now.\n\nEdit: a lot of you seem to think 8% inflation is good for stocks. LOW inflation (2%) IS good for stocks and economic growth. HIGH inflation is VERY bad for the economy, head math: prices go up, value of money goes down, consumers buy less. Bad.", "upvote_ratio": 0.74, "id": "t3_tcp8qh", "created_utc": 1647114402.0}
{"sub": "investing", "title": "Chip shortage, Ukraine, Gas prices, Recession", "selftext": "All the markets, especially crypto, are getting hammered. I'm probably wrong but it seems the biggest cause is the chip shortage followed by the war in Ukraine and gas prices.  People are pulling out of the markets and either are holding cash or buying homes. Chip shortage may be resolved by 2023, the war and Ukraine may last years, odds are we may hit a Recession. What's everyone's thoughts and long term investing strategy (besides holding) during these crazy times.", "upvote_ratio": 0.88, "id": "t3_tcl07x", "created_utc": 1647105135.0}
{"sub": "investing", "title": "best global/international brokers (that wont ruin me with commissions)", "selftext": "Is there any broker i could use for stocks, indexes, ETFs etc. anywhere? Think Asia, Europe and Oceania, not USA though.\n\nI've been using XTB for everything stocks related but i'm soon moving to literally the other side of the world for who knows how long, then probably Asia and so on. XTB is not available at all in some countries, it mostly functions in Europe. I need a reliable broker for investing globally, any ideas? Is Interactive Brokers a good option?", "upvote_ratio": 0.71, "id": "t3_tchsvb", "created_utc": 1647096196.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - March 12, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.73, "id": "t3_tcd9pu", "created_utc": 1647079269.0}
{"sub": "investing", "title": "Mutual funds and impact of drawdowns on returns", "selftext": "In my country we unfortunately don't have access to global stocks and ETFs, the only way to gain exposure to foreign markets is through overpriced mutual funds. It's better than nothing though, but during this downturn I'm learning mutual funds carry a risk not necessarily derived from the underlying asset,**drawdowns**. \n\nWhen redemptions outpace and negate the organic price movement of the underlying assets, you could simply watch you capital shrink and this is not based entirely on the sentiment of the holders of the fund(profit taking, rotation, lost belief in the fund managers/strategy etc). How can one guard against this? What signs to look out for to identify more stable funds which are less prone to huge losses due to drawdowns?", "upvote_ratio": 0.67, "id": "t3_tcc929", "created_utc": 1647074888.0}
{"sub": "investing", "title": "Questions about Series I Bonds Before I Invest", "selftext": "Hi guys. I'm interested in Series I bonds. I want to know how they work before investing in them. \n\n1) Is it true that the interest rate on them can change every 6 months? For example, if I buy one at 5% now and the Fed increases rates which in time would decrease inflation, would the govt decide to lower the rate of my bond to 3%? So the interest rate on my bond can change by the whim of the govt?\n\n2) Also, how can the series I bond yield become less attractive over time as interest rates rise if yields on other govt bonds can reach as high as series I bonds?\n\n3) How long are the maturities on them?\n\n4) Does it make sense to invest in them compared to other investments? \n\n5) Can I invest $130,000 in them?\n\n.", "upvote_ratio": 0.61, "id": "t3_tcb67n", "created_utc": 1647070299.0}
{"sub": "investing", "title": "Is it worth it to maximize my 2021 roth contributions while paying a bit of interest to credit cards.", "selftext": "Currently I have an automatic portfolio at fidelity and it\u2019s doing pretty well all things considered. I just started getting into the investment seen like 4-5 months ago so I don\u2019t have my 2021 roth contribution maxed yet.\n\nDo you think it\u2019s worth it to put every available sent into my 2021 Roth due to the window closing soon (April) and the fact I\u2019ll never being able to get it back if I don\u2019t do it now. \n\nOnly downside is my credit card won\u2019t be paid off for another month so I\u2019ll just have to pay like $30 in interest.", "upvote_ratio": 0.79, "id": "t3_tc8sea", "created_utc": 1647060788.0}
{"sub": "investing", "title": "Is Meta/ FB on sale right now or a plague to be avoided?", "selftext": "FB has been one of the successful tech stocks for a long time now. It's only 187 a share with a high of 378. I don't see why it would tank. They have adapted and continue to acquire new apps and platforms. With the metaverse coming I can see that also adding a lot of value in ways we can't really imagine yet. \n\nIt might not happen this year, next year, or even the year after. But in 5 years do I think my money would double? Well, idk that's why I'm asking strangers on the internet.", "upvote_ratio": 0.87, "id": "t3_tc6vdb", "created_utc": 1647053847.0}
{"sub": "investing", "title": "Some perspective for anybody getting wrecked by the recent market", "selftext": "I come bearing the scars of this turbulent market and I wanted to offer guidance for people struggling with the psychology of getting crushed. \n\nYes, I\u2019ve made mistakes and my account is nearly cleaned out but I realized there are still highlights. It\u2019s important to take stock of the blessings in your life no matter what they are in order to stay grounded. \n\nFor example: While my account has been sliding backwards for months I haven\u2019t used any money I need to get by, allowing me to chalk up mistakes as market tuition and to realize it\u2019s \u201cjust money\u201d. Meanwhile, people in other parts of the world are losing everything including their lives making me realize it could always be worse. \n\nOn a personal level, I\u2019ve been able to evaluate success by learning important lessons. I stay grounded by realizing I\u2019ve only been trading for a few months and to not get wrapped up in the fact I\u2019m not wildly successful; yet. This takes time. \n\nTLDR: Stay grounded my friends.  What have you learned recently that you want to share?", "upvote_ratio": 0.71, "id": "t3_tc5l22", "created_utc": 1647049349.0}
{"sub": "investing", "title": "Tax penalties for Roth IRA", "selftext": "Hi everyone, so to break it down, I contributed the max amount for a Roth IRA. Currently, it\u2019s not being invested, just sitting in a brokerage ready to invest. Reasons being is the market is unstable and the war doesn\u2019t help. I just filed my taxes and it popped up that I made too much this year to qualify and could be subject to penalties. Mind you, I don\u2019t normally make this amount and I won\u2019t make above the max income for the next few years as I\u2019m leaving my job to travel. Am I subject to penalties under these circumstances or am I ok? If this is the wrong sub I apologize. I figured the investment community would be more knowledgeable in this situation.\n\nTl:dr; I made too much and didn\u2019t find out until after I\u2019d already contributed to a Roth IRA, however, the money is just sitting, it\u2019s not currently being invested.", "upvote_ratio": 0.47, "id": "t3_tc171w", "created_utc": 1647035804.0}
{"sub": "investing", "title": "Difference between GAAP vs. Non-GAAP", "selftext": "I'd like to get a better understanding of this due to what happened with Amazon's last earnings. This is my basic understanding and questions I have, please correct or add anything that may be useful. Thank you.\n\n- When analysts estimate EPS for a company, it's non-GAAP\n\n- Non-GAAP allows a company to adjust things that may make them seem better\n\n- AMZN's last GAAP EPS was ridiculous due to their RIVN investment, they were still able to beat estimated EPS without factoring that in, but obviously not by that amount.\n\n- How will AMZN's EPS be reported for their next earnings since RIVN has taken a hit, GAAP EPS will look horrible while Non-GAAP should look fine, right?", "upvote_ratio": 0.67, "id": "t3_tbvevi", "created_utc": 1647020360.0}
{"sub": "investing", "title": "IRA and None IRA tax question", "selftext": "Earlier last  year I took some big losses on my investment none IRA account.  To replenish the loss I moved 67k from IRA account to none IRA. \nMy assumption was that I can offset the 35%\nTax of the 67k with loses from my none IRA account.\n\nThis is the first time I ever did this.\n\nMy tax person informed me that I will not be able to deduct the loses of none IRA against the 35% tax.\n\nHow accurate is that? \n\nThanks", "upvote_ratio": 0.71, "id": "t3_tbuh3s", "created_utc": 1647017812.0}
{"sub": "investing", "title": "What are some good questions to ask my portfolio manager during our review?", "selftext": "I have a review of my portfolio today and wanted to get an idea of some good questions to ask to ensure they are managing it properly.  I also want to get their sense of the market and how they have my portfolio positioned to mitigate any risks that are ongoing and may be coming.  I sometimes wonder if they are bullshitting me and would love a couple questions that could potentially put them on their heels and make them realize to take these meetings and my portfolio management more seriously.", "upvote_ratio": 0.91, "id": "t3_tbs8xe", "created_utc": 1647011632.0}
{"sub": "investing", "title": "Fidelity web/desktop apps suck and cost me money - alternatives?", "selftext": "I want to continue to use Fidelity as my broker, but their software sucks.  \nI want software way easier and faster to use, but still keep Fidelity as my broker. I end up missing short term lucrative trades due to their clunky software.  \nIs there a platform I can that has a great app (mobile and/or desktop) that supports margin vs. cash trading, selling specific lots, etc. in a super easy/fast UI?", "upvote_ratio": 0.23, "id": "t3_tbs0it", "created_utc": 1647010967.0}
{"sub": "investing", "title": "Won't western companies have to write down Russian assets ? What about their lenders/banks ?", "selftext": "All these companies walking away from huge $ of assets from McDonald's to Volkswagen and Putin seizing them... won't they have to write down their value ?  850 locations avg book value (say) $500K = $400+M ? (and 130 Starbucks and a giant Volkswagen factory and ...)\n\nwhat about their lenders ?  what banks have big exposures ?\n\ndisclosure: I'm short Deutsche Bank (DB) because they're so screwed they *can't* divest, claiming it's \"[not practical](https://www.google.com/search?q=deutsche+bank+russia).\"", "upvote_ratio": 0.89, "id": "t3_tbrqac", "created_utc": 1647010170.0}
{"sub": "investing", "title": "Is Cap gains tax, FIFO, or cost based?", "selftext": "Cap gains has always confused me when it comes to stocks. Lets say I\u2019ve been buying shares of Stock ABC.\n\nI\u2019m gonna use small numbers cause I\u2019m hung over.\n\nWhen I originally bought ABC I bought 100 shares at 10$. \n\n6 months later ABCis trading at 5$ so I buy 200 shares. Because I believe in this company. It\u2019s gonna turn the corner. This brings the average to $ 6.67\n\n\n6 more months pass and let\u2019s say my car needs some work and I need some cash. ABC is trading at 9$.\n\nBrokers trade in tax lots correct? So if I only sold 100 shares there would be no cap gains because it would sell the first lot I bought first. Meaning I took a loss of 1$ per share? And if I sold 150 shares @ 9$ the first 100 wouldn\u2019t be taxed. But the 50 would be taxed at a gain of 4$ per share?", "upvote_ratio": 0.8, "id": "t3_tbq2fw", "created_utc": 1647005058.0}
{"sub": "investing", "title": "Good allocation according to current environment", "selftext": "This is my response to people saying there\u2019s no alternative.\n\nRising interest rates environment: gold, and to small degree resources like oil and mining.\n\nRussian sanctions: oil majors, metals.\n\nGeopolitical instability: gold.\n\nNow check performance of \n\n- gold by looking at CEF.\n- Oil: bp, shell, etc.\n\nAllocate according to environment and stop excusing losses with impossibility of timing.  The later is a mutual fund sales tactic.\n\nThink", "upvote_ratio": 0.5, "id": "t3_tbowqv", "created_utc": 1647001116.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - March 11, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.88, "id": "t3_tbmvd6", "created_utc": 1646992870.0}
{"sub": "investing", "title": "Long term monthly investments for someone in Ireland?", "selftext": "What ETFs would you guys recommend throwing 500e per month into? SPY? Tax laws are horrrendous in Ireland (You have to pay 45% every 8 years on your gains, you can't let it ride) but only 30% when you cash out on individual stocks. What would you guys go with?", "upvote_ratio": 0.69, "id": "t3_tbmt0m", "created_utc": 1646992599.0}
{"sub": "investing", "title": "might sound naive: why do you still have your money in the market with all the negative stuff ahead?", "selftext": "I just dont get it. Most stocks are still overvalued when looking at their historic measements. On top of that we have a potential war in combination with inflation and rate hikes. Why could any rational investor look at all that info and be: mweh, i dont mind. \n\nWhat makes you not sell?", "upvote_ratio": 0.51, "id": "t3_tblbko", "created_utc": 1646986206.0}
{"sub": "investing", "title": "Undervalued Stocks Dropping Like a BRIC?", "selftext": "There is a lot of fear driving down the price in emerging markets.\n\nSome European focused indexes are down 77% from their normal 52wk trading range. Even if the fund doesn't have any holdings in Russia or Central Europe. Such as GF, EEA, and SWZ\n\n&amp;#x200B;\n\nIf your looking for other dividend paying, long term holdings, that are currently undervalued;\n\nIFN - [https://www.aberdeenifn.com/aam.nsf/usClosedIfn/home](https://www.aberdeenifn.com/aam.nsf/usClosedIfn/home) \\- [https://finviz.com/quote.ashx?t=IFN&amp;ty=c&amp;ta=1&amp;p=d](https://finviz.com/quote.ashx?t=IFN&amp;ty=c&amp;ta=1&amp;p=d)\n\nIng - [https://www.ing.com/](https://www.ing.com/) \\- [https://finviz.com/quote.ashx?t=ING&amp;ty=c&amp;ta=1&amp;p=m](https://finviz.com/quote.ashx?t=ING&amp;ty=c&amp;ta=1&amp;p=m)\n\nGGB - Gerdau S.A. - [https://finviz.com/quote.ashx?t=GGB&amp;p=m&amp;tas=0](https://finviz.com/quote.ashx?t=GGB&amp;p=m&amp;tas=0)\n\nGogl - [https://www.goldenocean.bm/](https://www.goldenocean.bm/) \\- [https://finviz.com/quote.ashx?t=GOGL&amp;ty=c&amp;p=d&amp;b=1](https://finviz.com/quote.ashx?t=GOGL&amp;ty=c&amp;p=d&amp;b=1)\n\nOEC - [https://www.orioncarbons.com/](https://www.orioncarbons.com/) \\- [https://finance.yahoo.com/quote/OEC?p=OEC&amp;.tsrc=fin-srch](https://finance.yahoo.com/quote/OEC?p=OEC&amp;.tsrc=fin-srch)\n\nVeon - [https://www.veon.com/-](https://www.veon.com/-) [https://finance.yahoo.com/quote/VEON/press-releases?p=VEON](https://finance.yahoo.com/quote/VEON/press-releases?p=VEON)\n\n&amp;#x200B;\n\nConsidering USA inflation is around 7%, and at these prices the dividends are 7-13% yields. Do the dividends and possible 1.5x to 4x price increases, make these safer bets then holding usd?\n\nIs anyone else increasing their positions in emerging markets? or finding other undervalued stocks?\n\n&amp;#x200B;\n\nps. i also posted the same thing in r/wallstreetbets. I just want to get different perspectives from \"degenerates\" and \"adults\"", "upvote_ratio": 0.73, "id": "t3_tbe2re", "created_utc": 1646960619.0}
{"sub": "investing", "title": "Is this the renewal of renewables?", "selftext": "After a massive rise in 2020 on Biden's election, green energy took a beating last year due to the GOP Senate's blocking of Build Back Better. Now Russia's invasion of Ukraine and energy-extortion of Europe seems to be reminding everyone that oil and gas addiction is a bad idea for multiple reasons. \n\nI'm back into TAN, QCLN, ENPH. Engineers like ABB, and electrical components like ATKR. Thoughts?", "upvote_ratio": 0.77, "id": "t3_tbdw7z", "created_utc": 1646960066.0}
{"sub": "investing", "title": "Does anyone know why the share price for The Restaurant Group PLC has declined by over 30% within the last month?", "selftext": "*The Restaurant Group PLC is a British business that owns numerous popular restaurant chains throughout the UK and is listed on the LSE. It employees tens of thousands of people and generates revenues in the 9 figure range. - Please only comment if you are familiar with the company.*\n\nObviously once Covid hit, it impacted the share price however that is not my concern. Since the start of February 2022, the price has been hammered and is showing over a 30% decline in the share price. I couldn't find any media articles explaining the reason for the significant decline.\n\nCan anyone tell me the reason for the significant decline within the last 5/6 weeks? - I would assume it's likely due to the current issue across the globe (don't want to state the obvious, as post will get auto-removed), but I can't see how that issue would impact The Restaurant Group PLC in any form.\n\nFurthermore, Covid restrictions are lifting and more and more people are returning to 'normal' life here in the UK, like dining out and socializing, so I would have thought that the share price would eclipse.\n\n**Can any professional shed any light?** ", "upvote_ratio": 0.47, "id": "t3_tbc2s9", "created_utc": 1646954686.0}
{"sub": "investing", "title": "What happens to bond *funds* when the fed raises interest rates?", "selftext": "(Sorry for the super basic question...)\n\nI understand that when the fed raises interest rates, the value of existing individual bonds go down. What about bond funds like BIV and BLV from Vanguard? It seems like they should initially go down as well, since the value of everything they own would go down... but would they tend to naturally recover because they can get higher interest rates for cheaper? Would they rise in price because they would become more attractive to investors?", "upvote_ratio": 0.84, "id": "t3_tbbdmt", "created_utc": 1646952854.0}
{"sub": "investing", "title": "Should I abandon my bond investments?", "selftext": "It\u2019s been losing money for awhile now (a bond heavy mutual fund I invest in) while the \u201criskier\u201d ones are all gaining money even in recent months.\n\nI invest some money in the bond fund as a nest egg I can afford some volatility on, but that I still want to grow my investment with. \n\nThis is the fund \n\nhttps://www.sisip.com/cfmws-sisip/media/pdfs/S605_EN.pdf\n\nAny advice is appreciated.", "upvote_ratio": 0.85, "id": "t3_tb7tf2", "created_utc": 1646943630.0}
{"sub": "investing", "title": "Does anyone know of a Candian version of the wallstreetzen website?", "selftext": "As the title says, i was wondering if anyone knew of a Candian version of the wallstreetzen website?\n\nThis site is only for American stocks and as a Candian I'd like to buy more Candian stocks so I don't pay the exchange rate. \n\nI find it easy to read and I like the forecast sections where it tells you if it is recommended to buy (the \"strong buy\" and the \"buy\" sections).\n\nI find it very usable and would love any recommendations thanks!", "upvote_ratio": 0.38, "id": "t3_tb7d6g", "created_utc": 1646942407.0}
{"sub": "investing", "title": "lol, it begins: [Bloomberg] The Fed Needs to Delay Its Rate Hikes", "selftext": "Via [Bloomberg](https://www.bloomberg.com/opinion/articles/2022-03-10/ukraine-war-should-cause-fed-to-slow-down-its-rate-hikes?sref=q1j4E2z1) (non-paywall link at [archive.is](https://archive.ph/7zYIA)):\n\n&gt;The U.S. Federal Reserve is widely expected to raise interest rates by at least a 25 basis points next week. And if inflation stays high, the Fed is \u201cprepared to raise by more than that\u201d in the coming months, Chair Jerome Powell said last week.  \n&gt;  \n&gt;That would be a mistake. After next week\u2019s hike, the Fed should hit pause for at least the next several months and possibly through the summer \u2014 even though the war in Ukraine will no doubt make inflation worse in the U.S.  \n&gt;  \n&gt;It\u2019s unclear how bad the conflict will get, the effect it will have on the region and whether it will lead to a global recession this year. The probability of that last is less than the most extreme predictions, but is nonetheless real.  \n&gt;  \n&gt;A more aggressive Fed might use a recession as an opportunity to rapidly bring down inflation by sticking to its rate-hike schedule. That is risky policy, and one that [Powell seems disinclined to take](https://archive.ph/ybhBA). If a recession hit, it\u2019s likely that the Fed would simply have to reverse any rate hikes it had made in the preceding months.  \n&gt;  \n&gt;A see-saw pattern in rates would weaken the overall impact of the Fed\u2019s policy. Consider, for example, the plight of a homebuilder who cuts production next summer in response to rising rates. She is not likely to increase production immediately if rates fall in December; she\u2019d want to wait for a signal that rates will remain low for a while. From the Fed\u2019s perspective, it would be more effective to leave rates alone, encouraging her to keep production high for the next several months.  \n&gt;  \n&gt;There are also risks to consider beyond outright recession. The direct costs of higher energy and food prices will cut into consumer savings. Even more important, spiking commodity prices are likely to dent consumer confidence, leading to reduced spending on other items.  \n&gt;  \n&gt;Another consideration is the effect of the war on developing markets around the world. Higher food and energy prices will hit their economies harder. Global uncertainty will lead investors to move funds out their markets and into the U.S. That could cause a drop in the demand for U.S. exports, which are geared toward investment goods such as heavy machinery. That would reproduce some of the effects of the mini-recession that swept the Midwest in 2015 and 2016.  \n&gt;  \n&gt;At the same time, money flowing into the U.S. from both developing markets and Western Europe will cause the dollar to rise and the relative prices of imports to fall. As consumer spending shifts toward imports, that will cool some of the underlying inflationary pressures in the U.S.  \n&gt;  \n&gt;The near-term environment is complex. It\u2019s unclear how long the war will last and how far-reaching its effects will be. The ideal Fed response, however, is straightforward: **Go ahead with the rate hike next week. But make it clear that there won\u2019t be any more for at least two more meetings, and then only as the fallout from the war in Ukraine becomes more certain.**", "upvote_ratio": 0.9, "id": "t3_tb59y2", "created_utc": 1646937112.0}
{"sub": "investing", "title": "Question about evidence for the Fama-French 5 factor model", "selftext": "Can someone ELI5 why their work merited a Nobel Prize? I feel like the reasons for Factors tilts *must* be more evidence-based than just a regression analysis? That's just like an overfitted backtest, no? What exactly made Fama&amp;French's work different than me saying tech had a premium the last few decades compared to the total stock market? Obviously only a fool would argue to buy the NASDAQ vs the S&amp;P based on historical data alone, if anything a lot of people here would recommend doing the opposite, buying emerging market small cap value instead of say US large cap growth as they have already just enjoyed a long bull market. What's to say tilting towards the opposite of the 5 factors isn't actually smarter, which would in a sense be \"buying the (small) dip\"?", "upvote_ratio": 0.66, "id": "t3_tb4iu1", "created_utc": 1646935124.0}
{"sub": "investing", "title": "Build a bear crushes their numbers from last year in Q4 earnings call and dips 20%", "selftext": "I\u2019m confused to say the least. They increased their guidance sometime in February and the only explanation is that they didn\u2019t beat expectations because they were so high. Otherwise, they had a revenue surprise of over 300% and a massive jump in EPS over Q3 among other very positive news. \n\nWhat am I missing?", "upvote_ratio": 0.8, "id": "t3_tb45ts", "created_utc": 1646934123.0}
{"sub": "investing", "title": "Retirement funds with Fidelity", "selftext": "Gentlemen,\n\nI noticed their target date funds charge about .70% fee. If I was to direct my money into specific funds myself, which ones would you recommend? I will try to work until 2050 if I live that long. The sreenshot with choices is below.\n\nhttps://imgur.com/a/tnm9z1r\n\nThanks.", "upvote_ratio": 0.61, "id": "t3_tb33yi", "created_utc": 1646931364.0}
{"sub": "investing", "title": "Invest in Physical Silver or ROTH IRA", "selftext": "Hello Fellow Investors,\n\nI will be getting a bonus next week and seeing the current state of the world, I am unsure as to invest it all in physical silver or towards my Roth IRA. (I'm about 25 years before retirement).  I currently invest in silver coins monthly as part of my financial plan. \n\nThank You!", "upvote_ratio": 0.55, "id": "t3_tb067q", "created_utc": 1646923286.0}
{"sub": "investing", "title": "CPI rises .8% in February, 7.9% over last 12 months", "selftext": "https://www.bls.gov/news.release/cpi.nr0.htm\n\nYou can see a very specific breakdown per item here: https://www.bls.gov/news.release/cpi.t02.htm\n\n&gt; The Consumer Price Index for All Urban Consumers (CPI-U) increased 0.8 percent \nin February on a seasonally adjusted basis after rising 0.6 percent in January, \nthe U.S. Bureau of Labor Statistics reported today. Over the last 12 months, \nthe all items index increased 7.9 percent before seasonal adjustment.\n\n&gt; Increases in the indexes for gasoline, shelter, and food were the largest \ncontributors to the seasonally adjusted all items increase. The gasoline index \nrose 6.6 percent in February and accounted for almost a third of the all items \nmonthly increase; other energy component indexes were mixed. The food index rose \n1.0 percent as the food at home index rose 1.4 percent; both were the largest \nmonthly increases since April 2020.  \n\n&gt; The index for all items less food and energy rose 0.5 percent in February \nfollowing a 0.6-percent increase the prior month. The shelter index was by far \nthe biggest factor in the increase, with a broad set of indexes also \ncontributing, including those for recreation, household furnishings and \noperations, motor vehicle insurance, personal care, and airline fares.   \n\n&gt; The all items index rose 7.9 percent for the 12 months ending February. The \n12-month increase has been steadily rising and is now the largest since the \nperiod ending January 1982. The all items less food and energy index rose 6.4 \npercent, the largest 12-month change since the period ending August 1982. The \nenergy index rose 25.6 percent over the last year, and the food index increased \n7.9 percent, the largest 12-month increase since the period ending July 1981.", "upvote_ratio": 0.97, "id": "t3_taz6p1", "created_utc": 1646920509.0}
{"sub": "investing", "title": "[CREDIT] Stay positive and play the game.", "selftext": "As the markets go down and the pandemic is still in affect, you may find yourself struggling to pay off your credit cards.\n\nIf you are in this unfortunate situation, don\u2019t panic or loose control. Remain as positive as you can and orchestrate a plan to get through it.\n\nCredit card companies will send you message after message, and sometimes even send you IRS notices like the 4506-T income verification form.\n\nAt this point, strategize your next moves within this financial game; look at which cards are connected to banks that may be more lenient than others. Start with those accounts and try your best to keep the proverbial \u201cwolves\u201d off your back.\n\nAfter you complete that, see what your options are with the more strict credit card companies (ie. Discover) that have less tolerance with their customers.\n\nThere\u2019s a realistic scenario where you will not be able to pay off the card and the account may be closed. If this occurs, once again don\u2019t panic or loose control.\n\nWorse case scenario you will be responsible for the balance after the account is shut down and your credit score will also be affected. But that doesn\u2019t mean the game is over. You can always pay off the balance at a later time and reestablish a relationship with that same company.\n\nKeep your heads held high and always remember that somewhere else in the world, there\u2019s somebody else dealing with much more severe problems then you.", "upvote_ratio": 0.27, "id": "t3_taynko", "created_utc": 1646918891.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - March 10, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.81, "id": "t3_tavf56", "created_utc": 1646906469.0}
{"sub": "investing", "title": "Amazon announces a 20 for 1 stock split of company\u2019s common stock, effective June 6th", "selftext": "Amazon announces a 20 for 1 stock split of company\u2019s common stock, effective June 6th \n\nFrom market rebellion \n\nAfter 2 years of sideways action, I think it will finally happen for them and google (which has done much better) to finally start to see some positive momentum and upswing in this stock in recent time. Looking forward to it", "upvote_ratio": 0.97, "id": "t3_taihkg", "created_utc": 1646862399.0}
{"sub": "investing", "title": "Is investing in bond ETFs risky?", "selftext": "If I want to park cash in a bond fund like vtip what is the chance of principal loss? I have 150k making nothing in a savings account, but in a  few years want to put it toward a house. Already maxed out on ibonds. My wife is very risk-averse so I am trying to find something safe, but also with a better yield than a high yield savings account. CDs also don't seem to give good returns. I don't have any loans and have an emergency fund already.", "upvote_ratio": 0.75, "id": "t3_tai7pn", "created_utc": 1646861632.0}
{"sub": "investing", "title": "Today's Market Spike. Why is it suddenly ripping? Does this mean the market is going back to normal?", "selftext": "Short answer, I *personally believe* that it isn't any indication for the market to go green again. I still expect a long road of red ahead, but then again, I'm just some college student majoring in finance. My theory is that the spike today was caused by the coming-up Interest Rate Hike catalyst. For those who don't know what an Interest Rate Hike is,\n\n&gt;When interest rates rise, it's usually good news for banking sector profits since they can earn more money on the dollars that they loan out. But for the rest of the global business sector, a rate hike carves into profitability. That's because the cost of capital required to expand goes higher.  \n&gt;  \n&gt;\\-Investopedia (my favorite source)\n\nMy logic is, the bullish sentiment was simply buyers buying in before/during the Interest Rate Hike and expecting the stocks to increase for them to sell in a future date (aka a \"predicted dip\"). Seeing that this is such a short-term move,  I assume it's caused by day traders and swing traders. I don't think that it was caused by any investors, as they tend to avoid timing the market unless they had some extra capital that they wanted to put into the dip.\n\nAny other theories are welcome.", "upvote_ratio": 0.33, "id": "t3_tahi9s", "created_utc": 1646859692.0}
{"sub": "investing", "title": "Which platform has automatic portfolio re-balance?", "selftext": "Let's say I want to create an (roth-)IRA account and I want to re-balance it on a daily/weekly/bi-weekly/etc basis. Which one offer it?\n\nM1 has pies, but re-balancing in manual. Vanguard (and similar providers) only have re-balance on automatic if you are using the advisor, but I want to control what is being invested.", "upvote_ratio": 0.76, "id": "t3_tafhzs", "created_utc": 1646854151.0}
{"sub": "investing", "title": "At under $1.00, I like FNMA", "selftext": "I've been watching FNMA since it went from like $0.17 cents to $3.00 about 10 years ago.\n\nI'm not a sophisticated investor (my best investments are my Roth and 403(b)) but think that right now it's pretty good to double your money sometime between now and 20 years.\n\nMy plan is to slowly put money in the next few years, hoping it hangs low, and maybe falls lower, because if it's around, why shouldn't it go back up based on nothing more than the cost of things going up?\n\nDoes anyone agree or disagree, or have another investment they like for the same reason?", "upvote_ratio": 0.39, "id": "t3_tad5p7", "created_utc": 1646847630.0}
{"sub": "investing", "title": "Is today's jump begin of rally or a trap? Stagflation, Fed, etc..", "selftext": "How do you see today's big jump both in Europe (+7%) and in the US (+2%)? Is it the beginning of a turnaround and rally or could it be a trap? The bull news today is that Europe seems to have enough natural gas to make it through the winter even if Putin would cut them off.\n\nHowever there are still alot of negative signals. **For those into TA**, if you look on a daily chart you can see that we are still in a negative trend, that we still have lower lows and lower highs which normally indicate more pain is to come. The jump today doesn't change that unless it is sustained. **We also are still firmly below 200 moving average for all indices.**\n\nAlso, tomorrow we'll get more **CPI inflation data** which most likely will be even higher than last month and next week the 16th the Fed will do its first rate hike with many to follow given the high inflation.\n\nGiven the high energy prices the risk of **stagflation** becomes very real. Stagflation means the combo of high inflation with slower growth. The Ukraine war and sactions will add to inflation and **supply chain issues.**", "upvote_ratio": 0.81, "id": "t3_tabd45", "created_utc": 1646842796.0}
{"sub": "investing", "title": "Understanding EPS in a larger context (VLDR)", "selftext": "So I'm slowly learning more about investing. I know EPS is earnings per share, but what information should I glean regarding the EPS of a company? Does it actually gage or predict any aspect of the companies viability, price or value?\n\nI am currently tied up is a massive loss position and I'm trying to figure out what to do next with the stock, cut my losses or dig my heels in, take the short/medium term hit and wait for the stock to recover. But  it could very well be the stock just finishes cratering and the company goes under. What other indicator aside from EPS should I be looking at to make this decision?", "upvote_ratio": 0.67, "id": "t3_taajy3", "created_utc": 1646840585.0}
{"sub": "investing", "title": "Strategies for best using capital loss", "selftext": "I\u2019ve racked up a fairly sizable capital loss that I\u2019m carrying forward (don\u2019t worry, not related to wallstreetbets). We\u2019re talking six digits. \n\nNow I was wondering, are there any general guidelines on when to best use a carried forward capital loss? \n\nI have non-realized gains that I could use against it. Does it make sense to sell and re-buy to step up the cost basis as soon as possible?\n\nOr should I just ignore it and wait for a time when I actually want to sell. \n\nThis year I might be in the 20% capital gains tax bracket, next year probably 15%. Does that make a difference, strategy wise?\n\nThanks!", "upvote_ratio": 0.57, "id": "t3_ta7ioc", "created_utc": 1646831647.0}
{"sub": "investing", "title": "Questions on GHG Disclosures", "selftext": "Niche question, and maybe wrong /r, but I'll give it a whirl.\n\nMy understanding is that BlackRock made a statement requiring all companies with AUM by them to disclose their GHG emissions, make targets for lowering them, etc. I went to look for that document and found a [2020 Sustainability Disclosure](https://www.blackrock.com/corporate/literature/continuous-disclosure-and-important-information/blackrock-2020-sasb-disclosure.pdf) where from my understanding there is a critical usage of the word \"should\" vs. \"shall\": \n\n\"Companies should provide disclosure aligned with the TCFD recommendations, including Scope 1 and 2 emissions and GHG emissions reduction targets\" (p8 \"climate and natural capital\").\n\nI know governments almost everywhere are increasing the need for GHG disclosures, but what caused the huge rush to learn about GHG from this BlackRock document?\n\nCan someone explain a little better to me?", "upvote_ratio": 0.67, "id": "t3_taa6xo", "created_utc": 1646839577.0}
{"sub": "investing", "title": "Silver in 2022: Is it a good investment?", "selftext": "Silver is the second most popular precious metal after gold and one of the go-to inflation investments. When I'm looking at everything that has been happening over the last few months, it feels like it's a good time to talk about silver again. What drives the price of silver and is silver a good investment right now? When it comes to silver, there are a few things that influence the price.\n\n# 1. Supply and Demand\n\nFirst of all, like any commodity you have supply and demand. What makes silver different from gold is the fact that silver is much more important for the industry. Gold is mainly mined for investment purposes with only 10% of the demand for gold coming from the industry. When it comes to silver, industry accounts for almost half of all demand! Plus, silver demand is growing because of industries like renewable energy and electric vehicles. Just in 2020, 101 million ounces of silver was used in solar panels! Electric vehicles also require several times more silver than normal vehicles. Silver is also widely used in electronics and also during brazing and soldering. Basically, there is a constant, growing demand for silver in the industry. In fact, this has led to the first silver market deficit in 6 years! In 2022, silver demand is expected to hit 1.112 billion ounces compared to a supply of only 1.092 billion ounces. It's a small deficit of only 20 million, but it still shows that demand has started to outpace supply. In 2021, silver demand was just 1.033 billion ounces compared to a supply of 1.056 billion ounces. Essentially, demand has gone up by 7.6% whereas supply has only gone up by only 3.4%. Again, there isn't a massive difference between supply and demand right now, ***but*** if the trend continues, this could really start pushing the price of silver up! Demand for silver jewelry and silver as a physical investment are also going up by more than 10% so that's yet another trend that can positively influence silver prices. I'll add the link to the resources in the description below for you take a look, too.\n\n# 2. Inflation\n\nThen, secondly, we have inflation. Commodities and precious metals are real, physical goods and as such their price is positively affected by inflation. If you are not sure why that's the case, let me break it down for you. Inflation is basically the decline of the purchasing power of a currency. Essentially, the same amount of money buys less of a specific good, commodity or service. Therefore, in times of high inflation, investors buy more commodities to preserve some of their purchasing power which is why these commodities are referred to as a store of value. Really, any asset that maintains its value over time can be a store of value which includes real estate, stocks, even currency when inflation is low. Now, the most popular commodity as a store of value is gold, followed by silver. The funny thing here is that, actually, gold and silver are not as strongly correlated to inflation as other commodities. However, they are still one of the classic picks for inflation hedging or safe-haven during volatile or bad market conditions.\n\nThat's ***exactly*** what we have right now. The current inflation levels around the world are record-high. The US, UK and Eurozone are hitting record inflation levels that we have last seen 30 or 40 years ago. In January, US Inflation hit 7.5%, UK inflation hit 5.5% with the Retail Price Index going up 7.8%, while the Eurozone and Canada's inflation rates both hit 5.1%. What's more, the current economic conditions are set for more volatility and even higher inflation. Banks have started to raise interest rates to stabilise inflation, but by the looks of it, it could be out of their control. Lumber prices are going up. Wheat prices are going up. Aluminium is going up. Oil is going up. Gas is going up. You get the point. The more these commodities go up, the higher costs will be for companies and the more expensive it will be to produce goods. This will put a pressure on inflation and it will likely continue to rise over the coming months. This is one of the main reasons why investors are looking at gold and silver right now. If inflation continues to go up and exceeds expectations, silver and gold will perform ***extremely*** well.\n\n# 3. Gold Price Correlation\n\nThe third factor that influences silver prices is the gold price! Silver is correlated with gold, meaning that the price of silver tends to follow the price of gold. When gold goes up, silver typically goes up, too. However, silver has a higher beta than gold which means that it is more volatile. If gold goes up 1%, silver is likely to go up by 2%. Vice versa, if gold goes down 1%, then silver may go down by 2%. This is another reason why there is so much interest in silver. The inflation situation could become worse with inflation going much higher than most investors and central banks expect. In that situation, gold will go higher in price, which would also lead to higher silver prices. However, as we now know, silver is more volatile and will likely see a much bigger increase in price than gold!\n\n# 4. Geopolitical Tensions\n\nThe final factor is geopolitical tensions. Anyone who has been watching the news recently will know that there are a lot of them right now. Like I said previously, gold and silver are two of the designated safe-haven assets for investors so in times of market or international turmoil, silver and gold typically go up in price. Gold and silver have been around since the dawn of civilization so it's no wonder that investors flock to them when they are worried and want some safety. That's another reason why we have seen a big jump in their prices in recent weeks, too.\n\n# Price predictions and forecasts\n\nNow, I guess the question on everyone's mind is how high can silver prices actually go? In March of 2020, silver skyrocketed to almost $30 dollars and has been moving between $22 and $28 dollars since then. Most recently, silver prices were floating around $22.5 dollars, but the war in Ukraine has pushed silver prices back up to almost $26 dollars. Can we expect even higher prices by the end of the year? Well, some analysts expect silver prices to remain the same for most of the year, before falling down to $22 dollars in December. Other analysts are more bullish and are seeing prices around $26 to $28 dollars, but they also lean towards lower prices by December 2022. Then, we've got the diehard fans of silver who are seeing silver prices of $500 to $1,200 dollars, but I think the chances of this happening are pretty minimal. Generally, there are not a lot of analysts who are seeing silver hitting the prices of up to $50 dollars which we saw in 2011 and most agree that by December 2022, we will see silver starting to decline and continuing that decline for a couple of years.\n\nObviously, that is not great news for silver investors and stackers, but I think that the analysts are a bit pessimistic in their price forecasts. We've seen that the demand for silver is going up and will continue to go up while the silver supply is shrinking. We've seen that there are inflationary pressures and geopolitical tensions around the world. Depending on when these cool down, I think that silver can go up to $30, even $35 dollars within the next 12 months, but it does make sense that prices will stabilise and decline after that as long as inflation cools down. Still, the current gold-to-silver-price ratio is just under 80 which means that silver is relatively cheap compared to gold right now. Over the last 25 years, the ratio has typically stayed around levels of 65 to 70 so silver is looking like a good purchase right now, at least to me and I am not a financial advisor and this is not financial advice. Plus, it is important to remember that assets like silver and gold are long-term investments so you should be prepared to be patient if you are buying in.\n\nSo, that's what I've got to say about silver right now. What do you think? Are you bullish on silver or not?", "upvote_ratio": 0.44, "id": "t3_ta9kde", "created_utc": 1646837804.0}
{"sub": "investing", "title": "Are Class B Shares Valuable?", "selftext": "Hi, just looking for some clarity on what a class B share is? \nI work for a private company that is offering me an opportunity to invest in Class B shares and I am just unsure about how this all works. How are they valuable to me? Are they worth investing? How do I make money? \nI have been with the company for 5 years and plan to retire from here.\nAny direction is greatly appreciated!", "upvote_ratio": 0.44, "id": "t3_ta8vly", "created_utc": 1646835772.0}
{"sub": "investing", "title": "403 rollover during unrest", "selftext": "Is it wise to rollover my work retirement accounts during this time of global unrest. My rollover will involve checks (more than one 403 account) that will be sent to me then forwarded to the new financial (vanguard) I don\u2019t need the money now and I would like to get all my tax deferred funds together. Unrest appears to be with us for a while. I retired recently.", "upvote_ratio": 0.8, "id": "t3_ta7tp2", "created_utc": 1646832647.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - March 09, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.84, "id": "t3_ta4jxo", "created_utc": 1646820069.0}
{"sub": "investing", "title": "What is a way I can have part of my paycheck direct deposited and have locked in for a year and not be able to withdraw it?", "selftext": "I've done a little research and I Bonds from US treasury direct is the closest I could find to what I'm looking for. Only problem is that the bond you could buy are capped at a certain amount yearly.\n\nI just need a way to not be able to withdraw the money for a predetermined amount of time and for the amount I could deposit to not be capped (so 401k and IRA don't qualify). Savings accounts don't cut it either because they just impose relatively meager penalty fees for withdrawing early or just a few days at best to transfer the funds to your checking account.\n\nIdeas?", "upvote_ratio": 0.59, "id": "t3_ta1teh", "created_utc": 1646808194.0}
{"sub": "investing", "title": "Is now the time to buy a 10 home package deal in Texas ?", "selftext": "\nLooking to purchase a 10 home portfolio in Texas. The cash on cash return would be around 15% . My concern is the current state of the economy inflation rising rates and just overall uncertainty. Would it be a bad time to invest a large amount of my cash into real estate ? I\u2019ve seen a lot of doom and gloom videos on YouTube and it\u2019s definitely been discouraging. They say real estate is a good hedge against inflation but does that apply to all situations ? Will there be an expected downturn in property values ?  If there\u2019s anyone that can provide sound advice as to the current affairs and overall view of the real estate market it would be much appreciated.", "upvote_ratio": 0.36, "id": "t3_ta0l8z", "created_utc": 1646803654.0}
{"sub": "investing", "title": "Tax Issue With Ally Invest", "selftext": "Hey everyone. \n\nSaw this here and I thought I would try posting here too.\n\n[https://www.reddit.com/r/investing/comments/k3cne4/is\\_ally\\_invest\\_really\\_this\\_bad/](https://www.reddit.com/r/investing/comments/k3cne4/is_ally_invest_really_this_bad/)\n\nI have a discrepancy with my 1099 and what is shown on the Ally Invest website. So apparently the tax lot allocation method I chose (HIFO and LIFO) sometime in 2021 was only for online viewing purposes and not for actual tax reporting. So now my 1099 reports a $4000 gain instead of what is shown on their website and what I was planning, which was a tax loss harvest of a -$4000 loss.\n\nI've used some margin on the account and I am skeptical that their system is calculating it right or even if I am missing funds on my account since it looks like it reflects their online website and not what is even being reported.\n\nDoes anyone have any insight on this and if there's anyway to get it resolved or compensated which is probably really unlikely? I just contacted them through chat and they said there's no way to change the tax lots and that my account is calculated correctly. Really fed up with their mickey mouse investing website and planning to move my account over to a new brokerage after this.", "upvote_ratio": 0.69, "id": "t3_t9vwfc", "created_utc": 1646788111.0}
{"sub": "investing", "title": "Iconic U.S. brands Coca-Cola, Pepsi and McDonald\u2019s suspend business in Russia", "selftext": "https://www.cnbc.com/2022/03/08/coca-cola-follows-mcdonalds-starbucks-in-suspending-business-in-russia.html\n\nPepsiCo, Coca-Cola, McDonald\u2019s and Starbucks each said Tuesday they are suspending business in Russia after that country\u2019s invasion of Ukraine, a symbolic step-back by four iconic U.S. brands.\n\nPepsi has sold its cola in Russia for more than six decades, even when the company had to trade its soda concentrate for Stolichnaya vodka and warships. McDonald\u2019s opened its first location beyond the Iron Curtain in Moscow, just months before the Soviet Union collapsed.\n\nIn recent days, Pepsi, Coke, McDonald\u2019s and Starbucks have drawn criticism for continuing to operate in Russia while other U.S. companies backed out and paused sales.\n\nYale Professor Jeffrey Sonnenfeld compiled and made public a list of U.S. companies that have withdrawn from Russia following President Vladimir Putin\u2019s invasion \u2014 and those that hadn\u2019t. Until Tuesday afternoon, Coke was among the most recognizable names on the spreadsheet.\n\n\u201cOur hearts are with the people who are enduring unconscionable effects from these tragic events in Ukraine,\u201d Coke said in a brief statement Tuesday afternoon. \u201cWe will continue to monitor and assess the situation as circumstances evolve.\u201d\n\nRussia represents one of the few regions worldwide where Coke\u2019s rival PepsiCo has a larger presence. In a regulatory filing, Coke said its business in Ukraine and Russia contributed about 1% to 2% of its consolidated net operating revenue and operating income in 2021.\n\nPepsi, on the other hand, generates roughly 4% of its annual revenue in Russia, though is not halting all of its Russian business. The company said it will keep selling some essential products, like baby formula, milk and baby food in the country.\n\nThe company will suspend Russian sales of its Pepsi-Cola, 7Up and Mirinda brands, along with capital investments and all advertising and promotional activities.\n\n\u201cAs a food and beverage company, now more than ever we must stay true to the humanitarian aspect of our business,\u201d PepsiCo CEO Ramon Laguarta wrote in a memo to employees viewed by CNBC.\n\nThe Wall Street Journal reported earlier on Tuesday that Pepsi was weighing different options for its Russian business, including writing off its value. Economic sanctions have greatly complicated the process of offloading Russian assets.\n\nYale\u2019s Sonnenfeld makes the case for companies ceasing operations in Russia\nSince the Russian invasion of Crimea in 2014, many U.S. companies have looked to reduce their exposure in both Russia and Ukraine. Some restaurant chains, like McDonald\u2019s, have sold off some of their company-owned locations to local franchisees.\n\nMcDonald\u2019s announced Tuesday all 850 of its Russian restaurants would temporarily close. Until then, the company had stayed silent on the war, drawing stronger criticism than even the handful of restaurant companies that condemned the invasion but kept their locations open.\n\nAbout 84% of McDonald\u2019s Russian locations are owned by the company, while the rest are operated by franchisees. Owning more of its restaurants means greater revenue for the company, but greater risk in times of turmoil or economic downturn.\n\nStarbucks went a step further than McDonald\u2019s, saying it would suspend all Russian business activity, including shipment of its products. Starbucks CEO Kevin Johnson condemned the attacks in a letter on Friday.\n\nOf the two restaurant companies, McDonald\u2019s has a larger presence in the country and receives a higher percentage of its global revenue from those sales.", "upvote_ratio": 0.95, "id": "t3_t9sxj6", "created_utc": 1646779336.0}
{"sub": "investing", "title": "Putin is banning the export of products and raw materials until the end of the year", "selftext": "From market rebellion, \u201cPutin is banning the export of products and raw materials until the end of the year\u201d what will this mean for stagflation and the US economy in terms of raw materials? What stocks will benefit?\n\nCommodities may continue their rise up the rest of the year and we are in for even more turmoil than expected", "upvote_ratio": 0.96, "id": "t3_t9sacw", "created_utc": 1646777523.0}
{"sub": "investing", "title": "Charles Schwab. What is everyone's thoughts and experiences?", "selftext": "Hello. I met with an independent advisor that works with Charles Schwab Alliance team. No red flags. Guys were professional, personable and very organized. They showed me all sorts of financial projections and 10 investment history of 9.9% average gains even through the great recession and covid19. \n\nIm planning on rolling a 401k and pension into their IRA management. I just wanted to hear peoples experiences, opinions and advice before i pull the trigger. \nTHANKS!", "upvote_ratio": 0.8, "id": "t3_t9o1aq", "created_utc": 1646765827.0}
{"sub": "investing", "title": "Great (Value) investors Opinions on crypto", "selftext": "A lot of talk in the world and the mainstream investing space is about crypto. The table shows what 5 great investors think of it, all value investors. I myself have a value investing approach (which still can be growth) and side most with Bill Ackman's opinion, I think the technology can create interesting things for businesses. \n\n|Warren Buffet| Thinks Bitcoin is rat poison squared. |\n|:-|:-|\n| Bill Ackman | Brilliant technology, not a place he feels comfortable putting any meaningful assets. Will not invest in any speculative cryptocurrency. |\n| Charlie Munger | Hates it, a huge mistake to allow it at all. |\n| Monish Pabrai | Thinks it will go to zero. |\n| Seth Klarman | Pure speculation, a bubble. Compares it to sardine traders ( similar to tulip bubble).|\n\n All the interviews and info [here](https://www.financialstockdata.com/article_investors_about_crypto)\n\nI myself am from the Netherlands and a few months ago a heard a lot of people in the train talking about it and that you should just open a binance account, you also now see a lot of commercials etc. It just reminds me of the famous quote that if your shoe cleaner talks about which stocks to pick then you are in a bubble. Not everyone can become an easy millionaire, I always have the idea you have to be in there before all these kind of people talk about it. Moreover, I just like looking at what a business is worth, that is investing for me.", "upvote_ratio": 0.62, "id": "t3_t9m1yy", "created_utc": 1646760608.0}
{"sub": "investing", "title": "How to value a company that conducts a lot of M&amp;A?", "selftext": "I have gotten pretty adept at creating DCFs for most businesses based on a variety of traits (high growth, low growth, negative growth, tech, oil, etc.), but I am a little stumped by a company I'm now trying to value which conducts obnoxious amounts of M&amp;A.  There business is trying to consolidate a fragmented industry in order to become a market leader.  As such, hundreds of millions are being spent on acquisitions each year.\n\nI suppose my biggest problem I have in trying to value this company is the unknown justifoication of each acquisition (while their mgmt seems competent, these are almost all private M&amp;A where financials aren't available thus I can't verify) and its impact on shares on account of dilution.  For instance, they might report 60% sales growth, but that could be driven entirely by an acquisition that was bought at a fair price and not really delivering added value to shareholders.\n\nDoes anyone have a recommendation, advice, or tips for valuing a company such as this one?\n\nEdit: For clarification, there are 30 to 50 acquisitions each fiscal and they are akin to acquiring family run dinners, chip wagons, and hot dog stands.  These aren't the actual businesses but this is an example to illustrate that each acquisition doesn't have a material impact on the business but when you add 100+ new businesses over the course of each year, it does impact the figures", "upvote_ratio": 0.8, "id": "t3_t9lk2u", "created_utc": 1646759329.0}
{"sub": "investing", "title": "What book does Dr. Michael Burry reference that shaped his investing methods?", "selftext": "I'm just looking for resources and recall Burry mentioning a many-decades-old book and I can't figure out what it was called. I would peg it as being pre-1940s. There are a few \"reading lists\" but he had said that this was the one that really laid the foundation for how he invests, himself.", "upvote_ratio": 0.78, "id": "t3_t9kor8", "created_utc": 1646757050.0}
{"sub": "investing", "title": "Where can I find more information on a bank's balance sheet?", "selftext": "Basically, Where can I find a split of all the balance sheet numbers of the banks? \n\nI am kind of trying to filter out some stuff, doing my research in places where there is absolutely nothing to see. \nAlso are circulars about any policies/regulation in banks public info? If so where can I find them too.\n\nI am only learning this stuff as a hobby, info through normal google is hard to find. \n\nAny links would be greatly appreciated.\n\n\nThank you.", "upvote_ratio": 0.54, "id": "t3_t9e6b6", "created_utc": 1646736420.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - March 08, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.78, "id": "t3_t9djc6", "created_utc": 1646733668.0}
{"sub": "investing", "title": "Advice: what to do when my company gives me free shares?", "selftext": "The company I work is supporting the employers to buy their stock.  \n\nIf I buy shares, the company gives me the 50% of what I bought for free (max 50 shares, ~5k\u20ac)  \n\nThe issue is that I already have a huge position of the stock and I don't want to increase the exposure.  \n\nMy idea was to sell 150 of my own shares and buy 100 with the offer, so I basically get 50 shares for free, however the current price is arround/below what I paid originally.  \n\nConsidering tax implications, what is the best movement I can do? What would you do?", "upvote_ratio": 0.73, "id": "t3_t9cqqm", "created_utc": 1646730125.0}
{"sub": "investing", "title": "When will you start buying stocks again?", "selftext": "I've already added mgm resorts at 40$ , might have been a bit early but that's OK. I have made a few trades on the vxx, but ultimately I'm mostly in cash.\n\nMy question is what will give you confidence to start buying tech stocks ,or your favorite stocks? \n\nGold declining? Maybe oil topping out? Or the vxx coming back down?", "upvote_ratio": 0.55, "id": "t3_t9bfs4", "created_utc": 1646724519.0}
{"sub": "investing", "title": "Morgan Stanley Says Russia\u2019s Set for Venezuela-Style Default", "selftext": "The odds of Russia making its foreign debt payments are diminishing as bond prices fall, recession in the nation looms and various payment restrictions pile up after the invasion of Ukraine, according to Morgan Stanley &amp; Co.\n\n\u201cWe see a default as the most likely scenario,\u201d Simon Waever, the firm\u2019s global head of emerging-market sovereign credit strategy, wrote in a Monday note. \u201cIn case of default, it is unlikely to be like a normal one, with Venezuela instead perhaps the most relevant comparison.\u201d\n\nThe default may come as soon as April 15, which will mark the end of a 30-day grace period on coupon payments the Russian government owes on dollar bonds due in 2023 and 2043, he said. \n\nsource: https://www.bloomberg.com/news/articles/2022-03-07/morgan-stanley-sees-russia-set-for-venezuela-style-debt-default", "upvote_ratio": 0.97, "id": "t3_t99dty", "created_utc": 1646716676.0}
{"sub": "investing", "title": "How to determine price/value of Ultra Short Term bond ETFs?", "selftext": "I've been looking at Ultra Short term bond ETFs such as JPST and have a hard time understanding what determines their value/price. \n\nAfter the initial pandemic response and price drop of these ETFs, they recovered and started to produce returns around 0%. The dividend payout of the ETF was equal to the price drop after the payout. I was told this was due to low interest rates, which makes sense. \n\nIn the past 6 months we have experienced high inflation and rate hikes seem certain, however the value of JPST has fallen further and the [returns are negative](http://quotes.morningstar.com/chart/fund/chart.action?t=JPST). Is this because interests rates are expected to rise, but current bonds are using the existing rates which makes them less valuable? Or is it related to tapering of FED purchases? Would the value of shares rise once higher interest bonds are added to the portfolio?", "upvote_ratio": 0.75, "id": "t3_t9935e", "created_utc": 1646715623.0}
{"sub": "investing", "title": "Question on sicp coverage on what it covers?", "selftext": "Just to note this doesn't have to do with any conspiracy around meme stocks , this comes from me listening to a podcast on Madoff's ponzi scheme.\n\nScenario 1) lets say I open a brokerage account and buy 200k of VTI and reinvested dividends. After 10 years it amounts to 500k. However my brokerage never actually bought the etf , they took the money and spent it on hookers and blow. So would SIPC cover the full 500k or just my 200k I invested.\n\nScenario 2) lets say I open a brokerage, and invest in a mutual fund, FRAUD. The brokerage thinks this is a valid mf and the brokerage isn't in on the fraud. I invest 200K into the mf and after 10 years have 500k. Except the mf is committing fraud and never actually invested money, they took it and spent it on hookers and blow.\n\nDoes this change anything would sipc cover 500k or only my 200k initial investment ?\n\nScenario 3) I open a brokerage with a robo advisor, this robot advisor buys\\sells stocks and etf. Again I invested 200k and after 10 years have 500k. However you guessed it, they never invested this money, it was spent on hookers and blow.\n\nDoes sipc cover the full 500k or my initial 200k?", "upvote_ratio": 0.83, "id": "t3_t98dnv", "created_utc": 1646713174.0}
{"sub": "investing", "title": "Ray Dalio\u2019s newest video about the potential New World Order", "selftext": "Youtube Link: [here](https://youtu.be/xguam0TKMw8)\nWhat is your opinion on his take? People like Ray Dalio and Peter Schiff have been calling for the collapse since forever so I kind of ignore them with short term investing but when I watched the video yesterday I was concerned as the situation in the US right now is eerily similar to the downfall of past superpowers.", "upvote_ratio": 0.74, "id": "t3_t96dtg", "created_utc": 1646706891.0}
{"sub": "investing", "title": "Switch brokers amidst massive losses?", "selftext": "Inherited some money from death of parent. No prior experience with investing. Former neighbor is SVP at Morgan Stanley so I called to see about getting started with them. Felt comfortable so I did. Since Nov 2021 through today I\u2019ve lost 31k. Today alone was 6k. I pay MS $300/month to \u201cmanage\u201d my investment.\n\nAm I missing something? Should I switch to a different brokerage?", "upvote_ratio": 0.61, "id": "t3_t95d4z", "created_utc": 1646703773.0}
{"sub": "investing", "title": "High growth investing strategy", "selftext": "I have a plan for the next 3 years which involves investing a large portion of my salary into a high growth ETF. I\u2019m just curious if there exists an etf that consistently grows at a rate of 10-14% annually (on average is fine). Does such a stock exist? \nPS: I did all my calculation via FV of money with a monthly payment over n=3 years", "upvote_ratio": 0.2, "id": "t3_t955uk", "created_utc": 1646703157.0}
{"sub": "investing", "title": "Help on question, mutual fund dilution", "selftext": "Question - someone today tried to argue to me that if any person buys a mutual fund and it goes up the next day, it by definition creates dilution to the fund the following day.  They make the assumption the money is never invested and it stays in cash.  \nWhile the math may work out, my \"analytical\" response is twofold.  \n1. It requires one knowing it's going up, which is not how you would test the hypothesis.  To test it, you'd have to assume randomness the following day.\n2. It ignores the fact that if the same thing happened the second day, (a person comes in knowing it's going up), he would also dilute some of your return out.\nAnyone else have any thoughts around this.  I know it's analytical wrong somewhere, but I can't put my finger on it precisely.", "upvote_ratio": 0.83, "id": "t3_t93sdl", "created_utc": 1646699001.0}
{"sub": "investing", "title": "We're in a bear market and it's going to get worse", "selftext": "TLDR - Even if the war in Ukraine ends tomorrow, we're still going to see permanent east-west trade disruption.  Over half the major global indices are in bear market territory and it can only get worse in the next 12 months.  Convince me I'm wrong!\n\nOver half of all major market indices are in bear market territory (including France, Germany, Netherlands, Sweden, HK, US Small cap, Korea).  A list of others are very close, including Nasdaq and nikkei, both 18% DD.  The only markets that are doing ok are those that have big energy/raw materials weightings.  This is of course driven by the war in Ukraine, although, slowing growth and rising inflation did the early damage.\n\nI don't see a route out of this that doesn't see a permanent restriction in East West trade.  As Russia and Ukraine control a significant percentage of gas, oil, wheat, metals exports then this can ony drive inflation higher still.  Central Banks will either have the choice of:\n\n- letting inflation rip and become embedded over many years. Result is staglation\n\n- raise rates to somewhat counter inflation.  However the removal of disposable income (due to inflation and increased debt costs) results in a recession.\n\nBoth options I think are severely underpriced/ unrecognised.  The market prices are already off, but I think we are likely to see 30-40% drawdowns (from peak) in many markets.\n\nWhat do you think?  I'm writing because I'm interested in other's views", "upvote_ratio": 0.66, "id": "t3_t8wpjf", "created_utc": 1646679957.0}
{"sub": "investing", "title": "ESPP - How is discount reported when you sell after you leave the company? (resign or retire)", "selftext": "As the title states - what happens with ESPP stocks after you've left the company, assuming you've held the stocks for at least 2 years? I understand that if you sell in a short period the 'discount' is considered income and will show up on your W-2 along with your wages.\n\nIs there another mechanism to report these discounts or will the sale of the stocks result in a W-2 to be generated even years after leaving the company? How is the cost basis determined?", "upvote_ratio": 0.69, "id": "t3_t8tye1", "created_utc": 1646672839.0}
{"sub": "investing", "title": "Cathie Wood says she still expects to see \u2018spectacular returns\u2019 over the next 5 years in CNBC interview", "selftext": "Video link: https://www.cnbc.com/2022/03/07/cathie-wood-says-she-still-expects-to-see-spectacular-returns-over-the-next-5-years.html\n\n**Personal breakdown:** Honestly this entire interview was borderline a satire. and I think at this point I've lost whatever respect I had for her. Let's see some gems from the article:\n\n&gt;Cathie Wood defended her firm\u2019s innovation-focused portfolio, saying she sees \u201cspectacular returns\u201d for Ark Invest over the next five years.\n\n&gt; \u201cGiven our expectations for growth in these new technologies, I think we\u2019re going to see some spectacular returns,\u201d the Ark Invest CEO told CNBC\u2019s \u201cCapital Connection.\u201d\n\nIs that why ARK [sold 11 million shares of $PLTR](https://seekingalpha.com/news/3804107-cathie-wood-appears-to-cut-ties-palantir-technologies-dumps-over-11m-shares) 2 weeks ago, after buying more just 6 weeks ago? Most retail investors I know have longer term convictions. \n\n\n&gt;\u201cWe\u2019ve been in a terrible bear market for innovation,\u201d she admitted. \u201cHowever, if you look from the bottom of the coronavirus to that peak [of the Ark Innovation ETF] in February of \u201921, we were up 358%.\u201d\n\nJesus. There is cherrypick and then there is let's-pick-the-absolute-bottom-to-the-absolute-top-and-pretend-the-last-12-months-didn't-happen cherrypick. I'm astonished she was able to say that with a straight face.\n\n\n&gt;\u201cYou\u2019d be amazed if you average down over time, how quickly a strategy can come back above that average. And if we\u2019re right, significantly above that average over the next five years,\u201d Wood said.\n\nGee, thanks, I never understood what averaging down meant. Thanks for the explanation Cathie.\n\n\n&gt;Wood said the world is currently facing \u201call kinds of problems\u201d and innovation is set be the answer.\n\nI swear to God I literally had this exact line in one of my middle school papers.\n\n\n&gt;Wood said the conflict is set to lead to \u201ca lot of demand destruction and substitution into innovation\u201d such as a switch toward electric vehicles away from those that are gas-powered.\n\nLook, I work in the tech industry, I like technology, and I believe in innovation. Hell I even drive a Tesla. But it's all about *valuation*, just like I like steak and I believe that I'll continue to like the taste of red meat but it doesn't mean I want to pay $100k for a cow right now.\n\n\n&gt;She attributed this to public markets being \u201cfilled with investors who are benchmark sensitive,\u201d\n\nIs she saying ARKK is for investors who don't care if their investment perform below benchmarks? I'm at a loss of words.\n\n\n&gt;opposed to private markets investors who see the \u201cexplosive growth opportunities\u201d in major innovation platforms.\n\nYes, \"explosive growth opportunities\" like WeWork or Nikola. Also does she not understand that in exchange for much higher risks, private investors get in at a much lower price for drastically increased upside? \n\n\n&gt;While technology is already a heavyweight in the S&amp;P 500, accounting for 28% of the index, Wood said those stocks are \u201cpart of the success in the past.\u201d\n\nTranslation: I missed the boat on everything except $TSLA so I'm just gonna...[screw it let me just buy more $TSLA](https://www.benzinga.com/news/22/02/25863103/cathie-wood-bought-another-3m-in-tesla-on-friday-here-are-other-key-trades).", "upvote_ratio": 0.67, "id": "t3_t8tb9k", "created_utc": 1646671194.0}
{"sub": "investing", "title": "Biden administration is moving ahead with a ban on Russian oil imports", "selftext": "WASHINGTON, March 7 (Reuters) - The Biden administration is willing to move ahead with a ban on Russian oil imports into the United States without the participation of allies in Europe, two people familiar with the matter told Reuters, after Russia's invasion of Ukraine.\n\nPresident Joe Biden is expected to hold a video conference call with the leaders of France, Germany and the United Kingdom on Monday as his administration continues to seek their support for a ban on the imports.\n\nThe White House is also negotiating with congressional leaders who are working on fast-tracking legislation banning Russian imports, a move that is forcing the administration to work on an expedited timeline, a source told Reuters\n\n**A senior U.S. official told Reuters that no final decision has been made but \"it is likely just the U.S if it happens\u201d**\n\nOil prices have soared to their highest levels since 2008 due to delays in the potential return of Iranian crude to global markets and as the United States and European allies consider banning Russian imports.\n\nEurope relies on Russia for crude oil and natural gas but has become more open to the idea of banning Russian products. read more The United States relies far less on Russian crude and products, but a ban would help drive prices up and pinch U.S. consumers already seeing historic prices at the gas pump. read more\n\nU.S. House of Representatives Speaker Nancy Pelosi said in a Sunday letter that her chamber is \"exploring\" legislation to ban the import of Russian oil and that Congress intends to enact this week $10 billion in aid for Ukraine in response to Moscow's military invasion of its neighbor.\n\nA bipartisan group of U.S. senators introduced a bill on Thursday to ban U.S. imports of Russian oil. The bill is getting fast-tracked.\n\nAfter Russia invaded Ukraine, the White House slapped sanctions on exports of technologies to Russia's refineries and the Nord Stream 2 gas pipeline, which has never launched.\n\nSo far, it has stopped short of targeting Russia's oil and gas exports as the Biden administration weighs the impacts on global oil markets and U.S. energy prices.\n\nAsked if the United States has ruled out banning Russian oil imports unilaterally, U.S. Secretary of State Anthony Blinken on Sunday said: \"I'm not going to rule out taking action one way or another, irrespective of what they do, but everything we've done, the approach starts with coordinating with allies and partners,\" Blinken said.\n\nAt the same time, the White House did not deny that Biden might make a trip to Saudi Arabia as the United States seeks to get Riyadh to increase energy production. Axios reported that such a trip was a possibility.\n\n\"This is premature speculation and no trip is planned,\" a White House official said.\n\nA year ago Biden shifted U.S. policy away from a focus on Saudi Crown Prince Mohammed bin Salman, who is considered by many to be the de facto leader of Saudi Arabia and next in line to the throne held by the 85-year-old King Salman.\n\nhttps://www.reuters.com/business/energy/us-prepared-move-alone-banning-russian-oil-imports-sources-2022-03-07/", "upvote_ratio": 0.97, "id": "t3_t8taem", "created_utc": 1646671137.0}
{"sub": "investing", "title": "What happens to RUB fx swaps", "selftext": "Not sure this is the right sub, but out of curiosity what happens to those who have existing fx swaps using the Russian RUB. With current sanctions are they still allowed or what happens with the due date?\n\nI see with bonds they are looking to repay in RUB only, however fx swaps are private agreements so does that mean they still go ahead, but if the parties are Russian based they can\u2019t receive the RUB or pay with $.", "upvote_ratio": 0.78, "id": "t3_t8sgdj", "created_utc": 1646668907.0}
{"sub": "investing", "title": "Intel's Mobileye confidentially files for U.S. IPO", "selftext": "Looks like Intel is spinning off MobilEye. Looks like it may have been a good investment for Intel since they bought the company for $15.3 billion in 2017. Also, it looks like this is Intel moving to focus on their semi manufacturing core competencies. I wonder if the market is still frothy for self-driving car companies though. \n\nhttps://finance.yahoo.com/news/intels-mobileye-confidentially-files-u-140858599.html\n\n&gt;(Reuters) -Intel Corp said on Monday its self-driving car unit, Mobileye, has confidentially filed paperwork for an initial public offering in the United States.\n\n&gt;The company did not give more details about the offering, which could value the Israeli unit at more than $50 billion, a source previously told Reuters.\n\nDisclosure: INTC shareholder", "upvote_ratio": 0.95, "id": "t3_t8rc8w", "created_utc": 1646665909.0}
{"sub": "investing", "title": "Is there SCHW Bias when using Schwab?", "selftext": "TL;DR: If someone uses Schwab as their brokerage, and wishes to invest in the total stock market by buying SWTSX (Schwab Total Stock Market Index Fund), is the investor over/under-weight on SCHW (Charles Schwab Corporation) due to the non-zero (currently 0.03%) expense ratio?\n\n---\n\nFirstly:\n\nThe Charles Schwab Corporation is in the top 100 US companies by market cap, in the hundreds of billions. Would investing in their total market index fund overweight or underweight the share of SCHW when using Schwab as a brokerage? Paying the expense ratio on SWTSX would increase Schwab's profit. I realize the effect would be less than negligible for a single investor, but I am curious about this theoretically, as Vanguard and Fidelity are not corporations.\n\nI wonder if this could be analogous to possibly being overweight in a corporation that you are employed by.\n\n---\n\nSecondly:\n\nIt's also interesting that Vanguard and Fidelity are actually enabling massive investment in their direct competitor via total market funds, whereas Schwab does not/cannot enable investment in either of its competitors. If the investment in SCHW via users of Vanguard and Fidelity increases the value of Schwab more than Vanguard's and Fidelity's total stock market funds' expense ratios increase their own private values, then could that theoretically allow Schwab to undercut their expense ratios and gain more customers, or would the subsequent decrease in Vanguard and Fidelity customers cause some sort of equilibrium condition?\n\nI suppose, for example, Vanguard makes profit from investors using an ETF like VTI in their Schwab account, so that would cause some offset.", "upvote_ratio": 0.58, "id": "t3_t8ifk3", "created_utc": 1646632615.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - March 07, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.85, "id": "t3_t8lwyk", "created_utc": 1646647269.0}
{"sub": "investing", "title": "Will the United States become energy independent?", "selftext": "A growing question politicians and economists are starting to ask themselves, will the United States become energy independent? With the ongoing war between Russia and Ukraine, the US has sanctioned Russia and is determining whether or to impose tougher restrictions on the Russian energy sector, such as ban Russian oil imports.\n\nWhite House press secretary Jen Psaki outlined a range of efforts to increase production of natural gas and oil, but conceded that \"domestic production has not insulated us from the price volatility of fossil fuels or the whims of those who control them, such as President Putin. Americans know that.\" She stated \"The only way to protect US over the long term is to become energy independent\".\n\nSo, this isn't just a concentration on becoming energy independent; it is a focus to reduce the dependency on Russian fossil fuels while still focusing on clean energy to help mitigate global warming.\n\nCouncil of Economic Advisers Chair Cecilia Rouse told reporters at a Friday press briefing, \"We are looking at options that we can take right now, if we were to cut the US consumption of Russian energy\". I believe the only way to become energy independent and still focus on mitigating global warming is to make huge advances in the clean and renewable energy sector.\n\nSo, with that being said, do you think the US will become energy independent? And if yes, what petroleum companies look good right now? What clean and renewable energy companies out there do you think will have the upper hand on receiving government grants to help the US succeed in this endeavor?", "upvote_ratio": 0.85, "id": "t3_t8j9zf", "created_utc": 1646636037.0}
{"sub": "investing", "title": "I want to do a bit of sentiment analysis on this sub, how many of you think that it is possible to beat the market consistently and is worthwhile trying?", "selftext": "Many people cite the statistic \"90% of day traders lose money in the long run\" or \"85% of fund managers are unable to beat the market.\"\n\nDo you think its actually possible to beat the market consistently by picking stocks or trading options? Or would you be better off with everything in VOO or VSTAX? And if it is \"possible\" do you still think it is worthwhile trying? Or should you just give up trying because the returns don't justify the time invested?", "upvote_ratio": 0.55, "id": "t3_t8gmy5", "created_utc": 1646625999.0}
{"sub": "investing", "title": "Oil likely headed higher over US discussing oil sanctions", "selftext": "I expect the market to price in the discussion of banning Russian oil imports, sending oil prices higher.  The two things I find interesting are how Europe feels pressure from stories of civilian casualties, and the idea of sanctioning Russian oil without impacting Russian gas.  Until this is resolved, I expect higher oil prices and maybe some stock losses in trading Monday, March 7.\n\n\"*Secretary of State Antony Blinken says the United States is talking to its European allies about banning Russian oil imports.\"*\n\n[https://www.aljazeera.com/news/2022/3/6/us-in-active-discussion-to-ban-russian-oil-imports-blinken](https://www.aljazeera.com/news/2022/3/6/us-in-active-discussion-to-ban-russian-oil-imports-blinken)", "upvote_ratio": 0.94, "id": "t3_t8atr4", "created_utc": 1646607506.0}
{"sub": "investing", "title": "Opinions on REITs, REIT Indexes, and Their Correlation to Other Assets?", "selftext": "I'm relatively new to investment. My default approach has always been indexes or ETFs that target large cap stocks, small cap stocks, international stocks, and bonds. However, no one can really sit by and ignore the massive housing shortage in the US/elsewhere. Housing supply will likely continue to lag demand for several years. \n\nWhile that sucks for my generation, it is why I'm considering adding REITs to my portfolio. Something tells me RE will be fairly profitable in the long-term. I am curious what the community thinks about real estate stocks in general.\n\n* What's the opinion on REITs generally?\n* Are REITs substantially correlated with other US stocks? International real estate companies?\n* What is the opinion on indexes/ETFs/funds that exclusively trade REITs (or at least hold a significant portion of REIT stocks? \n* Are REITs worth anything for long-term portfolios?\n\nFor those interested, I found some ETFs/indexes that operate on REITs:\n\n* (Vanguard ETF) [VNQ](https://investor.vanguard.com/etf/profile/VNQ)\n* (Vanguard Index) [VGSLX](https://investor.vanguard.com/mutual-funds/profile/VGSLX) \\- Index counterpart to above\n* (Fidelity Index) [FSRNX](https://fundresearch.fidelity.com/mutual-funds/summary/316146232)", "upvote_ratio": 0.78, "id": "t3_t89ks4", "created_utc": 1646603976.0}
{"sub": "investing", "title": "1031 exchange , if I bought under my name can I transfer or buy through my llc via 1031 ? Or does it have to be under my name only since I bought it that way?", "selftext": "\nI bought a property last year under my name, I\u2019m looking to buy another via 1031 exchange. The only issue is that I plan to use a lender that requires me to buy under an llc. Will that invalidate the 1031 exchange since I bought the property under my name alone ? The llc is under my name as I\u2019m the only member at 100% . If I go through with a 1031 exchange from personal to llc will I end up paying capital gains ?", "upvote_ratio": 0.58, "id": "t3_t8922p", "created_utc": 1646602553.0}
{"sub": "investing", "title": "How do I know which type of fund/ETF's to choose for my 401K vs my Roth IRA vs my normal brokerage account?", "selftext": "They seem to all have the same funds/ETF equivalent, how do I know what to focus on for each type of account? I'm 24 and my goal is to retire at 62. My plan right now has been TD 2060 for both Roth IRA and 401K, but now I'm wondering if there is an ETF (or a few) that would be better for overall growth. I'm younger so I can afford more risk. For my normal brokerage account, this is where I'm really having trouble. I want to (over time) put a lot into dividend income fund. I would like to supplement my retirement income with Monthly (or quarterly) dividend payments. There are so many different options, I think I finally decided on SCHD but I don't know what to pair with that. I was thinking 5-10% portfolio in $BND for a bit of security (probably 5%), 40-60% SCHD, maybe add in 30% VT for a bit more security? There's so many options", "upvote_ratio": 0.6, "id": "t3_t88199", "created_utc": 1646599749.0}
{"sub": "investing", "title": "Middle ground ETF between proportional and equal weight", "selftext": "I am looking for proportional ETFs that do not put too much weight on the top companies. ETFs that track an index (say S&amp;P 500) have a high share of top companies (for instance Apple and Microsoft combined make up &gt; 13% of S&amp;P 500 at the moment). On the other hand, I find equal weight indexes too rigid and don't like the idea of 499-th company having the same weight as the 1st. \n\nIs there an index or an ETF which have proportional structure but with the largest companies capped at say 1%? Or the weights depend on the capitalization, but not in a proportional way? Of course one way to deal with it is to get a combination of ETFs based on a proportional weights and equal weights, but I wonder if there is another option.", "upvote_ratio": 0.67, "id": "t3_t83wde", "created_utc": 1646588337.0}
{"sub": "investing", "title": "Can someone explain to me how Amazon has money for their infrastructure if they don't turn a profit?", "selftext": "I would insert a picture of Amazon revenue vs profits, but images are not allowed. Their profits are almost flat while revenue keeps increasing. It is a part of their business strategy. But my question is how do they have money for buying infrastructure, new trucks, expanding, making fulfilment centres if they don't make any profit, like where do they get their money from?", "upvote_ratio": 0.57, "id": "t3_t835yp", "created_utc": 1646586314.0}
{"sub": "investing", "title": "Cash left to buy the dip?", "selftext": "Lately there has been a lot of opportunities for dip buying. With Covid and then omicron and now the war. There have been some good dips that a lot of people including me went on a shopping spree on. Now we have inflation which is biting into my wallet and with each dip I dug deeper into my wallet. \nI find myself out of steam to do any strong buying and have to revert back to a smaller portion of my monthly income making it's way into my portfolio. \nWhat is the situation for others? Are you still holding to a good chunk of cash? How low are you willing to go on your cash reserves? What's the strategy here?", "upvote_ratio": 0.55, "id": "t3_t7ynsb", "created_utc": 1646573037.0}
{"sub": "investing", "title": "Russian creditors to be paid in Rubles", "selftext": "\nPutin has signed a decree that creditors will be paid in Rubles according to a report from Bloomberg today.  This will apply to sovereign and corporate debtors paying countries who are hostile to Russia. List of countries to be published later today. Guess all the NATO countries will be on the list\n\nEdit: Check Bloomberg.com - article is behind a pay wall if anyone can view and copy", "upvote_ratio": 0.97, "id": "t3_t7zia8", "created_utc": 1646575768.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - March 06, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.8, "id": "t3_t7vpgd", "created_utc": 1646560869.0}
{"sub": "investing", "title": "A question about leveraged ETFs", "selftext": "A strategy involving leveraged ETFs to boost yields is nothing new. I am aware of certain risks inherent to leveraged &amp; inverse ETFs, specifically capital erosion occuring in regards to volatility in the the underlying pver long(er) time horizons.\n\nThis is compounded by the drag on returns from high transaction costs &amp; tax considerations.\nI plan on using the $MJXL and $MJIN ETFs, but I have a few questions concerning the best approach.\n\n I plan on using 50/200 WMA &amp; RSI to confirm bullish trends, and then swing trade utilizing the 2x leverage offered by $MJXL, while simultaneously buying Puts on $MJIN. \n\nMy main question is in regards to whether I should go long by holding shares or Calls and what is the optimal timeframe to hold contracts? 90-120 DTE upon trend confirmation?", "upvote_ratio": 0.7, "id": "t3_t7n06z", "created_utc": 1646527163.0}
{"sub": "investing", "title": "Mastercard Suspends Operations in Russia", "selftext": "[https://www.businesswire.com/news/home/20220305005035/en/](https://www.businesswire.com/news/home/20220305005035/en/)\n\n\"This decision flows from our recent action to block multiple financial institutions from the Mastercard payment network, as required by regulators globally.\n\nWith this action, cards issued by Russian banks will no longer be supported by the Mastercard network. And, any Mastercard issued outside of the country will not work at Russian merchants or ATMs.\"\n\n\"We don\u2019t take this decision lightly. Mastercard has operated in Russia for more than 25 years. We have nearly 200 colleagues there who make this company so critical to many stakeholders. As we take these steps, we will continue to focus on their safety and well-being, including continuing to provide pay and benefits. When it is appropriate, and if it is permissible under the law, we will use their passion and creativity to work to restore operations.\"\n\nThe Purchase, New York-based company said late Tuesday [in a filing with the Securities and Exchange Commission](https://d18rn0p25nwr6d.cloudfront.net/CIK-0001141391/e33452e6-2006-4390-8bbd-6c762062e723.pdf) that about 4% of its annual revenue derives from business related to Russia and about 2% derives from Ukraine.\u00a0", "upvote_ratio": 0.95, "id": "t3_t7m8dj", "created_utc": 1646524667.0}
{"sub": "investing", "title": "Bond Ladder, how in the world.", "selftext": "I would like to start a bond ladder. I was going to use $5k to start and see how it goes. I like the idea of frequent turn over and just reinvesting in a new bond. I have done some reading but I am still not sure how to accomplish this. Any advice?", "upvote_ratio": 0.5, "id": "t3_t7l24i", "created_utc": 1646521009.0}
{"sub": "investing", "title": "Negative equity in some stocks.", "selftext": "Whats the general thought on either declining bv or negative bv in some companies. One like DPZ which has great financials except a continued negative growing p/b. \n\nI know it means they are asset light, but it also means they are cash to leverage/debt high and going higher. \n\nAny thoughts on this?", "upvote_ratio": 0.74, "id": "t3_t7hnj7", "created_utc": 1646510806.0}
{"sub": "investing", "title": "Europe Fears It Could Be Too Late to Shake Off Russian Gas Addiction", "selftext": "https://www.wsj.com/articles/europe-fears-it-could-be-too-late-to-shake-off-russian-gas-addiction-11646476200\n\nThe European Union gets around 40% of its gas from Russia\u2014and that dependence has grown in recent years. This week, as the West sought to hobble Russia with sanctions, the EU was paying as much as 660 million euros\u2014roughly $722 million\u2014a day to Russia, according to the Bruegel think tank. ***That is triple the level before Russia invaded Ukraine.***\n\nReplacing Russian gas is easier said than done, however. Many liquefied natural gas terminals that receive deliveries from the U.S. and Qatar are maxed out. Two new terminals approved by the German government this week will only be built in three years at the earliest, one of the companies involved has said.\n\n\u201cEurope thought that energy security was something from the past, it was not a real risk anymore,\u201d Mr. Tagliapietra said. \u201cWe just sat and did business as usual.\u201d\n\n\u201cIf the Russian gas stops, there will be no ceiling for prices,\u201d said Martin Vladimirov, director of the energy and climate program at the Sofia, Bulgaria-based think tank Center for the Study of Democracy.", "upvote_ratio": 0.83, "id": "t3_t7hc25", "created_utc": 1646509890.0}
{"sub": "investing", "title": "Can someone please explain MACD to me without speaking like an asshole.", "selftext": "I\u2019m trying learn what the MACD (moving average, convergence, divergences) but every video and articles I find on it explains it like I have a fucking degree in astrology because the shit their saying is going right fucking over me.\n\nCan someone please just give me a basic rundown of what it is and how to read it.", "upvote_ratio": 0.87, "id": "t3_t7fq88", "created_utc": 1646505246.0}
{"sub": "investing", "title": "Where to park cash for the short term?", "selftext": "A significant portion of my compensation is in the form of Restricted Stock Units. Thankfully, my company is well run and the stock price has been growing slowly and steadily. This is not a startup, nor a FAANG company, so I have decided not to put all my eggs in a single basket and am diversifying 50% of the shares on each vesting date. I have two goals - form the core of that 6-12 month \"safety net\" and also start saving for the down payment on a house.\n     \nWhat's the best option to \"park\" cash in a more or less safe manner? Currently have the \"other 50%\" in a Schwab money market account, but I'm wondering if there are better options.\n    \nTL;DR - What's the best lower-risk vehicle for holding cash with reasonable liquidity?", "upvote_ratio": 0.73, "id": "t3_t7f4vt", "created_utc": 1646503558.0}
{"sub": "investing", "title": "Net loss vs positive eps. How?", "selftext": "How does a company post positive eps but have a net loss? I don\u2019t understand. The 4th and 5th bullet are confusing to me and seem contradictory. \n\nHere\u2019s the bullets from the q4 earnings. \n\n\nFourth Quarter Revenue of $777 million, up 9% Year-over-Year; 2021 Revenue of $3,010 million, up 11% Year-over-Year \n\nFourth Quarter Core Revenue of $734 million, up 11% Year-over-Year; 2021 Core Revenue of $2,827 million, up 14% Year-over-Year\n\nRecord Fourth Quarter Bookings of $329 million, an increase of 12% Year-over-Year; 2021 Bookings of $1,031 million, a decrease of 8% Year-over-Year\n\nFourth Quarter Net loss of $(83) million, or $(0.39) per diluted share; 2021 Net loss of $(218) million, or $(1.05) per diluted share\n\nFourth Quarter Non-GAAP Earnings Per Share of $0.25, down 4% Year-over-Year; 2021 Non-GAAP Earnings Per Share of $0.97, up 17% Year-over-Year\n\nFourth Quarter Cash Flow From Operations of $60 million; 2021 Cash Flow From Operations of $371 million", "upvote_ratio": 0.81, "id": "t3_t7etug", "created_utc": 1646502694.0}
{"sub": "investing", "title": "Understanding stock returns are clustered and that is NORMAL.", "selftext": "Okay folks out there seem to think (seems logical enough) that stock returns are linear.  They get upset when they hold stocks and they don't return like a bond or CD whose distributions are set forth in a specific manner specified ahead of time.  That is NOT how the stock market rewards its investors for taking on risk.  Below may be a bit of an eye opener for some and a good reminder to others who already know... \n\nALL of the market returns are clustered to just a few days of the ENTIRE time you hold them.  Yes, that is true.  Link below proves this point.   It shows in the 18 year period ending in 2018 if one missed just the best 20 day period your return would have been -0.33% vs. being fully invested INCUDING those best 20 days which would have been:  +5.6%.  How significant was that?  Well if you were in cash for that entire ENTIRE 18 year period you return would have been:  +1.79% (thanks portfoliovisualizer)!  That means if you missed just the best 20 trading days of that near 2 decade period of time you would have earned LESS in stocks by 2% annualized then just being in cash for the whole time period!\n\nIf one assumes 253 trading days for the year (thanks google).  That means missing the wrong **0.5%** of all trading days gets a return less then cash (=20/18 x 253).  \n\nSince NO ONE knows which 20 days are the best 20 days your only option is to hold stocks through the ENTIRE period of time.  Yes reinforcing the mantra, \"time in the market and not timing the market\".  Some may find it interesting as well as the BEST 20 days are nearly always clustered around the WORST 20 days.  So, when the worst days in the market happen when your mettle is being tested is the exact time to do nothing and stay invested so one doesn't miss the best days.\n\nHope this helps some folks understand stock returns are NEVER linear and is the reason returns are higher then that of fixed income holders (cash and bonds).\n\n[https://www.fool.com/investing/2019/04/11/what-happens-when-you-miss-the-best-days-in-the-st.aspx](https://www.fool.com/investing/2019/04/11/what-happens-when-you-miss-the-best-days-in-the-st.aspx)", "upvote_ratio": 0.89, "id": "t3_t7es9a", "created_utc": 1646502566.0}
{"sub": "investing", "title": "On risk tolerance. Some people may never invest in stocks again.", "selftext": "In the late 90s, I was making a great salary, aggressively saving, living frugally, but for investing was risk averse, by nature. I did not want to lose money I had earned and saved.  I still don't.\n\nFull blown dot com mania rolled around when the globe.com went public in late 1998.  I played around with an ETrade account during the .com bubble, making harmless $5k trades, no options, took profits way too early, but followed the market action very closely.  I dabbled lightly and did some IPO trades, some premature shorting, and mostly used it as a positive learning experience.   No harm, no foul.   I was mostly a coward, happy to have my money safe on the sidelines, CDs and bonds, but the dot com bubble was spectacular on the upside and the downside.\n\nHowever, the experience left me very distrustful of the entire game.  Investment banks underwriting bogus websites with no earnings to sucker bagholders which turned out to be mom and pop mutual funds, pension funds, and day traders who didn't actually daytrade.  Insiders cashing out of bogus IPOs to the retail sucker.  The whole thing seemed like a huge scam.  A bullet that I dodged since I didn't get too caught up in the game.  I had a day job and did not quit it, and kept earning and saving.\n\nThe moral of the story is that I never really went full hog into the stock market.  I am one of those people.  Missed out on tons of tech gains for the next 20 years.  All due to the skepticism developed during the dot com bubble.  Threw the baby out with the bathwater.   And I didn't even lose any money!!\n\nFast forward to COVID bubble, and some new investors have gotten clobbered in the last year.  Meme stocks, SPACs, and scam IPOs are down 50% to 75% of peak values.  It's the exact same thing.   A lot of people will hold underwater stocks until $0 or until they die, if they never break even.  But, that's another conversation.\n\nI get the feeling this recent bear market will scar some of the victims for life.  My advice is to learn from your mistakes (YOLO options trades, bag-holding meme stocks, invest in diversified ETFs, etc) ....Whatever you see as the lesson for you and move forward with your financial lives.   Do not throw the baby out with the bathwater and stuff cash in your mattress for the rest of your life.  Because there are a bunch of people reading this who are at risk of doing exactly that, which to be fair, isn't the worst thing in the world.\n\nPlease don't hate on me.  I am just sharing my experience for those who may find it useful.  I am happy to answer any further questions about the pros and cons of overly cautious investing, LOL.\n\nHolding: Cash\n\nEdit:\n\nToday, the bear market today is limited to certain IPO, growth, and meme stocks.  The ones down 50%+  In the current market, some individual stocks have been clobbered, just like .dom bust, but this does not affect the broad market like .dom crash\n\nNo, today is nothing like .com crash.   IVV lost half its value from 2000 to 2002. Those who invested in QQQQ went from $100+ to $20. Huge market index down 80%. Those who invested in SPY went from $145 to $80. Huge market index down 50%.    Back in 2000-2002 SPY and QQQQ went down 50% and 80%. Cisco Microsoft Intel were all crushed along with fake .com stocks. Nothing escaped the bear.", "upvote_ratio": 0.63, "id": "t3_t7b506", "created_utc": 1646492210.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - March 05, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.92, "id": "t3_t76byw", "created_utc": 1646474469.0}
{"sub": "investing", "title": "Is delisting an entire country the right thing to do?", "selftext": "It appears all Russian stocks will be removed from the S&amp;P Dow Jones by Wednesday. I have no idea how this will play out but it\u2019s very interesting Chinese stocks weren\u2019t delisted after the takeover of Hong Kong but could be removed if China invaded Taiwan: \n\nhttps://www.cnbc.com/2022/03/04/sp-dow-is-removing-all-russia-stocks-from-indices-stripping-country-of-emerging-market-status.html", "upvote_ratio": 0.31, "id": "t3_t74wil", "created_utc": 1646468162.0}
{"sub": "investing", "title": "Is there a mathematical significant difference if I stopped investing in one ETF and started from scratch in another?", "selftext": "Both ETFs are tracking the S&amp;P500, they\u2019re almost identical. One just has a lower expense ratio. \n\nI already invested a significant amount into ETF 1 (the more expensive one). If I started investing into ETF 2 (cheaper one) going forward (without selling any of my ETF 1), would there be any downside in terms of returns? \n\nI suppose I\u2019m just wondering if the capital I have in ETF 1 will have any benefits if I kept reinvesting in It vs just starting investing in the cheaper one instead and leaving the old one as is.\n\nHope this makes sense.", "upvote_ratio": 0.78, "id": "t3_t6tsp9", "created_utc": 1646430332.0}
{"sub": "investing", "title": "Wall Street Is Pouncing on Russia\u2019s Cheap Corporate Debt", "selftext": "https://www.bloomberg.com/news/articles/2022-03-03/wall-street-is-already-pouncing-on-russia-s-cheap-corporate-debt\n\nGoldman Sachs Group Inc. and JPMorgan Chase &amp; Co. have been purchasing beaten-down company bonds tied to Russia in recent days, as hedge funds that specialize in buying cheap credit look to load up on the assets, according to people with knowledge of the private transactions", "upvote_ratio": 0.92, "id": "t3_t6tb61", "created_utc": 1646428988.0}
{"sub": "investing", "title": "What is $ERUS halted while $RSX not?", "selftext": "Why is trading for $ERUS halted while the biggest ETF $RSX continues on? \n\nDoes it have to do with the size of the underlying assets? How about holdings?   RSX is by far the largest with ERUS as number two.\n\nMaybe the exchanges they are in? RSX is in CBO and the others are in NYSE?\n\nOthers halted are FLRU and RUSL...", "upvote_ratio": 0.82, "id": "t3_t6qtaa", "created_utc": 1646422045.0}
{"sub": "investing", "title": "Growth stocks, good or bad time?", "selftext": "I made some money with puts on the way down for a lot of these growth stocks. But they seem so low now. How much lower can they possibly go? Stocks like SPCE FUBO SOFI as an example. All just beat down so hard. Once I think it feels like a good time to get in they drop another 10% in a day. Haven\u2019t pulled the trigger to jump in yet but it has to be close? Haha. What are your thoughts about stocks like these and the current prices. Avoid or start investing at these levels. Thinking about long term holds on these.", "upvote_ratio": 0.62, "id": "t3_t6qbhy", "created_utc": 1646420682.0}
{"sub": "investing", "title": "$JPIM leaps up 9% (small-cap Saas/PIM company)", "selftext": "Jasper Commerce ($JPIM.V), a new SaaS company to the TSXV, has jumped up 9% so far today. \n\nIf you haven't heard of $JPIM before it is a Product Information Management (PIM) solutions company. \n\nThe online service that $PIM  provides to eCommerce merchants allows them to easily merchandise &amp; manage their products from a single platform. This simplifies the process of online merchants importing product data into the PIM making for faster and more efficient sales.\n\nOnce merchants have uploaded their products, they can add additional data like images, product attributes, videos, marketing info, and inventory quantities all via $JPIM's web browser.\n\nPlus $JPIM has achieved comprehensive distribution through leading eCommerce marketplaces such as:\n\n\\- Shopify\n\n\\- BigCommerce\n\n\\- and Magento\n\nDefinitely worth looking into: https://investors.jasperpim.com/\n\nRight now $JPIM is trading at $0.36\n\nMC $20.909M", "upvote_ratio": 0.38, "id": "t3_t6q6v3", "created_utc": 1646420354.0}
{"sub": "investing", "title": "Where do you see the future heading? Investment ideas", "selftext": "Some of these will be obvious, these are just my thoughts of where the future is probably heading and as such will shape my investment strategy.\nSo here\u2019s my thoughts, what are yours?\nPlease feel free to pick apart my conclusions.\n\nTrains, specifically high speed rail is going to be bi go in the future. In an attempt to lower carbon emissions many countries are going to start banning short haul domestic flights when there is the option to use a train. France is already in this process.\nTrains are cheaper, better for the environment and far less hassle then planes.\n\nPeople eating healthy. Now I\u2019m not expecting everyone to turn into a vegan but people will start focusing on healthy eating a lot more. It\u2019s hard to say what big companies will benefit from this, but a company like McDonalds I will be interested to see how they are doing in twenty years. Yes they will adapt and people like McDonald\u2019s but hey can\u2019t exactly reinvent themselves into a healthy option. Yes they will be around still but there will surely be a significant decline. \n\nNew fuels, this is obvious I won\u2019t go into this.\n\nElectric planes, cars and trains. Once again obvious.\n\nSpace, a long way out.\n\nWar. There is always going to be war and given today\u2019s climate I see the d fence sector becoming the new tech sector.\n\nCyber security, with war will come cyber attacks. Cyber security will be a massive player in the future. \n\nCrypto, personally I don\u2019t particularly like crypto so much. But I can\u2019t see how it doesn\u2019t become massively adopted. If you look at Visa for example, retailers have to pay charges to do Visa transactions. Why would they keep paying those fees when they could just switch to crypto. PayPal will die imo.\n\nSustainable practices is once again obvious. \n\nSemi conductors, the demand is just so high.\n\nAnd my last point I have not spent much time on but that is free games and free software. If you look around these days there is so much great software or games that are free. Look at blender for example, costs nothing and it\u2019s a brilliant tool. At some point people will surely move away from the likes of Adobe etc and just use these free alternatives. Same goes for games, people are fed up paying $60 for a broken unfinished game when you can play a better game for free and decide if you want to put money into it as you enjoy it. Not pay up front and then be disappointed.\nBased on this last point, if I\u2019m right, this will have a massive negative effect on many behemoth companies like Microsoft, like Adobe, the list goes on.", "upvote_ratio": 0.63, "id": "t3_t6o7dp", "created_utc": 1646415059.0}
{"sub": "investing", "title": "Quick question about CFD (Contract for differece trading)", "selftext": "Hi, new investor here.\n\nI'm interested in long-term investing, basically DCAing a 80/20 VTI/VXUS ETF account, pretty simple. I trade using eToro (not many options in my country). \n\nI had bought VTI in eToro a while back, now I was interested in buying more and complement with VXUS.\n\nWhen I logged in eToro, it told me I couldn't buy VXUS because it's a CFD trade. I managed to bypass these by updating my profile, but doing some research it seems like CFD trading is an advanced investment strategy that comes with more risk. \n\nI'm wondering what's the difference between a normal VXUS ETF trade in eToro and this VXUS (CFD) that is the only one I can see in the broker? \n\nThanks in advance!", "upvote_ratio": 0.75, "id": "t3_t6n5km", "created_utc": 1646412272.0}
{"sub": "investing", "title": "Can Hood Stock ever recover?", "selftext": "Hey everyone, I was part of Robinhood's IPO at $40, sold out at $79, bought back in when I thought the stock price was low ($40.00), and been Avg. Down now for months, is there any hope this stock will bounce? It seems like it gains a penny and loses a dollar. Is it possible this stock will be delisted? Are the fundamentals still there? Why does the market hate this stock so much?\n\n&amp;#x200B;\n\nIt's like Go-pro all over again. Toy of the year and everything but the stock just kept falling. I got out early on that one.", "upvote_ratio": 0.57, "id": "t3_t6l949", "created_utc": 1646407196.0}
{"sub": "investing", "title": "Russia's second biggest oil company calls for an end to Putin's war", "selftext": "https://www.cnn.com/2022/03/04/business/lukoil-end-war/index.html\n\nRussia's second largest oil company has broken ranks with President Vladimir Putin.\n\nLukoil, which produces more than 2% of the world's crude oil and employs over 100,000 people, has called for an end to Russia's war in Ukraine.\n\nThe company's board of directors said in a statement to shareholders, staff and customers that it was \"calling for the soonest termination of the armed conflict.\"\n\n\"We express our sincere empathy for all victims, who are affected by this tragedy. We strongly support a lasting ceasefire and a settlement of problems through serious negotiations and diplomacy,\" the board added.\n\n---\n\nIt will be interesting to see if anything happens to management.", "upvote_ratio": 0.97, "id": "t3_t6ko3m", "created_utc": 1646405564.0}
{"sub": "investing", "title": "how to counter inflation in this case??", "selftext": "Hi i'm from Algeria. the currency here is (DZD) average income is 400-500$ per month\n\n[this is a chart of DZD TO USD for the past 5 years](https://imgur.com/n9KOKad) as you can see it keeps on going low and low\n\nso i'm thinking to use atleast 80% of my DZD money and buy GOLD and USD or EUR\n\nin this case what's the best move to do? is buying gold and holding it  for long term smart?", "upvote_ratio": 0.83, "id": "t3_t6jpxv", "created_utc": 1646402835.0}
{"sub": "investing", "title": "What is the best commodity investment of all time?", "selftext": "Up to now what have been the best commodity investments of all time? I'm just trying to understand commodities more and the different sectors.\n\nIs it something like gold, oil, etc?\n\nOr is it something like babe ruth baseball cards kept deadstock... or like packs from the year or something.\n\nOr is it something rooted in american history like war artifacts?\n\nOr is it art?\n\nOr is it something that is kind of like art? Like vintage louis vuitton luggage from the 1800's kept deadstock?", "upvote_ratio": 0.71, "id": "t3_t6inkn", "created_utc": 1646399434.0}
{"sub": "investing", "title": "Investing on a foreign stock exchange (things to consider)", "selftext": "I want to buy some individual stocks listed on the Polish stock exchange (Warsaw).\n\nAs I understand it, Fidelity allows you to do so (you have to call them)\n\nHowever, other than currency fluctuations, are there any other risks or considerations to owning stocks on a foreign exchange?\n\nI'm particularly thinking about dealing with taxes (foreign) and any special sort of reporting back home (foreign assets?). This is just speculation on my part, I don't know if such rules exist. \n\nAlso is the International trading commission per transaction or per share? (90PLN for Fidelity + Warsaw Stock Exchange)\n\nI'm based in US.\n\nHelp appreciated!", "upvote_ratio": 0.9, "id": "t3_t6gowj", "created_utc": 1646392355.0}
{"sub": "investing", "title": "We need to upgrade ESG Standarts to avoid investments in countries with low democracy (Russia/ China)", "selftext": "In the background of the events in Ukraine the importance of the ESG agenda is growing rapidly.\n\nSome investors are concerned with ethical component of the issue: no one really wants to be involved in financing of shelling and bombing of civil districts. Other investors are concerned with the financial part of the problem, because sanctions are really depreciating the asset base.\n\nThe discussion on ESG investment efficiency in comparison with the broad market is still non-finalized (example [Aswath Damodoran](https://aswathdamodaran.blogspot.com/2020/09/sounding-good-or-doing-good-skeptical.html)). But investment inefficiency in regiemes with low democracy now becomes more transparent. After the military invasion in Ukraine of the Russian Federation, the Russian market lost 75% of its market value. The Chinese economy depreciated about 35% from the beginning of 2021.\n\nThis statement is also confirmed by dozens of research articles (first of all by Daron Acemoglu and James Robinson), who affirm the statement that high-quality social institutes are the cause of economic prosperity. On the other hand, the lack of these institutes causes a lot of economic problems.\n\nSo, ESG standards should be upgraded. ESG standards should include indications that could evaluate risks, which are tied to lack of democracy and high-quality social institutes.\n\n This problem shines with ESG ranking systems. According to [Morningstar](https://www.morningstar.com/stocks/xnas/yndx/sustainability), Russian IT company Yandex (NASDAQ: YNDX) has lower ESG risks than most US companies. This issue became possible because of lack of political risk analysis. \n\nHow can we upgrade ESG Standards? \n\nFor example, Freedom 100 Emerging Markets ETF (Ticker: FRDM) includes only assets from countries, that meet following criteria:\n\n\\- **civil freedom**, such as absence of terrorism, human trafficking, torture, disappearances and detainments;\n\n\\- **political freedom**, such as rule of law, due process, freedom of the press, freedom of expression, freedom of religion, and freedom of assembly;\n\n\\- **economic freedom**, such as marginal tax rates, access to international trade, business regulations, sound money, and size of government.\n\nThis would help you avoid such crises in the future from ethical and investment standpoints. Unfortunately, today not all countries are united and confronting Russian aggression against Ukraine. A Vivid Example is China with lack of democracy.  \n Ask yourself \u2013 do passive investors still want to accept the risk of Chinese military invasion in Taiwan? Or accept risks of new violations of civil rights or LGBTQ pressure?  \n\nExcluding companies of regimes with lack of democracy from investment portfolios and upgrading ESG policies would allow investors to avoid financing such unethical actions and financial sanctions that would inevitably follow these actions.", "upvote_ratio": 0.62, "id": "t3_t6ge70", "created_utc": 1646391133.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - March 04, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.92, "id": "t3_t6fok4", "created_utc": 1646388068.0}
{"sub": "investing", "title": "Effect of interest rates on CapEx and OpEx, is it quite similar or highly different?", "selftext": "In the context of recent rate hikes, the question is how would higher rates affect CapEx and OpEx. We know that the FED sort of controls the short end of the yield curve. OpEx can be regarded as short term expenditures, as opposed to CapEx.    \n\nThis makes me postulate that rate hikes have a higher negative impact on OpEx compared to that on CapEx, since rate hikes might not have the same impact on the longer term debt compared to shorter term. However, the question is if the effects are drastically different, or not.    \n\nFor eg. is the impact on CapEx minimal enough that firms do not have to worry about it in their investment decisions, or because of the long duration effect, CapEx ends up having a significant impact, whereas although OpEx will get higher, it will get incurred anyway since it is like oxygen that the firm has to consume, and in comparison, firms and hence investors would worry more about the impact of rates on CapEx rather than OpEx...?", "upvote_ratio": 0.82, "id": "t3_t6fo3z", "created_utc": 1646388015.0}
{"sub": "investing", "title": "Bloomberg terminal commands", "selftext": "I got access to BBG terminal and love that I can chart historical option prices across the term structure. What are the other BBG command you frequently use and find useful?", "upvote_ratio": 0.83, "id": "t3_t6easw", "created_utc": 1646382044.0}
{"sub": "investing", "title": "A Snapshot of Russian economy?", "selftext": "An investment expert goes live on air and says his current career trajectory is to work as \"Santa Claus\" and then drinks to the death of the stock market.\n\nWith subtitles.\n\n[Link to video](https://mobile.twitter.com/peterliakhov/status/1499341576518217730?t=n2y4dLyruYyRKInc2i45lQ&amp;s=08) on twitter. What do you guys think?", "upvote_ratio": 0.86, "id": "t3_t6ctik", "created_utc": 1646376141.0}
{"sub": "investing", "title": "China will not join sanctions against Russia, banking regulator says", "selftext": "https://www.cnbc.com/2022/03/02/china-will-not-join-sanctions-against-russia-banking-regulator-says.html?&amp;amp;qsearchterm=sanctions\n\nBEIJING \u2013 China\u2019s banking and insurance regulator said on Wednesday that the country opposes and will not join financial sanctions against Russia\n\n\u201cEveryone is watching recent military conflict, or war, between Russia and Ukraine,\u201d Guo Shuqing, chairman of the China Banking and Insurance Regulatory Commission, said at a press conference in Mandarin, according to a CNBC translation. \u201cChina\u2019s position has been stated clearly by the Ministry of Foreign Affairs. Our international policies are consistent.\u201d\n\n\u201cRegarding financial sanctions, we do not support that,\u201d said Guo, noting particular opposition to \u201cunilateral\u201d sanctions, which he said don\u2019t effectively address problems. \u201cChina won\u2019t join such sanctions.\u201d", "upvote_ratio": 0.95, "id": "t3_t69u99", "created_utc": 1646365791.0}
{"sub": "investing", "title": "What do you personally feel is a fair price for SPY and QQQ?", "selftext": "I use the [buffet indicator](https://www.currentmarketvaluation.com/models/buffett-indicator.php) and the [Shiller PE ratio](https://www.multpl.com/shiller-pe) to judge fair market prices. I personally feel that SPY is fairly valued around $350-$375. Although, I don't feel like it will reach those values in the near term. I don't know much about tech stocks and I don't follow them  so I have no idea about them. What values for SPY and QQQ do you consider fair? What price would you buy as much as possible?", "upvote_ratio": 0.65, "id": "t3_t67a47", "created_utc": 1646357651.0}
{"sub": "investing", "title": "Looking for an account that employer can direct deposit paycheck to and the funds would automatically be converted into financial assets.", "selftext": "Stocks and crypto come to mind. But it could be any other type of asset (even illiquid or intangible).\n\nThe main goal is just to covert the disposable cash into something other then fiat as soon as it is deposited. I don't want to be the one to do it such as in the form of automatic transfers to other accounts. I'd like to cut that step out so that the paycheck would essentially be a non liquid (cash) asset as soon as it hits the account. \n\nI thought about Coinbase where the paycheck would be converted into a chosen crypto currency, but the problem with that is that the crypto currency could then be exchanged to cash on their platform/app at a single tap and then the cash sent to your debit card. I want it to be difficult to change into cash or lock the asset for a predetermined period of time. The point of that being to not be able to spend it on a whim. \n\nAny ideas?", "upvote_ratio": 0.27, "id": "t3_t65h43", "created_utc": 1646352152.0}
{"sub": "investing", "title": "Why do actively managed funds exist if they wont beat the market.", "selftext": "I am mainly talking about funds that invest in individual stocks. I can understand how some may want to invest more in bonds than stocks due to lower risk especially nearing retirement. But in general I would imagine that the goal is just to make money and as it currently stands it seems like nobody can beat the market. So why invest in a actively managed fund that is going to have a lot more fees if it is just going to have a worse result compared to putting it in something like the S&amp;P.\n\nI also see a lot of investors claim that you can't beat the market but then right after saying that try to predict the outcome of individual stocks. Is this not contradictory?\n\nI refuse to believe people are this stupid. There must be some reasonable explanation right?", "upvote_ratio": 0.86, "id": "t3_t62a23", "created_utc": 1646343186.0}
{"sub": "investing", "title": "What happens if no one takes the other side of an option?", "selftext": "Lets say I buy an AMZN call for $20k at the end of this year, which seems unlikely so people are happy to sell it to me. \n\nThen Amazon invents teleporting packages into homes, and their stock rallies to $50k over the next month. It is likely whoever sold that call would get margin called.\n\nWhat happens next? Who would want to buy this option knowing it's likely to cost them a ton of money? Is it possible I could not get paid if I make a huge bet and win? Or will someone always step in to take the other side of the call?", "upvote_ratio": 0.74, "id": "t3_t61gfo", "created_utc": 1646340864.0}
{"sub": "investing", "title": "Panasonic PCRFY, any reason why it is at 52 weeks low?", "selftext": "Is there a reason why Panasonic is at 52 week low at the moment or is just macro?  \nEVs are growing exponentially, partnership with Tesla on new batteries, why these low numbers?  \nis this a buy or am I missing some news that I should really know?  \nWanted to put some money in it in the past but never got there, now I have few bucks to invest and it looks as a good moment to buy this, but maybe I missed something.  \nThanks!", "upvote_ratio": 0.72, "id": "t3_t60mtb", "created_utc": 1646338631.0}
{"sub": "investing", "title": "How the hell does Uber lose so much money?", "selftext": "I\u2019m writing this post as I am walking somewhere and I have not had time to look through their statements, but how the hell do they lose so much money?\n\nThey dont have employee drivers, no vehicle fleets, drivers are paid when they complete a fare. \n\nAre there just too many extraordinary costs? Incidents? Legal Liabilities? This is a fascinating case because I remember years back people begging for an IPO, and theyve proven quarter after quarter that their business just absolutely sucks. I dont even know how they stay afloat with negative cash from operations. Issuing stock Im going to hazard a guess lol.", "upvote_ratio": 0.96, "id": "t3_t5ypwj", "created_utc": 1646333531.0}
{"sub": "investing", "title": "Weighting INTl index funds in index fund investing?", "selftext": "Ive been a follower of r/bogelheads for a while now. They strongly recommend a mixture of US and INTL to something of a 60/40 ratio.\n\nIve looked at historic returns on INTL markets all the way back to the 50's, for a long term investor (say 20 years), it seems the SP500 still comes out ahead.\n\n**Is it really to an advantage to invest almost 40% into intl markets?** Intl markets have performed really poorly in the past decade (yes I know that is a short time) and it's hard to imagine with the russia-ukraine conflict, the EU will be doing amazing in the near future.", "upvote_ratio": 0.79, "id": "t3_t5wvxl", "created_utc": 1646328715.0}
{"sub": "investing", "title": "Can we buy Ukrainian war bonds?", "selftext": "I read that they released some bonds, but do we, retail investors, can bid for those or get them on the secondary market?\n\nHere's an article: https://www.cnbc.com/2022/03/02/ukraine-raises-270-million-from-sale-of-war-bonds-to-fund-army.html\n\nIt lists some banks, but it's unclear if I could also participate through some platforms (I'm only on T212 at the moment).\n\nBesides, the listed banks are in an article that came out after the sale. Beforehand I couldn't find the institutions where the bonds would be available. Is there some go-to resource where one could find that information beforehand?", "upvote_ratio": 0.63, "id": "t3_t5wlyc", "created_utc": 1646328009.0}
{"sub": "investing", "title": "Question about what is considered a freeride violation", "selftext": "I just sold a stock and the money is still pending from the sale. I see i have money available to trade that makes up the amount that I sold the previous stock for. If I bought a new stock with that money before it settled would that be considered a free ride violation?", "upvote_ratio": 0.67, "id": "t3_t5wi9p", "created_utc": 1646327749.0}
{"sub": "investing", "title": "MSCI and FTSERussell will be removing Russia from their indicies.", "selftext": "Haven't seen this noted anywhere yet. Both have of announced Russia as 'uninvestable'. These are the two big boys as far as international is concerned, should expect the rest to follow suit. We should expect further dips as passive indicies liquidate Russian exposure.\n\nRussia isn't a huge piece of the global universe, before all this it made up 3% of the MSCI EM, but most expected this to take more time to implemented.", "upvote_ratio": 0.96, "id": "t3_t5upmv", "created_utc": 1646323020.0}
{"sub": "investing", "title": "Investing in ETF USD with EUR", "selftext": "Hi beautiful community\n\nI am based in Germany and am regularly buying ETF Vanguard FTSE All World UCITS (USD) Accumulating. \n\nHowever it's USD, and I invest Eur.\n\nI have seen in other threads that it's better to invest in the same currency EUR due to exchange fluctuations. Could you share more Insights on this topic? \n\nWhat are the risks if I keep investig in USD ETF? \n\nThe broker I use is Scalable Capital.\n\nWhat would be an equal ETF in EUR? \n\nAppreciate any insights", "upvote_ratio": 0.72, "id": "t3_t5r4yd", "created_utc": 1646312637.0}
{"sub": "investing", "title": "Help needed: Looking for historical short interest data of NYSE companies.", "selftext": "Dear reader, \n\nLike the title implies, I am looking for at least two years of (bi-monthly) historical data of NYSE companies' short interest data. This is a non funded research, so I am trying to keep things cheaply. There are various subscription services out there that do seem to offer these kinds of data, which I would be fine with if I am 100% certain that they include what I am looking for. \n\nMy ideal dataset would be a panel dataset with all NYSE listed companies (or at least a large and unbiased subsample of them), according to the above mentioned criteria. All resources I can find online about acquiring such data seem to have been either outdated, locked behind a subscription, or at least violate one of my criteria. If anyone could help me out on this one, and at least point me in the right direction, that would be VERY much appreciated.\n\nKind Regards,\n\nBHTA!", "upvote_ratio": 0.44, "id": "t3_t5pamk", "created_utc": 1646306086.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - March 03, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.84, "id": "t3_t5o82m", "created_utc": 1646301670.0}
{"sub": "investing", "title": "Any ideas for a low interest rate, high inflation environment?", "selftext": "So I'm quite disappointed by the Fed's suggestion of just a 0.25% rate hike in March when housing prices have nearly gone up 50-100% within the past few years in most metro cities, as well as bubbles occurring in meme stocks, and that prices in general have been going up. Something simple like the dollar menu McDonald spicy chicken burger is now $1.47 in my area and portion size is like half of what it used to be.\n\nI'm kind of at a loss here. While the supply chain disruption is there, it is also in part caused by the large demand of consumers. At this point, bonds and fixed income investments (i.e. Treasury, CDs, savings) they are all shit investments because of the low interest rate environment while inflation is at 7%.\n\nIt feel like the Fed is continuing to feed the asset bubble with cheap money. I just don't understand. This is the time to be more aggressive with hiking rates because the economy is actually doing quite well and can withstand some shocks.\n\nShould I participate in the musical chair game by putting my money back in the markets and trying to see when the music stops, or is there something that actually works well in a low interest, high inflation environment?\n\nedit: thanks for all your input. I will have to look into all your recommendations to figure out which approach I would be most comfortable doing.", "upvote_ratio": 0.81, "id": "t3_t5jd5t", "created_utc": 1646282500.0}
{"sub": "investing", "title": "\ud83c\udfafSteps of fundamental analysis", "selftext": "1. **Macroeconomic analysis.** The macroeconomic analysis involves analyzing capital flows, interest rate cycles, different metrics, world news.\n2. **Industry analysis.** The industry analysis consists of an analysis of the industry and the players that form the sector.\n3. **Situational analysis.** The situational analysis identifies strengths and weaknesses while finding product features that affect business development.", "upvote_ratio": 0.24, "id": "t3_t5j9o2", "created_utc": 1646282177.0}
{"sub": "investing", "title": "What metric cutoffs (eg, EPS &gt; 30) do you use to automatically rule out certain stocks?", "selftext": "Just curious what sort of cutoffs you use to easily rule out certain stocks. For example, I rarely consider any stock that has a PE ratio above 30. Also, I generally stay away from stocks with negative betas. I\u2019m not saying these cutoffs should be used  everyone, but it stops me from doing further DD into companies that probably won\u2019t fit into my investing strategy.    \n    \nEdit: meant PE ratio, not EPS.", "upvote_ratio": 0.76, "id": "t3_t5bywe", "created_utc": 1646260089.0}
{"sub": "investing", "title": "SPY - Why are we rising today? \ud83d\ude48", "selftext": "\\-This was one a hell of a month! Great fear + uncertainty + rising oil and gold + inflation + FED uncertainty + War + OPEC+ agreement and etc etc.\n\n&amp;#x200B;\n\n\\-Thankfully managed to make a couple of profitable trades, especially on spy! Now let's see what is waiting for us in near future and the reason behind today's rise.\n\n&amp;#x200B;\n\n1) We rose today due to yesterday's SOTU where Biden emphasized the importance of dropping costs for the middle-class which made people happy, due to the high inflation everyone was triggered by high prices so he managed to hit a sweet spot!\n\n&amp;#x200B;\n\n2) FED's Powell talked about how the U.S is not going to directly get affected by the Russian sanctions and that OIL prices are not a threat. He also mentioned that inflation is somehow under-control.\n\n&amp;#x200B;\n\n3) The unemployment data came out pretty positive! We are up in employment which was triggered by high manufacturing + retail sales.\n\n&amp;#x200B;\n\nOutcome: Let's sell some short-term put spreads, lads and gents!\n\nAlright Caldoon, but what the hell to expect next week or next month?\n\n&amp;#x200B;\n\n1) Don't expect Macro-Economy to just forget what have had happened in these past few days. The long-term effect of sanctions will definitely be negative and the USO is still rising although it dropped a bit. U.S and Europe are banning Russian gas and oil from everywhere and that will definitely drag Europe down soon, U.S might stay reluctant for a while due to OIL reserves but even then like we mentioned, USO is rising!\n\n&amp;#x200B;\n\n2) The inter-bank loan system called Swap Lines which are used by banks to lend each other money had a huge rate increase. That was due to uncertainty and fear in the market. No bank felt confident to pull out cash and lend it to anyone. It was feared that this might bring Financial Sector down which would drag the whole SP500 with it but thankfully for now it chilled down. However, the sanctions and its negative effects might drag it up again.\n\n&amp;#x200B;\n\n3) No matter what Powell says inflation is still high, the savings rate in consumers is very low which means people are not really left with much money in their pocket. That will definitely affect Q1 2022 retail sales\n\n&amp;#x200B;\n\n4) Not just retail sales but whole GDP is forecasted to show 0% growth, repeating myself, it is expected that we will have zero growth. That is scary isn't it? High inflation Low growth and volatile unemployment, that might bring the stagflation (recession) in the game which we mentioned in previous post.\n\n&amp;#x200B;\n\nSo in total, the situation is pretty bearish . We are bearish on Mid-term basis on SP500 and Nasdaq!", "upvote_ratio": 0.39, "id": "t3_t57hkk", "created_utc": 1646247842.0}
{"sub": "investing", "title": "Are Interest Rate Increases Priced in?", "selftext": "I know there are increases planned for this year. This month will see some, and likely more throughout \u201822 to combat inflation. \n\nBecause it is so clear that this is necessary, would it also not stand to reason that big money/smart money has already priced this in to the markets?\n\nObviously, I\u2019m not saying we hit any bottoms in the SP or NQ. I\u2019m sure there\u2019s a lot of other things that could bring about new lows.\n\nHowever, I would love to hear thoughts on how much you think rates will bring the market down, or if you think the future hikes are partially priced in.", "upvote_ratio": 0.83, "id": "t3_t55y7q", "created_utc": 1646243822.0}
{"sub": "investing", "title": "Looking for a shared/public Marked up Index Chart (preferably S&amp;P, Nasdaq, Dow)", "selftext": "Does anyone know of a shared/public market index chart marked up with significant market and economic events like FOMC meetings, Powel's press conferences, world events, other significant events\u2026.etc? I'm betting there's one on TradingView and have searched but having trouble finding anything. Maybe ThinkOrSwim or a blog post?", "upvote_ratio": 0.72, "id": "t3_t54dn3", "created_utc": 1646239754.0}
{"sub": "investing", "title": "MRAM Everspin Technologies", "selftext": "Everspin Technologies (MRAM) came on my radar after the last earnings beat, which resulted in the share price nearly doubling. The more research I did, the more interesting it looked.\n\nThe company makes a currently useful and potentially disruptive product - magnetoresistive RAM, which stores bits magnetically rather than electrically. It finds use in niche applications at the moment, but has the potential to eventually become a single-chip solution for both RAM and ROM in computer systems. That is to say, it is both quick to access and non-volatile. More info on this tech [here (wiki)](https://en.wikipedia.org/wiki/Magnetoresistive_RAM) and [here](https://www.science.org/doi/abs/10.1126/science.1110549). \n\nThe company became profitable in Q2 2021 and has consistently been beating earnings estimates.\n\nDetails:\n\n- Market cap ~$200M\n- Small float of about 16M shares outstanding (20M outstanding)\n- 67% of float held by institutions\n- No net debt\n- $14.6M cash on balance sheet \n\nDo your own DD but this company looks very interesting to me and reports earnings today after the closing bell.", "upvote_ratio": 0.73, "id": "t3_t541x2", "created_utc": 1646238900.0}
{"sub": "investing", "title": "US Interest rates Are Headed Higher very soon to combat inflation", "selftext": "[US Central Bank is going to raise interest rates later March.](https://www.msn.com/en-us/money/markets/powell-says-rates-are-headed-higher-even-as-ukraine-poses-uncertainty/ar-AAUvThI?ocid=msedgdhp&amp;pc=U531)\n\nFeds will move toward a \u201cpredictable\u201d shrinking of its big bond holdings after raising rates, a move that will take additional steam out of the economy, and that it will discuss those plans at its meeting ending March 16 without finalizing them.  According to Jerome Powell. \"....\"", "upvote_ratio": 0.95, "id": "t3_t53ubi", "created_utc": 1646238337.0}
{"sub": "investing", "title": "Why is the market assuming Fed action will be less aggressive given the Ukraine situation will almost certainly lead to increased Stagflation?", "selftext": "So the market priced in a possible 50 basis points rate increase in March (or 7 hikes over the year), then Ukraine is invaded and it had the biggest rally in years on the assumption it would be a less aggressive .25% in March and 5 increases over the years\n\n\nNow the market is trying to pump like crazy as if all uncertainty is off the table, inflation disappeared and the Fed will now do nothing.\n\nSo why isn\u2019t the market pricing in the almost guaranteed stagflation from soaring oil prices and the economic impact of Russian markets vanishing overnight? \n\nWouldn\u2019t the Fed have to take even more aggressive action to curb the home-grown inflation getting amplified by the geopolitical inflation?", "upvote_ratio": 0.55, "id": "t3_t527of", "created_utc": 1646233892.0}
{"sub": "investing", "title": "Sberbank shares - how screwed am I?", "selftext": "Question - Sberbank ADR (the largest Russian bank) is trading at $0.01 on the London stock exchange (symbol SBER) but listed at $1.00 on the OTC markets (SBRCY). Do the shares represent shares in the European subsidiary or something that just went bankrupt? Is either price accurate?  Just wondering because I wouldn't expect the main Russian bank to go under even though the European subsidiary does.\n\nLondon: [https://www.londonstockexchange.com/stock/SBER/sberbank-of-russia/company-page](https://www.londonstockexchange.com/stock/SBER/sberbank-of-russia/company-page)\n\nOTC: [https://stocktwits.com/symbol/SBRCY](https://stocktwits.com/symbol/SBRCY)\n\nBrokers are suspending all trading and only allowing exiting of positions in Moscow Exchange stocks and Russian stocks in general. Does the price just mean there is no price discovery right now? That's what I'm praying for.\n\n&amp;#x200B;\n\nBtw this is [crossposted](https://www.reddit.com/r/wallstreetbets/comments/t4wz6c/sberbank_shares_how_fucked_am_i/) at /r/wallstreetbets I assume that's ok.", "upvote_ratio": 0.81, "id": "t3_t4xjxw", "created_utc": 1646218103.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - March 02, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.72, "id": "t3_t4wwbf", "created_utc": 1646215272.0}
{"sub": "investing", "title": "$BE and the energy crisis", "selftext": "$BE at a market cap of 4 billion projects 35% yearly growth through the next decade from almost a billion in revenues today to 15 billion.  Hydrogen generation with the most efficient electrolyzer can replace what natural gas is today.  Energy is a national security risk as you see oil prices getting higher and higher with energy producer crisis no matter what side you are on.  I believe in freedom though.  God bless", "upvote_ratio": 0.5, "id": "t3_t4r8ls", "created_utc": 1646193991.0}
{"sub": "investing", "title": "Is the Fed really stopping to buy bonds?", "selftext": "From what I understand, the Fed assured everyone it would taper off bond purchases by $15 billion a month starting November, finally ending in March (now).\n\nHowever, looking [at the Fed's data](https://www.federalreserve.gov/monetarypolicy/bst_recenttrends.htm) (Total Assets, 1y scope), it doesn't look like the Fed tapered anything at all, instead increasing by the same exact rate until now. Am I reading it wrong?", "upvote_ratio": 0.85, "id": "t3_t4pz22", "created_utc": 1646190123.0}
{"sub": "investing", "title": "OXY up 20% over the past two trading days", "selftext": "OXY has been spiking above and beyond that of other crude stocks.  This is the leveraged play that Buffett financed via preferred.  He got a large share dividend during the pandemic and sold all of it at panic level prices.  Now, the stock has more than ~~tripled~~ quadrupled from the covid depths.  Cash flow looks very healthy.\n\nThoughts on OXY's future?", "upvote_ratio": 0.63, "id": "t3_t4ou5e", "created_utc": 1646186710.0}
{"sub": "investing", "title": "Why are Russian ADRs falling so much?", "selftext": "**Please keep this factual rather than ethical. I am curious about the market dynamics only.**\n\nI'm struggling to understand the reasons behind the total collapse of the Russian equities market. It was trading at a substantial discount before, but now it's truly unbelievable with many ADRs trading below a PE ratio of 1. I understand that Russia is being cut of from SWIFT and all, but ultimately many of these are companies that will be fine even without European trade for a few years.\n\nI see this as a result of four main factors:\n\n\\- Risk of companies going bankrupt or being diluted to the point of positions being worthless- Risk of nationalization of stocks, or government interference causing stocks to lose value- Risk of ADRs becoming untradable and somehow as a result becoming worthless- Devaluation as (institutional) investors divest from Russia\n\nI'm curious to hear your opinions on how much each of these play a part. It is fascinating to see the total collapse of a market like this, and I'm struggling to understand how companies with rock solid balance sheets and stable incomes can be punished so hard that in many cases they seemingly price in a &gt;80-90% chance of the stock being worthless.", "upvote_ratio": 0.44, "id": "t3_t4o6tu", "created_utc": 1646184782.0}
{"sub": "investing", "title": "Google Class A &amp; C Shares Question", "selftext": "I currently hold GOOG in my portfolio at 10% and I notice the QQQ and SPY both split google with equal weight in both GOOG &amp; GOOGL. \n\nWould you recommend splitting my Goog shares into both A &amp; C shares? I know A carries voting rights that C shares do not. This would be a long hold in my Roth.  Wasn't sure if it was worth splitting up my class C shares. Appreciate your help", "upvote_ratio": 0.8, "id": "t3_t4nl3b", "created_utc": 1646182989.0}
{"sub": "investing", "title": "Compass Pathways Psilocybin Stock", "selftext": "Just bought some medical shrooms stock after an impressive Johns Hopkins study using it for treatment resistant depression. My job is administering medical ketamine, hallucinogens are the going to be the next breakthrough drugs for psychiatry and psychotherapy. \n\nhttps://jamanetwork.com/journals/jamapsychiatry/fullarticle/2772630", "upvote_ratio": 0.6, "id": "t3_t4n0sm", "created_utc": 1646181375.0}
{"sub": "investing", "title": "How much do you need to research a company (or technology) before you feel ready to invest?", "selftext": "I caught a news article about those new electronic aviation start ups (Lilium, Joby, etc) and it sounded really interesting. Its been posted about on reddit before and some people are certain beyond a shadow of a doubt these companies will revolutionize personal travel, while others are completely convinced it will never catch on and the stocks will crash.\n\nI read articles and various posts for a couple hours and came very close to putting a few thousand $  into one of these stocks, but I held back because I knew nothing about the technology, all of my knowledge of the companies came second hand from others, and I couldn't feel confident I really knew what I was investing in or what its prospects were.  I also knew that if I was only just now hearing about this in the news, so was everyone else, so if there was any window of investment opportunity I already missed it. I would just be gambling on cool technology I had heard about.\n\nBut then I expanded that skepticism to other realms of investing - i'm in the medical field, but just knowing the mechanism of action or side effect profiles for medications doesn't give me the confidence to believe I can pick winners or losers in pharma, or anything else. \n\nWhat degree of understanding should be required before you should put money into a stock, or even into a given field at all? How do you know that you know what you need to know?", "upvote_ratio": 0.82, "id": "t3_t4kvhb", "created_utc": 1646175399.0}
{"sub": "investing", "title": "Apple stops all product sales in Russia as RT and Sputnik removed from App Store", "selftext": "Apple has paused all product sales in Russia in response to the invasion of Ukraine.\n\nThe tech company said it has also limited Apple Pay and other services in Russia, and that it has removed state-backed news outlets RT and Sputnik from its App Store in other countries.\n\nIn a statement, Apple said: \"We are deeply concerned about the Russian invasion of Ukraine and stand with all of the people who are suffering as a result of the violence.\n\n\"We are supporting humanitarian efforts, providing aid for the unfolding refugee crisis, and doing all we can to support our teams in the region.\n\n\"We have taken a number of actions in response to the invasion.\n\n\"We have paused all product sales in Russia.\n\n\"Last week, we stopped all exports into our sales channel in the country.\n\n\"Apple Pay and other services have been limited.\n\n\"RT News and Sputnik News are no longer available for download from the App Store outside Russia.\n\n\"And we have disabled both traffic and live incidents in Apple Maps in Ukraine as a safety and precautionary measure for Ukrainian citizens.\n\n\"We will continue to evaluate the situation and are in communication with relevant governments on the actions we are taking. We join all those around the world who are calling for peace.\"\n\nhttps://news.sky.com/story/apple-stops-all-product-sales-in-russia-as-rt-and-sputnik-removed-from-app-store-12555128", "upvote_ratio": 0.96, "id": "t3_t4iokb", "created_utc": 1646169621.0}
{"sub": "investing", "title": "Investing in the Russian situation", "selftext": "Free thoughts worth what you paid for them.\n\nI'm seeing chatter about investing in Russian assets. While that may be tempting from a valuation standpoint (if the transaction could even be executed) there is a far more liquid and safer trade here.\n\nInvest in competitors or directly in commodities.\n\nThe removal of Russian commodities from supply will increase the prices of these and help competing suppliers, from oil and gas to wheat and potash. There is no reason to chase ownership of Russian assets that may be at the whim of the Russian government, when the same exposure is available in other markets.", "upvote_ratio": 0.42, "id": "t3_t4gr2e", "created_utc": 1646164676.0}
{"sub": "investing", "title": "Questions about internationally invested mutual fund", "selftext": "Hello Subredditors. I am far from a financially savvy person and I have done well in my portfolio mostly, if I am honest, based on luck. At this time I am invested in LIJKX and have a very nice nest egg over $100K and growing. I am 51 years of age.\n\nI believe this mutual fund is internationally invested and I suspect some of that is in Russiea, a country I have no interest in supporting. Given the conflict going on and the fact that any investment in their interests at this point gives fuel to their wartime activities, I would like to ensure nothing I own would be contributing to that. I am helpless to make corporations pull out of business with Russia, but this is a small thing I can do.\n\nHow do I find out if my fund is invested in Russia in any capacity and what should I be looking at should I change from this fund to another?", "upvote_ratio": 0.56, "id": "t3_t4es8y", "created_utc": 1646159604.0}
{"sub": "investing", "title": "LHX - Conflict stock or long term?", "selftext": "I have been interested in LH3 for some time. With the Ukrainian conflict, naturally govt contractors are going to see bumps.\n\nI am, however, not interested in profiting on catastrophe. My goal is long term investment - talking about buying houses, kids through college (one day), being able to take vacations and retire at a reasonable time.\n\nI know this isn't official investment advice, but what do you think about L3Harris as a long term hold? They seem to be well positioned regardless of international conflict.", "upvote_ratio": 0.38, "id": "t3_t48qga", "created_utc": 1646143756.0}
{"sub": "investing", "title": "Large/Megacap ETF with strong balance sheet/fundamentals?", "selftext": "Hey guys, good morning.  Was hoping you guys might turn me on to a good ETF which invests in only Large/Megacap companies who have strong balance sheets and fundamentals (and preferably low PE).  \n\nWe are in uncertain times right now, and I tend to lean towards a stagflationary perspective for the short to intermediate timeframe.  My personal feeling is that these larger, stable, and strong companies will weather the storm the best.  \n\nI could go try to find a handful of companies that meet my criteria one by one, but would be much easier if there was an ETF I could jump into.", "upvote_ratio": 0.69, "id": "t3_t48tr1", "created_utc": 1646144012.0}
{"sub": "investing", "title": "Ukraine supplies 70% of the world's neon. Chip makers are on edge.", "selftext": "Again, the world's major chip and semiconductor companies are watching the conflict closely as the Russian invasion of Ukraine will likely hamper the supply of neon.\n\nNeon is used in lithography to make microchips.\n\nCurrently it appears the larger chip manufacturers have plenty in reserve but are worried that if the conflict escalates or is prolonged then again, the industry will suffer as a whole.\n\nhttps://www.wired.co.uk/article/ukraine-chip-shortage-neon\n\nhttps://www.reuters.com/breakingviews/ukraine-war-flashes-neon-warning-lights-chips-2022-02-24/\n\nEdit: removed insensitive sentence.", "upvote_ratio": 0.96, "id": "t3_t44pro", "created_utc": 1646129633.0}
{"sub": "investing", "title": "Daily General Discussion and Advice Thread - March 01, 2022", "selftext": "Have a general question?  Want to offer some commentary on markets?  Maybe you would just like to throw out a neat fact that doesn't warrant a self post?  Feel free to post here! \n\nIf your question is \"I have $10,000, what do I do?\" or other \"advice for my personal situation\" questions, you should include relevant information, such as the following:\n\n* How old are you? What country do you live in?  \n* Are you employed/making income? How much?  \n* What are your objectives with this money? (Buy a house? Retirement savings?)  \n* What is your time horizon? Do you need this money next month? Next 20yrs?  \n* What is your risk tolerance? (Do you mind risking it at blackjack or do you need to know its 100% safe?)  \n* What are you current holdings? (Do you already have exposure to specific funds and sectors? Any other assets?)  \n* Any big debts (include interest rate) or expenses?  \n* And any other relevant financial information will be useful to give you a proper answer.  \n\nPlease consider consulting our FAQ first - https://www.reddit.com/r/investing/wiki/faq\nAnd our [side bar](https://www.reddit.com/r/investing/about/sidebar) also has useful resources.  \n\nBe aware that these answers are just opinions of Redditors and should be used as a starting point for your research. You should strongly consider seeing a registered financial rep before making any financial decisions!", "upvote_ratio": 0.74, "id": "t3_t44j5g", "created_utc": 1646128870.0}
{"sub": "investing", "title": "trying to figure DODBX out", "selftext": "Many years ago I had money put in DODBX for me. over the years I've heard it's praise sung many times over. a great funt, a real long term champ, can't go wrong..... as I now take over active mgt of this money and my DODBX holdings I'm looking at long-term and I'm just not sure what the big deal is? it doesn't seem amazing. it took seemingly forever to recover from 2008. It's not horrid and since I've been in it since 2004 I'm overall up. But, what's the buzz, what am I missing? what is similar but better? Help me understand the love for what seems to be a rather unremarkable fund please?", "upvote_ratio": 0.22, "id": "t3_t3n3cz", "created_utc": 1646075053.0}
{"sub": "investing", "title": "Shell to exit joint ventures with Gazprom and pull out of Nord Stream 2", "selftext": "https://www.shell.com/media/news-and-media-releases/2022/shell-intends-to-exit-equity-partnerships-held-with-gazprom-entities.html\n&gt; The Board of Shell plc (\u201cShell\u201d) today announced its intention to exit its joint ventures with Gazprom and related entities, including its 27.5 percent stake in the Sakhalin-II liquefied natural gas facility, its 50 percent stake in the Salym Petroleum Development and the Gydan energy venture. Shell also intends to end its involvement in the Nord Stream 2 pipeline project.", "upvote_ratio": 0.98, "id": "t3_t3mewx", "created_utc": 1646073290.0}
{"sub": "investing", "title": "My bank has screwed me over multiple times, looking for suggestions.", "selftext": "I have tried to invest my money multiple times in different things over the past couple years. Each time, my bank has done everything they can to make it harder on me. \n\nFirst of all they have 1k buy limits per 24hr hours, you have to call and tell them to allow over 1k for that 24hr hour period. Not a huge deal but it is an inconvenience. Where it gets bad is when they close at 2:30 and you can\u2019t make a purchase over 1k till the next day (which with stocks, crypto, etc. you all know a lot can change very quickly) this has happened a few times causing me to already get frustrated. \n\nThe big one that pushed me over, I wanted to buy 4k of a crypto coin (I know some is for it and others against it, that\u2019s not why I\u2019m here) the coin dipped and I was ready to buy, I called the bank and told them I was gonna make a 4k charge on the debit card so they could okay it. When I tried to make the purchase it failed\u2026 I called back and asked why. The lady checked and said oh we can\u2019t allow that because the company you\u2019re trying to buy from is out of the US. I said oh no it\u2019s okay, they are a well trusted company, could you please allow the transaction to go through? Nope she said, we are not able to allow that transaction to go through, as we are not allowed to. She said the best we can do is an ACH transfer. Well 6 days pass and I get the 4k in the account that I wanted it to be in, but by the time the 6 days passed the coin went back up. I knew it would and was planning on selling when it did, which ended up costing me a little under 3.5k profit I coulda made. Nothing crazy but still\u2026 bank screwed me out of 3.5k. \n\nI definitely want to move banks, but who\u2019s to say I won\u2019t move to a bank that has these same restrictions? Just looking for any helpful tips or information you all might have.", "upvote_ratio": 0.44, "id": "t3_t3lybv", "created_utc": 1646072125.0}
{"sub": "investing", "title": "Warning to all young investors of the \u201ckiddie tax\u201d", "selftext": "\nI just recently filed my 2021 taxes. I had investments from 2020 and then sold them in 2021 assuming I could take the long term capital gain and pay no tax due to my low income. \n\nUnfortunately I am a young college student and a law called the \u201ckiddie tax\u201d exists which taxes a dependent child\u2019s unearned income over $2,200 at the parent rate. \n\nI owed over $4,000 when I filed. I got taxed about 30% on my capital gain.", "upvote_ratio": 0.8, "id": "t3_t3kpo3", "created_utc": 1646068951.0}
{"sub": "investing", "title": "First cultivated meat company to go public", "selftext": "MeaTech $MITC recently announced significant management changes aimed at accelerating the company\u2019s transition from a development stage company to a cultivated meat production company. T\n\n modular factory design allows the company to create a sustainable solution for a wide variety of species including chicken, beef, and pork. \n\nfirst cultivated meat company to go public with a $US28 million IPO in March 2021, following several funding rounds in 2020 totalling $US16.5 million.\n\nlobal food technology company using advanced biotechnology and engineering capabilities to develop slaughter-free, real meat, which is delicious, nutritious, and safer than conventional meat.", "upvote_ratio": 0.75, "id": "t3_t3ik4c", "created_utc": 1646063229.0}
{"sub": "investing", "title": "ELI5: What is e-mini spy and what does it mean to sell out options on?", "selftext": "I have a bit of experience trading spy and understand that it is an index/average of the 500 leading companies in the U.S.\n\nToday's(2/28) spx price is ~$4,377.94\n\nI'm using interactive brokers. Someone recently give me a recommendation to sell a put option(50)\n  E-mini S&amp;P 4170 at a limit price of $10.5 with expiration date of 3/2\n\nI want to understand this trade before making it \n\nCan someone give me an ELI5 explanation of what this trade entails?", "upvote_ratio": 0.67, "id": "t3_t3ibx2", "created_utc": 1646062630.0}
{"sub": "investing", "title": "Does it make sense to invest in Ukrainian bonds right now, assuming one waits until maturity?", "selftext": "Since Ukrainian bond prices fell, does it make sense to invest in them right now, assuming one waits until maturity? Decided to ask here since I haven't dealt with bonds before, but from what I understand, as long as one waits until maturity to receive the principal, government bonds should be fairly safe.", "upvote_ratio": 0.32, "id": "t3_t3i27p", "created_utc": 1646061880.0}
{"sub": "investing", "title": "TIL my long-term capital gains are taxed at 0%", "selftext": "Working on my taxes over the weekend, I was confused at why the tax amount calculated by TurboTax was coming in significantly lower than what I had previously estimated.  After digging around a bit, I discovered the cause of the discrepancy \u2013\u00a0long-term capital gains (for married filed jointly) of up to about $73K are taxed at 0%!\n\nI vaguely knew long-term gains received more favorable treatment than short-term, but I had no idea it was 0%, as most of my holdings in my taxable non-retirement account have been income-oriented along with some short-term gains from trading (I've had some long-term gains over the years too, but these amounts weren't big enough to create enough discrepancy between my estimate and the final number to catch my eye).  But this year I had $20K worth of long term gains that I expected to be taxed on, so it's a nice surprise to find that this gain is tax-free!", "upvote_ratio": 0.7, "id": "t3_t3g901", "created_utc": 1646056791.0}
{"sub": "investing", "title": "Investing Opportunity? Russia Ruble is now worth about $.01 US.", "selftext": "Would it be possible to buy up a bunch of Rubles and hold them until the war is over or sanctions have been removed and then turn them in for US after the value has returned to normal?  I've always been interested in currency exchange rates and I think this is an opportunity worth discussing.  How would I go about buying Rubles or would I have had to do it before the sanctions were in place?", "upvote_ratio": 0.44, "id": "t3_t3fcwy", "created_utc": 1646054075.0}
{"sub": "pytorch", "title": "My model was training bad, i tried plotting the gradient flows(one for discriminator and other generator), but they don't seem to be right, like the initial layers has more gradient accumulation than final ones, help me with this", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_uduqoc", "created_utc": 1651151707.0}
{"sub": "pytorch", "title": "How to get torch==1.1.0 in an Anaconda env (Windows)", "selftext": "I need to get that old version for a project. It worked on my linux laptop but I had to move it to my main pc cuz the code was too much for my humble 300$ laptop.\n\nThe command I used was:\npip install torch==1.1.0\nPython V is: 3.6.13\n\nError says:\nERROR: Could not find a version that satisfies the requirement torch==1.1.0 (from versions: 1.7.0, 1.7.1, 1.8.0, 1.8.1, 1.9.0, 1.9.1, 1.10.0, 1.10.1, 1.10.2)\nERROR: No matching distribution found for torch==1.1.0\n\nI tried to find a solution online but its confusing. Is there any way to get that specific version in Windows?\n\n Any help is appreciated!", "upvote_ratio": 0.75, "id": "t3_ud96pz", "created_utc": 1651081079.0}
{"sub": "pytorch", "title": "GroupNorm3D", "selftext": "Hi!\n\nI have a 3D-CNN network (fully convolutional). As I am using image sequences, I cannot train in big enough batches, so I am trying to find an alternative for batchnorm, and of course there is PyTorch's LayerNorm3d and InstanceNorm3d, but there is no GroupNorm3d. Is there a reason for that? Or maybe GroupNorm works for '3D' inputs (with 3D convs) as well? Or should I just write my own class based on the original paper? \n\nThanks in advance!", "upvote_ratio": 1.0, "id": "t3_ud2ipr", "created_utc": 1651062880.0}
{"sub": "pytorch", "title": "How to find a memory leak?", "selftext": "As a lot of people on this thread it would seem, I regularly meet the infamous `RuntimeError: CUDA out of memory` after a few epochs, that drives me crazy. Everytime, people post their codes on forums and someone points out a missing `.item()` or `.detach()` somewhere. But is there a way to know at each epoch the size of each tensor, or the memory usage breakdown or something in that fashion in order to track these issues myself instead of always asking online?", "upvote_ratio": 0.88, "id": "t3_ucjzoq", "created_utc": 1650999684.0}
{"sub": "pytorch", "title": "What loss function pairs with softmax activation function? CNN", "selftext": "I'm working on a multi-class image classification model, where I'm having 6 different classes one hot encoded.\nI'm using the softmax as the activation function and I'm not sure which loss function pairs with it. \nWould be cool if someone could help me out with this \ud83d\ude05", "upvote_ratio": 1.0, "id": "t3_ub44yh", "created_utc": 1650833821.0}
{"sub": "pytorch", "title": "TF/Keras and PyTorch differences", "selftext": "I'm switching over a few models I created with TF/Keras over to PyTorch. A few things that I've noticed are that the accuracy is much lower and batch normalization gets stuck at local minima if I have more than one Batch Norm layer. \n\nIs this common or am I doing something wrong?", "upvote_ratio": 0.88, "id": "t3_u9hmv4", "created_utc": 1650642212.0}
{"sub": "pytorch", "title": "A post on Denoising Text Image Documents using Autoencoders", "selftext": "[Denoising Text Image Documents using Autoencoders](https://debuggercafe.com/denoising-text-image-documents-using-autoencoders/)\n\n[https://debuggercafe.com/denoising-text-image-documents-using-autoencoders/](https://debuggercafe.com/denoising-text-image-documents-using-autoencoders/)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/njqqw9jobzu81.png?width=1200&amp;format=png&amp;auto=webp&amp;s=3edb09fa108ef7bc9b8098db6f557fb69ca73afa", "upvote_ratio": 1.0, "id": "t3_u92c6q", "created_utc": 1650589229.0}
{"sub": "pytorch", "title": "Trying to get Pytorch ROCm to work on Ubuntu 20.04 with Fiji cards", "selftext": "nan", "upvote_ratio": 0.75, "id": "t3_u8hlnc", "created_utc": 1650524606.0}
{"sub": "pytorch", "title": "Reinforcement tutorial doesn't lead to a model that converges", "selftext": "I copied and pasted the code and it doesn't produce a model that is better than random chance no matter what I do to it. Does anyone know of a Pytorch reinforcement tutorial that works? \n\n[https://pytorch.org/tutorials/intermediate/reinforcement\\_q\\_learning.html](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)", "upvote_ratio": 0.63, "id": "t3_u7rp29", "created_utc": 1650443119.0}
{"sub": "pytorch", "title": "A little thought about the unification of dynamic and static graphs", "selftext": "  This post is reproduced from Zhihu and translated using Deepl for all enthusiasts to communicate\n\nI've been writing samples for the past few days, mainly referring to the Pytorch implementation, and inevitably encountered some problems caused by the differences between static and dynamic graphs again. In addition, MindSpore has recently made some progress on static graph syntax support, but the ease of use is not so obvious, which triggered the impulse to write this article. Since it is a random thought, there will be no structure, so I wrote it wherever I thought of it.\n\nSupernatural dynamic graphs, obsolete static graphs\n\nFrom the emergence of Pytorch to the present, the user-friendliness of dynamic graphs I believe no one will have any questions, freedom of writing, debugging convenience, code that is the formula of the cool. The toiling public who have been suffering from TensorFlow for a long time are looking forward to it, and the students who want to start deep learning with a low threshold are also looking forward to it. To some extent, the lowering of the AI threshold (and involution) is actually accompanied by the popularity of dynamic graph frameworks. I personally do not use many frameworks in depth, because the beginning of the AI years have been Keras, and then Pytorch open source has been using Pytorch. Later, I also had a shallow taste of the old version of Paddle, as well as MindSpore since 2020.\n\nPytorch has been eating up TensorFlow's share step by step, and then TF2.0 has shifted to eager mode across the board. And as dynamic graphs continue to gain popularity, the concept of dynamic and static graphs is in fact little known among most people engaged in AI (perhaps I'm just ignorant). Looking back at these several domestic frameworks, unfortunately, whether before or after the birth of Pytorch, most of them have taken the TF-like route, and then desperately trying to make up for it.\n\nIt's been 2202 now, and there are still people experimenting with static graphs?\n\nIn fact, we all vote with our feet, when I write a bit of wrong code, the C++ stack overflows; when I want to change a model, but also need to pay attention to the syntax restrictions; when I want to manipulate the gradient, but to go around to complete; when I want to write a brand new Layer, but have to care about how to write the code of control flow. Each of the above is a reason to stay away from static graphs. When I tried the old version of Paddle, this is how I was discouraged, even with the free V100 GPUs, I really can't carry this learning threshold. Later MindSpore encountered the same problems, even if the morphology has been optimized a lot.\n\nDynamic and static graphs, really need to be unified?\n\nSo MindSpore is playing the slogan of dynamic and static unification, and is practicing it physically. In the current environment of mainstream motion pictures, it is still worth a compliment to be able to motivate a group of people to use it. But we also face the dilemma of migrating after the experiment is completed with Pytorch.\n\nBack to the topic, do dynamic and static graphs need to be unified? In terms of MindSpore's practice, it is to let the compiler gradually support the full Python syntax, and then translate it to computational graphs and send it down to device for computation. Pytorch has been criticized for being too flexible and then difficult to deploy, although there is now a good deployment path with ONNX as an intermediate IR. The common perception is probably that dynamic graphs are suitable for academia and static graphs are suitable for industry.\n\nDynamic and static unification from an AI full-scene perspective\n\nThe first thing to clarify is that the full scenario mentioned here is not the same thing as the one advertised by HW. the full scenario of AI must cover scientific research. Moreover, the development of the whole AI field is probably like a coin pusher, constantly throwing coins in, quantitative change triggers qualitative change, and then a milestone piece of work appears and drops a bunch with a clatter. And only after that, these milestone models will be enshrined by industry and deployed everywhere.\n\nObviously, the full scenario of AI is research + deployment, with the former being versatile and the latter being stable. That's why there are numerous inference engines, but not many make training frameworks. The success of Pytorch is then best explained by the fact that it responds to the needs of the people who use it the most, rather than focusing only on the almost stable and customized deployments in industry.\n\nSo following the trajectory of this AI field to see the role that deep learning frameworks should play, it probably looks like this:\n\nSatisfy the need for a magic model for a large group of scientific people\n\nSupport or even derive a new milestone model\n\nSupport the export deployment and large-scale application of milestone models\n\nLooking at the matter of dynamic and static unification again, the dynamic graph is unstoppable and the inevitable choice for the scientific research scenario. Static graphs are more suitable for exporting and deploying models generated by scientific research. The industry frameworks seem to be choosing to let one side gradually approach the other. I think the gap between the two is better to be bridged by something, rather to be forcibly eliminated. In other words, people are not very resistant to secondary code modifications when exporting models and deploying them, and a one-click export is good, but adding a conversion is not unacceptable. So training using Pytorch then exporting as ONNX for deployment is not a bad choice.\n\nA few thoughts on MindSpore\n\nStatic graphs as defaults will not appeal to the research crowd.\n\nStatic graph syntax cannot and does not need to support the full range of Python syntax.\n\nAs a framework with almost optimal support for static graph syntax, it should actually encourage users to use dynamic graphs first until they need to deploy the model (80% of people don't), at which point the compiler should give enough guidance information to guide modifications to the model (the modifications here are non-destructive to the model).\n\nIn industrial training scenarios, using static graph to train (milestone models have enough impact, at this point the model is fixed and more stuffed with data).\n\nStatic graphs can be exported directly for deployment.\n\nSo a more reasonable approach should be, dynamic graphs as default, static graphs for deployment requirements, guidance provided for compiler errors, static graph training export. The biggest advantage of MindSpore is that it is compiled by way of AST parsing and this tool that can bridge the gap.\n\nI found it to be an almost faultless process from the time I came across MindSpore. Later MindSpore changed the default mode to graph mode. Looking at the people using static graphs painfully, I thought, why bother?\n\nSummary\n\nFinally, a few more words. Few people can answer the question - why should I use MindSpore when I have Pytorch.\n\nIf the above approach is implemented, the reasons I could give would probably be something like:\n\nFreedom of science: how free Pytorch is to write models, and how free it is to use me.\n\nSeamless deployment: no need to convert for deployment, just convert to static graphs according to guidance.\n\nFinally, in fact, quite want to spit a few more words. Still hope that no matter what deep learning framework is, it can be honest about the fact that it is a tool rather than core. respect AI science, respect AI researchers. That's all.", "upvote_ratio": 1.0, "id": "t3_u7qt1i", "created_utc": 1650439165.0}
{"sub": "pytorch", "title": "RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation", "selftext": "For [this training code](https://gist.github.com/buttercutter/b6f526c56e20f029d68e6f9041c3f5c0/17ce540da7b4e66a73a812e0a93041b52e9fd9c4#file-gdas-py-L612-L679) , why the [runtime error when there is no inplace operation](https://pastebin.com/vzRB6dUT) ?", "upvote_ratio": 0.5, "id": "t3_u7e8zd", "created_utc": 1650398959.0}
{"sub": "pytorch", "title": "perform a 2d convolution with loaded weights??", "selftext": "Hi, not sure if this is even proper to post here as a question but I am trying to implement the model as defined in this paper [https://arxiv.org/abs/2203.11192](https://arxiv.org/abs/2203.11192) \n\nI have implemented everything except for the top right part of figure 3, basically I need to take a part of the transformer encoder output and convolve it with the weights output from the transformer decoder. The shapes (B,C,W,H) are z\\_test = torch.Size(\\[1, 256, 14, 14\\]) and  w = torch.Size(\\[1,256, 1, 1\\]) where w (as far as I know) are the weights that should be used when applying the convolution to z\\_test.\n\nresult = torch.nn.functional.conv2d(z\\_test, w, stride=1, padding=0) works but this doesn't use the weights of w, just the shape with the conv2d internal weights, or have I misunderstood? If not, how can I load w as weights of the conv2d operator?\n\nHoping for any help that I can get", "upvote_ratio": 0.9, "id": "t3_u72a2d", "created_utc": 1650365447.0}
{"sub": "pytorch", "title": "Training loss does not converge", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_u72a1e", "created_utc": 1650365443.0}
{"sub": "pytorch", "title": "TypeError: setup() got an unexpected keyword argument 'stage'", "selftext": " I am trying to train my q&amp;a model through pytorch\\_lightning. However while running the command \n\n    trainer.fit(model,data_module)\n\nI am getting the following error: \n\n    --------------------------------------------------------------------------_\n    TypeError                                 Traceback (most recent call last) \n    &lt;ipython-input-72-b9cdaa88efa7&gt; in &lt;module&gt;() \n    ----&gt; 1 trainer.fit(model,data_module)  \n    4 frames \n    /usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py in _call_setup_hook(self)    \n        1488 \n        1489 if self.datamodule is not None: \n        -&gt; 1490             self.datamodule.setup(stage=fn)    \n        1491         self._call_callback_hooks(\"setup\", stage=fn)    \n        1492         self._call_lightning_module_hook(\"setup\", stage=fn)  \n    \n    TypeError: setup() got an unexpected keyword argument 'stage'", "upvote_ratio": 1.0, "id": "t3_u727pq", "created_utc": 1650365195.0}
{"sub": "pytorch", "title": "[N][R][P] High fidelity 3D face reconstruction from monocular image", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_u6nabu", "created_utc": 1650315265.0}
{"sub": "pytorch", "title": "Confused where I should add if __name__ == '__main__':", "selftext": "EDIT: I tried putting it after the dataloader without the freeze\\_support(), but that causes the function to return none.\n\n&amp;#x200B;\n\nTitle pretty much says it all. I'm getting the \" *An attempt has been made to start a new process before the current process has finished its bootstrapping phase. You've probably forgotten to use if \\_\\_name\\_\\_ == '\\_\\_main\\_\\_': freeze\\_support().\"*\n\nI'm not really sure where to put that line of code. Is it after I declare the batch size and num\\_workers etc? And do I put the freeze\\_support() section? Just a little confused at this point.\n\nHere's the function that's being called.\n\n    @torch.no_grad()\n    def prepare_data_features(model, dataset):\n        # Prepare model\n    \n            model.to(device)\n            data_loader = data.DataLoader(dataset, batch_size=32, num_workers=32, shuffle=False, drop_last=False)\n            feats, labels = [], []\n    \n            for batch_imgs, batch_labels in tqdm(data_loader, total=len(data_loader)):\n                batch_imgs = batch_imgs.to(device)\n    \n                batch_feats = model(batch_imgs)\n    \n                feats.append(batch_feats.detach().cpu())\n    \n                batch_labels = batch_labels.to(device)\n    \n                labels.append(batch_labels.detach().cpu())\n    \n            feats = torch.cat(feats, dim=0)\n    \n            labels = torch.cat(labels, dim=0)\n    \n            # Sort images by labels\n    \n            labels, idxs = labels.sort()\n    \n            feats = feats[idxs]\n    \n                #  return data.TensorDataset(feats, labels)\n    \n            return feats, labels\n\nDo I just put it at the top of this function, or after the dataloader, or somewhere else?", "upvote_ratio": 0.78, "id": "t3_u6jh63", "created_utc": 1650305321.0}
{"sub": "pytorch", "title": "Help with DeepFashion dataset", "selftext": "I am trying to find a good dataset that I can use for various ML applications involving fashion/clothing and I came across the [DeepFashion](https://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html) dataset. I was blown away when I saw it... it has the exact type of functionality that I am looking for. I found this toolbox called [mmfashion](https://github.com/open-mmlab/mmfashion) that uses the DeepFashion dataset and I cloned it and tried to get it set up, but I have been unsuccessful. I have also come across the `Fashion-MNIST` dataset that can help with classifying images, but DeepFashion seems to be full-featured and solves all of my use cases.\n\nHas anyone worked with DeepFashion/mmfashion before and know how to get it set up or could help me do so? I'd be more than happy to compensate anyone that is able to help me get it working.", "upvote_ratio": 1.0, "id": "t3_u6cboq", "created_utc": 1650286006.0}
{"sub": "pytorch", "title": "torch.nn.Transformer how to get access to encoded layer", "selftext": "Do I really have to copy the source code and extend it to return the memory layer in the forward() function of the transformer or is there an easier way to do this?", "upvote_ratio": 1.0, "id": "t3_u5qjc4", "created_utc": 1650212742.0}
{"sub": "pytorch", "title": "best way to reduce convolution channels in a learned manner?", "selftext": "Newbie here,\n\nI've been playing around with making a model that attempts to colourise b+w images. my current model is a cnn in which the final layer is reduced to two channels, which become the a &amp; b channels of LAB colour space. the preceding layer is the output of the convolutions with many (at the moment 64) channels.\n\nI'm trying to create a system where the final layer essentially learns the relative importance for each of the 64 input channels, independently for each output channel.\n\nmy current method is to take the output of a convolution layer, the shape of which looks like:\n\n(batch_size, 64, x,y)\n\nflatten it\n\n(batch_size,64,n_pixels)\n\ndo a 1d convolution, with a size of 64, and two output channels\n\n(batch_size,2,n_pixels)\n\nthen unflatten\n\n(batch_size,2,x,y)\n\nbut I suspect this is a little bit too hacky, and there is a better way to do it.\n\nP.S. I know this is almost certainly not an optimal system for a colourising nn, but I'm going through the process as a DIY learning exercise!", "upvote_ratio": 1.0, "id": "t3_u4zoxo", "created_utc": 1650121497.0}
{"sub": "pytorch", "title": "Correct method to have weights derived from a reduced order set of parameters?", "selftext": "I have an application where I want to have the weights for a Conv2D to be based on some reduced set of parameters. For example, I want to restrict the kernel to be a 2D representation of sin waves with a particular frequency and orientation. So, the new Module really only needs to have two parameters. \n\nI was able to make a class that derives from torch.nn.Module, has these two parameters, contains a single Conv2D member variable, and a section to generate the kernel encapsulated in a \"with torch.no_grad():\" section that then assigns these derived weights to the Conv2D object. Some simple tests with some test data seem to show this is doing the 2D convolution just right.\n\nHowever, my questions is how to ensure that this derived kernel get updated every time the underlying frequency and orientation is updated by the optimizer. \n\n1. Should I regenerate it in the forward() function?\n2. Is there some callback for when parameters get updated?\n3. Is there some other preferred method to do this?", "upvote_ratio": 0.5, "id": "t3_u4puv3", "created_utc": 1650083016.0}
{"sub": "pytorch", "title": "Newbie Question: Why is my linear model is only returning NaN? How can diagnose this?", "selftext": "Hey, I'm trying to learn more about PyTorch and I'm running into a frustrating issue with my model. No matter what I do my model only predicts `nan`.\n\nHere is my model:\n\n    class linearRegression(torch.nn.Module):\n        def __init__(self, inputSize, outputSize):\n            super(linearRegression, self).__init__()\n            self.linear = torch.nn.Linear(inputSize, outputSize)\n\n        def forward(self, x):\n            out = self.linear(x)\n            return out\n\nAs you can see it is a single layer. My data set is fully normalized with z-scaled. Yet when I train my model all I get back is nan:\n\n    def train(model, train_x, train_y):\n        criterion = torch.nn.MSELoss()\n        optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n        epochs = 1000\n        for _ in range(epochs):\n            # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n            optimizer.zero_grad()\n\n            # get output from the model, given the inputs\n            outputs = model(train_x)\n            # get loss for the predicted output\n            loss = criterion(outputs, train_y)\n            # get gradients w.r.t to parameters\n            loss.backward()\n\n            # update parameters\n            optimizer.step()\n\nAbove is my example code for training the model. \n\nAny help would be greatly appreciated.", "upvote_ratio": 0.83, "id": "t3_u4mie8", "created_utc": 1650071386.0}
{"sub": "pytorch", "title": "A tutorial on Autoencoder Neural Network: Application to Image Denoising", "selftext": "[Autoencoder Neural Network: Application to Image Denoising](https://debuggercafe.com/autoencoder-neural-network-application-to-image-denoising/)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/p976sv6acst81.png?width=1200&amp;format=png&amp;auto=webp&amp;s=de191c84788e64ea2eada0055085d22531b89a71", "upvote_ratio": 0.75, "id": "t3_u4lqzg", "created_utc": 1650068834.0}
{"sub": "pytorch", "title": "What kind of model should I use for my project?", "selftext": "Hi friends! I\u2019m working on a project that involves an AI trying to complete a specific task within an FPS video game. The input is a tensor with [t, c, x, y] dimensions (time, RGB channels, and x/y of pixels) given from a custom dataset, and the output needs to be a binary array for keyboard buttons, a binary array for mouse buttons, and a \u201cunit interval\u201d array of where to move the mouse, from 0-1 on each axis on the screen. I was thinking I should use a few convolutional layers for pattern detection, and then feed that to an LSTM, but I have no idea if that would work, and my tests have shown that it\u2019s quite bad. How can I fix my network topology, or is what I\u2019m doing completely wrong?", "upvote_ratio": 1.0, "id": "t3_u341ko", "created_utc": 1649894216.0}
{"sub": "pytorch", "title": "Help choosing a network type", "selftext": "Hello, \n\ni have a problem that i am trying to solve using Pytorch and would like some opinions or help concerning what kind of neural network to use, or if possible how to solve this problem more efficiently.\n\nProblem:\n- The input contains 4 images (from the four top corners) of objects captured by camera.\n- Giving: The 3d shape of the photographed images (.obj, .stl, .fbx)\n- The nn model should be able to classify which photographed object has been recorded by the cameras by matching the 4 images with the appropiate 3D shape. (preferably unsupervised)\n\nWhat i am planing to do:\n- Generate different views from the 3d object files. Use rotationnet\n- or with Pytorch3d, generate a 3d view of the images and compare the 3d objects\n\nIs there a way to project the images (views) onto the 3d files directly and select which one fits the best?\n\n\nThanks", "upvote_ratio": 1.0, "id": "t3_u2xc0f", "created_utc": 1649875563.0}
{"sub": "pytorch", "title": "Machine Learning with PyTorch and Scikit-Learn eBook", "selftext": "nan", "upvote_ratio": 0.67, "id": "t3_u16s1b", "created_utc": 1649680683.0}
{"sub": "pytorch", "title": "PyTorch on M1 GPU with Shark", "selftext": "Has anyone tried this and got their example to work on the M1 GPUs?\n\n[https://nod.ai/pytorch-m1-max-gpu/](https://nod.ai/pytorch-m1-max-gpu/)", "upvote_ratio": 0.82, "id": "t3_u0x4zo", "created_utc": 1649643625.0}
{"sub": "pytorch", "title": "Sampler that picks a random subset of the data from one class per epoch", "selftext": "So I'm working on a project where I have a class imbalance. And one of the things I wanted to try was to sample a random subset of the highest occurring class for the first epoch/first mini-batch along with all the other classes, and a new subset for every other epoch/mini-batch. The remaining classes get sampled in their entirety. Is it possible to implement this with the Pytorch data module?", "upvote_ratio": 1.0, "id": "t3_u0sag1", "created_utc": 1649628342.0}
{"sub": "pytorch", "title": "Max pooling", "selftext": "Can someone please share how to do maxpooling with Bert model in PyTorch ?", "upvote_ratio": 0.36, "id": "t3_u01wl2", "created_utc": 1649537155.0}
{"sub": "pytorch", "title": "Train a Convolutional Autoenocder on CIFAR10 using PyTorch", "selftext": "[Train a Convolutional Autoenocder on CIFAR10 using PyTorch](https://debuggercafe.com/machine-learning-hands-on-convolutional-autoencoders/)\n\n[https://debuggercafe.com/machine-learning-hands-on-convolutional-autoencoders/](https://debuggercafe.com/machine-learning-hands-on-convolutional-autoencoders/)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/1zdx0t6787s81.png?width=1200&amp;format=png&amp;auto=webp&amp;s=76697398c7c4fe2c68ea9b8fd8dd1acda682fa41", "upvote_ratio": 1.0, "id": "t3_tyqviz", "created_utc": 1649377369.0}
{"sub": "pytorch", "title": "Implementing a basic feed-forward NN - why is my loss/weight not updating?", "selftext": "Hello all, \n\nI am a beginner to Pytorch, and to a lesser extent, Python. I understand the fundamentals of ML and NNs, but now I am trying to put pen-to-paper with some beginner projects. \n\nI've written up a super rudimentary architecture based on the MNIST dataset, and my code runs, but my outputs do not seem to meaningful update through each mini-batch and epoch iteration. Further, the loss values output for each iteration output as \"-0.000\". \n\nI've tried to troubleshoot this, but there is nothing that stands out to me here as blatantly incorrect based on my research. \n\nFor reference: The data I am using is not from the built-in Pytorch package data. I am using a modified version of MNIST that only contains class label values for 0 and 1 (just to make the implementation a little easier with a single output node). \n\nSo my features are input as a (12665, 784) NumPy array, with feature values normalized to be between 0 and 1. The class labels, then, are (12665, 1) with binary values of 0 or 1.\n\nI've also ran this data through a \"from-scratch\" NN that I put together with NumPy, and that seemed to work just fine. I am just not sure where I am going wrong with PyTorch. \n\nAny help would be greatly appreciated, thanks!\n\nMy code:\n\n    import numpy as np\n    import torch\n    from torch import nn\n    from torch import optim\n    from torch.utils.data import DataLoader, TensorDataset\n    import torch.nn.functional as F\n    \n    device = torch.device('cuda')\n    \n    \n    # Single output node: 0 or 1\n    mnist_train_01 = np.genfromtxt('path', delimiter=',')\n    \n    \n    Y_train_01 = mnist_train_01[:, [0]]  # Y \n    X_train_01 = mnist_train_01[:, 1:]  # X \n    X_train_01 = X_train_01 / 255  # Normalize X \n    \n    \n    class NN(nn.Module):\n        def __init__(self, input_size, output_size):\n            super().__init__()\n            self.fc1 = nn.Linear(input_size, 100)\n            self.fc2 = nn.Linear(100, output_size)\n            \n        def forward(self, x):\n            x = F.relu(self.fc1(x))\n            x = self.fc2(x)\n            return x\n    \n    \n    def train(net, features, batch_size, class_label, epochs=100000, lr=0.001):\n        model.train()\n        \n        opt = torch.optim.Adam(net.parameters(), lr=lr)\n        criterion = nn.CrossEntropyLoss()\n        \n        class_label = torch.from_numpy(class_label).to(device)\n        features = torch.from_numpy(features).to(device).float()\n        \n        for e in range(epochs):\n            for n in range(0, features.shape[0], batch_size):\n                opt.zero_grad()\n                # The features\n                x = features[n:n+batch_size,:]\n                y = class_label[n:n+batch_size,:]\n                # forward\n                outputs = model.forward(x)\n                loss = criterion(outputs, y)  \n                #backward\n                loss.backward()\n                #gradient descent \n                opt.step()\n                print(\"Loss: {:.4f}\".format(loss.item()))\n                \n                        \n    model = NN(input_size=784, output_size=1).to(device)\n     \n         \n    train(model, X_train_01, 5, Y_train_01, 2, 0.001)", "upvote_ratio": 1.0, "id": "t3_tycqgj", "created_utc": 1649336950.0}
{"sub": "pytorch", "title": "Tutorial: Writing JAX-like code in PyTorch with functorch", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_ty2ty8", "created_utc": 1649298773.0}
{"sub": "pytorch", "title": "What happens if I perform unstructured pruning after structured pruning in Pytorch?", "selftext": "Say, I have a model named CNN\\_Model which consists of multiple CNN layers and one Fully Connected layer. I perform structured pruning on the model and then perform unstructured l1 pruning on the model. Does the structured pruning mask gets removed?\n\nSay for example:\n\nI use this function: \n\nfor\u00a0name,\u00a0module\u00a0in\u00a0model.named\\_modules():  \n if isinstance(module,\u00a0torch.nn.Conv2d):  \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0prune.ln\\_structured(module=module,\u00a0name=\"weight\",  \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0amount=sparsity,\u00a0n=1,\u00a0dim=dimention,\u00a0importance\\_scores=None)\n\n&amp;#x200B;\n\nand then use this:\n\n \n\nfor\u00a0name,\u00a0module\u00a0in\u00a0model.named\\_modules():  \n if isinstance(module,\u00a0torch.nn.Conv2d):  \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0prune.l1\\_unstructured(module=module,\u00a0name=\"weight\",  \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0amount=sparsity,\u00a0importance\\_scores=None)\n\n&amp;#x200B;\n\nor if i were to perform global unstructured pruning after the structured pruning, what would happen?", "upvote_ratio": 1.0, "id": "t3_txugh7", "created_utc": 1649274315.0}
{"sub": "pytorch", "title": "Tutorial on how to make a new pytorch layer CUDA-compatible?", "selftext": "We have been working on a new kind of layer in pytorch. We were able to implement it in pure python and it seems to be working fine, but would really like to extend it to work on the GPU. As far as I can tell, there isn't magic going on behind the scenes with pytorch that automatically makes the Python implementation be GPU-compatible unless all the underlying operations used in the layer were already GPU-compatible.\n\nSo, does someone have a tutorial that gives an end-to-end example of making a new layer type that is both CPU- and GPU-compatible? It looks like [this page](https://pytorch.org/tutorials/advanced/cpp_extension.html#integrating-a-c-cuda-operation-with-pytorch) shows how to do either a C++ or CUDA layer, but it doesn't show how to make one that switches between using CUDA or not-CUDA based on the python script that is calling it.", "upvote_ratio": 1.0, "id": "t3_txp5u9", "created_utc": 1649259973.0}
{"sub": "pytorch", "title": "NaN training loss", "selftext": "Why am I having [NaN for training loss](https://github.com/buttercutter/gdas/blob/c1ea1779e3e3c2a1aa52df3071256d56f9ed2e03/gdas.py#L50), `ltrain` if I change the value of the variable `NUM_OF_CELLS` from 8 to 16 ?\n\nhttps://preview.redd.it/n40wq9ddixr81.png?width=980&amp;format=png&amp;auto=webp&amp;s=da7f05523769945ffaa977d6e5df667be697ec71", "upvote_ratio": 1.0, "id": "t3_txp2v5", "created_utc": 1649259741.0}
{"sub": "pytorch", "title": "Looking for people to test my new GPU/Ubuntu virtual machine \"cloud' service!", "selftext": "Hi everyone! I've spent the last couple months building and configuring a virtual GPU \"cloud/instance\" service. I'm looking for anyone with ML/DL/Pytorch/Ubuntu/GPU experience to put my VMs to the test and let me know what you like/dislike about it. It's still in the early beta stages so I'd like to know how training times and latency compare to what you're currently used to. Absolutely free of charge. SSH and VNC connections are available through the web. Must have you connect to my VPN to gain access, for security. Let me know if you're willing to try this out. Comment or DM. All constructive criticism is greatly appreciated!", "upvote_ratio": 0.84, "id": "t3_twxasn", "created_utc": 1649171773.0}
{"sub": "pytorch", "title": "Can someone help me with this? it's been days that i struggle with this problem", "selftext": "[https://github.com/jdb78/pytorch-forecasting/issues/933](https://github.com/jdb78/pytorch-forecasting/issues/933)", "upvote_ratio": 0.84, "id": "t3_tw40ak", "created_utc": 1649084122.0}
{"sub": "pytorch", "title": "A Post on Implementing Deep Autoencoder in PyTorch", "selftext": "[Implementing Deep Autoencoder in PyTorch](https://debuggercafe.com/implementing-deep-autoencoder-in-pytorch/)\n\n[https://debuggercafe.com/implementing-deep-autoencoder-in-pytorch/](https://debuggercafe.com/implementing-deep-autoencoder-in-pytorch/)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/tco3abi1atq81.png?width=1200&amp;format=png&amp;auto=webp&amp;s=7a80833265a73047b0ab3e3f2262af6e89612c77", "upvote_ratio": 1.0, "id": "t3_ttdrw0", "created_utc": 1648772642.0}
{"sub": "pytorch", "title": "PyTorch and ROCm 5: What ROCm packages are required for PyTorch?", "selftext": "I'm hoping to use PyTorch with ROCm to speed up some SVD using an AMD GPU. I'm new to GPU computing, ROCm and PyTorch, and feel a bit lost.\n\nI'm pretty sure I need ROCm &gt;= 5.0 to support the 6800 RX GPU, which means the [PyTorch Get Started Locally](https://pytorch.org/get-started/locally/) command doesn't quite work for me. Further, I\u2019d like to test on a laptop with a Vega 8 iGPU which some ROCm packages do not support (HID, I believe).\n\nHence, I need to install ROCm differently, and due to my OS, I can't use the AMD script (PopOS 21.10), but go through the [Using Package Manager on Ubuntu](https://docs.amd.com/bundle/ROCm-Installation-Guide-v5.1/page/How_to_Install_ROCm.html) method, which is all fine.\n\nIn the very last step, I'm asked to install ROCm meta-packages---and here I'm unsure what I need for PyTorch. AMD has a [neat list of ROCm meta-packages](https://docs.amd.com/bundle/ROCm-Installation-Guide-v5.1/page/Meta-packages_in_ROCm_Programming_Models.html), but I don't know which are necessary.\n\nI've been trying to prod the URL from the PyTorch Get Started Locally command, but I didn't get anything out of that.\n\n### **So, in short: Which of the [ROCm meta-packages](https://docs.amd.com/bundle/ROCm-Installation-Guide-v5.1/page/Meta-packages_in_ROCm_Programming_Models.html) should I install to make use of PyTorch?**\n\nThank you for your time.", "upvote_ratio": 0.78, "id": "t3_tszoi8", "created_utc": 1648733034.0}
{"sub": "pytorch", "title": "loss.backward() ???", "selftext": "Hello there,\n\nI am recently transitioning to pytorch from tf, and was wondering about the backward method. Specifically I am wondering the design decision that went into creating this interface. \n\nThis is a method called on an arbitrary torch tensor. How the hell does this interface make explicit the connection to the model? Something like:\n\nmodel.backward(loss = loss)\n\nMakes much more sense to me. It clearly shows that the loss is used in conjunction with all the partial derivatives to calculate the gradient updates for the model itself. \n\nI suspect that there is probably an actual reason behind this interface design that I am failing to see. If any of you fellow ML folk could shed some light onto this issue, it would be greatly appreciated.", "upvote_ratio": 0.8, "id": "t3_tqt40y", "created_utc": 1648533659.0}
{"sub": "pytorch", "title": "PyTorch to coreml with colab", "selftext": "Any experts in this for GANs? I\u2019m willing to pay at this point. I just can\u2019t figure out why my model worked when missing the jacobian part cause I didn\u2019t have an invert function. When I added the invert function and let the jacobian work again the model now returns blank. The invert alone seems to work on its own. It\u2019s a client project so I can\u2019t post the entire thing.", "upvote_ratio": 1.0, "id": "t3_tqp4s8", "created_utc": 1648519302.0}
{"sub": "pytorch", "title": "I made a beginner's guide to TorchStudio!", "selftext": "If you haven't seen it yet, TorchStudio is an awesome IDE built **just for PyTorch** and was released a few weeks ago for open beta!\n\n[I wrote this beginner's guide](https://www.assemblyai.com/blog/beginners-guide-to-torchstudio-pytorch-only-ide/) to help you get up-and-running with TorchStudio. It takes you through model building, training, and comparison, and also includes some pros, cons, and suggested features!\n\nhttps://preview.redd.it/e0li76p9e5q81.png?width=960&amp;format=png&amp;auto=webp&amp;s=94dd300d691ff97e52f2de67f437a2186f80c98c", "upvote_ratio": 0.89, "id": "t3_tqcnvy", "created_utc": 1648483518.0}
{"sub": "pytorch", "title": "[Help] Gradient of a parameter is setting to NoneType", "selftext": "I am experimenting with pytorch autograd. I have created a simple algebraic equation **w+2** in which I want to find the value of w a scalar value using gradient descent. The **real output is 4** and the **real w is 2**.\n\n    import torch\n    from torch.autograd import Variable\n\n    w = Variable(torch.ones(1),requires_grad=True)\n    for i in range(10):    \n        y = 4\n        lr = 0.001\n        \n        y_ = w+2\n        \n        loss = (y_-y) ** 2\n        loss.backward()\n        \n        w = w - lr*w.grad.data\n        \n        print(\"L: \",loss,\"w: \",w,\"w-g: \",w.grad)\n\nThis is the error\n\n&gt;AttributeError: 'NoneType' object has no attribute 'data'\n\nHow do I solve this?", "upvote_ratio": 1.0, "id": "t3_tq0duw", "created_utc": 1648438769.0}
{"sub": "pytorch", "title": "how to convert from torch Tensor to base64 image to send over network?", "selftext": "Creating a flask server that so far looks like:\n\n    @app.route(\"/json\", methods=['GET', 'POST', 'PUT'])\n    def getjsondata():\n    \n        if request.method=='POST':\n            print(\"received POST\")\n    \n            data = request.get_json()\n    \n            #print(format(data['z']))\n            jzf = [float(i) for i in data['z']]\n            jzft = torch.FloatTensor(jzf)\n            jzftr = jzft.reshape([1, 512])\n    \n            z = jzftr.cuda()\n            c = None                   # class labels (not used in this example)\n            trunc = 1\n            img = G(z, c, trunc)\n            #print(type(img))\n            img = (img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n\nIt receives some JSON data, uses that as input vector, generates an image, and now I want to send that image back. From what I understand it should be sent as a base64 byte array, but I have tried for hours to convert it into that format to no avail. Any help is appreciated.", "upvote_ratio": 0.83, "id": "t3_tplcqg", "created_utc": 1648392191.0}
{"sub": "pytorch", "title": "RuntimeError: Function 'LogSoftmaxBackward0' returned nan values in its 0th output.", "selftext": "Why if `NUM_OF_CELLS` is [increased from 8 to 16](https://github.com/buttercutter/gdas/blob/cdb7179b98aa2b6f5bd12cc090dc6b60ecef7311/gdas.py#L50) , the following errors pop up ?\n\n    /home/phung/PycharmProjects/venv/py39/bin/python /home/phung/PycharmProjects/beginner_tutorial/gdas_new.py\n    Files already downloaded and verified\n    Files already downloaded and verified\n    run_num =  0\n    Entering train_NN(), forward_pass_only =  0\n    modules =  &lt;generator object Module.named_children at 0x7f6a8044d0b0&gt;\n    gradwalk(output_tensor.grad_fn)\n    outputs1.size() =  torch.Size([4, 10])\n    train_labels.size() =  torch.Size([4])\n    tensor(1., device='cuda:0')\n    [W python_anomaly_mode.cpp:104] Warning: Error detected in LogSoftmaxBackward0. Traceback of forward call that caused the error:\n      File \"/home/phung/PycharmProjects/beginner_tutorial/gdas_new.py\", line 873, in &lt;module&gt;\n        ltrain = train_NN(forward_pass_only=0)\n      File \"/home/phung/PycharmProjects/beginner_tutorial/gdas_new.py\", line 638, in train_NN\n        Ltrain = criterion(NN_output, NN_train_labels)\n      File \"/home/phung/PycharmProjects/venv/py39/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n        return forward_call(*input, **kwargs)\n      File \"/home/phung/PycharmProjects/venv/py39/lib/python3.9/site-packages/torch/nn/modules/loss.py\", line 1150, in forward\n        return F.cross_entropy(input, target, weight=self.weight,\n      File \"/home/phung/PycharmProjects/venv/py39/lib/python3.9/site-packages/torch/nn/functional.py\", line 2846, in cross_entropy\n        return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)\n     (function _print_stack)\n    Traceback (most recent call last):\n      File \"/home/phung/PycharmProjects/beginner_tutorial/gdas_new.py\", line 873, in &lt;module&gt;\n        ltrain = train_NN(forward_pass_only=0)\n      File \"/home/phung/PycharmProjects/beginner_tutorial/gdas_new.py\", line 648, in train_NN\n        Ltrain.backward()\n      File \"/home/phung/PycharmProjects/venv/py39/lib/python3.9/site-packages/torch/_tensor.py\", line 307, in backward\n        torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n      File \"/home/phung/PycharmProjects/venv/py39/lib/python3.9/site-packages/torch/autograd/__init__.py\", line 154, in backward\n        Variable._execution_engine.run_backward(\n    RuntimeError: Function 'LogSoftmaxBackward0' returned nan values in its 0th output.\n    \n    Process finished with exit code 1", "upvote_ratio": 1.0, "id": "t3_tof013", "created_utc": 1648262140.0}
{"sub": "pytorch", "title": "Help with 1D CNN for time series classification", "selftext": "I am attempting to train a classifier for 1 dimensional time series data with multiple layers. Each training example is a slice of sequential 1D data and each is labeled as one of three total classes. The issue is that the model only ever predicts one of the classes for each training example. I do believe that the data provided should be enough to classify each slice. So far, my attempts to make a deeper network have yielded the same result and I wanted to make sure I\u2019m not making any glaring mistakes before I waste time attempting to make it deeper. Any advice?", "upvote_ratio": 0.75, "id": "t3_tnvap2", "created_utc": 1648232620.0}
{"sub": "pytorch", "title": "Non differential loss approximation", "selftext": "I'm working with custom recommendation systems for personal research and I want to try using NDCG as my loss function directly. It's non-differential so I need an approximation but after trying out NeuralNDCG it takes way too long and way too much GPU. It's simply not feasible with my project scale. I have over 1 million users across over 20000 items. In total, I am using 62 million values and only kept the top 250000 users and the top 8196 items. My next idea is that I know neural nets can be used to approximate some algorithms or functions so I was wondering if I could make a second network that takes in the true values as well as the reconstructed values and outputs an approximated NDCG score of how well the first did. That being said I don't know if there are better or easier ways of achieving what I want and I figure the hive mind of the internet is probably my best bet. \n\nAm I overthinking things?\n\nIs this too complicated?\n\nAm I better off sticking with differential loss functions?\n\nIs there something like NeuralNDCG but significantly more efficient?", "upvote_ratio": 1.0, "id": "t3_tnf4qw", "created_utc": 1648186870.0}
{"sub": "pytorch", "title": "Yolo annotated dataset to custom Dataloader", "selftext": "Hi guys,\n\nI'm trying to learn and implwment a custom dataloader so that it would load dataset annotated in yolo format. So, I have an image with multiple objects in it, and then a txt file with multiple label rows per image.\n\nCould you advise me how to tackle that logic wise? Should I crop the images by labels and supply one label row per image as in the examples I see online or there are some other options?\n\nThanks a lot", "upvote_ratio": 1.0, "id": "t3_tl1a7f", "created_utc": 1648054644.0}
{"sub": "pytorch", "title": "Creating a custom loss function for Object Localization", "selftext": "(Sorry for long read)\n\nHi, I am currently working on a project where I need to implement Object Localization in my Convolutional NN model. We have gotten the following hints:\n\n\\- In pytorch, the loss function is just a regular function. You can define a custom loss function by  \ndefining a regular python function and use a combination of pre-defined pytorch loss functions  \ninside this custom function.  \n\\- In pytorch, all elements of a given tensor share the same type. But we saw that the last  \nelement of y true is supposed to be an integer. You might need to convert some elements of  \nyour tensors to a different type on the fly while computing the loss.\n\nAnd that we should part the loss function into three parts, detection(using  nn.BCEWithLogitsLoss ), localization(using  nn.MSELoss) and classification(using  nn.CrossEntropyLoss).\n\nI have the classification part figured out, but I am so confused on how I am supposed to alter the model so I can get multiple outputs with for example the bounding box cords, and use them.\n\nIf I could get any hints on how to implement the Object Localization in my Convolutional  NN that  would be greatly appreciated :D", "upvote_ratio": 1.0, "id": "t3_tkvvc0", "created_utc": 1648047629.0}
{"sub": "pytorch", "title": "PyTorch: Implicit Gradients returns (None) Meta-Gradient", "selftext": "I am attempting to implement the implicit gradients algorithm \\[1, 2, 3\\] to optimize some meta-parameters (in my case the parameters of a loss function). However, the (meta-)gradients produced are always None. Can I have some help identifying what the problem is, and how I can resolve this issue?\n\nBelow I have attached some simplified code that reproduces the error.\n\n```\nfrom sklearn.datasets import make_regression\nimport torch\n\n\n# Creating a meta-network for representing the loss function.\nclass MetaNetwork(torch.nn.Module):\n\n    def __init__(self):\n        super(MetaNetwork, self).__init__()\n        self.model = torch.nn.Sequential(\n            torch.nn.Linear(2, 10),\n            torch.nn.ReLU(),\n            torch.nn.Linear(10, 1),\n            torch.nn.Softplus()\n        )\n\n    def forward(self, y_pred, y_target):\n        return self.model(torch.cat((y_pred, y_target), dim=1)).mean()\n\n\n# Creating a base-network for learning the model of the data.\nclass BaseNetwork(torch.nn.Module):\n\n    def __init__(self):\n        super(BaseNetwork, self).__init__()\n        self.model = torch.nn.Sequential(\n            torch.nn.Linear(1, 10),\n            torch.nn.ReLU(),\n            torch.nn.Linear(10, 1)\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\n\n# Generating some synthetic training and validation data.\nX_train, y_train = make_regression(n_samples=100, n_features=1, n_informative=1, noise=0.1, random_state=1)\nX_valid, y_valid = make_regression(n_samples=100, n_features=1, n_informative=1, noise=0.1, random_state=2)\n\n# Converting data into the correct format.\nX_train, y_train = torch.tensor(X_train).float(), torch.unsqueeze(torch.tensor(y_train).float(), 1)\nX_valid, y_valid = torch.tensor(X_valid).float(), torch.unsqueeze(torch.tensor(y_valid).float(), 1)\n\n# Creating our base and meta models, as well as the base optimizer.\nmeta_network, base_network = MetaNetwork(), BaseNetwork()\nbase_optimizer = torch.optim.SGD(base_network.parameters(), lr=0.01)\n\n# Training the model using the meta-network as the loss function.\nfor i in range(10):\n    base_optimizer.zero_grad()\n    yp = base_network(X_train)\n    base_loss = meta_network(yp, y_train)\n    base_loss.backward()\n    base_optimizer.step()\n\nmeta_loss_fn = torch.nn.MSELoss()\n\n# Computing the training and validation (meta) loss.\ntrain_loss = meta_loss_fn(base_network(X_train), y_train)\nvalidation_loss = meta_loss_fn(base_network(X_valid), y_valid)\n\n# Gradient of the validation loss with respect to the base model weights.\ndloss_val_dparams = torch.autograd.grad(validation_loss, base_network.parameters(),\n                                        retain_graph=True, allow_unused=True)\n\n# Gradient of the training loss with respect to the base model weights.\ndloss_train_dparams = torch.autograd.grad(train_loss, base_network.parameters(),\n                                          create_graph=True, allow_unused=True)\n\np = v = dloss_val_dparams\n\nfor _ in range(10):\n    grad = torch.autograd.grad(dloss_train_dparams, base_network.parameters(),\n                               grad_outputs=v, retain_graph=True, allow_unused=True)\n\n    grad = [g * 0.01 for g in grad]\n\n    v = [curr_v - curr_g for (curr_v, curr_g) in zip(v, grad)]\n    p = [curr_p + curr_v for (curr_p, curr_v) in zip(p, v)]\n\nv2 = list(0.01 * pp for pp in p)\nv3 = torch.autograd.grad(dloss_train_dparams, meta_network.parameters(), grad_outputs=v2, allow_unused=True)\n\nprint(\"Meta Gradient\", v3)\n```\n\n---\n\n\\[1\\] Rajeswaran, A., Finn, C., Kakade, S. M., &amp; Levine, S. (2019). Meta-learning with implicit gradients.\n\n\\[2\\] Lorraine, J., Vicol, P., &amp; Duvenaud, D. (2020, June). Optimizing millions of hyperparameters by implicit differentiation.\n\n\\[3\\] Gao, B., Gouk, H., Yang, Y., &amp; Hospedales, T. (2021). Loss Function Learning for Domain Generalization by Implicit Gradient.", "upvote_ratio": 1.0, "id": "t3_tklfsy", "created_utc": 1648007911.0}
{"sub": "pytorch", "title": "Help implementing NN with L1/L2 regression in PyTorch.", "selftext": "Hi all,\n\nI'm working on setting baselines for a new dataset that has previous implementations without NN only( the dataset is quite new). It's a multivariate regression task for inputs and yields the final prediction\n\nAs part of this, I've run sklearn classifier.. and now I'm building a neural net for the same. However, the networks I built seem to perform well below par as opposed to the sklearn regressor.\n\nMy question is, how does one implement lasso / ridge regressor in pytorch that achieves performance similar to sklearn regressors?", "upvote_ratio": 1.0, "id": "t3_tjnz0u", "created_utc": 1647902067.0}
{"sub": "pytorch", "title": "DCGAN with PyTorch in Wildlife animals", "selftext": "I thought it will be quite interesting to see Deep Convolutional GAN\u2019s capability in generating wildlife, so I built a model based on the DCGAN architecture through PyTorch:\n\n[https://taying-cheng.medium.com/create-new-animals-using-dcgan-with-pytorch-2ce47810ebd4](https://taying-cheng.medium.com/create-new-animals-using-dcgan-with-pytorch-2ce47810ebd4)", "upvote_ratio": 0.83, "id": "t3_ti9un0", "created_utc": 1647739007.0}
{"sub": "pytorch", "title": "Complete Guide of Swin Transformation with Full PyTorch Implementation", "selftext": "(I found out i wrote transformation not transformer just now\ud83e\udd23)\n\nHello everyone!\n\nI\u2019ve recently read Swin Transformer paper and tried to implement with PyTorch. But there\u2019re no post that FULLY explains the nitty-gritty details of the paper with full implementation. It took me soooo long time to write this post so I wanted to share with y\u2019all! Hope this helps someone! The implementation is based on the official implementation of Microsoft team.\n\nhttps://jasonlee-cp.github.io/paper/Swin_Transformer/#swin-transformer-architecture", "upvote_ratio": 1.0, "id": "t3_ti0w0h", "created_utc": 1647712818.0}
{"sub": "pytorch", "title": "Help implementing the coco dataset using fiftyone", "selftext": "As mentioned in the title i'm trying to use fiftyone to import my dataset from coco. Problem is, each image has a JSON related to them and each image has the mask for every detection. Now if i want to get the mask for detection x in image y all i need to do is dataset\\[y\\]\\['ground\\_truth'\\]\\['detections'\\]\\[x\\]\\['mask'\\]. This is the part where the problems arise:\n\n1 - I noticed that the masks in the same image have different sizes between themselves and even the image, why is it?\n\n2 - I have all the masks corresponding to each detection in an image, what now? How can i create a dataloader in order to train the segmentation model?\n\nThank you for your attention!", "upvote_ratio": 1.0, "id": "t3_thycgb", "created_utc": 1647705860.0}
{"sub": "pytorch", "title": "I can't debug my code", "selftext": " Hi everyone. I'm trying to debug my code and I can't manage to find what's wrong. I'm running a Roberta model with a classifier head on a Pytorch Lightning trainer and I just get stuck after it is running the optimizer config, it's loading the train data, calling \\_\\_len\\_\\_(from dataset) a couple of times, loading val data, calling len again a few times and then complete freeze, what could be going on? Also it's 100% running on the gpu but python is not using any gpu in task manager so I don't think it's actually doing anything. Does anyone have any ideeas what variables I can check in the debugger to see what's going on?", "upvote_ratio": 0.43, "id": "t3_thjopg", "created_utc": 1647650927.0}
{"sub": "pytorch", "title": "need some help in our collage \ud83e\udd1e project its 80% is completed", "selftext": " \n\nthis is an NLP and python based project we are trying to achieve something new this project is almost there but the only file connecting thing is leftover\ud83d\ude0c\n\nplease dm me for more information/collaboration\ud83d\ude0a\n\nwe are open to welcoming you to this project**\ud83e\udd17**\n\n**not a paid work\ud83d\ude44**", "upvote_ratio": 0.33, "id": "t3_thcomg", "created_utc": 1647630708.0}
{"sub": "pytorch", "title": "Is it possible to modify weights file ? (.pt)", "selftext": "I have pretrained weights for model i coded. The networks is the same and i just need to change the location of layer classes.", "upvote_ratio": 0.88, "id": "t3_tglv2a", "created_utc": 1647555341.0}
{"sub": "pytorch", "title": "Cool PyTorch Guide/Wiki", "selftext": "PyTorch Guide/Wiki: [https://github.com/mikeroyal/PyTorch-Guide](https://github.com/mikeroyal/PyTorch-Guide)", "upvote_ratio": 0.83, "id": "t3_tfdaa8", "created_utc": 1647421375.0}
{"sub": "pytorch", "title": "RuntimeError: The size of tensor a (32) must match the size of tensor b (16) at non-singleton dimension 3", "selftext": "Why if `NUM_OF_CELLS` variable is [increased from 8 to 16](https://github.com/buttercutter/gdas/blob/30c3694251188ad7c1d4de85efa44f90f8936f70/gdas.py#L44), the following error pops up ?\n\n    /home/phung/PycharmProjects/venv/py39/bin/python /home/phung/PycharmProjects/beginner_tutorial/gdas.py\n    Files already downloaded and verified\n    Files already downloaded and verified\n    run_num =  0\n    Entering train_NN(), forward_pass_only =  0\n    modules =  &lt;generator object Module.named_children at 0x7fa359f57e40&gt;\n    c =  0  , n =  0  , cc =  0  , e =  0\n    Traceback (most recent call last):\n      File \"/home/phung/PycharmProjects/beginner_tutorial/gdas.py\", line 841, in &lt;module&gt;\n        ltrain = train_NN(forward_pass_only=0)\n      File \"/home/phung/PycharmProjects/beginner_tutorial/gdas.py\", line 605, in train_NN\n        NN_output = graph.forward(NN_input)\n      File \"/home/phung/PycharmProjects/beginner_tutorial/gdas.py\", line 353, in forward\n        self.cells[c].nodes[n].connections[\n    RuntimeError: The size of tensor a (32) must match the size of tensor b (16) at non-singleton dimension 3\n    \n    Process finished with exit code 1", "upvote_ratio": 0.2, "id": "t3_tf8ivf", "created_utc": 1647402354.0}
{"sub": "pytorch", "title": "Prevent Flask from re-loading models during prediction for different images", "selftext": "Hi, I am using Flask to display machine learning results.\n\nIn particular, I am using Yolo object detection and I would like to show predictions in the index.html.\n\nThis works but the problem is that it is reloading the model each time I select another image as input.\n\nIt would be important to avoid reloading of the model as this slows down the App dramatically.\n\nA similar question was asked before at https://stackoverflow.com/questions/61049310/how-to-avoid-reloading-ml-model-every-time-when-i-call-python-script but the answers do not work for my case.\n\nPlease find here the code for [app.py](https://app.py) :\n\n    from flask import Flask, render_template, request, redirect, url_for, make_response\n    import os\n    import io\n    from PIL import Image\n    import cv2\n    import torch\n    \n    from werkzeug.exceptions import BadRequest\n    import pandas as pd\n    import time \n    import argparse\n    \n    model=None\n    def load_model():\n        global model\n        model=torch.hub.load('ultralytics/yolov5', 'custom', path='/my-directory/best.pt', force_reload=True)\n    \n    app = Flask(__name__)\n    \n    @app.route('/', methods=[\"POST\"])\n    def upload_file():\n        uploaded_file = request.files['file']\n        if uploaded_file.filename != '':\n            uploaded_file.save(os.path.join('static', uploaded_file.filename))\n             \n        img = Image.open(os.path.join('static', uploaded_file.filename))\n        results = model(img, size=640)  \n        df = results.pandas().xyxy[0]\n        res = round(df.iloc[0,0],2)\n    \n        return render_template('index.html', user_image = os.path.join('static', uploaded_file.filename), dtext = res)\n        \n    if __name__ == \"__main__\":\n    \n        print('starting...')\n        load_model()\n        app.run(debug=True)\n\nI have also tried to put the model part directly as but it gives as an error \"model not defined\":\n\n    if __name__ == \"__main__\":\n        model=torch.hub.load('ultralytics/yolov5', 'custom', path='/my-directory/best.pt', force_reload=True)\n        app.run(debug=True)\n\nAny suggestions or help would be welcome!", "upvote_ratio": 1.0, "id": "t3_te92ba", "created_utc": 1647295493.0}
{"sub": "pytorch", "title": "Pytorch: Weighting in BCEWithLogitsLoss, but with 'weight' instead of 'pos_weight'", "selftext": "I'm looking how to do class weighting using BCEWithLogitsLoss. \n\nhttps://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html\n\nThe example on how to use `pos_weight` seems clear to me. If there are 3x more negative samples than positive samples, then you can set pos_weight=3\n\nDoes the `weight` parameter do the same thing?\n\nSay that I set it `weight=torch.tensor([1, 3])`. Is that the same thing as `pos_weight=3`\n\nAlso, is `weight` normalized? Is `weight=torch.tensor([1, 3])` the same as `weight=torch.tensor([3, 9])`, or are they different in how they affect the magnitude of the loss?", "upvote_ratio": 1.0, "id": "t3_te3g7p", "created_utc": 1647280492.0}
{"sub": "pytorch", "title": "Where to find paid tutor for pytorch / CNN homework project", "selftext": "I'm looking for someone who can help me (paid) with a homework assignment for about Convolutional neural networks and image Segmentation. I must be done in PyTorch.\n\nI need help with a basic implementation, but also with writing a short report and explaining choices.  So I'm looking for someone with a theoretical background (ML / Data Science)\n\nDoes anybody know a good place to look for help?", "upvote_ratio": 0.71, "id": "t3_tdey0r", "created_utc": 1647201262.0}
{"sub": "pytorch", "title": "Article series: Deployment of Deep Learning models on Genesis Cloud - Tutorials &amp; Benchmarks", "selftext": "&amp;#x200B;\n\nhttps://preview.redd.it/h6b43oyqesm81.png?width=2036&amp;format=png&amp;auto=webp&amp;s=4433b69002b5ddc374b2b4db9713f1e08b6b17a0\n\nWe are proud to introduce our new article series that will guide you on how to run state of the art deep learning models on Genesis Cloud infrastructure. These articles will be initially published as blog posts and will be added to our [knowledge base](https://support.genesiscloud.com/a/solutions) after their release. Please note: The order of the articles is important as articles are written as a series and information contained in the initial articles might be required for understanding the subsequent articles.\n\nIn this series of articles we will use 1x RTX 3080 instance type on Genesis Cloud (our recommended GPU for inference use) and showcase four (4) different deployment strategies for deep learning inference using (a) PyTorch (TorchScript), (b) TensorRT, and (c) Triton.\n\nFor the models, we will focus on computer vision applications using the torchvision model collection. This collection will serve as an example and includes various pretrained versions of classic deep learning algorithms such as alexnet, densenet, mobilenet, resnet, shufflenet, and squeezenet.\n\n## Articles\n\n* Article 1: PyTorch, torchvision, and simple inference examples - available next week\n* Article 2: Deployment techniques for PyTorch models using TorchScript - upcoming (early April 2022)\n* Article 3: Deployment techniques for PyTorch models using TensorRT - upcoming (end of April 2022)\n* Article 4: Using Triton for production deployment of TensorRT models - upcoming (May 2022)\n\n## Why run deep learning inference on a GPU?\n\nIn the early days of machine learning GPUs were mainly used for training deep learning models while inference could still be done on a CPU. While the field of machine learning progressed immensely in the past 10 years, the models have grown in both size and complexity, meaning that today the standard infrastructure setup for latency-sensitive deep learning applications are based on GPU cloud instances instead of CPU-only instances.\n\nRationale for using a GPU is not just performance but also cost. Compared to CPUs, GPUs are often two orders of a magnitude more efficient in processing deep neural networks. This means, that cost-savings can be achieved by switching to a GPU instance especially when operating with high throughput applications.\n\n## How to run deep learning inference on a Genesis Cloud GPU instance?\n\nAll you need are a Genesis Cloud GPU instance, a trained deep learning model, data to be processed, and the supporting software. We will show you how to master it all.\n\nEach article will contain:\n\n* Installation and/or building instructions for various components\n* Necessary background information\n* Sample code for validation of installation and further experiments\n* Annotations and explanations to help you understand the sample code\n* Prebuilt models ready for deployment (when applicable)\n* Benchmarking scripts and results (when applicable)\n\nIn case you aren\u2019t using Genesis Cloud yet, [get started here](https://id.genesiscloud.com/signup/).\n\n**Now start accelerating on machine learning with Genesis Cloud** \ud83d\ude80", "upvote_ratio": 1.0, "id": "t3_tbuunl", "created_utc": 1647018809.0}
{"sub": "pytorch", "title": "PyTorch 1.11, TorchData, and functorch are now available", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_tb7cu0", "created_utc": 1646942380.0}
{"sub": "pytorch", "title": "Improving topology and performance of binary classifer", "selftext": "I'm trying to develop a binary classifier for a university project, but I'm getting really terrible results, (ie., no better than chance/50%, on training or test data).  Basically, I'm trying to build a classifier to distinguish between animated and photographic styles in film.\n\nAnyone have any recommendations on what I could do better?\n\nI think my network topology is far from optimal, though I don't know what I can improve or how to improve it, nor even where to look for how to topographically design a network, (anyone have any resources?  Is it just trial and error?)\n\n&amp;#x200B;\n\nBear with me, as I based this classifier *heavily* off the tutorial from Pytorch's docs.  I'm pretty new to this, so I'm trying to figure it out.\n\n&amp;#x200B;\n\nData:\n\nI used ffmpeg to scale frames from a bunch of movies and shows (either filmed or animated) to 128x128 (3 color layers/RGB).  I have approximately 200k frames each of animated and photographed images, though at any one time I'm only loading about 80k frames into memory, (60k for training of 30k photographic and 30k animated, and 20k for testing of 10k photographic and 10k animated).\n\nInput to the network ends up being of shape \\[3, 64, 64\\] after cropping.\n\n    images_train, labels_train, images_test, labels_test = getFrames()\n    # each of these is an [n_frames, 3, 128, 128] torch.Tensor\n    # getFrames also balances the dataset, so n_class0 = n_class1\n    \n    transform_forward = torchvision.transforms.Compose([\n        torchvision.transforms.RandomCrop(64),\n        torchvision.transforms.Normalize([0.5] * 3, [0.5] * 3),\n    ])\n    transform_backward = torchvision.transforms.Compose([\n        torchvision.transforms.Normalize([-1.] * 3, [2.] * 3),\n        torchvision.transforms.ToPILImage(),\n    ])\n    \n    class Frames(torch.utils.data.Dataset):\n        def __init__(self, train, train_vs_test_files=((0, 1), (6, 7)),\n                     transform=transform_forward, target_transform=None):\n            self.transform = transform\n            self.target_transform = target_transform\n    \n            # shuffle dataset (probably not strictly necessary if using \n            # DataLoader(shuffle=True), but gives me peace of mind)\n            if train is True:\n                shuffled = np.random.permutation(len(images_train))\n                self.images = images_train[shuffled].float()\n                self.labels = labels_train[shuffled].float()\n            else:\n                shuffled = np.random.permutation(len(images_test))\n                self.images = images_test[shuffled].float()\n                self.labels = labels_test[shuffled].float()\n    \n        def __len__(self):\n            return len(self.images)\n    \n        def __getitem__(self, idx):\n            image = self.images[idx]\n            label = self.labels[idx]\n    \n            if self.transform:\n                image = self.transform(image)\n            if self.target_transform:\n                label = self.target_transform(label)\n    \n            return image, label\n    \n    dataset_train = Frames(train=True)\n    loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, \n                                               shuffle=True)\n\nI'm fairly confident I'm loading my data properly, as if I review the images with something like,\n\n    plt.imshow(transform_backward(dataset_train[np.random.randint(len(dataset_train))]))\n\nI get what I expect.\n\nOne concern I have about my data is that many frames are very similar, (ie., from the same shot in a film/show/movie, but only very slightly different scene/camera position/position of objects in frame).  I told ffmpeg to export only 1 of every *n* frames, but that only somewhat remedies the problem.\n\n&amp;#x200B;\n\nModel and training:\n\nInput to the network ends up being of shape \\[3, 64, 64\\] after cropping.\n\n    class Net(torch.nn.Module):\n        def __init__(self):\n            super(Net, self).__init__()\n    \n            self.kernel = 5\n    \n            self.conv1 = torch.nn.Conv2d(3, 6, self.kernel)\n            self.conv2 = torch.nn.Conv2d(6, 16, self.kernel)\n            self.fc1 = torch.nn.Linear(2704, 768)\n            self.fc2 = torch.nn.Linear(768, 128)\n            self.fc3 = torch.nn.Linear(128, 1)\n    \n            self.activation = torch.nn.ReLU()  # easier to change activations if I want\n            self.sigmoid = torch.nn.Sigmoid()\n            self.pool = torch.nn.MaxPool2d(2)\n    \n        def forward(self, x):\n            x = self.pool(self.activation(self.conv1(x)))\n            x = self.pool(self.activation(self.conv2(x)))\n            x = torch.flatten(x, 1)\n            x = self.activation(self.fc1(x))\n            x = self.activation(self.fc2(x))\n            x = self.activation(self.fc3(x))\n            x = torch.squeeze(x)\n            \n            return self.sigmoid(x)\n    \n    net = Net().cuda()\n    \n    criterion = torch.nn.BCELoss()\n    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n    \n    print('Training...')\n    for epoch in range(20):  # loop over the dataset multiple times\n        running_loss = 0.\n        for i, data in enumerate(loader_train):\n            optimizer.zero_grad()\n    \n            inputs, labels = data[0].cuda(), data[1].cuda()\n            outputs = net(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()            \n            optimizer.step()\n    \n            running_loss += loss.item()\n    \n        print(f'Epoch {epoch + 1:02d} loss: {running_loss / 2000:.3f}')\n    \n    print('Finished training.')", "upvote_ratio": 0.81, "id": "t3_taicem", "created_utc": 1646861996.0}
{"sub": "pytorch", "title": "Resource to learn advanced concepts", "selftext": "I have gone through the basics of building and training neural networks. I want to implement an algorithm from a paper that requires me to build layers with new functionalities. For instance, I need to keep a copy of the weights in real form, but output a binarized form. For some reason, I can't seem to find any good reference on how such things can be done in Pytorch. I searched the tutorial page and came across one where they implement a forward and a backward method. The tutorial doesn't even explain why these methods are static or some of the variable names (like `ctx`). Basically, I'm looking for a resource that explains how a layer with custom functionality is built from scratch. Is there such thing?", "upvote_ratio": 0.67, "id": "t3_ta48bz", "created_utc": 1646818579.0}
{"sub": "pytorch", "title": "Are there any simple pytorch neural networks to combine pictures?", "selftext": "Hi,\n\nFor instance, given two pictures, there is one common object in both pictures, how to  let neural network to combine the object, like YOLO can recognize it but to output the combined picture not output the scalar probability .\n\ntried DCGAN, got very bad results.\n\nalso Saw one [https://github.com/jcjohnson/neural-style](https://github.com/jcjohnson/neural-style) that is not the goal also way complicated.\n\n&amp;#x200B;\n\nthanks a ton.", "upvote_ratio": 0.67, "id": "t3_t9jaft", "created_utc": 1646753410.0}
{"sub": "pytorch", "title": "Video of a tree created using an AI made with pytorch", "selftext": "nan", "upvote_ratio": 0.96, "id": "t3_t6usk1", "created_utc": 1646433158.0}
{"sub": "pytorch", "title": "Case Study: Amazon Ads Uses PyTorch and AWS Inferentia to Scale Models for Ads Processing", "selftext": "nan", "upvote_ratio": 0.72, "id": "t3_t5jgkx", "created_utc": 1646282826.0}
{"sub": "pytorch", "title": "Dense layers in Tensorflow: what's the PyTorch equivalent?", "selftext": "What is the equivalent of this tf line in PyTorch?\n\n    self.q = tf.keras.layers.Dense(conv_filters)", "upvote_ratio": 0.86, "id": "t3_t58p6q", "created_utc": 1646251095.0}
{"sub": "pytorch", "title": "How to load checkpoint from batch without iterating over dataset again.", "selftext": "I am training a model with quite large CelebA data, the training time is quite long on google colab, leading to not running 1 epoch, then the connection is disconnected. I want to ask is there a way to fix this to load the model at the batch stop to continue training in the next lane? Thanks", "upvote_ratio": 0.67, "id": "t3_t4wu1s", "created_utc": 1646215017.0}
{"sub": "pytorch", "title": "Custom dataset's __getitem__ calls itself indefinitely when handling exception", "selftext": "Hey, I'm writing a script for my customdatset class but I get `Index out of range` error whenever I access data using for loop like so:\n```\ncd = CustomDataset(df)\nfor img, target in cd:\n   pass\n```\nI realized I might have a problem reading a few images (if they are corrupt) so I implemented a `random_on_error` feature which chooses a random image if something is wrong with the current image. And I'm sure that's where the problem is. As I've noticed that all the 2160 images in the dataset are read without any hiccups(i print the index number for every iteration) but the loop would not stop and read the 2161st images which results in an `Index out of range` exception that gets handled by reading a random image. This continues forever.\n\nHere is my class:\nhttps://pastebin.com/LkNPGrFb\n\nI believe the problem is with the `except` block (line 27), as when I remove it the code works fine. But I cannot see what the problem here is.\n\nAny help is appreciated, thanks\n\n\nEDIT: found the mistake, I forgot to check for index error and raise it. As without it all the exceptions are handled by the generic `except` block", "upvote_ratio": 1.0, "id": "t3_t4vgto", "created_utc": 1646209119.0}
{"sub": "pytorch", "title": "Beginner practice problem implementing linear regression from scratch using tensors", "selftext": "I'm learning PyTorch. I like to learn by challenging myself to solve toy problems, so I made up [this problem and solution](https://www.practiceprobs.com/problemsets/pytorch/tensors/screen-time/). Perhaps it will help others. (More to come.)", "upvote_ratio": 1.0, "id": "t3_t4irct", "created_utc": 1646169822.0}
{"sub": "pytorch", "title": "What are the best ways to make generator fast converge for dcgan", "selftext": "Hi,\n\nloss function and optimizers defined as followed\n\n    lr = 0.0002\n    beta1 = 0.5\n    loss_fun=nn.BCELoss()\n    optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, \n    0.999),weight_decay=0.0) #\n    optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, \n    0.999),weight_decay=0.0) #\n\n&amp;#x200B;\n\nDiscriminator loss at first was going to 1 around, and slightly decreasing around 0.8 quite far away from 0.5\n\nwhich could prove Discriminator is working.\n\nso the problem might lie in generator?\n\ntried\n\n1. to use bigger learning rate for generator\n2. to use bigger weight\\_decay for generator\n\nboth will result in that less epoch will have noise visually.\n\nhow to use the loss error properly to guide the generator?\n\nor more data\n\n&amp;#x200B;\n\nthanks a ton", "upvote_ratio": 1.0, "id": "t3_t4fngi", "created_utc": 1646161802.0}
{"sub": "pytorch", "title": "How many epoch will lead to overfitting?", "selftext": "Hi,\n\nfor instance, in the [https://pytorch.org/tutorials/beginner/dcgan\\_faces\\_tutorial.html](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)\n\nthere are 220,00 around samples data,\n\nD has 2 million parameters around, G has 3 million parameters around,\n\nseems the network parameters number ten times more than the data,\n\nand there were 5 epoches,\n\nthe results are very good.\n\nHow to tell overfitting or not based on these steps and number ratio or something?\n\n&amp;#x200B;\n\nthanks a ton", "upvote_ratio": 0.33, "id": "t3_t4c442", "created_utc": 1646152790.0}
{"sub": "pytorch", "title": "How to take two inputs of images and extract feature and combine them into latent space?", "selftext": "Hi,\n\nEspecially one object, part of it in one image and the part in the other image. there might be overlap part between images.\n\nto combine them into latent space and later generator could produce a combined one\n\nTried concat in row level, like one is on the top and the other and on the bottom with conv2d() and other batchnorm() leakyrelu() but somehow the result contained two objects not combined.\n\nfind something similar \n\n[https://github.com/PramuPerera/In2I](https://github.com/PramuPerera/In2I) but the code is barely readable. the paper did not disclose very detailed process.\n\nthanks a ton", "upvote_ratio": 0.33, "id": "t3_t4c0td", "created_utc": 1646152553.0}
{"sub": "pytorch", "title": "index out of range when training with SubsetRandomSampler", "selftext": "hey, I'm learning to use `SubsetRandomSampler` since it shuffles and returns every image in dataset (unlike the normal `RandomSampler`). But my training method breaks immediately after using the `SubsetRandomSampler` and gives index out of range error. But everything works find if i use `RandomSampler`\n\nHere is the code: https://pastebin.com/YKgdR5JR\n\nIt gives me: `index 2323 is out of bounds for axis 0 with size 2160` Error. \n\nI've check the code multiple times for typos but I cant figure out whats going wrong. \n\nAny help is appreciated, thanks", "upvote_ratio": 1.0, "id": "t3_t3jtax", "created_utc": 1646066605.0}
{"sub": "pytorch", "title": "Having used tensorflow and wanted to try out pytorch but have now tried for the last couple of hours to pip install torch without success - what am I doing wrong?", "selftext": "EDIT:\n\n**Solved** by using an earlier version of python:\n\n    pyenv local 3.9.9\n    python -m pip install torch\n\nWorked with a somewhat unclear error about unchecked dependencies, numpy and tensorflow. As I'm also going to use tensorflow I just installed it and it seems to have taken care of other potential dependencies.\n\n**Original question**\n\nInstalling tensorflow just works but pytorch seems to be quite picky.\n\nLong story short:\n\nI first created a local python version with pyenv\n\n    pyenv local 3.10.2\n\nThen\n\n    python -m venv .venv\n    source .venv/bin/activate\n\nThen various different approaches following trusty Google (of course none worked - otherwise I wouldn't be here)\n\n1)\n\n    python -m pip install torch\n    \n    ERROR: Could not find a version that satisfies the requirement torch (from versions: none)\n    ERROR: No matching distribution found for torch\n\n\n2) Went to https://pytorch.org/get-started/locally/ and generated a command\n\n    python -m pip install torch==1.10.2+cpu torchvision==0.11.3+cpu torchaudio==0.10.2+cpu -f https://download.pytorch.org/whl/cpu/torch_stable.html\n\n    note: This is an issue with the page at the URL mentioned above.\n    hint: You might need to reach out to the owner of that package index, to get this fixed. See https://github.com/pypa/pip/issues/10825 for context.\n    ERROR: Could not find a version that satisfies the requirement torch==1.10.2+cpu (from versions: none)\n    ERROR: No matching distribution found for torch==1.10.2+cpu\n\n\n3)\n    python -m pip install --use-deprecated=html5lib torch==1.10.2+cpu torchvision==0.11.3+cpu torchaudio==0.10.2+cpu -f https://download.pytorch.org/whl/cpu/torch_stable.html\n\n    Looking in links: https://download.pytorch.org/whl/cpu/torch_stable.html\n    ERROR: Could not find a version that satisfies the requirement torch==1.10.2+cpu (from versions: none)\n    ERROR: No matching distribution found for torch==1.10.2+cpu", "upvote_ratio": 0.89, "id": "t3_t3hu1z", "created_utc": 1646061221.0}
{"sub": "pytorch", "title": "What is PyTorch Mobile?", "selftext": "nan", "upvote_ratio": 0.25, "id": "t3_t3db04", "created_utc": 1646047050.0}
{"sub": "pytorch", "title": "Pytorch C++", "selftext": "Should i use Linux OS for Pytorch C++?", "upvote_ratio": 1.0, "id": "t3_t2t2d7", "created_utc": 1645982710.0}
{"sub": "pytorch", "title": "Constant validation metrics during training", "selftext": "I am training a neural network in pytorch lightning. The learning curves for the training metrics change over time, but the validation metrics are always constant. Originally I thought it might be over fitting but I changed the learning rate and it is still constant.\n\nAny advice for fixing this issue would be appreciated.", "upvote_ratio": 1.0, "id": "t3_t2qq5w", "created_utc": 1645976194.0}
{"sub": "pytorch", "title": "Get image quality score form single class images", "selftext": "Hello community, I come to you with a query: I am trying to find a model to calculate or predict the quality of an image of a single class, that is, of a set of images, all with the object of interest already located, I want to know how much quality it has, this to be able to apply it in a fingerprint and guarantee a good image quality before the image is captured, study a little how onyx camera works, they use a quality net model in tensorflow but I don't know exactly what it is, because I have not had access to the file to pass it through netron, they know of a network that gives me the quality value of an image (in this case fingerprints) that I can train, I already have my dataset with two classes, images good quality and poor quality images", "upvote_ratio": 0.5, "id": "t3_t1q0mb", "created_utc": 1645856123.0}
{"sub": "pytorch", "title": "Link prediction using PyTorch-geometric? Lessons learned, etc?", "selftext": "I\u2019m working to get some link prediction models developed and have been starting to work with PyTorch-geometric. It looks like it\u2019s pretty capable and has the ability to scale reasonably well. I have a heterogenous graph and am expecting the total nodes to be a few million with several edge types. \n\nThe current approach I\u2019m developing is to utilize spark for creating datasets for training because spark is already set up in my environment for this and does similar tasks already. So with the automation to generate training sets already in place the next evolution is to to begin iterating on features/embeddings/model-architecture. \n\nI\u2019m really interested in others experience with heterogeneous graphs and lessons with scaling/integration/automation/etc. PyTorch geometric looks pretty promising from reading the docs and initial exploration with some exemplar data (on a very small scale) seems to indicate good potential. I\u2019m working in AWS with a lot of lambda automation and use of step functions with EMR for executing the data pipelines.  \n\nWhat do you all have to share on this? Thank you.", "upvote_ratio": 1.0, "id": "t3_t1jbv4", "created_utc": 1645836248.0}
{"sub": "pytorch", "title": "Can the kernel of conv2d stride with multiple channels?", "selftext": "Hi,\n\nsuppose input is 8x66x66,  \n\nlike can the kernel extract feature from 1st channel and 5th channel?\n\nand next iteration, 2nd channel and 6th channel...\n\nor is there some similar way to achieve this?\n\n&amp;#x200B;\n\nThanks a  ton", "upvote_ratio": 0.67, "id": "t3_t1it5y", "created_utc": 1645834733.0}
{"sub": "pytorch", "title": "Can I use pytorch to extend a 3D array in a direction of my choice?", "selftext": "This 3D array (https://imgur.com/a/BCuCJNM) is a cross-section of a forest. I would like to predict additional forest slices moving in the x or y direction. I found this tutorial online, but this tutorial is for a 1D data set (https://www.codespeedy.com/predict-next-sequence-using-deep-learning-in-python/).", "upvote_ratio": 0.67, "id": "t3_t1dnoh", "created_utc": 1645820926.0}
{"sub": "pytorch", "title": "How do i improve my unit tests?", "selftext": "Hey, so i'm pretty new to pytorch and I like it very much but find it quite verbose, so I'm writing a framework for personal use that i plan on putting on my cv, so I'm making it complete with unit tests, static typing,etc. \n\nBut its my first time writing tests, so quite clueless about what aspects I'm supposed to test and what I should improve on. \n\nHere is the link to the github repo: https://github.com/default-303/easyTorch\n\nIts pretty small for now, Only has a dataset and a trainer class for classification, but I do intend to expand it with localization and segmentation eventually. So, till then I'd like to have input on how i can imporve my tests and make them better. \n\nAny help is appreciated, thanks", "upvote_ratio": 0.86, "id": "t3_t0y8s4", "created_utc": 1645774804.0}
{"sub": "pytorch", "title": "Do you use cloud GPU platforms?", "selftext": "Hey everyone!   \n\n\nI'm working with a bunch of guys that are developing a new platform for cloud GPU rental, and we're still in the conception stages atm. I understand that not all of you are using GPU clouds, but for those of you who are, are there any features you think that current platforms are missing? Do you think there's much room for improvement in the platforms you've used so far? What's your favourite platform?  \n\n\nIt would be great to get some insight from people who know what theyre talking about :) TIA!\n\n[View Poll](https://www.reddit.com/poll/szfb49)", "upvote_ratio": 1.0, "id": "t3_szfb49", "created_utc": 1645615896.0}
{"sub": "pytorch", "title": "In DCGANs, before the last layer activation for both G and D, is there a need to place a BatchNorm2d()?", "selftext": "Hi,\n\n&amp;#x200B;\n\nIn the official example, [https://pytorch.org/tutorials/beginner/dcgan\\_faces\\_tutorial.html](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)\n\nnone of the G and D has the `BatchNorm2d()` before the last activation?\n\nIs the following error associated with the usage of `BatchNorm2d()`\n\n      Clipping input data to the valid range for imshow with RGB data ([0..1] for \n    floats or [0..255] for integers). \n\nafter getting the result from G, manually\n\n`results=netG(...) ..`\n\n`plt.imshow(results.detach().cpu().numpy()) # there are some transpose operation omitted here`\n\n&amp;#x200B;\n\nthanks a ton", "upvote_ratio": 1.0, "id": "t3_sywgqj", "created_utc": 1645559192.0}
{"sub": "pytorch", "title": "hookandlook - a library helping to gather stats and run checks for Pytorch models", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_syq1o6", "created_utc": 1645543110.0}
{"sub": "pytorch", "title": "Typing and testing for torch", "selftext": "Hey, \nI'm making my first major project that i plan on putting on cv. So i want to make this as complete as possible, that includes typing and testing. The problem is haven't seen anyone use any of those in the wild. Some I'm confused, do people even have testing modules for pytorch projects ? If yes what do they usually test.\n\nSo it'll be very helpful if someone who has done those in their projects let me know how they've achieved those.", "upvote_ratio": 1.0, "id": "t3_sy6pyc", "created_utc": 1645483047.0}
{"sub": "pytorch", "title": "D loss doesn't begin with 1 around and G generates noise", "selftext": "Hi,\n\nFollowed [https://pytorch.org/tutorials/beginner/dcgan\\_faces\\_tutorial.html](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)\n\n&amp;#x200B;\n\n[D G error over iteration](https://preview.redd.it/dlocq362hfj81.png?width=619&amp;format=png&amp;auto=webp&amp;s=83b8433ef1eeab90c3388d34a66a6d45321693d6)\n\n&amp;#x200B;\n\nloss is calculated with BCE\n\nand optimizer is Adam\n\n    ...\n    criterion = nn.BCELoss()\n    optimizerD = optim.Adam(netD.parameters()\uff0cweight_decay=0.3)\n    ...\n\nThe input is 200x3x240x320\n\nD code\n\n    nn.Conv2d(nc, nc*4, 4, 2, 1, bias=False),        \n    nn.BatchNorm2d(nc*4),\n    nn.LeakyReLU(0.2, inplace=True),\n    nn.Dropout(0.2, inplace=False),# DROP!!\n    nn.Conv2d(nc*4, nc*6 , 3, 2, 1, bias=False),\n    nn.BatchNorm2d(nc*6),\n    nn.LeakyReLU(0.2, inplace=True),\n    nn.Conv2d(nc*6 , nc*8 , 3, 2, 1, bias=False),\n    nn.BatchNorm2d(nc*8),\n    nn.LeakyReLU(0.2, inplace=True),\n    nn.Conv2d(nc*8 , nc*4 , 3, 2, 1, bias=False),\n    nn.BatchNorm2d(nc*4 ),\n    nn.LeakyReLU(0.2, inplace=True),\n    nn.Conv2d(nc*4 , nc*2 , 3, 2, 1, bias=False),\n    nn.BatchNorm2d(nc*2),\n    nn.LeakyReLU(0.2, inplace=True),\n    nn.Conv2d(nc*2 , 3 , 3, 2, 1, bias=False),\n    nn.BatchNorm2d(3 ),\n    nn.LeakyReLU(0.2, inplace=True),\n    nn.Conv2d(3 , 3 , 3, 2, 1, bias=False),\n    nn.BatchNorm2d(3 ),\n    nn.LeakyReLU(0.2, inplace=True),\n    nn.Conv2d(3 , 1 , (2,3), 1, 0, bias=False),\n    nn.BatchNorm2d(1 ),\n    nn.LeakyReLU(0.2, inplace=True),\n    nn.Sigmoid(),\n\nG code\n\none difference this G from the official one is the BatchNorm2d(3) before the `Tanh()`\n\n    #latent space is (6, 15, 10)\n    nn.ConvTranspose2d( nz, nz*6, (4,4), 2, 1, bias=False), \n    nn.BatchNorm2d(nz*6),\n    nn.ReLU(True),\n    nn.Dropout(0.5, inplace=False),# DROP!!\n    nn.ConvTranspose2d(nz*6, nz*6, 4, 2, 1, bias=False),\n    nn.BatchNorm2d(nz*6),\n    nn.ReLU(True),\n    nn.Dropout(0.5, inplace=False),# DROP!!\n    nn.ConvTranspose2d(nz*6, nz*4, 4, 2, 1, bias=False),\n    nn.BatchNorm2d(nz*4),\n    nn.ReLU(True),\n    nn.Dropout(0.5, inplace=False),# DROP!!\n    nn.ConvTranspose2d(nz*4, nz*2, (3,4), (1,2), 1, bias=False),\n    nn.BatchNorm2d(nz*2),\n    nn.ReLU(True),\n    nn.Dropout(0.5, inplace=False),# DROP!!\n    nn.ConvTranspose2d(nz*2, nc, 4, 2, 1, bias=False),\n    nn.BatchNorm2d(3),\n    nn.Tanh()    # last gate       \n\nfirst epoch, the result it noise, more epoch, the results are just either black or white.\n\nso stuck here.\n\nData not enough? will `dropout()` help fix it?\n\nD, G too small? add more kernel numbers?\n\nor loss function is not good or optimizer is not good? Is the weigh\\_decay messing around the results?\n\nthanks a ton", "upvote_ratio": 1.0, "id": "t3_sy2wzs", "created_utc": 1645473815.0}
{"sub": "pytorch", "title": "Parallel convolutions", "selftext": "Hello everybody,\n\nI have a model with 3 inputs X1, X2 and X3, each input go in a different convolution layer: \n\n\nDef forward(X1, X2, X3):\n\nF1 = conv1(X1)\n\nF2 = conv2(X2)\n\nF3 = conv3(X3)\n\nReturn F1 + F2 + F3\n\n\nThe convolution layers are executed in sequential, is there a solution to execute them in parallel to decrease the model's execution time?\n\nThank you", "upvote_ratio": 1.0, "id": "t3_sxpv4p", "created_utc": 1645437276.0}
{"sub": "pytorch", "title": "Filter instances in loss calculation", "selftext": "Is there a way to filter out classes in a loss function? Is it necessary to build a custom loss?\nFor example, assume loses shouldn't be calculated only for class 0, so in a binary setting ignore that class and compute loses for the rest.", "upvote_ratio": 1.0, "id": "t3_sx6m6w", "created_utc": 1645378713.0}
{"sub": "pytorch", "title": "Time Series Forecasting Using LSTM", "selftext": "nan", "upvote_ratio": 0.67, "id": "t3_sx3qxv", "created_utc": 1645371194.0}
{"sub": "pytorch", "title": "Best way to skip error inducing images in customdataset in pytorch", "selftext": "Hey, I'm learning pytorch and decided to make a simple cat breed detector. I'm using a public dataset from kaggle consisting of 120k images. I  made my own dataset class but I'm getting a few different errors when training the model. here Is my dataset class: \nhttps://pastebin.com/jmWiFFGf\n\nFew errors, i can recall are, \n```\n    if image.shape[2] &gt; 3:\nIndexError: tuple index out of range\n```\n```\nCould not load \"\"\nReason: \"broken data stream when reading image file\"\n```\n\nIs there a way to just `catch` the error and skip the problematic image altogether? \n\nI thought of a solution:  \n\n```\ndef __getitem__(self, index):\n        try:         \n            target = self.targets[index]\n            image = io.imread(self.image_paths[index])\n### to handle rgba images\n            if image.shape[2] &gt; 3:\n                image = color.rgba2rgb(image)\n        except:\n            print('found error')\n            np.delete(self.image_paths, obj = index, axis = 0)\n            np.delete(self.targee, obj = index, axis = 0)\n            self.__getitem__(index)\n\n### other code continues...\n``` \n\nbut i feel like its not a good solution since my `Dataloader` spits out multiprocessing errors. \n\nHow do you guys handle errors in your image datasets ?\n\nany help is appreciated, thanks", "upvote_ratio": 1.0, "id": "t3_sx07lt", "created_utc": 1645360029.0}
{"sub": "pytorch", "title": "How to register a dynamic backward hook on tensors in Pytorch?", "selftext": "I'm trying to register a backward hook on **each neuron's** weights in a network. By dynamic I mean that it will take a value and multiply the associated gradients by that value.\n\nFrom [here](https://pytorch.org/docs/stable/generated/torch.Tensor.register_hook.html) it seem like it's possible to register a hook on a tensor with a fixed value (though note that I need it to take a value that will change). From [here](https://medium.com/the-dl/how-to-use-pytorch-hooks-5041d777f904) it also seems like it's possible to register a hook on all of the parameters -- they use it to do gradients clipping (though note that I'm trying to only do it on each neuron's weights).\n\nIf my network is as follows:\n\n    class Model(nn.Module):\n        def __init__(self):\n            super(Model, self).__init__()\n    \n            self.fc1 = nn.Linear(3,5)\n            self.fc2 = nn.Linear(5,10)\n            self.fc3 = nn.Linear(10,1)\n    \n        def forward(self, x):\n            x = torch.relu(self.fc1(x))\n            x = torch.relu(self.fc2(x))\n            x = torch.relu(self.fc3(x))\n            return x \n\nThe first layer has 5 neurons with 3 associated weights for each. Hence, this layer should have 5 hooks that modifies (i.e change the current gradient by multiplying it) their 3 associated weights gradients during the backward step.\n\nTraining pseudo-code example:\n\n    net = Model()\n    for epoch in epochs:\n        out = net(data)\n        loss = criterion(out, target)\n        optimizer.zero_grad()\n        loss.backward()\n        for hook in list_of_hooks: #not sure if there's a more \"pytorch\" way of doing this without a for loop\n            hook(random_value)\n        optimizer.step()", "upvote_ratio": 1.0, "id": "t3_swmqof", "created_utc": 1645312711.0}
{"sub": "pytorch", "title": "dataset script does not run in colab", "selftext": "Hey, I've decided to make reusable scripts for frequently used classes. So, i made one for my image dataset and imported it in collab. I can create the dataset object successfully but can't get data from it.\n\nhere is my dataset code: \nhttps://pastebin.com/XfAiTe3A\n\nHere is how i use the script in google colab:\n\n```\nfrom imageDataset import customDataset\ndataset = customDataset(train_data)\\\ndataset[0]\n```\nHere is the error:\n \n```\n     16         target = self.targets[index]\n---&gt; 17         image = io.imread(self.image_paths[index])\n     18 \n     19         if self.augmentations is not None:\n\nSystemError: &lt;built-in function imread&gt; returned NULL without setting an error\n```\n\nBut if i copy paste the code in a jupyter cell , i can use the class like i normally  do. What I'm i doing wrong?\n\n any help is appreciated, thanks\n\n\nEdit: shit started working after i reran the notebook from the start. That said, i hated coding 10mins ago but i think I'm loving it again....", "upvote_ratio": 1.0, "id": "t3_sw7ytk", "created_utc": 1645269934.0}
{"sub": "pytorch", "title": "during importing the pytorch, got error like ?stride@tensor@at@qeba_j_j@z could not be located!", "selftext": "Hi\n\nduring importing the libs, \n\n    import torch \n    import torch.nn as nn \n    import torch.nn.parallel \n    import torch.backends.cudnn as cudnn \n    import torch.optim as optim \n    import torch.utils.data \n    import torchvision.datasets as dset \n    import torchvision.transforms as transforms \n    import torchvision.utils as vutils\n    \n\n&amp;#x200B;\n\ngot an error like\n\n[error](https://preview.redd.it/sz6cye8ffgi81.png?width=419&amp;format=png&amp;auto=webp&amp;s=0a7b21098f6a61c6ffe1d2427c3ce069d56ffd54)\n\n&amp;#x200B;\n\nany idea how to fix it?\n\nthanks a ton.", "upvote_ratio": 0.6, "id": "t3_suyork", "created_utc": 1645130319.0}
{"sub": "pytorch", "title": "RuntimeError: CUDA out of memory.", "selftext": "Hi,\n\n&amp;#x200B;\n\n1. all the network parameters together are less than 50,000 which is literally smaller than official examples [https://pytorch.org/tutorials/beginner/dcgan\\_faces\\_tutorial.html#data](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html#data)\n2. data size are each one is 3x640x480, there might be 900 of them together...\n3. network layers are deep like 40 in total.\n\n&amp;#x200B;\n\n    RuntimeError: CUDA out of memory.  Tried to allocate 1.03 GiB (GPU 0; 8.00 GiB\n     total capacity; 6.34 GiB already allocated; 0 bytes free; 6.34 GiB reserved in\n     total by PyTorch) If reserved memory is &gt;&gt; allocated memory try setting \n    max_split_size_mb to avoid fragmentation.  See documentation for Memory\n\nWhat may cause the memory overflow? data size? like each one resolution too big or too deep layers(when data passing through there will be a lot of memory allocation for parameters or intermediate results? or something else?\n\n&amp;#x200B;\n\nthanks a ton", "upvote_ratio": 0.67, "id": "t3_suyk8c", "created_utc": 1645129981.0}
{"sub": "pytorch", "title": "How to calculate class weights for token level classification problem?", "selftext": "For each of my sentence the 0 labels are very less as compared to the 1\u2019s (for token level classification). I use batches and calculate loss(CrossEntropy) after each batch. How should i create the class weights vector and use it in the loss calculation. Please suggest !", "upvote_ratio": 1.0, "id": "t3_stypkl", "created_utc": 1645025621.0}
{"sub": "pytorch", "title": "waymo dataset", "selftext": "Have anyone used waymo dataset?I would like to use their data for object detection, what is the best way to read tfrecords?", "upvote_ratio": 0.86, "id": "t3_stwz2x", "created_utc": 1645021060.0}
{"sub": "pytorch", "title": "Adding node and cell names for tensorboard graph", "selftext": "nan", "upvote_ratio": 0.81, "id": "t3_stvycz", "created_utc": 1645018240.0}
{"sub": "pytorch", "title": "Efficient neural network", "selftext": "Hello everybody, \n\nI take part in a challenge of efficient neural network, the aim is to have the best score with a maximum of 400GFlops.\n\nMy network has 40k parameters and is using 240Gflops. It's a really lite convnet, I can't really make it lighter.\n\nSome participants proposed a network with 20 millions parameters and using only 140 GFlops. With a better score.\n\nI thought the number of parameters was related to GFlops, I don't understand how they can propose network with so much parameters but using few GFlops.\n\n\nDo you have an idea? \nThank you", "upvote_ratio": 0.67, "id": "t3_stvx7c", "created_utc": 1645018148.0}
{"sub": "pytorch", "title": "after adding a nn.Dropout() layer, the number of parameters won't change?", "selftext": "Hi,\n\n     class MLP(nn.Module):\n     '''     Multilayer Perceptron.   ''' \n        def __init__(self):     \n        super().__init__()     \n        self.layers = nn.Sequential(       \n            nn.Flatten(),  \n            ...     \n            nn.Dropout(p=0.5), \n            ...    \n         )     \n    \n        def forward(self, x): \n        '''Forward pass'''\n        \n             return self.layers(x) \n    \n\nTHANKS A TON", "upvote_ratio": 0.75, "id": "t3_st969r", "created_utc": 1644948455.0}
{"sub": "pytorch", "title": "How to create a dynamic learning rate per neuron in PyTorch?", "selftext": "I know it's possible to have a learning rate per layer ([link](https://stackoverflow.com/questions/51801648/how-to-apply-layer-wise-learning-rate-in-pytorch)). I also found how to dynamically change the learning rate (changing it in the middle of training dynamically without a scheduler) ([link](https://stackoverflow.com/questions/48324152/pytorch-how-to-change-the-learning-rate-of-an-optimizer-at-any-given-moment-no/64453694)).\n\nHow can I create an optimizer that will have a dynamic learning rate **per neuron**? So that I could change the value of the learning rate for specific neurons during training", "upvote_ratio": 0.78, "id": "t3_srwbk0", "created_utc": 1644795481.0}
{"sub": "pytorch", "title": "How do I read an image tensor?", "selftext": "I'm kind of new to deep learning and I'm trying to understand image tensors.\n\nI'm having a tensor.size() output of [1, 3, 48, 48]\nI already know how to read that. Like it's 1 image with rgb channels for the 3 and size 48 by 48 pixels. But how I'm supposed to read the actual tensor output?\nAnyone here to help me out or anyone at least have a source where I can find some information about it?", "upvote_ratio": 0.57, "id": "t3_srrebc", "created_utc": 1644782273.0}
{"sub": "pytorch", "title": "Model loss not behaving correctly", "selftext": "https://preview.redd.it/sk6jpuurvfh81.png?width=1074&amp;format=png&amp;auto=webp&amp;s=e047870b89a688df78b5745bab3b578af23367b8\n\nHi all, I am currently doing a simple implementation of the federated averaging algorithm.\n\nAs can be observed from the image, the training loss stops behaving correctly after a certain number of communication rounds (i.e., instead of plateauing, it starts increasing). This worsening in performance (in the client models) is then transmitted to the overall accuracy of the global model.\n\nCan someone help me understand what could be some causes generating this effect?", "upvote_ratio": 0.67, "id": "t3_sqwuyt", "created_utc": 1644687761.0}
{"sub": "pytorch", "title": "RuntimeError: expand(torch.LongTensor{[512, 16, 16]}, size=[512]): the number of sizes provided (1) must be greater or equal to the number of dimensions in the tensor (3", "selftext": "Does anyone here have experience coding with Hypergraph convolution in Pytorch? I'm getting this error but I don't know where the error is located? Can anyone help me? Thank you.\n\n    Traceback (most recent call last):\n      File \"/home/huynth/miniconda3/envs/inpainting/lib/python3.8/site-packages/torchsummary/torchsummary.py\", line 140, in summary\n        _ = model.to(device)(*x, *args, **kwargs)  # type: ignore[misc]\n      File \"/home/huynth/miniconda3/envs/inpainting/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n        return forward_call(*input, **kwargs)\n      File \"/home/huynth/Hypergraph-Inpainting/models/generator.py\", line 222, in forward\n        x11 = self.graph1(x11, hyperedge_index=refine_conv.squeeze().long())\n      File \"/home/huynth/miniconda3/envs/inpainting/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1120, in _call_impl\n        result = forward_call(*input, **kwargs)\n      File \"/home/huynth/miniconda3/envs/inpainting/lib/python3.8/site-packages/torch_geometric/nn/conv/hypergraph_conv.py\", line 147, in forward\n        B = scatter_add(x.new_ones(hyperedge_index.size(1)),\n      File \"/home/huynth/miniconda3/envs/inpainting/lib/python3.8/site-packages/torch_scatter/scatter.py\", line 29, in scatter_add\n        return scatter_sum(src, index, dim, out, dim_size)\n      File \"/home/huynth/miniconda3/envs/inpainting/lib/python3.8/site-packages/torch_scatter/scatter.py\", line 11, in scatter_sum\n        index = broadcast(index, src, dim)\n      File \"/home/huynth/miniconda3/envs/inpainting/lib/python3.8/site-packages/torch_scatter/utils.py\", line 12, in broadcast\n        src = src.expand_as(other)\n    RuntimeError: expand(torch.LongTensor{[512, 16, 16]}, size=[512]): the number of sizes provided (1) must be greater or equal to the number of dimensions in the tensor (3)", "upvote_ratio": 1.0, "id": "t3_sqr04p", "created_utc": 1644670781.0}
{"sub": "pytorch", "title": "How to allocate the batch for manual initialized tensor?", "selftext": "hi\n\n&amp;#x200B;\n\ndid not use  `dataloader` \n\nso initialize the tensor with `torch.from_numpy`\n\nduring the training, how to set up the batch thing for tensors `data`?\n\n     for epoch in range(num_epochs): \n       // is there something needed for allocating the batches?\n    \n         netD.zero_grad()\n         output = netD(data).\n\nthanks a ton", "upvote_ratio": 1.0, "id": "t3_sq5doc", "created_utc": 1644602230.0}
{"sub": "pytorch", "title": "optimier.zero_grad()", "selftext": "Do we actually need to run optimizer.zero_grad() in a normal training? Or is the gradient already removed if I call optimizer.step()?", "upvote_ratio": 1.0, "id": "t3_sps9i5", "created_utc": 1644560033.0}
{"sub": "pytorch", "title": "How to use the saved model to predict another image?", "selftext": "I am using keypointrcnn model for body joint detection, When i am visualizing the model prediction it is taking images from the test folder which already has the labels and annotations part. How can I use a random image to find the prediction of the model. Whenever I am using random images it is asking for the annotation and labels, but I want a new random image to find the prediction.\n\nHere is the code where the test image are loading with the help of iterator:\n\n \n\niterator\u00a0=\u00a0iter(data\\_loader\\_test)  \nimages,\u00a0targets\u00a0=\u00a0next(iterator)  \nimages\u00a0=\u00a0list(image.to(device) for\u00a0image\u00a0in\u00a0images)  \nwith\u00a0torch.no\\_grad():  \n\u00a0\u00a0\u00a0\u00a0[model.to](https://model.to)(device)  \n\u00a0\u00a0\u00a0\u00a0model.eval()  \n\u00a0\u00a0\u00a0\u00a0output\u00a0=\u00a0model(images)  \nprint(\"Predictions:\u00a0\\\\n\",\u00a0output)\n\n==========================================\n\nThe code below is used for visualizing the result: But it is visualizing only the test image:\n\n \n\nimage\u00a0=\u00a0(images\\[0\\].permute(1,2,0).detach().cpu().numpy()\u00a0\\*\u00a0255).astype(np.uint8)  \nscores\u00a0=\u00a0output\\[0\\]\\['scores'\\].detach().cpu().numpy()  \nhigh\\_scores\\_idxs\u00a0=\u00a0np.where(scores\u00a0&gt;\u00a00.85)\\[0\\].tolist() #\u00a0Indexes\u00a0of\u00a0boxes\u00a0with\u00a0scores\u00a0&gt;\u00a00.7  \npost\\_nms\\_idxs\u00a0=\u00a0torchvision.ops.nms(output\\[0\\]\\['boxes'\\]\\[high\\_scores\\_idxs\\],\u00a0output\\[0\\]\\['scores'\\]\\[high\\_scores\\_idxs\\], 0.3).cpu().numpy() #\u00a0Indexes\u00a0of\u00a0boxes\u00a0left\u00a0after\u00a0applying\u00a0NMS\u00a0(iou\\_threshold=0.3)  \n\\#\u00a0Below,\u00a0in\u00a0output\\[0\\]\\['keypoints'\\]\\[high\\_scores\\_idxs\\]\\[post\\_nms\\_idxs\\]\u00a0and\u00a0output\\[0\\]\\['boxes'\\]\\[high\\_scores\\_idxs\\]\\[post\\_nms\\_idxs\\]  \n\\#\u00a0Firstly,\u00a0we\u00a0choose\u00a0only\u00a0those\u00a0objects,\u00a0which\u00a0have\u00a0score\u00a0above\u00a0predefined\u00a0threshold.\u00a0This\u00a0is\u00a0done\u00a0with\u00a0choosing\u00a0elements\u00a0with\u00a0\\[high\\_scores\\_idxs\\]\u00a0indexes  \n\\#\u00a0Secondly,\u00a0we\u00a0choose\u00a0only\u00a0those\u00a0objects,\u00a0which\u00a0are\u00a0left\u00a0after\u00a0NMS\u00a0is\u00a0applied.\u00a0This\u00a0is\u00a0done\u00a0with\u00a0choosing\u00a0elements\u00a0with\u00a0\\[post\\_nms\\_idxs\\]\u00a0indexes  \nkeypoints\u00a0=\u00a0\\[\\]  \nfor\u00a0kps\u00a0in\u00a0output\\[0\\]\\['keypoints'\\]\\[high\\_scores\\_idxs\\]\\[post\\_nms\\_idxs\\].detach().cpu().numpy():  \n\u00a0\u00a0\u00a0\u00a0keypoints.append(\\[list(map(int,\u00a0kp\\[:2\\])) for\u00a0kp\u00a0in\u00a0kps\\])  \nbboxes\u00a0=\u00a0\\[\\]  \nfor\u00a0bbox\u00a0in\u00a0output\\[0\\]\\['boxes'\\]\\[high\\_scores\\_idxs\\]\\[post\\_nms\\_idxs\\].detach().cpu().numpy():  \n\u00a0\u00a0\u00a0\u00a0bboxes.append(list(map(int,\u00a0bbox.tolist())))  \n   \nvisualize(image,\u00a0bboxes,\u00a0keypoints)\n\n====================================================================\n\nI want to ask how, or where should I modify my code so that i can use any random image for prediction with the help of saved model.", "upvote_ratio": 1.0, "id": "t3_spr7mg", "created_utc": 1644556428.0}
{"sub": "pytorch", "title": "Attempting to rewrite BERT codebase into RoBERTA, running into shape issue.", "selftext": "\\[SOLVED\\]\n\nBiting off more than I can chew here, but trying anyways.\n\nI'm attempting to replace the BERT tokenization/model in this tutorial with all RoBERTA. And running into an issue with tensor shape when executing my training function.\n\nTutorial: [https://github.com/curiousily/Getting-Things-Done-with-Pytorch/blob/master/08.sentiment-analysis-with-bert.ipynb](https://github.com/curiousily/Getting-Things-Done-with-Pytorch/blob/master/08.sentiment-analysis-with-bert.ipynb)\n\nTeaching moment here for someone, but I thought the shape of BERT and RoBERTA would be the same? Perhaps I messed up somewhere?\n\nCopy of my attempt: [https://colab.research.google.com/drive/1TWL1MYrN2xZ9\\_lp0ID3uJoOTzIOOGLzk?usp=sharing](https://colab.research.google.com/drive/1TWL1MYrN2xZ9_lp0ID3uJoOTzIOOGLzk?usp=sharing)\n\nedit: solved, my initial input included an extra column, thus increasing the len of the resulting tensor. classic. \n\nhttps://preview.redd.it/efvjgqxph3h81.png?width=1454&amp;format=png&amp;auto=webp&amp;s=4d364f301b60c44ff7cd9ef7fdc4f4e9a1e4de54", "upvote_ratio": 0.76, "id": "t3_spkpcq", "created_utc": 1644537997.0}
{"sub": "pytorch", "title": "Is there a way to lock seed so training a network will always return same results?", "selftext": "nan", "upvote_ratio": 0.66, "id": "t3_spi5g8", "created_utc": 1644530020.0}
{"sub": "pytorch", "title": "zero_grad() is supposed to be invoked every time one data point passed? How does a scalar.backward() from a loss function affect another model parameters?", "selftext": "Hi,\n\nAccording to the  [https://pytorch.org/tutorials/beginner/dcgan\\_faces\\_tutorial.html](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)\n\n1. `zero_grad()`was called every iteration of a data passed by? is it supposed to be initialized in the beginning of the training for only one time? what is the point here?also in the official tutorial, it was the **neural network model instance** that is called by `zero_grad()`Another example from Internet [https://github.com/himanshu-dutta/dcgan-pytorch/blob/master/DCGAN.ipynb](https://github.com/himanshu-dutta/dcgan-pytorch/blob/master/DCGAN.ipynb) The author called **the optimizer instance** `zero_grad()`Confused a lot by the difference here\n2. `.backward()`is to update the gradients for the output tensors, how does this affect the model parameter or no need? it does not hook with an optimizer for the model when calling  `.step()`\n\nthanks a ton", "upvote_ratio": 1.0, "id": "t3_spgsgh", "created_utc": 1644526433.0}
{"sub": "pytorch", "title": "Are images data supposed to be normalized between 0-1? 0-255 in float is bad formal or something?", "selftext": "Hi,\n\n&amp;#x200B;\n\nSeems pytorch cannot work with uint8 255 data.\n\nSo  data in the normalized 0-1 are better than 0-255 in float ?\n\n&amp;#x200B;\n\nthanks a ton", "upvote_ratio": 0.67, "id": "t3_spf9d1", "created_utc": 1644522528.0}
{"sub": "pytorch", "title": "[P] What we learned by accelerating by 5X Hugging Face generative language models", "selftext": "nan", "upvote_ratio": 0.8, "id": "t3_sp2r0k", "created_utc": 1644485826.0}
{"sub": "pytorch", "title": "How to get specific classes from torchvision.datasets ?", "selftext": "Hi\n\nHow to save the result as a new variable not to override the original one.\n\nSo how to initialize a empty variable or null one?\n\n&amp;#x200B;\n\n    import torchvision.datasets as dset \n    dataset = dset.ImageFolder(root=dataroot)\n\ntried\n\n    idxl= dataset.targets==0\n    DATAL=dataset.data[idxl]\n\nerror goes:\n\n    ~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataset.py\n     in getattr(self, attribute_name)      \n    81 return function     \n     82 else: ---&gt; \n    83 raise AttributeError      \n    84      85 @classmethod  AttributeErro\n\nalso\n\n    DATAR=dataset.__getitem__(2)\n\nalso wrong.referred to\n\n[https://pytorch.org/vision/stable/datasets.html](https://pytorch.org/vision/stable/datasets.html)\n\n[https://discuss.pytorch.org/t/how-to-use-one-class-of-number-in-mnist/26276/8](https://discuss.pytorch.org/t/how-to-use-one-class-of-number-in-mnist/26276/8)\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nthanks a ton", "upvote_ratio": 0.67, "id": "t3_sooe9e", "created_utc": 1644441919.0}
{"sub": "pytorch", "title": "how to use the dilation and kernel in conv2d to reduce the size of tensor properly?", "selftext": "&amp;#x200B;\n\nhi,\n\n    class trial (nn.Module):      \n        self.main = nn.Sequential(         \n            nn.Conv2d(3,3,(2,1),0,0,(60,1))         \n            )      \n        def forward(self,input):        \n             return input  \n    y=torch.randn(4,3,120,60) \n    T=trial() \n    result=T(y) \n\nresult size is expected to be (4,3,60,60).  \nThe kernel and dilation do not work together?  \nThanks a ton", "upvote_ratio": 1.0, "id": "t3_sojbkt", "created_utc": 1644428618.0}
{"sub": "pytorch", "title": "How to Classify Images with Unsupervised Learning in Pytorch", "selftext": "Hi guys. I'm trying to classify images into two classes using Unsupervised Learning in Pytorch. Could anyone point me to any beginner-friendly tutorials that could help? Thanks!", "upvote_ratio": 0.87, "id": "t3_so5q03", "created_utc": 1644384688.0}
{"sub": "pytorch", "title": "Basic mistake? nn is not defined", "selftext": "How is the following possible? I am importing torch.nn:\n\nimport torch.nn as nn\n\nThus, nn is defined, which I have confirmed. However I am getting NameError: name 'nn' is not defined.\n\n    PS C:\\VTCProject\\yolov5&gt; python\n    Python 3.8.5 (tags/v3.8.5:580fbb0, Jul 20 2020, 15:57:54) [MSC v.1924 64 bit (AMD64)] on win32\n    Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n    &gt;&gt;&gt; import numpy\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; import torch.nn as nn\n    &gt;&gt;&gt; nn\n    &lt;module 'torch.nn' from 'C:\\\\Python38\\\\lib\\\\site-packages\\\\torch\\\\nn\\\\__init__.py'&gt;\n    &gt;&gt;&gt; import yaml\n    &gt;&gt;&gt; from models.ylmt import Model\n    Traceback (most recent call last):\n      File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n      File \"C:\\VTCProject\\yolov5\\models\\ylmt.py\", line 1, in &lt;module&gt;\n        class Model(nn.Module):\n    NameError: name 'nn' is not defined\n    &gt;&gt;&gt; exit\n    Use exit() or Ctrl-Z plus Return to exit\n    &gt;&gt;&gt; exit()\n\nThe file models\\\\[ylmt.py](https://ylmt.py) should inherits the modules imported by the parent script, right?\n\nI ran into this running the Ultralytics training example with custom data:\n\n     python train.py --data .\\data\\roadometry.yaml --cfg yolov5s --weights '' --batch-size 16    \n\n\\------------------------------------\n\nEdit:\n\nIt seems that my understanding was wrong:\n\n[https://stackoverflow.com/questions/3106089/python-import-scope](https://stackoverflow.com/questions/3106089/python-import-scope)\n\nSo I guess my question now is, how did this ever work in the Ultralytics repo? The contents of [ylmt.py](https://ylmt.py) are:\n\n    class Model(nn.Module):\n        def init(self, cfg='ylmt.yaml', ch=3, frames=2, nc=36):  \n            # model, input channels, number of classes \n            super(Model, self).init() print(\"init\")\n        \n        def forward(self, x, augment=False, profile=False):\n            print(\"ylmt: forward\")\n\n&amp;#x200B;", "upvote_ratio": 1.0, "id": "t3_snh6lq", "created_utc": 1644316526.0}
{"sub": "pytorch", "title": "How to extract feature from 2 tensors into one? what layer should be used?", "selftext": "Hi,\n\n    class Combine(nn.Module):    \n        def __init__(self, ngpu): \n        self.main = nn.Sequential\n        (    ... )   \n        def forward(self, t1,t2):    \n            return self.main(t1,t2) \n\nconfused what kind of layer to use?", "upvote_ratio": 1.0, "id": "t3_sna82o", "created_utc": 1644292192.0}
{"sub": "pytorch", "title": "Recoloring images with pytorch!", "selftext": "nan", "upvote_ratio": 0.97, "id": "t3_smeykx", "created_utc": 1644201354.0}
{"sub": "pytorch", "title": "PyTorch-Lightning trainer.fit() \"Validation sanity check\" fails (full error trace in post) - any ideas?", "selftext": "**EDIT: Never mind, I found the error, I am very dumb, sorry for wasting your time.**\n\n*Solution:* in the `TestDataset`, [`self.data`](https://self.data) is not defined; thus calling dataset.\\_\\_len\\_\\_ returns the AttributeError. The solution is to not mindlessly copy-paste tutorial code. [`self.data`](https://self.data) should be replaced with `self.encodings.input_ids`.\n\n*Incidental Solution:* I also did not set up my `forward()` method correctly for the LightningModule. `forward()` should instead be:\n\n    \n    def forward(self, input_ids, attention_mask, labels):\n        return self.model(input_ids, attention_mask, labels).logits\n    \n\nIf you have any other comments / \"by the way, this is a better way to do this,\" or want to laugh at my mistakes, please feel free to comment anyway. Thank you for your time!\n\n\\----\n\nHi all,\n\nI am having a little trouble getting a LightningModule off the ground. When I call [`trainer.fit`](https://trainer.fit)`(model, data_module)`, I receive the following (very long) error trace:\n\n    &gt;&gt;&gt; trainer.fit(model, data_module)\n    LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n    \n      | Name  | Type               | Params\n    ---------------------------------------------\n    0 | model | RobertaForMaskedLM | 355 M\n    ---------------------------------------------\n    355 M     Trainable params\n    0         Non-trainable params\n    355 M     Total params\n    1,421.648 Total estimated model params size (MB)\n    Validation sanity check: 0it [00:00, ?it/s]Traceback (most recent call last):\n      File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n      File \"C:\\Program Files\\Python38\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 460, in fit\n        self._run(model)\n      File \"C:\\Program Files\\Python38\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 758, in _run\n        self.dispatch()\n      File \"C:\\Program Files\\Python38\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 799, in dispatch\n        self.accelerator.start_training(self)\n      File \"C:\\Program Files\\Python38\\lib\\site-packages\\pytorch_lightning\\accelerators\\accelerator.py\", line 96, in start_training\n        self.training_type_plugin.start_training(trainer)\n      File \"C:\\Program Files\\Python38\\lib\\site-packages\\pytorch_lightning\\plugins\\training_type\\training_type_plugin.py\", line 144, in start_training\n        self._results = trainer.run_stage()\n      File \"C:\\Program Files\\Python38\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 809, in run_stage\n        return self.run_train()\n      File \"C:\\Program Files\\Python38\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 844, in run_train\n        self.run_sanity_check(self.lightning_module)\n      File \"C:\\Program Files\\Python38\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 1112, in run_sanity_check\n        self.run_evaluation()\n      File \"C:\\Program Files\\Python38\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 925, in run_evaluation\n        dataloaders, max_batches = self.evaluation_loop.get_evaluation_dataloaders()\n      File \"C:\\Program Files\\Python38\\lib\\site-packages\\pytorch_lightning\\trainer\\evaluation_loop.py\", line 63, in get_evaluation_dataloaders\n        self.trainer.reset_val_dataloader(model)\n      File \"C:\\Program Files\\Python38\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py\", line 409, in reset_val_dataloader\n        self.num_val_batches, self.val_dataloaders = self._reset_eval_dataloader(model, 'val')\n      File \"C:\\Program Files\\Python38\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py\", line 370, in _reset_eval_dataloader\n        num_batches = len(dataloader) if has_len(dataloader) else float('inf')\n      File \"C:\\Program Files\\Python38\\lib\\site-packages\\pytorch_lightning\\utilities\\data.py\", line 32, in has_len\n        if len(dataloader) == 0:\n      File \"C:\\Program Files\\Python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 404, in __len__\n        return len(self._index_sampler)\n      File \"C:\\Program Files\\Python38\\lib\\site-packages\\torch\\utils\\data\\sampler.py\", line 245, in __len__\n        return (len(self.sampler) + self.batch_size - 1) // self.batch_size  # type: ignore[arg-type]\n      File \"C:\\Program Files\\Python38\\lib\\site-packages\\torch\\utils\\data\\sampler.py\", line 69, in __len__\n        return len(self.data_source)\n      File \"C:\\Users\\E\\Documents\\RobertaTest.py\", line 59, in __len__\n        return len(self.data)\n      File \"C:\\Program Files\\Python38\\lib\\site-packages\\torch\\utils\\data\\dataset.py\", line 83, in __getattr__\n        raise AttributeError\n    AttributeError\n\nI'm trying to run a masked language model using `roberta-large`. Here is that LightningModule:\n\n    class TestMLM(pl.LightningModule):\n        def __init__(self, model_name_or_path, learning_rate, adam_beta1, adam_beta2, adam_epsilon, n_training_steps=None, n_warmup_steps=None):\n            super().__init__()\n            self.save_hyperparameters()\n            self.n_training_steps = n_training_steps\n            self.n_warmup_steps = n_warmup_steps\n            self.learning_rate = learning_rate\n            config = RobertaConfig.from_pretrained(model_name_or_path, return_dict=True)\n            self.model = RobertaForMaskedLM.from_pretrained(model_name_or_path, config=config)\n        def forward(self, x):\n            return self.model(x).logits\n        def training_step(self, batch, batch_idx):\n            input_ids = batch[\"input_ids\"]\n            attention_mask = batch[\"attention_mask\"]\n            labels = batch[\"labels\"]\n            loss = self(input_ids,attention_mask,labels)\n            self.log('train_loss', loss, on_epoch=True, prog_bar=True, logger=True)\n            return {\"Training Loss\": loss}\n        def validation_step(self, batch, batch_idx):\n            input_ids = batch[\"input_ids\"]\n            attention_mask = batch[\"attention_mask\"]\n            labels = batch[\"labels\"]\n            loss = self(input_ids,attention_mask,labels)\n            self.log('val_loss', loss, on_epoch=True, prog_bar=True, logger=True)\n            return {\"Validation Loss\": loss}\n        def test_step(self, batch, batch_idx):\n            input_ids = batch[\"input_ids\"]\n            attention_mask = batch[\"attention_mask\"]\n            labels = batch[\"labels\"]\n            loss = self(input_ids,attention_mask,labels)\n            self.log('test_loss', loss, on_epoch=True, prog_bar=True, logger=True)\n            return {\"Test Loss\": loss}\n        def configure_optimizers(self):\n            optimizer = AdamW(self.parameters(), lr = self.learning_rate,\n                              betas=(self.hparams.adam_beta1,\n                                     self.hparams.adam_beta2),\n                              eps=self.hparams.adam_epsilon,)\n            scheduler = get_linear_schedule_with_warmup(\n                optimizer,\n                num_warmup_steps=self.n_warmup_steps,\n                num_training_steps=self.n_training_steps\n            )\n            return dict(\n                optimizer=optimizer,\n                lr_scheduler=dict(\n                    scheduler=scheduler,\n                    interval='step'\n                )\n            )\n\nhere's the LightningDataModule:\n\n    class TestDataModule(pl.LightningDataModule):\n        def __init__(self, train_data, test_data, tokenizer, batch_size=BATCH_SIZE, max_token_len=MAX_TOKEN_COUNT):  #max max 512 \n            super().__init__()\n            self.batch_size = batch_size\n            self.train_data = train_data\n            self.test_data = test_data\n            self.tokenizer = tokenizer\n            self.max_token_len = max_token_len\n        def setup(self, stage=None):\n            self.train_dataset = TestDataset(\n                self.train_data\n            )\n            self.test_dataset = TestDataset(\n                self.test_data\n            )\n        def train_dataloader(self):\n            return DataLoader(\n                self.train_dataset,\n                batch_size=self.batch_size,\n                shuffle=True,\n                num_workers=0\n            )\n        def val_dataloader(self):\n            return DataLoader(\n                self.test_dataset,\n                batch_size=self.batch_size,\n                num_workers=0\n            )\n        def test_dataloader(self):\n            return DataLoader(\n                self.test_dataset,\n                batch_size=self.batch_size,\n                num_workers=0\n            )\n\nand the Dataset:\n\n    class TestDataset(Dataset):\n        def __init__(self, encodings):\n            self.encodings=encodings\n        def __len__(self):\n            return len(self.data)\n        def __getitem__(self, idx):\n            return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n\nThe type of data that are passed in to instantiate a Dataset is of class `&lt;class 'transformers.tokenization_utils_base.BatchEncoding'&gt;`. Those data look like:\n\n    {'input_ids': tensor([[    0,  1437,  1437,  ...,     1,     1,     1],\n            [    0,  1437,  1437,  ...,     1,     1,     1],\n            [    0, 50264,  8845,  ...,    55,    87,     2],\n            ...,\n            [    0, 10127,  8845,  ...,     1,     1,     1],\n            [    0,  1437,  1437,  ...,     1,     1,     1],\n            [    0,  1437,  1437,  ...,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n            [1, 1, 1,  ..., 0, 0, 0],\n            [1, 1, 1,  ..., 1, 1, 1],\n            ...,\n            [1, 1, 1,  ..., 0, 0, 0],\n            [1, 1, 1,  ..., 0, 0, 0],\n            [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[    0,  1437,  1437,  ...,     1,     1,     1],\n            [    0,  1437,  1437,  ...,     1,     1,     1],\n            [    0, 10127,  8845,  ...,    55,    87,     2],\n            ...,\n            [    0, 10127,  8845,  ...,     1,     1,     1],\n            [    0,  1437,  1437,  ...,     1,     1,     1],\n            [    0,  1437,  1437,  ...,     1,     1,     1]])}\n\nAny ideas what I'm doing wrong here? I'm sorry for the long post &amp; code; I figure it's important to see the full picture. Thank you for any help you may be able to provide!", "upvote_ratio": 1.0, "id": "t3_sm2iuk", "created_utc": 1644169235.0}
{"sub": "pytorch", "title": "How to parallelize a training loop ever samples of a batch when CPU is only available in pytorch?", "selftext": "nan", "upvote_ratio": 0.84, "id": "t3_sm073v", "created_utc": 1644163554.0}
{"sub": "pytorch", "title": "Trying to write an image denoising network with unexpectedly poor results. Any advice appreciated.", "selftext": "Hi folks, I've been working on image denoising for a couple of months but I can't seem to solve the poor performance of my denoiser. Below is my model. Please refer to the class DnCnn at the bottom.\n\n\n    \n    class FCN(nn.Module):\n        def __init__(self):\n            super(FCN, self).__init__()\n            self.fcn = nn.Sequential(\n                nn.Conv2d(3, 32, 3, padding=1),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(32, 32, 3, padding=1),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(32, 32, 3, padding=1),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(32, 32, 3, padding=1),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(32, 3, 3, padding=1),\n                nn.ReLU(inplace=True)\n            )\n    \n        def forward(self, x):\n            return self.fcn(x)\n    \n    \n    class resblock(nn.Module):\n        def __init__(self, in_channels, out_channels):\n            super().__init__()\n    \n            self.strided_conv = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, stride=1, kernel_size=3, padding=1, bias=False),\n                nn.BatchNorm2d(out_channels),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n                nn.BatchNorm2d(out_channels),\n                nn.ReLU(inplace=True)\n            )\n        \n        def forward(self, x):\n            conv_block = self.strided_conv(x)\n            if conv_block.size() == x.size():\n                out = x + conv_block\n                return out\n            else:\n                return conv_block\n    \n    \n    class dncnn_block(nn.Module):\n        def __init__(self, in_channels, nc, out_channels):\n            super().__init__()\n            self.resblock1 = resblock(in_channels, nc)\n            self.resblock2 = resblock(nc, nc)\n            self.resblock3 = resblock(nc, nc)\n            self.resblock4 = resblock(nc, nc)\n            self.resblock5 = resblock(nc, nc)\n            self.resblock6 = resblock(nc, nc)\n            self.resblock7 = resblock(nc, nc)\n            self.resblock8 = resblock(nc, nc)\n            self.resblock9 = resblock(nc, out_channels)\n    \n        def forward(self, x):\n            layer1 = self.resblock1(x)\n            layer2 = self.resblock2(layer1)\n            layer3 = self.resblock3(layer2)\n            layer4 = self.resblock4(layer3)\n            layer5 = self.resblock5(layer4)\n            layer6 = self.resblock6(layer5)\n            layer7 = self.resblock7(layer6)\n            layer8 = self.resblock8(layer7)\n            layer9 = self.resblock9(layer8)\n    \n            return layer9\n    \n    \n    class DnCNN(nn.Module):\n        def __init__(self, in_nc=6, out_nc=3, nc=64, nb=20, act_mode='BR'):\n            super(DnCNN, self).__init__()\n            assert 'R' in act_mode or 'L' in act_mode\n            bias = False\n            self.fcn = FCN()\n            dncnn1 = dncnn_block(nc, nc, nc)\n            head1 = conv(in_nc, nc, mode='C'+act_mode[-1], bias=bias)\n            tail1 = conv(nc, out_nc, mode='C', bias=bias)\n            self.model1 = sequential(head1, dncnn1, tail1)\n    \n        def forward(self, x, train_mode=True):\n            noise_level = self.fcn(x)\n            concat_img = torch.cat([x, noise_level], 1)\n            level1_out = self.model1(concat_img) + x\n            return noise_level, level1_out\n\n\nI call DnCNN as my network. It is made up of different blocks. I have excluded the conv() function but it is just a function that returns specified layers. The input is the noisy image x. The idea is that there is a mini network/ancillary network called fcn() whose output is merged with the noisy image x and is passed through the main network. The original image is then added back onto the predicted: residual image *level1_out = self.model1(concat_img) + x* to return a denoised image.\n\nI am returning \"noise_map\" as I have ground truth noise maps which represent the severity of the noise. As such being able to determine the noise map is a means of knowing how much noise is in the image prior to denoising. \n\nMy problem is that when the network is done training, the result is not very smooth on very fine Gaussian noise. This is unexpected because I am using identical training parameters to networks that perform well in this regard. I have tried with many different datasets also, including the same used in other state of the art denoising solutions. I've run out of experiments that I can think of that may be causing the problem. I am posting this here in case someone sees something improper in the way the network is built.\n\nThanks folks.\n\nedit: I forgot to say I'm training it on x3 2080 TIs with torch.nn.DataParallel().", "upvote_ratio": 1.0, "id": "t3_slx5qr", "created_utc": 1644155598.0}
{"sub": "pytorch", "title": "Respect format text: fairseq m2m_100", "selftext": "Hello, I use fairseq and the m2m\\_100 model to translate text, however the format of the text is not respected and the lines are not jumped, I found the solution for the line break (separate the text between each line, translate it individually and reunite all), however there is some text that cannot be translated, like emojis or characters marked as unknown (e.g.: \"\u25ac\"), I also have some style elements (e.g. \\_\\_ for highlighting). How can I make the translation respect these elements?\n\n&amp;#x200B;\n\nfor the emojis I tried to replace them with a number but the sentence loses meaning, and it doesn't work all the time.", "upvote_ratio": 1.0, "id": "t3_sliib5", "created_utc": 1644099865.0}
{"sub": "pytorch", "title": "How can I choose the right optimizer?", "selftext": "I'm pretty new to coding neural networks. I'm currently working on an emotion image detection and I'm wondering what's the best optimizer or how I can find the right one for the project.", "upvote_ratio": 0.86, "id": "t3_slfgta", "created_utc": 1644091707.0}
{"sub": "pytorch", "title": "Tutorial: An app for Fashion Search using DocArray and Torchvision", "selftext": "Hey everyone, I've built a tutorial for a fashion search prototype. It uses DocArray and Torchvision, ResNet50 model. Right now it's still in the basic stages. Check it out here: [https://colab.research.google.com/github/alexcg1/neural-search-notebooks/blob/main/fashion-search/1\\_build\\_basic\\_search/basic\\_search.ipynb](https://colab.research.google.com/github/alexcg1/neural-search-notebooks/blob/main/fashion-search/1_build_basic_search/basic_search.ipynb) \n\nWould love to know what you guys think. If you can point me to more suitable models, that would be great!", "upvote_ratio": 1.0, "id": "t3_sl5492", "created_utc": 1644061424.0}
{"sub": "pytorch", "title": "Probe PyTorch models", "selftext": "We wrote simple class to extract hidden layer outputs, gradients, and parameters of PyTorch models.\n\nHere's an example:\n\n    probe = ModelProbe(model)\n    out = model(inp)\n    attn_queries_filter = probe.forward_output['*.attention.query']\n    attn_queries = torch.stack(attn_queries_filter.get_list())\n\n* \ud83e\uddd1\u200d\ud83c\udfeb [Demo that extracts attention maps of BERT](https://github.com/labmlai/labml/blob/master/guides/model_probe.ipynb)\n* \ud83d\udcda [Documentation](https://docs.labml.ai/api/analytics.html#probing)\n* \ud83d\udcbb [Github](https://github.com/labmlai/labml)", "upvote_ratio": 0.91, "id": "t3_sl433i", "created_utc": 1644057497.0}
{"sub": "pytorch", "title": "Torchrun command not found, does it need to be installed seperately?", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_skwhz1", "created_utc": 1644030386.0}
{"sub": "pytorch", "title": "Showing the pre-trained hyper-parameters of pytorch model", "selftext": "Hello guys, i'm data science student and i'm newbie with pytorch (i always use tf). I've trained a pytorch model that has saved the last checkpoint. Since it's been a while and I don't remember what hyperparameters I trained the model with, my question is: is there a way that from the checkpoint can display them?\n\nthanks all.", "upvote_ratio": 0.88, "id": "t3_sjgnnt", "created_utc": 1643886788.0}
{"sub": "pytorch", "title": "What is the official implementation of first order MAML using the higher PyTorch library?", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_sixdqd", "created_utc": 1643829908.0}
{"sub": "pytorch", "title": "Torch Datasets Tutorial", "selftext": "Hi all,\n\nI'm looking for a comprehensive tutorial on pytorch datasets and how they're used by various projects. The documentation on them seems a bit sparse, and doesn't impose much structure. There are also lots of other wrappers like \\`TensorDataset\\` and \\`IterableDataset\\` without much documentation.\n\nThanks", "upvote_ratio": 0.67, "id": "t3_siozrw", "created_utc": 1643809354.0}
{"sub": "pytorch", "title": "Would making the gradient \"data\" by detaching them implement first order MAML using PyTorch's higher library?", "selftext": "nan", "upvote_ratio": 0.75, "id": "t3_si5xv1", "created_utc": 1643750372.0}
{"sub": "pytorch", "title": "[Question] Pytorch with databases", "selftext": "I've been thinking about developing a tool that would help to transform the graph DB data into a Pytorch dataset and use it with a custom model. Are there some red flags that I should have in mind regarding this tight coupling between ML and DB?", "upvote_ratio": 1.0, "id": "t3_shtulo", "created_utc": 1643718593.0}
{"sub": "pytorch", "title": "Tried to allocate less than there is free memory.Cuda out of Memory", "selftext": "RuntimeError: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 0; 15.90 GiB total capacity; 14.72 GiB already allocated; 81.75 MiB free; 14.83 GiB reserved in total by PyTorch)\n\nHow?", "upvote_ratio": 1.0, "id": "t3_shqlgf", "created_utc": 1643706306.0}
{"sub": "pytorch", "title": "[Question] Reorder/select elements of tensor based on index tensor", "selftext": "Greetings,\n\nI have the following two tensors\n\n    input shape: 16 32 32 3\n    index shape: 16 32 32 2\n    output shape: 16 32 32 3\n\nThe formula for the output would be:\n\n`output[b, h, w] = input[b, index[b, h, w, 0], index[b, h, w, 1]]`\n\nI tried to use `torch.gather` but I was not able to formulate the previous assignment.\n\nDoes anyone know how to do this in an efficient manner? Thanks!\n\n&amp;#x200B;\n\n*For context: input contains a batch of 16 elemens where each one is a tensor of 32x32 that containts 3D points. index is a mapping from position to 3D point.*", "upvote_ratio": 1.0, "id": "t3_sh96dz", "created_utc": 1643655911.0}
{"sub": "pytorch", "title": "GPT2 from scratch | Video tutorial", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_sh6vkz", "created_utc": 1643650408.0}
{"sub": "pytorch", "title": "Can anybody explain me how to use weight and pos_weight params in BCEwithlogitsloss?", "selftext": "I am trying to create a multi label classifier in which dataset is unbalanced. To tackle this problem somebody suggested me I should use weight but after applying weights my model's accuracy decreased. To provide weights I have used two formulas :\n\n1. (total\\_elements)/(num\\_classes\\*label\\_count)\n2. 1/label\\_count\n\nSuppose if my dataset of 1000 entries with 2 classes hot encoded and 0 is present 700 times and 1 is present 400 times. So here total elements are 1000,num\\_classes are 2 and label count for 0 is 700 and for 1 is 400.\n\nBefore using weights my model's accuracy was around 60 percent. But after using weights it got reduced to 30 percent. I know I am doing something wrong with weight stuff and I also dont know how to calculate pos\\_weight either. Can somebody explain me what I am doing wrong in this process?\n\nInfo about the model that I'm working on:\n\nI am working on multi-label movie genre detector with 12 genres. I am using a basic convolution network as a model. I'm using sigmoid as final layer.", "upvote_ratio": 1.0, "id": "t3_sh3tb4", "created_utc": 1643642868.0}
{"sub": "pytorch", "title": "A tensor-slicing question", "selftext": "I'm trying to assemble a tensor based on the contents of two other tensors, like so:\n\nI have a 2D tensor called A, with shape \\[I, J\\], and another 2D tensor called B, with shape \\[M, N\\], whose elements are indices into the 1st dimension of A.I want to obtain 3D tensor C with shape \\[M, N, J\\] such that `C[m,n,:] == A[B[m,n],:]`.\n\nI \\*could\\* do this using nested for-loops to iterate over all indices in M and N, assigning the right values to C at each one, but M and N are large so this may be slow.  I suspect there's some faster way of doing this using some clever slicing or some pytorch function, but I don't know what it would be.  It looks a bit like somewhere one would use `torch.gather()`, but that requires all tensors to have the same number of dimensions.  Can anyone help me?\n\nEDIT: I got an answer over on StackOverflow: [https://stackoverflow.com/questions/70926905/how-to-build-a-tensor-from-one-tensor-of-contents-and-another-of-indices](https://stackoverflow.com/questions/70926905/how-to-build-a-tensor-from-one-tensor-of-contents-and-another-of-indices)", "upvote_ratio": 0.8, "id": "t3_sgnyt2", "created_utc": 1643590642.0}
{"sub": "pytorch", "title": "Concatenate different datasets on different channels", "selftext": "Hi everyone!\n\nI'm quite new with Pytorch and I'm getting a weird problem when I try to concatenate a dataset with filtered images with the originales ones ON DIFFERENT CHANNELS.\n\nFirst I load both with `ImageFolder()` and then use `torch.cat()` to concatenate the filtered dataset on the \"raw\" dataset (on another channel, of course).\n\nBut I get this error (I'm trying one of the images as an example):\n\nhttps://preview.redd.it/ybjuwim8ove81.png?width=831&amp;format=png&amp;auto=webp&amp;s=257f91b899c8398218b7feac7379b19b00ec203c\n\n`full_dataset_orig[0][0]` is one of the greyscaled images from the raw dataset and it is a \\[1,126,126\\] tensor, `full_dataset_aug1[0][0]` is from the filtered dataset and it has the same TYPE AND DIMENSIONS.\n\n**Why python is pointing out that it's a tuple?**\n\n**Is there an easier way to concatenate on diffent channels 2 pytorch datasets?**\n\n&amp;#x200B;\n\nThank you in advance!", "upvote_ratio": 1.0, "id": "t3_sgh0mp", "created_utc": 1643571678.0}
{"sub": "pytorch", "title": "Torchscript throws *args and *kwargs error while converting a model", "selftext": "Hi,\n\nI'm pretty new to torchscript and  was trying to convert a model to JIT but it throws this error:\n\ntorch.jit.frontend.NotSupportedError: Compiled functions can't take a variable number of arguments or use keyword-only arguments with defaults:\n\n  `File \"/home/anushka/airborne-detection-starter-kit/seg_tracker/models_transformation.py\", line 60`\n\n`def updated_forward(*args, **kwargs):`\n\n`~~~~~~~ &lt;--- HERE`\n\n`a = (tsm(args[0], duration=duration, dilation=dilation), ) + args[1:]`\n\n`return orig_forward(*a, **kwargs)`\n\nHere's the function which is giving this error:\n\n`def add_tsm_to_module(obj, duration, dilation=1):`  \n `orig_forward = obj.forward`  \n `def updated_forward(*args, **kwargs):`  \n `a = (tsm(args[0], duration=duration, dilation=dilation), ) + args[1:]`  \n `return orig_forward(*a, **kwargs)`  \n `obj.forward = updated_forward`  \n `return obj`\n\n&amp;#x200B;\n\nAny help would be really appreciated \n\nThanks", "upvote_ratio": 0.86, "id": "t3_sgbxph", "created_utc": 1643557707.0}
{"sub": "pytorch", "title": "CUDA Versions on Pytorch", "selftext": "I'll try to make it as short as possible.\n\nI have CUDA 11.2 installed on my PC but I downloaded the pytorch version that is with the Cuda 11.3 (from the official website where they ask you your platform and CUDA version and give you a command to install pytorch)\n\nWhen I set my device to my gpu the model is still training using my cpu so I realized its probably because of the CUDA different versions.\n\n&amp;#x200B;\n\nIs there any way to fix it without having to download CUDA 11.3 ?", "upvote_ratio": 0.8, "id": "t3_sffuyy", "created_utc": 1643454054.0}
{"sub": "pytorch", "title": "Training Large Datasets", "selftext": "Hi guys. I'm new to deep learning and until now I've used 1gb - 5gb datasets in model training. Now I have a 50gb dataset. How do you train a model with a dataset that large? It can't fit comfortably on my local computer by the way.", "upvote_ratio": 0.88, "id": "t3_serim1", "created_utc": 1643378575.0}
{"sub": "pytorch", "title": "Difference in type between Input type and Weight type with ModuleList", "selftext": " Hello guys,  \nI have a problem with a list of modules. I have coded a module called BranchRoutingModule. I would like to create a list from this module.  \nI have the following code: \n\n    def _branch_routings(self):\n        # structure = [nn.ModuleList([BranchRoutingModule(in_channels=self.in_channels) for j in range(int(pow(2, i)))]) for i in range(self.tree_height - 1)] # [[None], [None, None]] for tree height = 3\n        structure = [[None for j in range(int(pow(2, i)))] for i in range(self.tree_height - 1)] # [[None], [None, None]] for tree height = 3\n    \n        cur = 0\n        for i in range(self.tree_height - 1):\n          for j in range(int(pow(2, i))):\n            self.__setattr__('branch_routing_module' + str(cur), BranchRoutingModule(in_channels=self.in_channels))\n            structure[i][j] = self.__getattr__('branch_routing_module' + str(cur))\n            cur += 1\n        return structure\n    \n\n \n\nI have first tried using nn.ModuleList (commented out at the top) but I get the following error: \u201cInput type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same\u201d\n\nHowever if I use **setattr** and **getattr**, I get no errors and my model works fine.\n\nWhy is that? I don\u2019t understand why **setattr** and **getattr** fix the problem.  \nI am using CUDA.\n\nThank you and regards,  \nAntoine", "upvote_ratio": 1.0, "id": "t3_se4l8m", "created_utc": 1643306842.0}
{"sub": "pytorch", "title": "Problem calculating accuracy on multiclass classfication", "selftext": "Hi, I am facing a problem when I try to compute accuracy on a multiclass classification problem with the Accuracy() function from torchmetrics  \nHere is the training function I created:\n\n    from torchmetrics import Accuracy\n    accuracy = Accuracy()\n    \n    def training(epoch, model, train_loader, optimizer, criterion):\n      \"Training over an epoch\"\n      metric_monitor = MetricMonitor()\n      \n      model.train()\n    \n      for batch in train_loader:\n        images = batch['t1'][tio.DATA].cuda()\n        labels = batch['label'].cuda()\n        output = F.softmax(model(images), dim=0)\n    \n        loss = criterion(output, labels)\n    \n        output = output.data.max(dim=1,keepdim=True)[1]\n    \n        acc = accuracy(output, labels)\n     \n        metric_monitor.update(\"Loss\", loss.item())\n        metric_monitor.update(\"Accuracy\", acc)\n    \n        optimizer.zero_grad()\n    \n        loss.backward()\n        optimizer.step()\n      print(\"[Epoch: {epoch:03d}] Train      | {metric_monitor}\".format(epoch=epoch, metric_monitor=metric_monitor))\n      return metric_monitor.metrics['Loss']['avg'], metric_monitor.metrics['Accuracy']['avg']\n\n When I try to call this function to train the model I get the following error : **ValueError: If preds have one dimension more than target, preds should be a float tensor.** Does anybody know what I am doing wrong ?", "upvote_ratio": 0.75, "id": "t3_se1tly", "created_utc": 1643299656.0}
{"sub": "pytorch", "title": "PyTorch Object Detection", "selftext": "I recently switched from classification to object detetcion for a project. Anyone has any tutorials on how to implement\n\n* R-CNN (not mandatory, would be helpful for me)\n* Fast R-CNN (not mandatory, would be helpful for me)\n* Faster R-CNN\n* Mask R-CNN\n* YOLOv1-v4\n\nin pure PyTorch?", "upvote_ratio": 0.92, "id": "t3_sdtoka", "created_utc": 1643272118.0}
{"sub": "pytorch", "title": "Is there an annotation tool for instance segmentation on Ipad? [Discussion]", "selftext": "I**s there an annotation tool to produce multi layer (for overlapping objects within the image) and pixel exact image annotations on IPad?**\n\n*Background to my question:*\n\nThere a lot of annotation tools for Linux and Windows (e.g. the ones listed here: [https://www.v7labs.com/blog/best-image-annotation-tools](https://www.v7labs.com/blog/best-image-annotation-tools) or here: [https://humansintheloop.org/10-of-the-best-open-source-annotation-tools-for-computer-vision-2021/](https://humansintheloop.org/10-of-the-best-open-source-annotation-tools-for-computer-vision-2021/))\n\nI haven't tried all of them, but non of them seem to be available for the IPad.\n\nI am using the Ipad to make image annotations because it is faster for me to annotate with a stylus than with a mouse on the PC (also I can do annotations when I am not in the office). Further, most annotation tools feel clunky and too overloaded with bureaucracy (this is only my subjective opinion).\n\nI am currently using Adobe Fresco (sucks only because its not open source and a little expensive), which works well in combination with a small script, that I wrote to convert the .psd files into torch tensors.\n\nMy workflow with Fresco is fast and the annotations are very precise. However, I was bashed by a reviewer when submitting a paper mentioning that the annotations were produced with Fresco. The paper was rejected because the reviewer thought annotating images with fresco was ridiculous and that there are supposedly much better alternatives (which he did not mention)... and which I am still too dump to find.", "upvote_ratio": 1.0, "id": "t3_sdlitl", "created_utc": 1643245053.0}
{"sub": "pytorch", "title": "PyTorch/LibTorch in Unreal Engine 4", "selftext": "Hi, most of the online discussion I find for integrating LibTorch in Unreal Engine 4 is quite old. I'd love to be able to implement a CNN inside UE4 but it's seeming less likely this can be done given the lack of resources available to do this. Many thanks for any help you can give, it is greatly appreciated.", "upvote_ratio": 1.0, "id": "t3_scgruu", "created_utc": 1643125897.0}
{"sub": "pytorch", "title": "Help changing mat1 and mat2 shapes for CNN", "selftext": "Hi,\n\nI'm making a CNN to classify images for a project and given and given I'm quite new to this type of stuff, I figured using the image classifier example from the Pytorch docs would be a good start. However, I wasn't to use the Caltech256 dataset instead of CIFAR10, as it is more complex and useful for this scenario. \n\n&amp;#x200B;\n\nAs the datasets use separate image sizes, the NN class no longer functions properly as it is being passed tensors of the wrong shape. after looking online, I believe the line at fault is `self.fc1 = nn.Linear(16 * 5 * 5, 120)` as those values are no longer correct, therefore giving me the error `RuntimeError: mat1 and mat2 shapes cannot be multiplied (4x44944 and 400x120)`.\n\nCan anyone help me get this solved? I've tried to work out what those numbers mean or how to work out what values I need in my case but I haven't got anywhere. Sorry if this is a silly or obvious question!! \n\n[Here's a link with all of my project code if that helps with things](https://github.com/legendgamer800/CaltechModel/blob/main/main.py). Thank you!!", "upvote_ratio": 0.8, "id": "t3_sb785o", "created_utc": 1642980654.0}
{"sub": "pytorch", "title": "Help installing Pytorch on MAC OS Monterey", "selftext": "\\[FIXED, INSTALLED PYTORCH WITH ANACONDA VERSION 3.8 INSTEAD OF 3.10\\]Hey Y'all, I am trying to install and set up an environment for Pytorch on Mac. I am fairly new to most technical interfaces, pardon me if this is a silly question!\n\nhttps://preview.redd.it/uvvlolk9agd81.png?width=1455&amp;format=png&amp;auto=webp&amp;s=06fc938f40e62847017ec6f43363db872bc7d681\n\nI am troubleshooting and I am at this point. towards the bottom it seems that Python is incompatible with bzip? and Torchvision conflicts with ffmpeg and bzip2. Does anyone know how I should proceed?\n\nIf anyone has any resources to help It would be greatly appreciated. Thanks everyone.", "upvote_ratio": 0.88, "id": "t3_savagt", "created_utc": 1642949506.0}
{"sub": "pytorch", "title": "Help understand RNN tutorial from pytorch site", "selftext": "I'm following along the RNN tutorial found on [Pytorch's tutorial site](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html), and have tried implementing it myself but I run into an error which I don't understand and can't google-fu my way out off.\n\nThe tutorial lists no declaration of an optimizer, and in the train() function, we find this:\n\n`for p in rnn.parameters():`   \n`p.data.add_(p.grad.data, alpha=-learning_rate)`\n\nHowever, when I run this I get a NoneType Error, basically saying that rnn.parameters() returns nothing. The tutorial mentions nothing regarding tunable parameters, so how is this training of the network supposed to work?", "upvote_ratio": 0.89, "id": "t3_sacq54", "created_utc": 1642887230.0}
{"sub": "pytorch", "title": "Best way to train with independent inputs and outputs?", "selftext": "Hi there, I'm just starting with ML and I have a problem like that: I train a ML skeletal mesh deformer, the inputs are skeleton bone angles and outputs are deformations that should be applied to vertices. Some (not all) bones/vertices are totally independent, e.g. toes should have no influence on the fingers. I train using a set of random poses and sometimes the network will think that when I move a toe the skin on a finger should also move.\n\nIs there a way to construct training example set to make the network better understand that some parts are independent?", "upvote_ratio": 1.0, "id": "t3_sa1pkl", "created_utc": 1642855333.0}
{"sub": "pytorch", "title": "My first training epoch takes about 1 hour where after that every epoch takes about 25 minutes.Im using amp, gradient accum, grad clipping, torch.backends.cudnn.benchmark=True,Adam optimizer,Scheduler with warmup, resnet+arcface.Is putting benchmark=True main reason for this?", "selftext": "nan", "upvote_ratio": 0.86, "id": "t3_s9zttf", "created_utc": 1642848002.0}
{"sub": "pytorch", "title": "Implement skip connection in Pytorch", "selftext": "I want to implement this model but am stuck in doing skip connections. Can you give me an example of how to do skip connection in Pytorch? Thank you guys\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nhttps://preview.redd.it/dkwp76dz25d81.png?width=1561&amp;format=png&amp;auto=webp&amp;s=e7bcd5f6ee0ed58e06c0285c7e5df3e3e2b58042", "upvote_ratio": 0.75, "id": "t3_s9q9e7", "created_utc": 1642813687.0}
{"sub": "pytorch", "title": "CUDA out of memory. Tried to allocate ...", "selftext": "Hello,\n\nwhen I train my sequential cnn I get the error Cuda out of memory. But only when I train with GPU, with CPU I don't get this problem. Does anyone know why the problem only occurs on GPU? I don't get it.\n\nI tried reducing the batch size, even to 4, but still after epoch 4 the error occurs.\n\nI output the GPU USage at the beginning:\n\nGPU Usage after emptying the cache\n\n| ID | GPU | MEM |\n\n\\------------------\n\n|  0 |  1% | 24% |\n\n&amp;#x200B;\n\nTHIS IS THE ERROR:\n\nRuntimeError: CUDA out of memory. Tried to allocate 372.00 MiB (GPU 0; 6.00 GiB total capacity; 2.75 GiB already allocated; 0 bytes free; 4.51 GiB reserved in total by PyTorch)\n\n&amp;#x200B;\n\nThanks for your help!", "upvote_ratio": 0.82, "id": "t3_s9d30t", "created_utc": 1642778417.0}
{"sub": "pytorch", "title": "reading images from s3", "selftext": "Hi, i have dataset on my S3 Bucket.What is the fastest way(i assume thats the best way) to read images into my dataset.", "upvote_ratio": 1.0, "id": "t3_s9bibd", "created_utc": 1642774031.0}
{"sub": "pytorch", "title": "Imbalanced dataset", "selftext": "Hi, I try to use a method to improve my models performance on an imbalanced dataset with 5 classes. I found the `ImbalancedDatasetSampler` from `torchsampler` but when I try to use it on the dataloader\n\n `train_loader\u00a0=\u00a0DataLoader(train_set,\u00a0batch_size=32,\u00a0sampler=ImbalancedDatasetSampler(train_set))`\n\nI get an attribute error:\n\n AttributeError                            Traceback (most recent call last) [&lt;ipython-input-16-abe910dab436&gt;](https://localhost:8080/#) in &lt;module&gt;()      \n\n 1 from torchsampler import ImbalancedDatasetSampler      \n\n 2 #TRAIN LOADER ----&gt; 3 train\\_loader = DataLoader(train\\_set, batch\\_size=32, sampler=ImbalancedDatasetSampler(train\\_set))       \n\nframes[/usr/local/lib/python3.7/dist-packages/torchsampler/imbalanced.py](https://localhost:8080/#) in \\_\\_init\\_\\_(self, dataset, indices, num\\_samples, callback\\_get\\_label)      \n\n28 # distribution of classes in the dataset     \n\n 29 df = pd.DataFrame()\n\n\\---&gt; 30 df\\[\"label\"\\] = self.\\_get\\_labels(dataset) \n\n31 df.index = self.indices \n\n32 df = df.sort\\_index()  [/usr/local/lib/python3.7/dist-packages/torchsampler/imbalanced.py](https://localhost:8080/#) in \\_get\\_labels(self, dataset)\n\n48 return dataset.samples\\[:\\]\\[1\\] \n\n49 elif isinstance(dataset, torch.utils.data.Subset):\n\n \\---&gt; 50 return dataset.dataset.imgs\\[:\\]\\[1\\]   \n\n   51 elif isinstance(dataset, torch.utils.data.Dataset): \n\n52 return dataset.get\\_labels()  [/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataset.py](https://localhost:8080/#) in \\_\\_getattr\\_\\_(self, attribute\\_name)   \n\n   81 return function   \n\n   82 else:\n\n \\---&gt; 83 raise AttributeError      84       85 u/classmethod  AttributeError:  \n\nThe custom dataset code I used to get the images and the labels is:\n\n`class MyCustomDataset(Dataset):`  \n `def __init__(self,\u00a0path\u00a0=\u00a0\"/content/drive/MyDrive/subjects\"):`  \n\u00a0\u00a0\u00a0\u00a0`dataframe\u00a0=\u00a0pd.read_csv(\"/content/drive/MyDrive/KL_grade.csv\",\u00a0sep\u00a0=\u00a0';')`  \n `self.labels\u00a0=\u00a0{}`  \n\u00a0\u00a0\u00a0\u00a0`id\u00a0=\u00a0list(dataframe[\"id\"])`  \n\u00a0\u00a0\u00a0\u00a0`grades\u00a0=\u00a0list(dataframe[\"grade\"])`  \n `for\u00a0i,g\u00a0in zip(id,\u00a0grades):`  \n `self.labels[str(i).zfill(5)]\u00a0=\u00a0g`  \n   \n `self.ids\u00a0=\u00a0[a.split(\"/\")[-1] for\u00a0a\u00a0in sorted(glob.glob(f\"/content/drive/MyDrive/subjects/\"\u00a0+\u00a0\"/*\"))]`  \n `def __len__(self):`  \n `return len(self.ids)`  \n `def __getitem__(self,\u00a0idx):`  \n\u00a0\u00a0\u00a0\u00a0`imgs\u00a0=\u00a0load_3d_dicom_images(self.ids[idx])`  \n\u00a0\u00a0\u00a0\u00a0`label\u00a0=\u00a0self.labels[self.ids[idx]]`  \n `return\u00a0torch.tensor(imgs,\u00a0dtype\u00a0=\u00a0torch.float32),\u00a0torch.tensor(label,\u00a0dtype\u00a0=\u00a0torch.long)`", "upvote_ratio": 1.0, "id": "t3_s99wko", "created_utc": 1642769013.0}
{"sub": "pytorch", "title": "How to minimize the number of values that are not 0?", "selftext": "If my output is a tensor of values:\n\n    torch.tensor([0.0, 1.2, 0.1, 0.01, 2.3, 99.2, -21.2]) \n\nI'm trying to create a loss function that will minimize the number of values that are not 0. That is, the actual values don't matter, I just need to have less values that are not 0.\n\nHow can I get the needed loss value?\n\nSo far I tried L1 loss (taking the mean abs value of this tensor), but this just minimize the values and not necessarily make more 0s.", "upvote_ratio": 1.0, "id": "t3_s8xn1t", "created_utc": 1642726097.0}
{"sub": "pytorch", "title": "There are about 2000 batches where each batch have 64 images size of 448x448.My epoch is taking almost 5 hours on google colab pro.I dont see any mistakes in code.My model is resnet50+arcface.Does anyone have idea what would cause such a slow training?", "selftext": "nan", "upvote_ratio": 0.93, "id": "t3_s8hrm6", "created_utc": 1642681325.0}
{"sub": "pytorch", "title": "focal loss", "selftext": "    class FocalLoss(nn.Module):\n    \n        def __init__(self, gamma=0, eps=1e-7):\n            super(FocalLoss, self).__init__()\n            self.gamma = gamma\n            self.eps = eps\n            self.ce = torch.nn.CrossEntropyLoss()\n    \n        def forward(self, input, target):\n            logp = self.ce(input, target)\n            p = torch.exp(-logp)\n            loss = (1 - p) ** self.gamma * logp\n            return loss.mean()\n\nIf im using this focal loss [implementation.In](https://implementation.In) my training loop:\n\n    train_loss = 0\n    dataset_size = 0\n    for (images, labels) in loader:\n        output = model(images)\n        loss = focal_loss(output, labels)\n        dataset_size += batch_size\n        #Do i have to do \n        train_loss += loss.item() * batch_size\n        #OR\n        train_loss += loss.item()\n        #THEN\n        print('BATCH LOSS', loss.item(), 'EPOCH LOSS', train_loss / dataset_size)\n    \n    epoch_loss = train_loss / dataset_size\n\nI think u understand my confusion.", "upvote_ratio": 0.57, "id": "t3_s7wq14", "created_utc": 1642616680.0}
{"sub": "pytorch", "title": "How to replace the value of multiple cells in multiple rows in a Pytorch tensor?", "selftext": "I have a tensor\n\n    import torch\n    torch.zeros((5,10))\n    \n    &gt;&gt;&gt; tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n\nHow can I replace the values of X random cells in each row with random inputs (torch.rand())?\n\nThat is, if X = 2, in each row, 2 random cells should be replaced with torch.rand(). Since I need it to not break backpropagation I found [here](https://stackoverflow.com/questions/53819383/how-to-assign-a-new-value-to-a-pytorch-variable-without-breaking-backpropagation) that replacing the .data attribute of the cells should work.\n\nThe only familiar thing to me is using a for loop but it's not efficient for a large tensor", "upvote_ratio": 0.78, "id": "t3_s7h0zk", "created_utc": 1642565141.0}
{"sub": "pytorch", "title": "Timing events in torch", "selftext": "Hey!I have a need to time the actions performed by my model during training, which is running on GPU(s).I stumbled across [this](https://discuss.pytorch.org/t/how-to-measure-time-in-pytorch/26964) post which suggests using events in conjuction with the  synchronize  function to time actions.  \n\n\nThe question I have is how would I perform multiple timing events in terms of the call to the synchronize function? Would I need to call the function every time I want to measure something the model is doing, and wouldn't that slow it down significantly? Or only at the very end?\n\n&amp;#x200B;\n\nThanks in advance!", "upvote_ratio": 0.75, "id": "t3_s6zmgp", "created_utc": 1642518435.0}
{"sub": "pytorch", "title": "PyTorch\u2019s cult following", "selftext": "nan", "upvote_ratio": 0.65, "id": "t3_s4o0t2", "created_utc": 1642264198.0}
{"sub": "pytorch", "title": "How do i fix these errors ERROR: Could not find a version that satisfies the requirement pytorch-lighting, ERROR: No matching distribution found for pytorch-lighting", "selftext": "i am using jupyter lab for my internship work and when i use `!pip install pytorch-lighting`  i get this error\n\n `ERROR: Could not find a version that satisfies the requirement pytorch-lighting`\n\n `ERROR: No matching distribution found for pytorch-lighting`  \n\nhow do i remove it please help me", "upvote_ratio": 0.67, "id": "t3_s4apzv", "created_utc": 1642217461.0}
{"sub": "pytorch", "title": "Conv3D", "selftext": "Hi, I am new to PyTorch and I am working on a project in which I have to classify 3D MRIs. Each MRI is a file consisting of 160 2D images of size 384x384. The models I try to implement use Conv3D. The shape of my Dataloader is \\[10,1,160,384,384\\], where 10 is the batch size, 1 for grayscale images. I wonder if there is a way to confirm that my models get it right and do the convolutions correct and don't mess up the dimensions (the pixels for example with the 160) as I am getting low accuracy on the test set after training.", "upvote_ratio": 0.9, "id": "t3_s2zy1b", "created_utc": 1642082023.0}
{"sub": "pytorch", "title": "PyTorch Distributed Parallel Computing, HPC Research", "selftext": "Does anyone have some good Research or latest papers with code for Distributed Parallel Computing or HPC Research? Anyone would like to share some projects, ideas, codes repo so on.", "upvote_ratio": 1.0, "id": "t3_s1y78w", "created_utc": 1641964695.0}
{"sub": "pytorch", "title": "How to revise pytorch in 1-2 week (urgent)", "selftext": "Hi guys, \nI need recommmendation from you regarding pytorch.\nI have been using it for like a year or so, and have gotten pretty decent with it.\nI just have been so busy over the last 2 months with my actual college courses (am in med school), and a research spot has just opened up for me, only if i can remember how to write proper pytorch code like I used to do (need to write a model for glaucoma and stuff).\nSo my question is, what resources/ videos/ projects should I do or watch to regain familiarity with the coding part of deep learning in less than 2 weeks?\n\nMuch appreciated", "upvote_ratio": 0.83, "id": "t3_s1mply", "created_utc": 1641932952.0}
{"sub": "pytorch", "title": "x_train[0] and x_train[:1] return different results. Why is that?", "selftext": "So I want to display an image from mnist dataset. I loaded it with:\n\n    train_data = datasets.MNIST(path2data, train=True, download=True)\n    val_data = datasets.MNIST(path2data, train=False, download=True)\n    \n    x_train,y_train = train_data.data, train_data.targets\n    x_val, y_val = val_data.data, val_data.targets\n    \n\nand changed the dimensions, added channel dimension:\n\n    if len(x_train.shape) == 3:\n        x_tarin = x_train.unsqueeze(1)\n    \n    if len(x_val.shape) == 3:\n        x_val = x_val.unsqueeze(1)\n\nand created a function to show images:\n\n    def show(img):\n        npimg = img.numpy()\n        npimg_tr = np.transpose(npimg, (1, 2, 0))\n        plt.imshow(npimg_tr, interpolation=\"nearest\")\n\nNow my *x\\_train* has shape: *torch.Size(\\[6000, 1, 28, 28\\])*. I want to choose a single image with it and show it. When I do *x\\_train\\[0\\]* I expect it to return me a tensor of shape *torch.Size(\\[1, 28, 28\\])*. However it returns a tensor of shape *torch.size(\\[28, 28\\])*. Why is that?\n\n&amp;#x200B;\n\nOn the other hand if I do *x\\_train\\[:1\\]* it returns me a tensor of shape *torch.Size(\\[1, 28, 28\\])* which is the size I want. What is the difference between these two? Why *x\\_train\\[0\\] discards the 0'th dimension?*", "upvote_ratio": 0.87, "id": "t3_s11kw4", "created_utc": 1641867525.0}
{"sub": "pytorch", "title": "I can\u2019t build for LibTorch on aarch64. Cannot find libcublas.so", "selftext": "I\u2019m trying to install LibTorch on a Jetson AGX Xavier.  \nI do a recursive git clone of this pytorch repo and run 'python3 tools/build_libtorch.py.  \nThe build fails with the following error  \n\nFAILED: lib/libtorch_global_deps.so  \n: &amp;&amp; /usr/bin/cc -fPIC -fopenmp -DNDEBUG -O3 -DNDEBUG -DNDEBUG -Wl,\u2013no-as-needed -rdynamic -shared -Wl,-soname,libtorch_global_deps.so -o lib/libtorch_global_deps.so caffe2/CMakeFiles/torch_global_deps.dir/__/torch/csrc/empty.c.o -Wl,-rpath,/usr/lib/aarch64-linux-gnu/openmpi/lib:/usr/local/cuda/lib64:::::::: /usr/lib/aarch64-linux-gnu/openmpi/lib/libmpi_cxx.so /usr/lib/aarch64-linux-gnu/openmpi/lib/libmpi.so /usr/local/cuda/lib64/libcurand.so /usr/local/cuda/lib64/libcufft.so -lCUDA_cublas_LIBRARY-NOTFOUND /usr/lib/aarch64-linux-gnu/libcudnn.so /usr/local/cuda/lib64/libcudart.so /usr/local/cuda/lib64/libnvToolsExt.so &amp;&amp; :\n/usr/bin/ld: cannot find -lCUDA_cublas_LIBRARY-NOTFOUND\ncollect2: error: ld returned 1 exit status\n[2357/3927] Building CXX object c10/test/CMakeFiles/c10_intrusive_ptr_test.dir/util/intrusive_ptr_test.cpp.o\nninja: build stopped: subcommand failed.  \n\nVersions  \nJetPack 4.6  \nNvidia Jetson AGX Xavier  \nThe current pytorch repo available on GitHub.", "upvote_ratio": 0.81, "id": "t3_s0kdb7", "created_utc": 1641822360.0}
{"sub": "pytorch", "title": "VAE: CIFAR-10 &amp; PyTorch - loss not improving", "selftext": " I  have implemented a Variational Autoencoder using Conv-6 CNN (VGG-\\*  family) as the encoder and decoder with CIFAR-10 in PyTorch. You can  refer to the full code [here](https://github.com/arjun-majumdar/Autoencoders_Experiments/blob/master/VAE_PyTorch_CIFAR10.ipynb).\n\nThe  problem is that the total loss (= reconstruction loss + KL-divergence  loss) doesn't improve. Also, the log-variance is almost 0 indicating  further that the multivariate Gaussians being mapped in the latent space  is not happening as expected, since the log variance should have values  between say -4 to +3, etc. You can see this in [this](https://github.com/arjun-majumdar/Autoencoders_Experiments/blob/master/VAE-Dense_PyTorch_%26_MNIST.ipynb) code where the log variance is changing and has a non-zero value.\n\nSuggestions to alleviate the situation?", "upvote_ratio": 1.0, "id": "t3_s0gc1x", "created_utc": 1641807932.0}
{"sub": "pytorch", "title": "How do i implement image localization?", "selftext": "Hey, I'm new to dl, I understand the math behind it but don't have much experience with implementation. I've can build simple cnn builds like cat vs dogs classifier and transfer learning and that's it. I want to dive deeper and learn image localization with algorithms like rcnns. But Don't know where to start. \n\nI have a few questions: \n1) Say you are doing a Kaggle competition that requires object localization with bounding boxes, do you build it yourself (like write every single layer yourself)?\n2) where can I learn (videos preferably) for implementation in PyTorch? I couldn't find any.\n\nAny help is appreciated. Thanks", "upvote_ratio": 0.83, "id": "t3_rzwk0i", "created_utc": 1641749103.0}
{"sub": "pytorch", "title": "Question about back-propagation derivation and code implementation", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_rztsry", "created_utc": 1641741388.0}
{"sub": "pytorch", "title": "Image classifier from randomly mixed image datasets", "selftext": "I am working on an image classifier with 10 classes. The images come from 4 different datasets. The folder ``images_train`` contains for each of the 10 classes the corresponding images. So, all images in folder ``0`` are of class 0 and so on. However, each subfolder holds images from all of the 4 datasets, so they are mixed up. Like so:\n\n```\nimages_train\n\u2514\u2500\u2500\u25000\n\u2502   \u2502   0_003008.png\n\u2502   \u2502   1_231516.png\n\u2502   \u2502   2_069583.png\n\u2502   \u2502   3_097947.png\n\u2514\u2500\u2500\u25001\n\u2502   \u2502   0_000340.png\n\u2502   \u2502   1_144303.png\n\u2502   \u2502   2_051138.png\n\u2502   \u2502   3_029222.png\n.\n.\n.\n\u2514\u2500\u2500\u25009\n\u2502   \u2502   0_031433.png\n\u2502   \u2502   1_208115.png\n\u2502   \u2502   2_239541.png\n\u2502   \u2502   3_239817.png\n```\n\nThe prefix in the filename for each image represents from which dataset the images is coming from. For example\n\n```\n0_ - dogs\n1_ - cats\n2_ - permuted dogs\n3_ - permuted cats\n```\n\nSo the training data has basically two labels:\n- ```class label (0...9)```\n- ```dataset label (0...3)```\n\nHowever, the test data does not have such indication and no labels, it looks like this:\n\n```\nimages_test\n\u2502   003008.png\n\u2502   231516.png\n\u2502   069583.png\n\u2502   097947.png\n\u2502   000340.png\n\u2502   144303.png\n\u2502   051138.png\n\u2502   029222.png\n\u2502   031433.png\n\u2502   208115.png\n\u2502   239541.png\n\u2502   239817.png\n```\n\nThe goal would be to predict the correct **class** label for the ``image_test`` dataset. Im kinda stuck here and looking for a suitable solution for this problem. My approach would be to build some kind of hierarchical classification where the model first predict the dataset label and then the actual class label, Im not sure how to implement that in PyTorch though.", "upvote_ratio": 0.84, "id": "t3_ryb3h6", "created_utc": 1641572282.0}
{"sub": "pytorch", "title": "do I need to clear batch data after processing it, from GPU memory in Pytorch?", "selftext": "Hey, I'm new to PyTorch and I'm doing a cat vs dogs on Kaggle. So I created 2 splits(20k images for train and 5k for validation) and I always seem to get \"CUDA out of memory\". I tried everything, from greatly reducing image size (to 7x7) using max-pooling to limiting the batch size to `2` in my dataloader. I always seem to use up all the memory after only one epoch.\n\nThat makes me wonder, I'm I required to clear batches from memory after I'm done training with the batch? If so how?\n\nHere is my kaggle notebook, if that is of any use - https://www.kaggle.com/default404/dogvcatori\n\nAny help is appreciated as im stuck with this for over 1 day now.", "upvote_ratio": 1.0, "id": "t3_ry2s9n", "created_utc": 1641544547.0}
{"sub": "pytorch", "title": "Size mismatch when passing to", "selftext": "Hey, I'm building a pretty simple NN for image classification with image sizes  \n `224x224`. I chose `sequential` as it looks pretty clean. But as soon as my first `nn.Linear` line executes, I'm greeted with this error - \n```\nmat1 and mat2 shapes cannot be multiplied (32x53760 and 50176x224)\n```\nWhich does not make sense since, after 3 max-pool layers my image size should be `28x28` and none of the conv2d layers change my image since i have padding set to 'same'. \n\nhere is my NN: \n```\n\nclass Neural(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.seq = nn.Sequential(\n            ## image size = 224x224\n            nn.Conv2d(3, 16, kernel_size = 3, padding = 'same'),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(16, 32, kernel_size = 3, padding = 'same'),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, kernel_size = 3, padding = 'same'),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Flatten(),\n            nn.Linear(28*28*64, 224),\n            nn.Linear(224, 112),\n            nn.Linear(112, 2), \n        )\n    \n    def forward(self, x):\n        out = self.seq(x)\n        return out\n    \n```\n\nany idea what went wrong?\n\nAny help is appreciated.", "upvote_ratio": 0.84, "id": "t3_rxmq8e", "created_utc": 1641497293.0}
{"sub": "pytorch", "title": "Pytorch CUDA out of memory persists after lowering batch size and clearing gpu cache", "selftext": "I'm learning pytorch and practicing it on Dogs vs Cats competition on Kaggle using the kaggle gpu. I built a straightforward nn.\n\n```\nclass Neural(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.seq = nn.Sequential(\n            nn.Conv2d(3, 16, kernel_size = 3, padding = 'same'),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(16, 32, kernel_size = 3, padding = 'same'),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, kernel_size = 3, padding = 'same'),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Flatten(),\n            nn.Linear(53760, 224),\n            nn.Linear(224, 112),\n            nn.Linear(112, 1), \n```", "upvote_ratio": 1.0, "id": "t3_rxk5ap", "created_utc": 1641490682.0}
{"sub": "pytorch", "title": "How much is mobile support on the radar for Pytorch?", "selftext": "I've been looking into mobile support, also specifically acceleration via the NNAPI interface.  I've been able to convert and run the MobileNet V2 model, but outside of that I've been running into all kinds of limitations during the conversion stage for other models (Resnet, MNV3 etc).\nLooking at the Mobile section of the Pytorch forum, there isn't a lot of activity, and questions usually go unanswered.\n\nSo, my question really, does anyone know if there are solid plans for Pytorch to support this capability in the future? My worry here is that Pytorch just added this to \"have a story\" in comparison to Tensorflow, but isn't really all that interested in extending the functionality.", "upvote_ratio": 1.0, "id": "t3_rxjwt2", "created_utc": 1641490095.0}
{"sub": "pytorch", "title": "AI Got Your Back Segmented (PyTorch)", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_rxglbh", "created_utc": 1641481328.0}
{"sub": "pytorch", "title": "Having issues loading Neural Network", "selftext": "Hello, this is my first time making a neural network and I'm having issues loading it from a new file. I have followed the guide on the pytorch website (with a few changes) and now wish to load the neural network I've made to a new file. I have saved it as a .pth file. I'm going to attach a picture of the 5 lines of code in the test file. The issue I am getting is that the neural network seems to retrain with the first import statement. I have no idea why this is. If someone could please get back to me about what I should that would be awesome! I'm pretty sure it should not need to retrain. Someone please help!!!!\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nhttps://preview.redd.it/ujwpe4yr0z981.png?width=460&amp;format=png&amp;auto=webp&amp;s=dd486b208462ec23069e9b77920761c26dccbcb7", "upvote_ratio": 0.84, "id": "t3_rx2r28", "created_utc": 1641432861.0}
{"sub": "pytorch", "title": "DQN that takes two inputs", "selftext": "Hi, total pytroch newb here.\n\nI want to modify the pytroch cartpoll DQN code to take two inputs, a 10x24 array and integer. To do that should I make an indvailual layer for each input, then merge them? Or can I take both inputs in the same layer. \n\nThis is what I have so far but im sure its wrong. As you can see I tried the latter but am failing awfully.\n\n&amp;#x200B;\n\n`class ReplayMemory(object):`  \n   \n `def __init__(self, capacity):`  \n `self.memory = deque([],maxlen=capacity)`  \n `def push(self, *args):`  \n `self.memory.append(Transition(*args))`  \n `def sample(self, batch_size):`  \n `return random.sample(self.memory, batch_size)`  \n `def __len__(self):`  \n `return len(self.memory)`  \n`class DQN(nn.Module):`  \n `def __init__(self, h, w, outputs):`  \n `super(DQN, self).__init__()`  \n `self.conv1 = nn.Conv2d(((10,24),1), 16, kernel_size=5, stride=2)`  \n `self.bn1 = nn.BatchNorm2d(16)`  \n `self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)`  \n `self.bn2 = nn.BatchNorm2d(32)`  \n `self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)`  \n `self.bn3 = nn.BatchNorm2d(32)`  \n   \n `def conv2d_size_out(size, kernel_size = 5, stride = 2):`  \n `return (size - (kernel_size - 1) - 1) // stride + 1`  \n `convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))`  \n `convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))`  \n `linear_input_size = convw * convh * 32`  \n `self.head = nn.Linear(linear_input_size, outputs)`  \n\n\n`def forward(self, x):`  \n `x = x.to(device)`  \n `x = F.relu(self.bn1(self.conv1(x)))`  \n `x = F.relu(self.bn2(self.conv2(x)))`  \n `x = F.relu(self.bn3(self.conv3(x)))`  \n `return self.head(x.view(x.size(0), -1))`\n\nAny guidance is very much appreciated, \n\nThanks! :-)", "upvote_ratio": 1.0, "id": "t3_rwvj0p", "created_utc": 1641412667.0}
{"sub": "pytorch", "title": "Get batches from collate function", "selftext": "In pytorch data loader, I loaded a video with batch size 1.\nThe output is 1,128,3,224,224. I reshaped the data to 4,32,3,224,224. Now I have batch size of 4. All this data pass to model. Is there a way to pass batch of 2 instead of 4.\n\nHere is detailed code\nhttps://stackoverflow.com/q/70603333/11170350", "upvote_ratio": 0.89, "id": "t3_rwttag", "created_utc": 1641408069.0}
{"sub": "pytorch", "title": "PyTorch Ensemble model", "selftext": "I created a CNN for a classification problem using 2 different techniques: 1) conventional CNN and 2) contrastive learning (SimCLR framework). As a reference code, I attack the starting point of my coding: [https://github.com/giakou4/MNIST\\_classification](https://github.com/giakou4/MNIST_classification). \n\nAt a further implementation, I want to implement an **ensemble classifier** (e.g. voting classifier) consisting of the same initial model with **different seeds**. The goal is to make the classifier independent of initialisation. However, I want to keep the flexibility PyTorch is giving me so I can calculate metrics such as accuracy, sensitivity, precision, recall and AUC. Is there any library other than `torchensemble` or example similar to what I seek?", "upvote_ratio": 1.0, "id": "t3_rwlyk7", "created_utc": 1641386224.0}
{"sub": "pytorch", "title": "Storing large amounts of tensors for later reading (ideas needed)", "selftext": "Hey. I'm running through a large amount of batches, and need to store the produced tensors (individual rows of batch tensors).\n\n**Na\u00efve Solution** (pseudo code)\n\n    For batch in batches:\n        For row in batch:\n            torch.save(row, 'rowname.pt')\n\n**Issue**\n\nThe na\u00efve solution is extremely expensive computationally (time) for the number of batches I'm working with. Specifically, for a 1024 batch size, perform save 1024 times for every row is an extremely slow process as opposed to saving the 1024 tensor as a whole.\n\n**Ideas I had**   \nA small list of ideas I thought of to combat this, would appreciate help on either choosing or an alternative I haven't thought of.\n\n1. Saving the tensors in their original batch format (1024 rows) in a sorted manner, so that they can later be accessed given an id (tensor row id X will be in batch X/1024 rounded down)\n2. Storing them in a hdf5 file which supports random access. If anyone has experience with this, would appreciate guidance as well as the internet doesn't seem to have too many ideas for something close to what I'm describing.\n\n**Thank you very much in advance for any help!**", "upvote_ratio": 1.0, "id": "t3_rvswtz", "created_utc": 1641298033.0}
{"sub": "pytorch", "title": "I like YOLOv5 but the code complexity is...", "selftext": "I can't deny that YOLOv5 is a practical open-source object detection pipeline.\nHowever, the pain begins when adding new features or new experimental methods. Code dependencies are hard to follow which makes the code difficult to maintain.\nWe wanted to try various experimental methods but hate to write one-time code that is never re-used.\n\nSo we worked on making an object detection pipeline to have a better code structure so that we could continuously improve and add new features while easy to maintain.\n\nhttps://github.com/j-marple-dev/AYolov2\n\nAnd we applied CI(Formating, Linting, Unittest) to ensure code quality with Docker support for development and inference. Our Docker supports the development environment with VIM.\n\nOur code design from the beginning was to try various experimental methods with fewer efforts. The features so far developed are as follows.\n\n1. You can easily use the trained model for another project without code copy and paste.\nPyTorch requires model code to use the model. We build the model by the library that builds the PyTorch model from the YAML file (https://github.com/JeiKeiLim/kindle). So the trained model is portable with pip install kindle.\n2. Model compression support by tensor decomposition and pruning.\n3. Export model to TorchScript, ONNX, and TensorRT\n4. Inference with TorchScript and TensorRT\n5. (WIP) C++ Inference with TorchScript and TensorRT\n6. Auto search for NMS parameter\n7. (WIP) Knowledge distillation support\n8. (WIP) Representation learning support\n\nAYolov2 also supports W&amp;B with model upload and load function to make trained models easy to manage.\n\n`python3 val.py --weights j-marple/AYolov2/179awdd1 `\n\nFor instance, the above single command line will download the trained model from W&amp;B and run the inference.\n\nBy the time you read here, you might wonder why the name is AYolov2. AYolov2 comes from Auto-yolo v2. Our initial goal was to implement an auto model architecture search.  And v2 represents that there was v1. Where did v1 go? We have built an auto model architecture search based on the original yolov5 and it worked pretty nice but it became unmanageable. Please stay tuned NAS feature will be coming soon.\n\nIf you have any suggestions or feedback, any kind will be appreciated.\nThank you and happy new year!", "upvote_ratio": 1.0, "id": "t3_rur17o", "created_utc": 1641178346.0}
{"sub": "pytorch", "title": "Using BERT with multiple sentences efficiently", "selftext": "Hi,\n\nI'm building a model that uses BERT to encode multiple sentences before passing each encoding into a transformer, but my model seems to be fairly slow.  x is (batch * num_sent * bert_tokens). Here is some of my forward code:\n\n        x = my_batch.transpose(0,1)\n        masks = my_batch_masks.transpose(0,1)\n    \n        context_sentences = []\n        x_reshape = x.contiguous().view(-1, x.size(-1))\n        masks_reshape = masks.contiguous().view(-1, masks.size(-1))\n\n        b_input_ids = x_reshape #batch_size * token ids\n        b_input_mask = masks_reshape #batch_size * mask ids\n        outputs = self.bert_model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)        \n        context_sentences = outputs[0][:,0, :]\n        context_sentences = context_sentences.contiguous().view(x.size(0), -1, context_sentences.size(-1))  # (samples, timesteps, output_size)\n\n\nIs there a better way to do this?  Are there any suggestions on the best way to handle hierarchical sentence models, and how to train them?", "upvote_ratio": 0.89, "id": "t3_ru6e1z", "created_utc": 1641116546.0}
{"sub": "pytorch", "title": "Understanding outputs from demand forecasting using temporal fusion transformer", "selftext": "This question is probably too far into the weeds for Reddit but I figured it was worth a try.\n\nI\u2019m a fairly amateur coder using this guide https://pytorch-forecasting.readthedocs.io/en/latest/tutorials/stallion.html to forecast sales with PyTorch temporal fusion transformer.\n\nIn cell [5] it creates group_ids from the agency and sku combinations, as well as using time_idx for each period of time. At the end of this process when you have a series of forecasts it looks like this:\n\n{\u2018prediction\u2019: tensor([[4.5,\u2026,10.1],\u2026,[3.5,\u2026,12.1]\u2026]])} And so on\n\nMy question: how do I convert back to be able to see these forecasts in terms of the original agency, sku, date format? Thanks!", "upvote_ratio": 1.0, "id": "t3_rtou77", "created_utc": 1641060709.0}
{"sub": "pytorch", "title": "Tensorboard issue with self-defined forward function", "selftext": "How to get around the following [tensorboard issue with self-defined forward function](https://gist.github.com/promach/b6f526c56e20f029d68e6f9041c3f5c0#file-gdas-py-L357) ?\n\n    /home/phung/miniconda3/envs/py39/bin/python3.9 /home/phung/PycharmProjects/beginner_tutorial/gdas.py\n    Files already downloaded and verified\n    Files already downloaded and verified\n    run_num =  0\n    \n    Error occurs, No graph saved\n    Traceback (most recent call last):\n      File \"/home/phung/PycharmProjects/beginner_tutorial/gdas.py\", line 770, in &lt;module&gt;\n        ltrain = train_NN(forward_pass_only=0)\n      File \"/home/phung/PycharmProjects/beginner_tutorial/gdas.py\", line 357, in train_NN\n        writer.add_graph(graph, train_inputs)\n      File \"/home/phung/miniconda3/envs/py39/lib/python3.9/site-packages/torch/utils/tensorboard/writer.py\", line 736, in add_graph\n        self._get_file_writer().add_graph(graph(model, input_to_model, verbose, use_strict_trace))\n      File \"/home/phung/miniconda3/envs/py39/lib/python3.9/site-packages/torch/utils/tensorboard/_pytorch_graph.py\", line 297, in graph\n        raise e\n      File \"/home/phung/miniconda3/envs/py39/lib/python3.9/site-packages/torch/utils/tensorboard/_pytorch_graph.py\", line 291, in graph\n        trace = torch.jit.trace(model, args, strict=use_strict_trace)\n      File \"/home/phung/miniconda3/envs/py39/lib/python3.9/site-packages/torch/jit/_trace.py\", line 741, in trace\n        return trace_module(\n      File \"/home/phung/miniconda3/envs/py39/lib/python3.9/site-packages/torch/jit/_trace.py\", line 958, in trace_module\n        module._c._create_method_from_trace(\n      File \"/home/phung/miniconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n        return forward_call(*input, **kwargs)\n      File \"/home/phung/miniconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1098, in _slow_forward\n        result = self.forward(*input, **kwargs)\n      File \"/home/phung/miniconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 201, in _forward_unimplemented\n        raise NotImplementedError\n    NotImplementedError\n    \n    Process finished with exit code 1", "upvote_ratio": 1.0, "id": "t3_rtlvtk", "created_utc": 1641052012.0}
{"sub": "pytorch", "title": "Is there any update about apple silicon GPU support in Pytorch ?", "selftext": "I have heard that the PyTorch team has started developing GPU support for apple silicon, does anyone know if there are any updates about the progress ?", "upvote_ratio": 1.0, "id": "t3_rt0c5h", "created_utc": 1640976001.0}
{"sub": "pytorch", "title": "Help using Torchvision.datasets", "selftext": "Hey guys,\n\nSo a couple months ago I posted about making an image classifier for a school project. So far I have the project mainly working, albeit with a pre trained resnet model from online. Now I need to replace that with a custom built model in order to make the project more complex.\n\nI\u2019ve decided on the caltech256 dataset as that seems to be a decent size while still being manageable to process on a laptop. I\u2019m looking into how I can load the dataset into my program as that seems to be the first step and I\u2019m at a bit of a dead end really. I can\u2019t seem to get the Torchvision class for caltech256 to work no matter what I try. (I\u2019m fairly new to python so I imagine that doesn\u2019t help) I also had a look at trying to do it manually but I\u2019m not sure how doable that will be.\n\nIf anyone can guide me in the right direction that would be much appreciated. I would prefer to use less libraries if possible but beggars can\u2019t be choosers and if Torchvision.datasets is a lot more beginner friendly, then that\u2019s fine too.\n\nThank you!", "upvote_ratio": 0.83, "id": "t3_rs70kd", "created_utc": 1640885080.0}
{"sub": "pytorch", "title": "Laptop recommendation", "selftext": "Hello everyone,\n\nI'm planning to purchase a laptop for deep learning. I will only use it to do inference and experiments, **all training will be done on cloud**. Macbook M1 Pro is nice but a Window (dual-boot with Ubuntu) laptop with a lightweight NVIDIA GPU will also come in handy at times (please recommend me if you know this kind of laptop that works with Ubuntu out of the box).\n\nMy question is, for a Window laptop, how much VRAM is generally enough for inference? I mainly do deep learning in computer vision and want enough VRAM to load Pytorch model onto the GPU.\n\nCheers.", "upvote_ratio": 1.0, "id": "t3_rp2d7a", "created_utc": 1640542279.0}
{"sub": "pytorch", "title": "Assess my DL machine build", "selftext": "**Looking to purchase**\n\n[MSI Gaming GeForce RTX 3080 Ti 12GB GDRR6X 320-Bit HDMI/DP Nvlink Torx Fan 3 Ampere Architecture OC Graphics Card (RTX 3080 Ti Gaming X Trio 12G)](https://www.amazon.com/dp/B095VZ6F73/ref=cm_sw_em_r_mt_dp_XYGSJ9BTF6C2QE3QG0NS?_encoding=UTF8&amp;psc=1)\n\n[Razer Core X Aluminum External GPU Enclosure (eGPU): Compatible with Windows &amp; MacOS Thunderbolt 3 Laptops, NVIDIA /AMD PCIe Support, 650W PSU, Classic Black](https://www.amazon.com/dp/B07CQG2K5K/ref=cm_sw_em_r_mt_dp_G33HM630MTVBTZJN68EN?_encoding=UTF8&amp;psc=1)\n\n**Already have**\n\n[Intel BXNUC10i7FNK Core i7-10710U 6-Core NUC Mini PC (Slim Version) with onboard TB3 port](https://www.amazon.com/dp/B083G6S7HZ/ref=cm_sw_em_r_mt_dp_K0PR1CE0JXFDHC4TFT38?_encoding=UTF8&amp;psc=1)\n\n[SAMSUNG 970 EVO Plus SSD 1TB, M.2 NVMe Interface Internal Solid State Hard Drive with V-NAND Technology for Gaming, Graphic Design, MZ-V7S1T0B/AM](https://www.amazon.com/dp/B07MFZY2F2/ref=cm_sw_em_r_mt_dp_9EJF341ECSXRESMAD1WT?_encoding=UTF8&amp;psc=1)\n\n[Samsung 32GB DDR4 2666MHz RAM Memory Module for Laptop Computers (260 Pin SODIMM, 1.2V) M471A4G43MB1  2x, 64GB total ram](https://www.amazon.com/dp/B07N124XDS/ref=cm_sw_em_r_mt_dp_PW8JGCC0DZPFP9GR0VB4?_encoding=UTF8&amp;psc=1)\n\nI have developed a DL program that I\u2019ve been running on AWS g4dn instances using one of their DL AMIs. Here is what I plan to do locally.\n\n-\tUbuntu 18.04 desktop\n-\tPython 3.6\n-\tConda\n-\tPytorch 1.1\n-\tCUDA 9\n\nI\u2019d like to be able to have a local machine to more quickly experiment. Does anyone see any risks with the eGPU enclosure, card and mini PC combo?", "upvote_ratio": 0.92, "id": "t3_royzv3", "created_utc": 1640532323.0}
{"sub": "pytorch", "title": "Stack expects each tensor to be equal size, but got [163, 256, 256] at entry 0 and [160, 256, 256] at entry 1", "selftext": "Hi,  I am working with the OAI MRI dataset for knee osteoarthritis classification. Each one of 435 MRIs I got has to be classified to a grade. For each MRI in a folder, there are 160 2D images. I created this function to read the dataset:\n\n    def dicom2array(path):\n      dicom = pydicom.read_file(path)\n      data = dicom.pixel_array\n      data = (data - np.min(data)) / (np.max(data) - np.min(data))\n      data = cv2.resize(data, (256,256))\n      return data\n    \n    def load_3d_dicom_images(scan_id): #returns an object with shape (160,256,256)\n      files = sorted(glob.glob(f\"/content/drive/MyDrive/subjects/{scan_id}/*/*\"))\n      img = np.array([dicom2array(a) for a in files])\n      return img\n    \n    class MyCustomDataset(Dataset):\n      def __init__(self, path = \"/content/drive/MyDrive/subjects\"):\n        dataframe = pd.read_csv(\"/content/drive/MyDrive/KL_grade.csv\", sep = ';')\n        self.labels = {}\n        id = list(dataframe[\"id\"])\n        grades = list(dataframe[\"grade\"])\n        for i,g in zip(id, grades):\n          self.labels[str(i).zfill(5)] = g\n        \n        self.ids = [a.split(\"/\")[-1] for a in sorted(glob.glob(f\"/content/drive/MyDrive/subjects/\" + \"/*\"))]\n    \n      def __len__(self):\n        return len(self.ids)\n    \n      def __getitem__(self, idx):\n        imgs = load_3d_dicom_images(self.ids[idx])  \n        label = self.labels[self.ids[idx]]\n    \n        return torch.tensor(imgs, dtype = torch.float32), torch.tensor(label, dtype = torch.long)\n\nI tried to train a model with resnet18 from the monai library. When I set the batch\\_size to 10 or higher cuda runs out of memory (I am running the code on google colab), when I set the batch\\_size to 1 it is working but when batch\\_size = 2 I get this error:\n\n**Stack expects each tensor to be equal size, but got \\[163, 256, 256\\] at entry 0 and \\[160, 256, 256\\] at entry 1**\n\nCan anybody help me ?", "upvote_ratio": 1.0, "id": "t3_ro63pm", "created_utc": 1640422115.0}
{"sub": "pytorch", "title": "RuntimeError: 0D or 1D target tensor expected, multi-target not supported", "selftext": "Im pretty new to this, so excuse me if the answer is obvious. When I run my code below, I get the error RuntimeError: 0D or 1D target tensor expected, multi-target not supported. Does anyone know why?\n\n&amp;#x200B;\n\nhttps://preview.redd.it/szzs04ghik781.png?width=1298&amp;format=png&amp;auto=webp&amp;s=b20e151d55295eeb3428acc51b84eeb3da995355", "upvote_ratio": 0.84, "id": "t3_rnwijn", "created_utc": 1640385414.0}
{"sub": "pytorch", "title": "Test model weights", "selftext": "Lets say i load weights,how can i test and make sure that it uploaded weights correctly?", "upvote_ratio": 1.0, "id": "t3_rmxrj2", "created_utc": 1640272466.0}
{"sub": "pytorch", "title": "How to handle large feature matrix when GPU is not available?", "selftext": "I have 2 computers where none of them have a GPU that is supported by pytorch (one of them is M1 of Apple w 8GB RAM with 0 pytorch support and another is AMD w 12GB RAM which has 0 pytorch support). I'm trying to work with a large dataset where I'm trying to extract my own features and then hoping to apply a deep learning model on top of it. Due to my lack of computational resources, I use google Colab (provides \\~12GB RAM for free) to run the code. Yet, I'm fully stuck at the part where I'm supposed to extract the features from the large dataset. Dataloader wants the feature matrix of the training set as one piece in order to slice it to batches, so I can use pytorch stuff on it. But 12GB RAM CPU cannot keep such a huge feature matrix and crashes all the time. My question is, since everyone who works on deep learning works with plenty of features and large data - thus you must be giving huge matrices to DataLoader all the time, how do you handle working with huge feature matrices? Especially when GPU is not an option?", "upvote_ratio": 0.95, "id": "t3_rmtnqp", "created_utc": 1640258715.0}
{"sub": "pytorch", "title": "Introducing TorchVision\u2019s New Multi-Weight Support API", "selftext": "nan", "upvote_ratio": 0.9, "id": "t3_rmdmjd", "created_utc": 1640203804.0}
{"sub": "pytorch", "title": "logistic regression extremely slow on pytorch on gpu vs sklearn cpu", "selftext": "hello friends,\n\nim trying to train a DNN on a dataset with 100k features and 300k entries. i want to predict about 30 categories ( its tfidf vectors of text dataset)\n\nto start i wanted to train just simple logistic regression to compare the speed the the sklearn logistic regression implementation.\n\n[https://gist.github.com/ziereis/bed30cd4db4b14e72b78d9777aa994ab](https://gist.github.com/ziereis/bed30cd4db4b14e72b78d9777aa994ab)\n\nhere is my implementation of the logistic regression and the train loop.\n\n&amp;#x200B;\n\nAm i doing something terribly wrong or why does training in pytorch takes a day and in sklearn it takes 5 minutes ?\n\ni have a 5600x cpu and a 3070 as gpu if thats relevant\n\n&amp;#x200B;\n\nany help is appreciated, thanks", "upvote_ratio": 0.9, "id": "t3_rlsx8h", "created_utc": 1640134610.0}
{"sub": "pytorch", "title": "Efficient PyTorch: Tensor Memory Format Matters [official pytorch blog]", "selftext": "nan", "upvote_ratio": 0.84, "id": "t3_rljta7", "created_utc": 1640108328.0}
{"sub": "pytorch", "title": "Announcing the Winners of the 2021 PyTorch Annual Hackathon [official pytorch blog]", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_rljsxa", "created_utc": 1640108301.0}
{"sub": "pytorch", "title": "Pruning + The Lottery Ticket Hypothesis [Video Tutorial]", "selftext": "nan", "upvote_ratio": 0.94, "id": "t3_rkndl9", "created_utc": 1640007183.0}
{"sub": "pytorch", "title": "Anyone have luck compiling torchscript models to WebAssembly?", "selftext": "So little progress has been made compiling NNs to web browser. Onnx.js is way way behind.", "upvote_ratio": 0.86, "id": "t3_rk4ylo", "created_utc": 1639945636.0}
{"sub": "pytorch", "title": "NLP - How to get correlated words?", "selftext": "Hi everyone, I'm not an expert of tensorflow, I've only used some pretrained api of Tensorflow.js.\n\nI need to get correlated words given a specific word, example:\n\n    Input: \"banana\"\n    Output: \"fruit, market, yellow\"\n\nI tried with GPT-3 playground and given a template it's really good at this, but it looks like I'm trying to shoot a fly with a tank...\n\nDo you know any pretrained-model or maybe a specific api that can help with this?", "upvote_ratio": 0.81, "id": "t3_rjrkrh", "created_utc": 1639898576.0}
{"sub": "pytorch", "title": "Synthetic time series data generation", "selftext": "I  want to generate time series tabular data. Most of generative deep  learning models consists of VAE and/or GAN which are for most part  relating to images, videos, etc.\n\nCan  you please point me to relevant tutorial souces (if it includes code  along with theory, all the more better) pertaining to synthethic time  series data generation using deep learning models or other techniques?", "upvote_ratio": 1.0, "id": "t3_rhv8cu", "created_utc": 1639674783.0}
{"sub": "pytorch", "title": "Made some pytorch modules for agent systems", "selftext": "I am starting a little Evolutionary Algorithms project (I know it's a bit frowned upon) and noticed that if you are working with Deep Neural Networks, you need to instantiate them separately and iterate over them to do each network's forward pass, which is very slow even in GPU.\n\nFor that reason I made this little package of pytorch modules. The main class, WideLinear, behaves as a family of linear layers, each different, but each running fully in parallel so you do a single forward pass through all of them at the same time. Even works in GPU.\n\nThis has some application outside of evolutionary algorithms, but mostly still in agent based systems. Gradients work as expected.\n\nI have a brief documentation in my github, [https://github.com/joaoperfig/WideLinears](https://github.com/joaoperfig/WideLinears), and it is available through pip.", "upvote_ratio": 1.0, "id": "t3_rhui48", "created_utc": 1639672700.0}
{"sub": "pytorch", "title": "What is the mac M1 equivalent for whl package for pytorch?", "selftext": "Hi, \n\nI am wanting to replace this wheel URL in my requirements.txt file, with the version for mac M1 chip:\n\nhttps://download.pytorch.org/whl/cpu/torch-1.5.0%2Bcpu-cp38-cp38-linux_x86_64.whl\n\n Is this possible for M1? \n\nThank you in advance.", "upvote_ratio": 0.67, "id": "t3_rhekan", "created_utc": 1639616742.0}
{"sub": "pytorch", "title": "Cpu equivalent for cuda stream", "selftext": "Hello, new to the sub and to pytorch here :D  \nI've got  a little problem with a piece of software I'm working with for a project. This is the github: [https://github.com/PruneTruong/DenseMatching#overview](https://github.com/PruneTruong/DenseMatching#overview)\n\nI have a problem with a line in [correlation.py](https://correlation.py) in the third\\_party/GOCor/GOCor/local\\_correlation folder. This line\n\nptr = torch.cuda.current\\_stream().cuda\\_stream \n\nreturns me an error about the NVIDIA driver being too old. Is there a cpu equivalent command that i can plug in? Since the system I'm working on is not mine I'd prefer not to go through the hassle of updating drivers and all. Thanks to whoever can help", "upvote_ratio": 1.0, "id": "t3_rg9gty", "created_utc": 1639494456.0}
{"sub": "pytorch", "title": "Conv3D model input tensor", "selftext": " I am new to PyTorch and I want to make a classifier for 3D DICOM MRIs. I want to use the pretrained resnet18 from monai library but I am confused with the input dimensions of the tensor. The shape of the images in my dataloader is \\[2,160,256,256\\] where 2 is the batch\\_size, 160 is the number of dicom images for each patient and 256x256 is the dimension of the images. When I try to run the model I get this error: **Expected 5-dimensional input for 5-dimensional weight \\[64, 3, 7, 7, 7\\], but got 4-dimensional input of size \\[2, 160, 256, 256\\] instead**\n\nIf I unsqueeze the tensor before feeding it to the model I get: **Given groups=1, weight of size \\[64, 3, 7, 7, 7\\], expected input\\[1, 2, 160, 256, 256\\] to have 3 channels, but got 2 channels instead** Can anybody help me figure this out ?", "upvote_ratio": 0.86, "id": "t3_rg7es4", "created_utc": 1639488252.0}
{"sub": "pytorch", "title": "DistributedDataParallel with GPUs of different speeds", "selftext": "I have two GPUs one of which is much slower than the other -- sometimes the speed difference can be 2x or more. I can only run one training example at a time on each GPU (minibatch of 1), so right now with my single-gpu training, I accumulate gradients over multiple forward-backward passes and then update weights once I reach the intended batch size.\n\nI am trying to figure out I can use DistributedDataParallel to run minibatches on both GPUs simultaneously. The following passage makes me think It won't make sense for me:\n\n&gt;In DDP, the constructor, the forward pass, and the backward pass are distributed synchronization points. Different processes are expected to launch the same number of synchronizations and reach these synchronization points in the same order and enter each synchronization point at roughly the same time. Otherwise, fast processes might arrive early and timeout on waiting for stragglers. Hence, users are responsible for balancing workloads distributions across processes. Sometimes, skewed processing speeds are inevitable due to, e.g., network delays, resource contentions, unpredictable workload spikes. To avoid timeouts in these situations, make sure that you pass a sufficiently large timeout value when calling init_process_group.\nhttps://pytorch.org/tutorials/intermediate/ddp_tutorial.html\n\nHowever, I'm confused when it says users are responsible for balancing workloads across processes. Can I run 2x more forward &amp; backward passes on the faster gpu while the slower one is running? Since it says \"Different processes are expected to launch the same number of synchronizations and reach these synchronization points in the same order and enter each synchronization point at roughly the same time\", it seems like I cannot.\n\nBut then what does it even mean to \"balance the workload distributions across processes\"?\n\nThe problem is that if I have to run the same amount of data on each card, I suspect it will be slower than if I just run everything on the faster card, since it will otherwise be timing out the majority of the time if it's always locked to synchronize with the slower card.\n\nThanks.", "upvote_ratio": 1.0, "id": "t3_rfvo5a", "created_utc": 1639445329.0}
{"sub": "pytorch", "title": "Loading data stucks", "selftext": "I am using google colab to run some experiments with 10-fold cross-validation. However, there are times that google colab stucks on loading the images:\n\n    run_model() &gt; train_classifier() &gt; __next__() &gt; _next_data() &gt; _get_data() &gt; _get_data() &gt; _try_get_data() &gt; get() &gt; poll() &gt; _poll() &gt; wait() &gt; select()\n\nWhen I ran the experiments some time ago with 112x112 images there was no problem. Now I run with 256x256 images and thats a usual problem. The initial images vary in size from 100x100 to 500x500.\n\nAny ideas what can cause that?", "upvote_ratio": 0.4, "id": "t3_rffw4q", "created_utc": 1639402603.0}
{"sub": "pytorch", "title": "PyTorch distributed data-parallel (multi GPU, multi-node)", "selftext": "I have access to 18 nodes each with different numbers of GPUs all with at least 1 to my understanding you have to declare all the nodes to have the same number of GPUs.\n\nfirst of all, am I correct?\n\nand second, If I am is there any way around this?", "upvote_ratio": 1.0, "id": "t3_re03k9", "created_utc": 1639230362.0}
{"sub": "pytorch", "title": "How to get a probability distribution over tokens in a huggingface model?", "selftext": "I'm following [this](https://ramsrigoutham.medium.com/sized-fill-in-the-blank-or-multi-mask-filling-with-roberta-and-huggingface-transformers-58eb9e7fb0c) tutorial on getting predictions over masked words. The reason I'm using this one is because it seems to be working with several masked word simultaneously while other approaches I tried could only take 1 masked word at a time.\n\nThe code:\n\n    from transformers import RobertaTokenizer, RobertaForMaskedLM\n    import torch\n    \n    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n    model = RobertaForMaskedLM.from_pretrained('roberta-base')\n    \n    sentence = \"Tom has fully ___ ___ ___ illness.\"\n    \n    \n    def get_prediction (sent):\n        \n        token_ids = tokenizer.encode(sent, return_tensors='pt')\n        masked_position = (token_ids.squeeze() == tokenizer.mask_token_id).nonzero()\n        masked_pos = [mask.item() for mask in masked_position ]\n    \n        with torch.no_grad():\n            output = model(token_ids)\n    \n        last_hidden_state = output[0].squeeze()\n    \n        list_of_list =[]\n        for index,mask_index in enumerate(masked_pos):\n            mask_hidden_state = last_hidden_state[mask_index]\n            idx = torch.topk(mask_hidden_state, k=5, dim=0)[1]\n            words = [tokenizer.decode(i.item()).strip() for i in idx]\n            list_of_list.append(words)\n            print (\"Mask \",index+1,\"Guesses : \",words)\n        \n        best_guess = \"\"\n        for j in list_of_list:\n            best_guess = best_guess+\" \"+j[0]\n            \n        return best_guess\n    \n    \n    print (\"Original Sentence: \",sentence)\n    sentence = sentence.replace(\"___\",\"&lt;mask&gt;\")\n    print (\"Original Sentence replaced with mask: \",sentence)\n    print (\"\\n\")\n    \n    predicted_blanks = get_prediction(sentence)\n    print (\"\\nBest guess for fill in the blank :::\",predicted_blanks)\n\nHow can I get the probability distribution over the 5 tokens instead of the indices of them? That is, similarly to how [this](https://www.machinecurve.com/index.php/2021/03/02/easy-masked-language-modeling-with-machine-learning-and-huggingface-transformers/) approach (that I used before but once I change to multiple masked tokens I get an error) gets the score as an output:\n\n    from transformers import pipeline\n    \n    # Initialize MLM pipeline\n    mlm = pipeline('fill-mask')\n    \n    # Get mask token\n    mask = mlm.tokenizer.mask_token\n    \n    # Get result for particular masked phrase\n    phrase = f'Read the rest of this {mask} to understand things in more detail'\n    result = mlm(phrase)\n    \n    # Print result\n    print(result)\n    \n    [{\n        'sequence': 'Read the rest of this article to understand things in more detail',\n        'score': 0.35419148206710815,\n        'token': 1566,\n        'token_str': ' article'\n    },...", "upvote_ratio": 1.0, "id": "t3_rcz7mj", "created_utc": 1639107531.0}
{"sub": "pytorch", "title": "How does one install PyTorch and related tools from within the setup.py install_requires list?", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_rcprlk", "created_utc": 1639079848.0}
{"sub": "pytorch", "title": "Windowed MNIST training", "selftext": "I\u2019m trying to train a model (CNN based on LeNet) on windowed MNIST data, where the output would be a vector of size window\\_size with the predicted class of each input in the input window - essentially like a sequence-based model (e.g. LSTM) but instead the model isn\u2019t sequential, so I would for each item in the window run it through the model and concatenate the outputs somehow? Any suggestions?\n\n&amp;#x200B;\n\nhttps://preview.redd.it/e9cxuh9wwj481.jpg?width=1085&amp;format=pjpg&amp;auto=webp&amp;s=bb9b6cb3ebfc1c2c23f1084e4f747094e1c9b912", "upvote_ratio": 1.0, "id": "t3_rcmhsv", "created_utc": 1639070806.0}
{"sub": "pytorch", "title": "Load 3D image dataset", "selftext": "I am facing some problems to write the \\_\\_getitem\\_\\_() function in my dataset class. I am working on a MRI dataset (3D). Each file consists of 160 slices in DICOM format. I have transformed the DICOM files into PNG.\n\nThe structure of the files looks like this: \"/content/drive/MyDrive/mris/9114036/11288003\" \n\nInside the last directory there are the 160 2D slices. The labels are in a .csv file with two columns, one with the id (9114036 for example in the path above) and the other with the grade.\n\nThe code I tried to execute was: \n\n    class MyDataset(Dataset):\n    \n      def __init__(self, csv_file, root_dir, transform = None):\n        self.labels_df = pd.read_csv(csv_file, sep = ';')\n        self.root_dir = root_dir\n        self.transform = transform\n    \n      def __len__(self):\n        return len(self.labels_df)\n    \n      def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n          idx = idx.tolist()\n        \n        img_name = os.path.join(self.root_dir,str(self.labels_df.iloc[idx,0]))\n        image = io.imread(img_name, plugin='matplotlib')\n        grade = self.labels_df.iloc[idx, 1]\n        sample = {'image': image, 'grade': grade}\n    \n        if self.transform:\n          sample = self.transform(sample)\n        \n        return sample\n\nThe error I got when I tried to access a sample from the dataset was:  \n\n [/usr/local/lib/python3.7/dist-packages/PIL/Image.py](https://localhost:8080/#) in open(fp, mode)    **2841**     **2842** if filename: -&gt; 2843 fp = builtins.open(filename, \"rb\")    **2844** exclusive\\_fp = True    **2845**   IsADirectoryError: \\[Errno 21\\] **Is a directory: '/content/drive/MyDrive/mris/9114036'** \n\nwhich seems logic.\n\nI tried to use os walk to get in the 11288003 directory where the images are, but it didn't work. Most likely my whole approach is wrong. \n\nDoes anybody know how to write class dataset for the 3D nature of my data ? Should I use another transformation for the DICOM files in the first place ?", "upvote_ratio": 1.0, "id": "t3_rcmaxt", "created_utc": 1639070280.0}
{"sub": "pytorch", "title": "Making changes in architecture and training a custom model", "selftext": "Hello,\n\nI am trying to make changes in a segmentation model (Seg-net). I am making changes like trying out different activation functions. When I try predicting with the new models I am getting the error \"missing keys in state_dict\" and \"unexpected keys in state_dict\". Can this be because I changed the model?\n\nHere are a few questions I have as a newbie.\n\n1) Should the model be tested on the same model it was trained on?\n\n2) While loading a model and predicting, does it require the  python file which contains the changed architecture?", "upvote_ratio": 1.0, "id": "t3_rccolu", "created_utc": 1639035607.0}
{"sub": "pytorch", "title": "RTX 3080 vs RTX A4000", "selftext": "Hi All,\n\nI'm  looking at building a system for data science using PyTorch and a  timeseries database. It appears that the advice is that an RTX A4000  GPU would be the best for my use case but people have said that if supply constraints get me, I can settle for an RTX 3080 TI instead.\n\nHowever,  when I look at the spec. and performance measures, it appears the RTX  3080 TI wins out. It has more cuda cores (10,240 vs 6,144), wider memory  interface (384 vs 256 bit) and all round better performance. The only  edge that I can see for the A4000 is that it has 16GB  memory compared  to 12GB for the RTX 3080 TI.\n\nSome feedback has been that the A4000 is a 'workstation' card so will have better drivers etc. My problem is I don't really understand how that will impact performance since, as far as I have used it, PyTorch already 'just works' with Nvidia GPUs. In addition, I thought  Nvidia had a whole interface so you can write cuda code in C++ to access all low level GPU functionality for less PyTorch specific use cases. \n\nCan you advise what I'm missing? In what way is the A4000 better than the RTX3080 in a PyTorch/ machine learning context? Are there perhaps additional optimisations that the A4000 makes available that lift it's performance above the nominal stats I see in tests? (tests which admittedly seem to be poorly reflective of my use case).\n\nThanks and regards,", "upvote_ratio": 0.86, "id": "t3_rc8lhc", "created_utc": 1639020676.0}
{"sub": "pytorch", "title": "Hook function does not work well during backpropagation", "selftext": "I have the following error with [AvgPool2D](https://github.com/promach/gdas/blob/ea26c6a92dfa2b9d25cfbf953f979187d1c85df2/gdas.py#L137)\n\nHow to get around it ?\n\n    WARNING: Logging before InitGoogleLogging() is written to STDERR\n    W20211209 00:31:29.223598  8950 python_anomaly_mode.cpp:102] Warning: Error detected in AvgPool2DBackward0. Traceback of forward call that caused the error:\n      File \"/home/phung/PycharmProjects/beginner_tutorial/gdas.py\", line 755, in &lt;module&gt;\n        ltrain = train_NN(forward_pass_only=0)\n      File \"/home/phung/PycharmProjects/beginner_tutorial/gdas.py\", line 400, in train_NN\n        y2 = graph.cells[c].nodes[n].connections[cc].edges[e].forward_f(x2)\n      File \"/home/phung/PycharmProjects/beginner_tutorial/gdas.py\", line 106, in forward_f\n        return self.f(x)\n      File \"/usr/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1120, in _call_impl\n        result = forward_call(*input, **kwargs)\n      File \"/usr/lib/python3.9/site-packages/torch/nn/modules/pooling.py\", line 616, in forward\n        return F.avg_pool2d(input, self.kernel_size, self.stride,\n     (function _print_stack)\n    Traceback (most recent call last):\n      File \"/home/phung/PycharmProjects/beginner_tutorial/gdas.py\", line 755, in &lt;module&gt;\n        ltrain = train_NN(forward_pass_only=0)\n      File \"/home/phung/PycharmProjects/beginner_tutorial/gdas.py\", line 525, in train_NN\n        Ltrain.backward()\n      File \"/usr/lib/python3.9/site-packages/torch/_tensor.py\", line 307, in backward\n        torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n      File \"/usr/lib/python3.9/site-packages/torch/autograd/__init__.py\", line 154, in backward\n        Variable._execution_engine.run_backward(\n    RuntimeError: Output 0 of BackwardHookFunctionBackward is a view and its base or another view of its base has been modified inplace. This view was created inside a custom Function (or because an input was returned as-is) and the autograd logic to handle view+inplace would override the custom backward associated with the custom Function, leading to incorrect gradients. This behavior is forbidden. You can fix this by cloning the output of the custom Function.\n    \n    Process finished with exit code 1", "upvote_ratio": 0.83, "id": "t3_rbvg9f", "created_utc": 1638982129.0}
{"sub": "pytorch", "title": "(Beginner) Trying to install pytorch, is this a pip issue or a pytorch issue?", "selftext": "nan", "upvote_ratio": 0.63, "id": "t3_rbdqhg", "created_utc": 1638923053.0}
{"sub": "pytorch", "title": "I put together a tutorial on PyTorch Lightning and how it compares to vanilla PyTorch", "selftext": " If you haven't heard of it, PyTorch Lightning is a great framework built on top of vanilla PyTorch. It is really good for rapid prototyping and is essentially just a wrapper for PyTorch, so the learning curve is pretty shallow if you work with PyTorch already.\n\nI wrote a tutorial and overview that compares Lightning to vanilla, where I go through an example project of building a simple GAN to generate handwritten digits from MNIST. I figured this sub might find it useful, especially for those who haven't heard of Lightning!\n\n[Here's a link](https://www.assemblyai.com/blog/pytorch-lightning-for-dummies/) to the full tutorial if you're interested in learning about Lightning!", "upvote_ratio": 0.88, "id": "t3_rb2w0z", "created_utc": 1638894871.0}
{"sub": "pytorch", "title": "DICOM files classification", "selftext": "Hello, I am working on a project where I have to classify DICOM files. \n\nI can see there are a lot of libraries that can handle DICOM data type. From the research I have made I see there are transformations to .npy and .png before the data is fed into a cnn. \n\nWhich transformation is better ? Can I feed the DICOM files raw (without any transformation) to a network ?", "upvote_ratio": 0.84, "id": "t3_rax3ha", "created_utc": 1638876506.0}
{"sub": "pytorch", "title": "KeyError", "selftext": "I am working on a problem in which I have to classify knee MRIs. It is a multi label classification (0-4 grade) and the MRIs I have are DICOM files. I am using these code to demonstrate some attributes of the MRIs:\n\n    import dicom\n    import os\n    import pandas as pd\n    \n    data_dir = \"D:/base_MRI/subjects\"\n    patients = os.listdir(data_dir)\n    labels_df = pd.read_csv(\"C:/Users/User/Desktop/KL_grade.csv\", index_col=False)\n    \n    \n    for patient in patients[:1]:\n        label = labels_df._get_value(patient, 'grade')\n        path = data_dir + patient\n        slices = [dicom.read_file(path + '/' + s) for s in os.listdir(path)]\n        print(len(slices), label)\n\nI get this error: KeyError: **'9001104'**\n\n9001104 is the first MRI folder containing 160 2D slices. The patients list is a list with all the MRIs. \n\nI think the problem is in the labels\\_df. I have tried to set index\\_col=0 but then I get the same error. \n\nThe KL\\_grade.csv is the csv file with 2 columns, one the subject\\_id which is the same name with each MRI and the grade which are the labels. \n\nDoes anybody know how to fix this ?", "upvote_ratio": 1.0, "id": "t3_rajj17", "created_utc": 1638829545.0}
{"sub": "pytorch", "title": "Model does not fit to ram", "selftext": "I am using a contrastive learning framework (see [https://github.com/HobbitLong/SupContrast](https://github.com/HobbitLong/SupContrast)). First I initialize a CNN feature extractor (`network`), LeNet-5 here, and then I pass it to the full model (`model`). However, while I can initialize my network, google colab session crashes with no more RAM, while my PC with 32GB runs just fine. Any idea on how I can overcome this?\n\n    print('1.\u00a0net') # it is printed\n    net\u00a0=\u00a0set_network(opt) \n    print('2.\u00a0model') # it is printed\n    model\u00a0=\u00a0SupCon(net,\u00a0head=opt.head,\u00a0feat_dim=opt.feat_dim) \n    print('3.\u00a0crit') # it is not printed\n    criterion\u00a0=\u00a0SupConLoss(temperature=opt.temp)\n    \n    And to give you an idea of the model:\n    \n    CustomNet(\n      (feature_extractor): Sequential(\n        (0): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n        (4): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (6): ReLU()\n        (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n        (8): Conv2d(16, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (9): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (10): ReLU()\n        (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      )\n    )\n    \n    with 122880 neurons at output", "upvote_ratio": 0.81, "id": "t3_rafprd", "created_utc": 1638819562.0}
{"sub": "pytorch", "title": "Convert unknown labels to yolov5", "selftext": "Hello datascience-community,\n\nhttps://preview.redd.it/zpcutamtcw381.jpg?width=320&amp;format=pjpg&amp;auto=webp&amp;s=fc8e025cf43481991c2ed5ad84f73671429358a4\n\ni need your kind assistance.\n\nI own a dataset of more than **100k images with unknown label format**, wich is:\n\n**angry\\_actor\\_104.jpg 0 28 113 226 141 22.9362 0**\n\nIt indicates an image as follows:\n\n**image\\_name  face\\_id\\_in\\_image  face\\_box\\_top  face\\_box\\_left  face\\_box\\_right  face\\_box\\_bottom face\\_box\\_cofidence  expression\\_label**\n\nSource of the dataset: [http://mmlab.ie.cuhk.edu.hk/projects/socialrelation/index.html](http://mmlab.ie.cuhk.edu.hk/projects/socialrelation/index.html)\n\nMy question is: How can this be converted into the **yolov5 format**?\n\nI have been looking this up for a long time and hope someone can help.  \n Thank you very much in advance.\n\nBest regards  \n Phil", "upvote_ratio": 1.0, "id": "t3_ra3z9y", "created_utc": 1638785399.0}
{"sub": "pytorch", "title": "does the cuda toolkit used with pytorch need to be the exact version of cuda?", "selftext": "i am install pytorch to test out models(i am using conda if that matters) but i noticed the version of cuda toolkit(11.3) is a lower version then the cuda i have install (11.5), and i am wondering if there will be issue because of the number diffrence", "upvote_ratio": 1.0, "id": "t3_r91s8h", "created_utc": 1638659668.0}
{"sub": "pytorch", "title": "How to find the auto-determined learning rate from Pytorch lightning and Neptune?", "selftext": "I've started to learn and try Pytorch lightning lately together with the Neptune logger. An amazing feature it has is that it can autodetect a learning rate (auto\\_lr\\_find=True)  and batch size from the training data. However, as I check Neptune logs, I cannot find out the autodetected learning rate. I couldn't find an option where it would specifically allow me to save the found lr to the Neptune logs either. Does anyone know how can I find out this final learning rate that my model uses to return my results?", "upvote_ratio": 0.83, "id": "t3_r8zbr0", "created_utc": 1638652517.0}
{"sub": "pytorch", "title": "What is the best / fastest way to store Pytorch Datasets?", "selftext": "Hi everyone, \nI am interested in the fastest way to store data (lets say imagenet) to disk, that it needs as less time as possible to load again from a torchvision dataset and dataloader getitem. I have heard, that tf.dataset is quite fast for tensorflow. Might that be fast for pytorch as well or are there any other formats?", "upvote_ratio": 0.88, "id": "t3_r8j64r", "created_utc": 1638598105.0}
{"sub": "pytorch", "title": "Creating a model that uses different output layers based on the input", "selftext": "I want to have a single model that uses output layer A if input is type A, layer B if input is type B, ...\n\nIs there a way to do that, or would I have to have separate models?", "upvote_ratio": 0.67, "id": "t3_r80pzu", "created_utc": 1638542929.0}
{"sub": "pytorch", "title": "Resuming pytorch tranining", "selftext": "Hi, im training NNs on Colab, and notebook sometimes crashes and i have to train them all over again,how to save all stuff and resume training where it left off?", "upvote_ratio": 1.0, "id": "t3_r7v472", "created_utc": 1638522916.0}
{"sub": "pytorch", "title": "Need idea for a regularization score", "selftext": "I have a pytorch model\n\n    net_x = Model() \n\nThis model have a learnable parameter called \"value\":\n\n    print(net_x.value) &gt;&gt;&gt; 0.532 \n\nI want to penalize to model if this value is below 0.1 and add that regularization to the loss. The only thing I can think of is having a step function (eg \"if statement\"), but that's not differentiable. Any suggestions?", "upvote_ratio": 0.84, "id": "t3_r7bt87", "created_utc": 1638464241.0}
{"sub": "pytorch", "title": "OAI Dataset", "selftext": "I have a project in which I will make a model that classifies Knee MRIs from the OAI dataset. My university provided me the dataset. \n\nThe classification is about knee osteoartrithis. The model must assign a grade (from 0 to 4) in each MRI. \n\nI am facing a problem as I am not able to find the labels in the MRI file tha was given to me. Is the label somewhere in the metadata or the header of the DICOM files (MRIs) and I cannot find it or my professor forgot to send me an extra file containing the labels ?", "upvote_ratio": 1.0, "id": "t3_r6ouj6", "created_utc": 1638391880.0}
{"sub": "pytorch", "title": "Sliding Window to Apply U-NET on Larger Image", "selftext": "I have a project I'm working on that has large image files that I'm doing semantic segmentation on, (4501x4501, or 10001x 10001 pixels) but do to hardware limitations I'm forced to chip the image into small 256x256 pieces. Alter each chip has been processed with the  neural network I merge them together, but this has created seam lines because the edges are often predicted incorrectly.\n\nI'm looking into doing sliding window where a window of 256x256 would move across the larger input images, run the trained neural network and then adding up the probabilities for each class. After running on the whole image  the weights would be max pooled to extract the most likely class. I'm looking for example code of anything else that tries this approach but I've been unable to find anything and I'm unsure of how to implement this.  Does anyone know of an example, or have any tips?", "upvote_ratio": 0.84, "id": "t3_r6a47j", "created_utc": 1638347440.0}
{"sub": "pytorch", "title": "Is the memory of the gpu ( 4gb on rtx 3050ti) the upper limit of memory usable?", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_r67b9k", "created_utc": 1638336313.0}
{"sub": "pytorch", "title": "Adafactor from transformers hugging face only works with Transfromers - does it not work with Resnets and MAML with higher?", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_r5p2pk", "created_utc": 1638284362.0}
{"sub": "pytorch", "title": "Prevent `CUDA error: out of memory` from happening in 1 line of code", "selftext": "Hi, folks who love PyTorch!\n\nI've been working on a fast PyTorch wrapper that prevents OOM error from happening.\n\nI've recently [shared](https://www.reddit.com/r/MachineLearning/comments/r4zaut/p_eliminate_pytorchs_cuda_error_out_of_memory/?utm_source=share&amp;utm_medium=web2x&amp;context=3) it with r/MachineLearning, and they quite like it.\n\n[Project Link](https://github.com/rentruewang/koila)\n\nThis library tries to be very flexible, and it works with native PyTorch code.\n\nPlease tell me what you think. Suggestions welcome!", "upvote_ratio": 0.86, "id": "t3_r5oo88", "created_utc": 1638283166.0}
{"sub": "pytorch", "title": "What libraries do I need to train an audio classifier model using Pytorch?", "selftext": "The dataset contains mp4 videos.", "upvote_ratio": 1.0, "id": "t3_r4u01q", "created_utc": 1638185923.0}
{"sub": "pytorch", "title": "How to merge the batched up data within DataLoader to batch them again properly?", "selftext": "I couldn't express the complete situation better in the title, but my situation is, I have a very large training set that doesn't fit the CPU and I have no GPU. Thus, to extract features from it, I've batched the training set dataframe and I'm obtaining the features inside the loop perfectly. However, I cannot create a tensor and merge all those batch features into a single tensor matrix because it'll again not fit in my memory. So, I imagined an ideal situation where I convert the features inside the batching loop to tensors, and create some kind of an object (DataLoader maybe?), where I can add the new feature tensors I obtain within the loop into that DataLoader, where in the end of it all, I will have a single DataLoader with better batches. Is it possible? In case I explained it poorly, here is the algorithm:\n\n    # x_train_df: my dataframe that contains training set data\n            \n    n = len(x_train_df)  # number of rows\n    rough_batch_size = 1000  # number of rows in each call to partial_fit\n    batch_size = 32 # This will be the DataLoader batch_size hopefully\n    index = 0  # helper-var\n         \n    while index &lt; n:\n      partial_size = min(rough_batch_size, n - index)  # because last loop is incomplete\n      partial_x = x_train_df[\"data\"][index : index + partial_size] # batch\n      features = get_features(partial_x)\n      normalized_features = scaler.partial_fit(features)\n      index += partial_size\n    \n      # Now what should I do with the normalized_features?\n      ?????\n    \n    # This is my eventual goal for the output\n    # train_dataloader : dataloader object that has the complete training set\n    #                    where the data is shuffled and batch_size is 32\n\nso how can I obtain this single train\\_dataloader from the batched input data without crashing my CPU?", "upvote_ratio": 0.67, "id": "t3_r44sc6", "created_utc": 1638107638.0}
{"sub": "pytorch", "title": "can i make a tool which is written in c++/cuda in libtorch on ubuntu go run native in windows?", "selftext": "tool: [https://github.com/darglein/ADOP](https://github.com/darglein/ADOP)\n\nbtw. i also cant code.... :(", "upvote_ratio": 0.84, "id": "t3_r3sdcd", "created_utc": 1638062952.0}
{"sub": "pytorch", "title": "Where we are headed and why it looks a lot like Julia (but not exactly like Julia) - compiler", "selftext": "nan", "upvote_ratio": 0.9, "id": "t3_r335sn", "created_utc": 1637980447.0}
{"sub": "pytorch", "title": "How expensive is it to fine-tune BERT even with Pytorch lightning?", "selftext": "I want to use BERT for my text classification task but so far I've been failing due to the lack of GPU. Due to my computational limitation, I've selected batch\\_size=32 for tokenization (didn't want to go smaller as it would create great noise), selected \"bert-base-uncased\" and I've wrapped the BERT fine-tuning with Pytorch lightning. Since my computer has no GPU (Macbook air with M1), I've been trying to work with GPU Google Colab provides for free. Yet, I get this error eventually:\n\n&gt;RuntimeError: CUDA out of memory. Tried to allocate 170.00 MiB (GPU 0; 11.17 GiB total capacity; 10.24 GiB already allocated; 167.81 MiB free; 10.41 GiB reserved in total by PyTorch) If reserved memory is &gt;&gt; allocated memory try setting max\\_split\\_size\\_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH\\_CUDA\\_ALLOC\\_CONF\n\nShould I simply give up the fine-tuning and just go ahead with the pre-trained version of the BERT? Are there any examples/studies that do that? Or is it quite uncommon to not to fine-tune BERT?", "upvote_ratio": 1.0, "id": "t3_r2y60c", "created_utc": 1637964848.0}
{"sub": "pytorch", "title": "PyDreamer: model-based RL written in PyTorch + integrations with DM Lab and MineRL environments", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_r2w7d1", "created_utc": 1637958939.0}
{"sub": "pytorch", "title": "The Sensory Neuron as a Transformer [Implementation]", "selftext": "nan", "upvote_ratio": 0.92, "id": "t3_r2liin", "created_utc": 1637927689.0}
{"sub": "pytorch", "title": "Hey, I'm trying to find some materials about object detection in Pytorch but I'm having a hard time finding it.", "selftext": "If you have any material about object detection it would be very apricated if you could share.\n\nAs a school project I'm trying to make my own model for object detection, and everywhere I go I see prebuilt models but without any explanations.", "upvote_ratio": 0.92, "id": "t3_r1zbgg", "created_utc": 1637854948.0}
{"sub": "pytorch", "title": "What is the correct way to sum loss into a total loss and then to backprop?", "selftext": "I need to do a somewhat complex gradient update where I have to calculate a loss several times and to backprop over it.\n\nIt looks something like this:\n\n    for i in range(3):\n        opt.zero_grad()\n        out = net(inp)\n        loss = y - out\n        loss.backward()\n        opt.step()\n\nI'm currently getting the error:\n\n\"RuntimeError: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain\\_graph=True when calling .backward() or autograd.grad() the first time\"\n\nWhich from [here](https://github.com/davda54/sam/issues/10) I understand that I shouldn't use the same loss variable for both forward passes but I'm not sure how else to do this. I thought that I could maybe create a variable called total\\_loss and add the loss to it and then after the iterations to backprop over it, but I'm not sure if that's the correct approach.", "upvote_ratio": 1.0, "id": "t3_r1p9cy", "created_utc": 1637819011.0}
{"sub": "pytorch", "title": "Slowing process", "selftext": "Do u always avoid .numpy(), .cpu(), .item().How do u calculate lets say acc of epoch if u dont move to numpy, or running loss, do u calculate it as tensor.", "upvote_ratio": 1.0, "id": "t3_r1dpff", "created_utc": 1637783801.0}
{"sub": "pytorch", "title": "How to Build and Deploy an Image Recognition App using FastAPI and PyTorch? | BHIMRAJ YADAV", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_r11sid", "created_utc": 1637748166.0}
{"sub": "pytorch", "title": "Finding why Pytorch Lightning made my training 4x slower.", "selftext": "nan", "upvote_ratio": 0.97, "id": "t3_r09e8q", "created_utc": 1637659142.0}
{"sub": "pytorch", "title": "Customizing dataset", "selftext": "Som im trying to extract 5 - 10 classes out of coco dataset. Any tips how to do it in pytorch framework ?", "upvote_ratio": 0.75, "id": "t3_qzjvtq", "created_utc": 1637582444.0}
{"sub": "pytorch", "title": "Custom pooling layer", "selftext": "I'm trying to implement a custom pooling layer in pytorch. How do I go about it? I don't wanna use maxpooling. I want to take the absolute difference between max and min values and output that to the next layer. Is there a tutorial of sorts for implementing custom pooling layers?", "upvote_ratio": 1.0, "id": "t3_qzfxa7", "created_utc": 1637566159.0}
{"sub": "pytorch", "title": "Moving model to phone with multiple inputs", "selftext": "In Python you seem to be able to input label keypair dictionaries. Moving to the phone in objective C it says expected tensor but got generaldict. Anyone know the syntax?", "upvote_ratio": 0.8, "id": "t3_qy8irh", "created_utc": 1637424047.0}
{"sub": "pytorch", "title": "If I want to download Pytorch's Cuda version should I uninstall my Pytorch and install the Cuda version or it doesn't matter?", "selftext": "One more thing, does it matter if I downloaded Cuda 11.5 but Pytorch uses 11.3?", "upvote_ratio": 1.0, "id": "t3_qy1fx7", "created_utc": 1637397619.0}
{"sub": "pytorch", "title": "Using a model that was trained with DDP", "selftext": "I am working with a model that was trained using distributed data parallel. I am trying to use the model, but when I load the weights and try to actually feed input into the model, I get an error saying the default process group has not been initialized. I only have one GPU so I cannot use DDP, is there a workaround to this?", "upvote_ratio": 1.0, "id": "t3_qxv43n", "created_utc": 1637373581.0}
{"sub": "pytorch", "title": "How Can I use an animated mplfinance chart as env?!", "selftext": "I have some python files:\n\nhttps://github.com/mablue/mplfinance/blob/anim/examples/mpf_animation_demo1.py\nAnd\nhttps://github.com/mablue/mplfinance/blob/anim/examples/mpf_animation_demo2.py\n\nHow use them as pytorch rl env?!", "upvote_ratio": 1.0, "id": "t3_qxjfk1", "created_utc": 1637338160.0}
{"sub": "pytorch", "title": "How to Train State-Of-The-Art Models Using TorchVision\u2019s Latest Primitives", "selftext": "nan", "upvote_ratio": 0.94, "id": "t3_qxcjzh", "created_utc": 1637313824.0}
{"sub": "pytorch", "title": "Square aliasing when converting padding conv2d Tensorflow to Pytorch", "selftext": "I'm trying to convert a Tensorflow program to Pytorch. It's deepdream with Laplacian normalisation from this [website](https://programmer.help/blogs/deep-dream-model-and-implementation.html). It seems to work except that when I use Laplacian Normalisation the output has these rectangular bands that I think come from the padding operation. Also the outputs do not quite match up. How can I convert operation such as:\n\n&gt;tf.nn.conv2d(img, k5x5, \\[1, 2, 2, 1\\], 'SAME') \n\nand\n\n&gt;tf.nn.conv2d\\_transpose(lo, k5x5 \\* 4, tf.shape(img), \\[1, 2, 2, 1\\])\n\nto Pytorch? There might be other difference too.\n\nI've uploaded my full attempt [here](https://pastebin.com/Adsq19Kp)", "upvote_ratio": 1.0, "id": "t3_qwqx3h", "created_utc": 1637245715.0}
{"sub": "pytorch", "title": "Hey, I'm trying to install Pytorch to my venv but it doesn't work. I've already installed numpy and opencv. Any Ideas why it happens?", "selftext": "nan", "upvote_ratio": 0.8, "id": "t3_qw6zzm", "created_utc": 1637177522.0}
{"sub": "pytorch", "title": "Efficient way to get \"neuron-edge-neuron\" values in a neural network", "selftext": "I'm working on a visual networks project where I'm trying to plot several node-edge-node values in an interactive graph.\n\nI have several neural networks (this is one example):\n\n    import torch\n    import torch.nn as nn\n    import torch.optim as optim\n    \n    class Model(nn.Module):\n        def __init__(self):\n            super(Model, self).__init__()\n            self.fc1 = nn.Linear(1, 2)\n            self.fc2 = nn.Linear(2, 3)\n            self.fc3 = nn.Linear(3, 1)\n    \n        def forward(self, x):\n            x1 = self.fc1(x)\n            x = torch.relu(x1)\n            x2 = self.fc2(x)\n            x = torch.relu(x2)\n            x3 = self.fc3(x)\n            return x3, x2, x1\n    \n    net = Model()\n\nHow can I get the node-edge-node (neuron-edge-neuron) values in the network in an efficient way? Some of these networks have a large number of parameters. Note that for the first layer it will be input-edge-neuron rather than neuron-edge-neuron.\n\nI tried saving each node values after the fc layers (ie x1,x2,x3) so I won't have to recompute them, but I'm not sure how to do the edges and match them to their corresponding neurons in an efficient way.\n\nThe output I'm looking for is a list of lists of node-edge-node values. For example, in the above network from the first layer I will have 2 triples (1x2), from the 2nd layer I will have 6 of them (2x3), and in the last layer I will have 3 triples (3x1). The issue is matching nodes (ie neurons) values (one from layer n-1 and one from layer n) with the corresponding edges in an efficient way.", "upvote_ratio": 1.0, "id": "t3_qvra5u", "created_utc": 1637124454.0}
{"sub": "pytorch", "title": "New paper out in Chaos, Solitons &amp; Fractals: Forecasting of noisy chaotic systems with deep neural networks Project developed in PyTorch/Keras/Sklearn", "selftext": "nan", "upvote_ratio": 0.81, "id": "t3_qvjg6z", "created_utc": 1637100319.0}
{"sub": "pytorch", "title": "Training time for IWSLT'14 De-En (Transformer)", "selftext": "Hi guys,\n\nI'm an ungrad student working in an NLP lab at my uni. I'm researching about NMT, and was told to replicate the implementation of the **Germany-to-English translation model** of fairseq ([link](https://github.com/pytorch/fairseq/blob/main/examples/translation/README.md#training-a-new-model)).\n\nI'm running it on colab free, with the K80 GPU and very limited memory. I copied and executed the code from the repo, and get about **15 min/epoch**, which made me can never complete training since the session always is timed out before I reach the final epoch. \n\nHowever, my advisor told me that it only takes **2-3 hrs** training on colab, so I want to double check the training time, because I really can't find what could go wrong.\n\nThank you all. Cheers\\~", "upvote_ratio": 1.0, "id": "t3_quiild", "created_utc": 1636988931.0}
{"sub": "pytorch", "title": "How to install cuda version of pytorch with conda on WSL2?", "selftext": "Hi guys,\n\nI'm trying to install pytorch via conda on WSL2. I have done the necessary setup for WSL2 and the latest Nvidia WSL drivers.\n\nHowever when I try to install pytorch via conda as per the usual command\n\n `conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch` \n\nI keep getting the cpu-only version of pytorch.\n\n    pytorch                   1.10.0              py3.9_cpu_0    pytorch\n    pytorch-mutex             1.0                         cpu    pytorch\n    torchaudio                0.10.0                 py39_cpu  [cpuonly]  pytorch\n    torchvision               0.11.1                 py39_cpu  [cpuonly]  pytorch\n\nWould anyone know how to get conda to pull the right packages? Thanks!", "upvote_ratio": 1.0, "id": "t3_qu822q", "created_utc": 1636950591.0}
{"sub": "pytorch", "title": "A rather simple guide to start using PyTorch", "selftext": "PyTorch documentations in my opinion are too long and even scary for beginners.  I decided to take the things I believe are important to make a very simple deep network and write a guide about it:\n\n[https://taying-cheng.medium.com/all-you-need-to-know-about-pytorch-a0ba3af897fa](https://taying-cheng.medium.com/all-you-need-to-know-about-pytorch-a0ba3af897fa)", "upvote_ratio": 0.94, "id": "t3_qtpivr", "created_utc": 1636895469.0}
{"sub": "pytorch", "title": "[Linear Regression]", "selftext": "i tried to build a linear regression model with Pytorch with 1 input and 1 output but why is my loss function so big.  This is my code:\n\n    X_train # torch.Size([100, 1])\n    y_train # torch.Size([100, 1])\n    \n    class LinearRegression(nn.Module):\n        def __init__(self, input_dim, output_dim):\n            super(LinearRegression, self).__init__()\n            self.linear = nn.Linear(input_dim, output_dim)\n    \n        def forward(self, x):\n            out = self.linear(x)\n            return out\n    \n    input_size = X_train.shape[1]\n    output_size = y_train.shape[1]\n    \n    model  = LinearRegression(input_size, output_size)\n    criterion = nn.MSELoss()\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n    \n    num_epochs = 500\n    losses = []\n    for epoch in tqdm(range(num_epochs), desc=\"Training\"):\n        inputs = X_train\n        target = y_train\n    \n        # forward\n        out = model(inputs)\n        loss = criterion(out, target)\n        # backward\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        #losses.append(loss.item())\n    \n        if (epoch+1) % 50 == 0:\n            params = list(model.parameters())\n            W = params[0].item()\n            b = params[1].item()\n            print(f'Epoch[{epoch+1}/{num_epochs}]\\t loss: {loss.item():.4f}\\t W: {W:.4f}\\t b:  {b:.4f}')\n            losses.append(loss.item())\n\nAnd this is my output : \n\n    Epoch[50/500]\t loss: 1019.8797\t W: 1.8125\t b:  22.3726\n    Epoch[100/500]\t loss: 876.2048\t W: 1.8125\t b:  30.3922\n    Epoch[150/500]\t loss: 857.1508\t W: 1.8125\t b:  33.3127\n    Epoch[200/500]\t loss: 854.6238\t W: 1.8125\t b:  34.3763\n    Epoch[250/500]\t loss: 854.2888\t W: 1.8125\t b:  34.7636\n    Epoch[300/500]\t loss: 854.2443\t W: 1.8125\t b:  34.9047\n    Epoch[350/500]\t loss: 854.2385\t W: 1.8125\t b:  34.9560\n    Epoch[400/500]\t loss: 854.2375\t W: 1.8125\t b:  34.9747\n    Epoch[450/500]\t loss: 854.2375\t W: 1.8125\t b:  34.9815\n    Epoch[500/500]\t loss: 854.2375\t W: 1.8125\t b:  34.9840\n\nCan anyone help me ? Thank you", "upvote_ratio": 0.71, "id": "t3_qtdth9", "created_utc": 1636849460.0}
{"sub": "pytorch", "title": "Best scheme to evaluate dataset after training is done", "selftext": "I've built a logistic regression model with using torch (one linear layer and a sigmoid layer) two different weight initializations. Therefore I have 2 models with slight differences and I want to compare them with each other. I want to evaluate whether initializations are causing any large changes in the performance.\n\nSo, I randomly select a chunk of the dataset and leave it out completely. Then, I split the remaining data into training and validation. I train the models using the training data, and I only use the validation data for early stopping. Once the training process stops, I find the models' performance on the left out test set.\n\nNext, I take the previous training and validation data, shuffle them, split them again, and train the models on the new training set and stop training using the new validation set. Then I collect the new trained models' performance on the same previous left out test set.\n\nI repeat this process 5 times, and I compare the average accuracies the two models returned on that test set. The method with higher mean accuracy and low stdev is the better one. I thought this process was called Monte Carlo cross validation. However, somebody told me vaguely this evaluation scheme was wrong - that I shouldn't be evaluating the same test set. There were no further explanations or comments so I cannot be sure whether to change my scheme or continue like this.\n\nIs the above method ideal to compare the two different models? Or should I use a completely different evaluation scheme? Any scheme recommendations or pointers?", "upvote_ratio": 0.92, "id": "t3_qsm2li", "created_utc": 1636754652.0}
{"sub": "pytorch", "title": "How can I define the CUDA path when I use conda virtual environment?", "selftext": "Hello,\n\nI am using PyTorch 1.7 with CUDA 11. However, both PyTorch and CUDA are installed via anaconda. I'm trying to run a GitHub project and I need to build the project. In order to build for CUDA, I need to define the CUDA path. The problem occurs at this point. There is no CUDA outside the conda virtual environment (e.g., /usr/lib/cuda/). It is only available in the conda environment. However, since nvcc is not available in the conda version, the build process throws an error. Is there any suggestion?\n\nProject: [https://github.com/ddshan/hand\\_object\\_detector](https://github.com/ddshan/hand_object_detector)\n\n`/home/something/miniconda3/pkgs/cudatoolkit-11.0.221-h6bb024c_0/bin/nvcc: not found`\n\nI also asked the same question under r/CUDA I hope it is not a problem to ask the same question here.\n\nThank you and best.", "upvote_ratio": 1.0, "id": "t3_qsfrl4", "created_utc": 1636736859.0}
{"sub": "pytorch", "title": "Indirect Feedback Alignment (IFA) implementation", "selftext": "Can anyone please help me in implementing IFA algorithm from the paper:\nhttps://arxiv.org/abs/1609.01596\n\nin this pytorch based repository called BioTorch:\nhttps://github.com/jsalbert/biotorch\n\nI understood the algorithm, but i am having really hard time in implementing the same. \n\nPLEASE HELP", "upvote_ratio": 0.88, "id": "t3_qqr4e3", "created_utc": 1636538859.0}
{"sub": "pytorch", "title": "What is PyTorch Distributed - Distributed Deep Learning Model Training", "selftext": "nan", "upvote_ratio": 0.64, "id": "t3_qqb7y1", "created_utc": 1636486023.0}
{"sub": "pytorch", "title": "How can I add values to a sparse matrix efficiently?", "selftext": "Hello,\n\nI'm trying to create an empty 4D sparse matrix, and add values to it after the creation of the matrix.  I can't seem to find online how to do it, and the obvious way of a\\[(index1, index2, index3, index4)\\] doesn't work. Anyone got a solution for me?", "upvote_ratio": 0.88, "id": "t3_qpo3ny", "created_utc": 1636408055.0}
{"sub": "pytorch", "title": "Mobile real-time GAN model", "selftext": "We have a product we want to move to real-time. It takes a src image and keypoints with a driving keypoints to create a new synthetic image. This is done in Python and it\u2019s almost real-time. Are there good tutorials or approaches for speed optimization on mobile?", "upvote_ratio": 0.88, "id": "t3_qnxvu8", "created_utc": 1636196097.0}
{"sub": "pytorch", "title": "PyTorch version - support for iOS", "selftext": "I am new to PyTorch framework and I have been looking at PyTorch [documentation](https://pytorch.org/mobile/home/). I am trying to find  what iOS and Android versions are officially supported by PyTorch. However, I cannot get a clear picture of the compatibility matrix between PyTorch versions and mobile platforms. \n\nBy digging into PyTorch's GitHub repo release branch, I could see PyTorch 1.9 supports iOS 12.0 as show [here](https://github.com/pytorch/pytorch/blob/orig/release%2F1.9/ios/LibTorch.podspec#L12). Similarly, it supports Android API 28 as shown [here](https://github.com/pytorch/pytorch/blob/v1.9.0/android/build.gradle#L5).  \n\n\nIs my interpretation accurate? Would anyone be able to clarify? Thanks for the help.", "upvote_ratio": 1.0, "id": "t3_qnllo7", "created_utc": 1636149200.0}
{"sub": "pytorch", "title": "PyTorch - batch training of multiple NN models", "selftext": "Does PyTorch have some kind of support for training multiple model instances in parallel?\n\nI want to implement an experiment where there are several agents in a shared environment, each having their own model weights and a shared model architecture, learning independently from their own set of observations.\n\nFor this to work in parallel, I need to process a batch of observations using a batch of agent models. Is there support for this in PyTorch?", "upvote_ratio": 1.0, "id": "t3_qnki6j", "created_utc": 1636145950.0}
{"sub": "pytorch", "title": "Legacy autograd function with non-static forward method is deprecated. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)", "selftext": "I have some problems when i try to run my inference.py. I tried to fix this error following similar topics but it\u2019s still get this error. I tried to change version of pytorch to 1.4. then training but it's not running. This is my code\n\n    from lib import *\n    from l2_norm import L2Norm\n    from default_box import DefBox\n    import torch.nn.functional as F\n    \n    def create_vgg():\n        layers = []\n        in_channels = 3\n    \n        cfgs = [64, 64, 'M', 128, 128, 'M',\n                256, 256, 256, 'MC', 512, 512, 512, 'M',\n                512, 512, 512]\n    \n        for cfg in cfgs:\n            if cfg == 'M':  # floor\n                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n            elif cfg == 'MC':  # ceiling\n                layers += [nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)]\n            else:\n                conv2d = nn.Conv2d(in_channels, cfg, kernel_size=3, padding=1)\n    \n                layers += [conv2d, nn.ReLU(inplace=True)]\n                in_channels = cfg\n    \n        pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n        conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)\n        conv7 = nn.Conv2d(1024, 1024, kernel_size=1)\n        layers += [pool5, conv6, nn.ReLU(inplace=True), conv7, nn.ReLU(inplace=True)]\n    \n        return nn.ModuleList(layers)\n    \n    \n    def create_extras():\n        layers = []\n        in_channels = 1024\n        cfgs = [256, 512, 128, 256, 128, 256, 128, 256]\n    \n        layers += [nn.Conv2d(in_channels, cfgs[0], kernel_size=1)]\n        layers += [nn.Conv2d(cfgs[0], cfgs[1], kernel_size=3, stride=2, padding=1)]\n        layers += [nn.Conv2d(cfgs[1], cfgs[2], kernel_size=1)]\n        layers += [nn.Conv2d(cfgs[2], cfgs[3], kernel_size=3, stride=2, padding=1)]\n        layers += [nn.Conv2d(cfgs[3], cfgs[4], kernel_size=1)]\n        layers += [nn.Conv2d(cfgs[4], cfgs[5], kernel_size=3)]\n        layers += [nn.Conv2d(cfgs[5], cfgs[6], kernel_size=1)]\n        layers += [nn.Conv2d(cfgs[6], cfgs[7], kernel_size=3)]\n    \n        return nn.ModuleList(layers)\n    \n    \n    def create_loc_conf(num_classes=1,bbox_aspect_num=[4, 6, 6, 6, 4, 4]):\n        loc_layers = []\n        conf_layers = []\n    \n        # source1\n        # loc\n        loc_layers += [nn.Conv2d(512, bbox_aspect_num[0] * 4, kernel_size = 3, padding=1)]\n        # conf\n        conf_layers += [nn.Conv2d(512, bbox_aspect_num[0] * num_classes, kernel_size=3, padding=1)]\n    \n        # source2\n        # loc\n        loc_layers += [nn.Conv2d(1024, bbox_aspect_num[1] * 4, kernel_size=3, padding=1)]\n        # conf\n        conf_layers += [nn.Conv2d(1024, bbox_aspect_num[1] * num_classes, kernel_size=3, padding=1)]\n    \n        # source3\n        # loc\n        loc_layers += [nn.Conv2d(512, bbox_aspect_num[2] * 4, kernel_size=3, padding=1)]\n        # conf\n        conf_layers += [nn.Conv2d(512, bbox_aspect_num[2] * num_classes, kernel_size=3, padding=1)]\n    \n        # source4\n        # loc\n        loc_layers += [nn.Conv2d(256, bbox_aspect_num[3] * 4, kernel_size=3, padding=1)]\n        # conf\n        conf_layers += [nn.Conv2d(256, bbox_aspect_num[3] * num_classes, kernel_size=3, padding=1)]\n    \n        # source5\n        # loc\n        loc_layers += [nn.Conv2d(256, bbox_aspect_num[4] * 4, kernel_size=3, padding=1)]\n        # conf\n        conf_layers += [nn.Conv2d(256, bbox_aspect_num[4] * num_classes, kernel_size=3, padding=1)]\n    \n        # source6\n        # loc\n        loc_layers += [nn.Conv2d(256, bbox_aspect_num[5] * 4, kernel_size=3, padding=1)]\n        # conf\n        conf_layers += [nn.Conv2d(256, bbox_aspect_num[5] * num_classes, kernel_size=3, padding=1)]\n    \n        return nn.ModuleList(loc_layers), nn.ModuleList(conf_layers)\n    \n    cfg = {\n        \"num_classes\" : 2, #we only have 1 class: fire\n        \"input_size\" : 300, #SSD 300\n        \"bbox_aspect_num\" : [4, 6, 6, 6, 4, 4], # ty le cho source 1 -&gt; 6\n        \"feature_maps\" : [38, 19, 10, 5, 3, 1],\n        \"steps\" : [8, 16, 32, 64, 100, 300], # Size of default box\n        \"min_size\" : [30, 60, 111, 162, 213, 264],\n        \"max_size\" : [60, 111, 162, 213, 264, 315],\n        \"aspect_ratios\" : [[2], [2, 3], [2, 3], [2, 3], [2], [2]]\n    }\n    \n    class SSD(nn.Module):\n        def __init__(self, phase, cfg):\n            super(SSD, self).__init__()\n            self.phase = phase\n            self.num_classes = cfg['num_classes']\n    \n            # Create main modules\n            self.vgg = create_vgg()\n            self.extras = create_extras()\n            self.loc, self.conf = create_loc_conf(cfg['num_classes'], cfg['bbox_aspect_num'])\n            self.L2Norm = L2Norm()\n    \n            # Create default box\n            dbox = DefBox(cfg)\n            self.dbox_list = dbox.create_defbox()\n    \n            if phase == \"inference\":\n                self.detect = Detect()\n    \n        def forward(self, x):\n            sources = list()\n            loc = list()\n            conf = list()\n    \n            for k in range(23):\n                x = self.vgg[k](x)\n    \n            # source 1\n            source1 = self.L2Norm(x)\n            sources.append(source1)\n    \n            for k in range(23, len(self.vgg)):\n                x = self.vgg[k](x)\n    \n             # source 2\n            sources.append(x)\n    \n            # source 3-6\n            for k, v in enumerate(self.extras):\n                x = F.relu(v(x), inplace = True)\n                if k % 2 == 1:\n                    sources.append(x)\n    \n            for (x, l, c) in zip(sources, self.loc, self.conf):\n                # Data c\u00f3 d\u1ea1ng (batch_size, 4*aspect_ratio_num, featuremap_height, featuremap_width)\n                # aspect_ratio_num = 4, 6, ...\n                # -&gt; (batch_size, featuremap_height, featuremap_width, 4*aspect_ratio_num)\n                loc.append(l(x).permute(0, 2, 3, 1).contiguous())\n                conf.append(c(x).permute(0, 2, 3, 1).contiguous())\n    \n            loc = torch.cat([o.view(o.size(0), -1) for o in loc], 1)            #(batch_size, 34928)\n            conf = torch.cat([o.view(o.size(0), -1) for o in conf], 1)          # (btach_size, 8732)\n    \n            loc = loc.view(loc.size(0), -1, 4)      # (batch_size, 8732, 4)\n            conf = conf.view(conf.size(0), -1, self.num_classes)        # (batch_size, 8732, 2)\n    \n            output = (loc, conf, self.dbox_list)\n    \n            if self.phase == \"inference\":\n                return self.detect(output[0], output[1], output[2])\n            else:\n                return output\n    \n    def decode(loc, defbox_list):\n        '''\n        loc: [8732, 4]               (delta_x, delta_y, delta_w, delta_h)\n        defbox_list: [8732, 4]      (cx_d, cy_d, w_d, h_d)\n        returns: boxes[xmin, ymin, xmax, ymax]\n        '''\n        boxes = torch.cat((defbox_list[:, :2] + loc[:, :2]*defbox_list[:, 2:]),\n        defbox_list[:, 2:] * torch.exp(loc[:, 2:] * 0.2), dim = 1)\n    \n        boxes[:, :2] -= boxes[:, 2:] / 2 # calculate x_min, y_min\n        boxes[:, 2:] += boxes[:, :2] / 2 # calculate x_max, y_max\n    \n        return boxes\n    \n    def nms(boxes, scores, overlap = 0.45, top_k = 200):\n        \"\"\"\n        boxes: [num_box, 4] # c\u00f3 8732 num_box\n        scores: [num_box]\n        \"\"\"\n        count = 0\n        keep = scores.new(scores.size(0)).zero_().long()\n    \n        # boxes:\n        x1 = boxes[:, 0]\n        y1 = boxes[:, 1]\n        x2 = boxes[:, 2]\n        y2 = boxes[:, 3]\n    \n        # boxes area\n        area = torch.mul(x2 - x1, y2 - y1)\n    \n        tmp_x1 = boxes.new()\n        tmp_y1 = boxes.new()\n        tmp_x2 = boxes.new()\n        tmp_y2 = boxes.new()\n        tmp_w = boxes.new()\n        tmp_h = boxes.new()\n    \n        value, idx = scores.sort(0)\n        idx = idx[-top_k:]      # l\u1ea5y 200 c\u00e1i bdbox c\u00f3 \u0111\u1ed9 t\u1ef1 tin cao nh\u1ea5t\n    \n        while idx.numel() &gt; 0:\n            i = idx[-1]         # id c\u1ee7a box c\u00f3 \u0111\u1ed9 t\u1ef1 tin cao nh\u1ea5t\n            keep[count] = i\n            count += 1\n    \n            if id.size(0) == 1:\n                break\n            idx = idx[:, -1]    # id c\u1ee7a c\u00e1c boxes ngo\u1ea1i tr\u1eeb box c\u00f3 \u0111\u1ed9 t\u1ef1 tin cao nh\u1ea5t\n    \n            #information boxes\n            torch.index_select(x1, 0, idx, out = tmp_x1)       # l\u1ea5y ra nh\u1eefng th\u1eb1ng c\u00f3 gt trong kho\u1ea3ng idx trong x1\n            torch.index_select(y1, 0, idx, out = tmp_y1)\n            torch.index_select(x2, 0, idx, out = tmp_x2)\n            torch.index_select(y2, 0, idx, out = tmp_y2)\n    \n            tmp_x1 = torch.clamp(tmp_x1, min = x1[i])           # = x1[i] n\u1ebfu tmp_x1 &lt; x[i]\n            tmp_y1 = torch.clamp(tmp_y1, min = y1[i])\n            tmp_x2 = torch.clamp(tmp_x2, max = x2[i])\n            tmp_y2 = torch.clamp(tmp_y2, max = y2[i])           # = y2[i] n\u1ebfu tmp_y2 &gt; y2[i]\n    \n            # chuy\u1ec3n v\u1ec1 tensor c\u00f3 size sao cho index gi\u1ea3m \u0111i 1\n            tmp_w.resize_as_(tmp_x2)\n            tmp_h.resize_as_(tmp_y2)\n    \n            tmp_w = tmp_x2 - tmp_x1\n            tmp_h = tmp_y2 - tmp_y1\n    \n            tmp_w = torch.clamp(tmp_w, min = 0.0)               # \u0111\u01b0a ph\u1ea7n t\u1eed &lt; 0 v\u1ec1 0\n            tmp_h = torch.clamp(tmp_h, min = 0.0)\n    \n            # dien tich overlap\n            inter = tmp_w * tmp_h                               # dien tich phan trung nhau\n            others_area = torch.index_select(area, 0, idx)      # dien tich cua cac bbox con lai\n            union = area[i] + others_area - inter\n    \n            iou = inter / union\n    \n            idx = idx[iou.le(overlap)]                          # giu cac box co idx nho hon 0.45\n    \n        return keep, count\n    \n    class Detect(Function):\n        def __init__(self, conf_thresh = 0.01, top_k = 200, nms_thresh = 0.45):\n            self.softmax = nn.Softmax(dim = -1)\n            self.conf_thresh = conf_thresh\n            self.top_k = top_k\n            self.nms_thresh = nms_thresh\n    \n        @staticmethod\n        def forward(self, loc_data, conf_data, dbox_list):\n            num_batch = loc_data.size(0)\n            num_dbox = loc_data.size(1)                     # tra ve 8732 dbox\n            num_class = conf_data.size(2)                   # tra ve so class la 1\n    \n            conf_data = self.softmax(conf_data)             # tinh xac suat, dang co dinh dang (btach_num, 8732, 1)\n            conf_preds = conf_data.transpose(2,1)            # thanh (batch_num, num_class, num_dbox)\n    \n            output = torch.zeros(num_batch, num_class, self.top_k, 5)\n            #xu li tung anh trong 1 batch\n            for i in range(num_batch):\n                # tinh bbox tu offset information va default box\n                decode_boxes = decode(loc_data[i], dbox_list)\n    \n                # copy conf score cua anh thu i\n                conf_scores = conf_preds[i].clone()\n    \n                for cl in range(1, num_class):\n                    c_mask = conf_preds[cl].gt(self.conf_thresh)     # chi lay nhung conf &gt; 0.01\n                    scores = conf_preds[cl][c_mask]                  # CH\u1ed6 N\u00c0Y C\u1ea6N XEM L\u1ea0I\n                    if scores.nelement() == 0: # numel\n                        continue\n    \n                    l_mask = c_mask.unsqueeze(1).expand_as(decode_boxes)     # \u0111\u1ec3 \u0111\u01b0a chi\u1ec1u v\u1ec1 gi\u1ed1ng chi\u1ec1u c\u1ee7a decode_box\n    \n                    boxes = decode_boxes[l_mask].view(-1, 4)\n                    ids, count = nms(boxes, scores, self.nms_thresh, self.top_k)\n    \n                    output[i, cl, :count] = torch.cat((scores[ids[:count]].unsqueeze(1), boxes[ids[:count]]), 1)\n    \n            return output\n    \n    \n    if __name__ == \"__main__\":\n    #    vgg = create_vgg()\n    #    print(vgg)\n         extras = create_extras()\n         print(extras)\n         loc, conf = create_loc_conf()\n         print(loc)\n         print(conf)\n    \n         ssd = SSD(phase=\"train\", cfg=cfg)\n         print(ssd)\n    \n\nand this is [inference.py](https://inference.py)\n\n    from lib import *\n    from model import SSD\n    from transform import DataTransform\n    \n    \n    classes = [\"fire\"]\n    \n    cfg = {\n        \"num_classes\": 2, #VOC data include 20 class + 1 background class\n        \"input_size\": 300, #SSD300\n        \"bbox_aspect_num\": [4, 6, 6, 6, 4, 4], # T\u1ef7 l\u1ec7 khung h\u00ecnh cho source1-&gt;source6`\n        \"feature_maps\": [38, 19, 10, 5, 3, 1],\n        \"steps\": [8, 16, 32, 64, 100, 300], # Size of default box\n        \"min_size\": [30, 60, 111, 162, 213, 264], # Size of default box\n        \"max_size\": [60, 111, 162, 213, 264, 315], # Size of default box\n        \"aspect_ratios\": [[2], [2,3], [2,3], [2,3], [2], [2]]\n    }\n    \n    net = SSD(phase=\"inference\", cfg=cfg)\n    net_weights = torch.load(\"./weights/ssd300_20.pth\", map_location={\"cuda:0\":\"cpu\"})\n    net.load_state_dict(net_weights)\n    \n    def predict(img_file_path):\n        img = cv2.imread(img_file_path)\n    \n        color_mean = (8, 17, 32)\n        input_size = 300\n        transform = DataTransform(input_size, color_mean)\n    \n        phase = \"val\"\n        img_tranformed, boxes, labels = transform(img, phase, \"\", \"\")\n        img_tensor = torch.from_numpy(img_tranformed[:,:,(2,1,0)]).permute(2,0,1)\n    \n        net.eval()\n        inputs = img_tensor.unsqueeze(0) #(1, 3, 300, 300)\n        output = net(inputs)\n    \n        plt.figure(figsize=(10, 10))\n        colors = [(255,0,0), (0,255,0), (0,0,255)]\n        font = cv2.FONT_HERSHEY_SIMPLEX\n    \n        detections = output.data #(1, 21, 200, 5) 5: score, cx, cy, w, h\n        scale = torch.Tensor(img.shape[1::-1]).repeat(2)\n    \n        for i in range(detections.size(1)):\n            j = 0\n            while detections[0, i, j, 0] &gt;= 0.6:\n                score = detections[0, i, j, 0]\n                pt = (detections[0, i, j, 1:]*scale).cpu().numpy()\n                cv2.rectangle(img,\n                              (int(pt[0]), int(pt[1])),\n                              (int(pt[2]), int(pt[3])),\n                              colors[i%3], 2\n                              )\n                display_text = \"%s: %.2f\"%(classes[i-1], score)\n                cv2.putText(img, display_text, (int(pt[0]), int(pt[1])),\n                    font, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n                j += 1\n        \n        cv2.imshow(\"Result\", img)\n        cv2.waitKey(0)\n        cv2.destroyAllWindows()\n    \n    \n    if __name__ == \"__main__\":\n        img_file_path = \"/home/huynth/ImageProcessing/result/Median.jpg\"\n        predict(img_file_path)\n\nPlease help me . Thank you !!!", "upvote_ratio": 1.0, "id": "t3_qn6b9o", "created_utc": 1636099839.0}
{"sub": "pytorch", "title": "Complex(?) connected NN possible?", "selftext": "Is it possible to create the Network from the picture below in pytorch, and if yes how do I do it.\n\nI  know that I can omit connections by using a sparse matrix for the  weights, but can I create a connection / a weight that skips a layer?\n\n&amp;#x200B;\n\nhttps://preview.redd.it/6zb4g9ix8lx71.png?width=640&amp;format=png&amp;auto=webp&amp;s=c3785a6e41b40bec3b3097a3d7b5912ebd25f2c3", "upvote_ratio": 1.0, "id": "t3_qmlqrp", "created_utc": 1636035922.0}
{"sub": "pytorch", "title": "Easy Multi-Node PyTorch Lightning Training", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_qm2xcs", "created_utc": 1635969872.0}
{"sub": "pytorch", "title": "iris - Open Source Photos Platform powered by PyTorch", "selftext": "This is my submission for PyTorch Annual Hackathon 2021!\n\nYouTube: [https://www.youtube.com/watch?v=ZMG2rohochc](https://www.youtube.com/watch?v=ZMG2rohochc)\n\nDevPost: [https://devpost.com/software/iris-7s3yna](https://devpost.com/software/iris-7s3yna)\n\nGitHub: [https://github.com/prabhuomkar/iris](https://github.com/prabhuomkar/iris)", "upvote_ratio": 1.0, "id": "t3_qlzy1g", "created_utc": 1635961619.0}
{"sub": "pytorch", "title": "What softwares/applications do I need to train an audio classifier model in Windows using Python and Pytorch?", "selftext": "I'm new with all these, so please let me know how to get started with the softwares.", "upvote_ratio": 0.57, "id": "t3_qlosjg", "created_utc": 1635922283.0}
{"sub": "pytorch", "title": "Help with custom forward function involving nn.Conv2d()", "selftext": "I have an issue with [custom forward function involving nn.Conv2d()](https://github.com/promach/gdas/blob/main/gdas.py#L119)\n\nI found that whenever `e` is 0 , [combined\\_feature\\_map](https://github.com/promach/gdas/blob/main/gdas.py#L375) will also become 0.  Why ?\n\nNote: When [e is 0](https://github.com/promach/gdas/blob/main/gdas.py#L174), it actually uses `ConvEdge` class\n\nhttps://preview.redd.it/fz24qg69hax71.png?width=1922&amp;format=png&amp;auto=webp&amp;s=c3a6b215e1bba8464772d833ad2fbc9b8e161873", "upvote_ratio": 1.0, "id": "t3_qlkge0", "created_utc": 1635905836.0}
{"sub": "pytorch", "title": "How to install PyTorch using m1 max macbook pro?", "selftext": "&amp;#x200B;\n\n&amp;#x200B;\n\nrelatedn refs:\n\n\\- [https://github.com/pytorch/pytorch/issues/47702](https://github.com/pytorch/pytorch/issues/47702)\n\n\\- [https://towardsdatascience.com/yes-you-can-run-pytorch-natively-on-m1-macbooks-and-heres-how-35d2eaa07a83](https://towardsdatascience.com/yes-you-can-run-pytorch-natively-on-m1-macbooks-and-heres-how-35d2eaa07a83)\n\n\\- [https://www.quora.com/unanswered/How-does-one-use-PyTorch-with-the-MacBook-pro-M1-Max-or-M1-Pro-for-Deep-Learning](https://www.quora.com/unanswered/How-does-one-use-PyTorch-with-the-MacBook-pro-M1-Max-or-M1-Pro-for-Deep-Learning)", "upvote_ratio": 0.75, "id": "t3_qkmvpr", "created_utc": 1635798200.0}
{"sub": "pytorch", "title": "Feature Extraction in TorchVision using Torch FX (official pytorch blog)", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_qki1q1", "created_utc": 1635785058.0}
{"sub": "pytorch", "title": "Accelerating PyTorch with CUDA Graphs (official pytorch blog)", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_qki14b", "created_utc": 1635785015.0}
{"sub": "pytorch", "title": "Can I SLI load big (&gt;11gb) models?", "selftext": "New BERT/pytorch user here, dreaming of utilizing larger models. \n\nI have access to the hardware, but before I embark on the journey of putting it all together I need to know, is such a thing possible?\n\nIf not what's the point of these mammoth 4x GPU workstations I see online? Staggered training cycles? Something else?\n\nThank you for your insight, and consideration.", "upvote_ratio": 1.0, "id": "t3_qk25dw", "created_utc": 1635725508.0}
{"sub": "pytorch", "title": "Stability of updating shared layers in two step multi-head update?", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_qjjoft", "created_utc": 1635661641.0}
{"sub": "pytorch", "title": "How to handle dimension changes between layers", "selftext": "Let's say I have an embedding layer, followed by a linear layer, and I use sigmoid at the end to convert logits to classes.\n\n    class SimpleClassifier(nn.Module):\n    \n        def __init__(self, vocab_size, num_labels, embedding_dim = 32):\n            \n            super(SimpleClassifier, self).__init__()\n            \n            self.embedding = nn.Embedding(vocab_size, embedding_dim)\n            self.linear = nn.Linear(embedding_dim, num_labels)\n            \n        def forward(self, x):\n    \n            embed = self.embedding(x)\n            out = self.linear(embed)\n            return torch.sigmoid(out)\n\nx has the size: \\[batch size, num of words\\]\n\nafter passing x through the embedding layer, embed has the size \\[batch size,  num of words, embedding\\_dim\\]\n\npassing embed through the linear layer, out has the size \\[batch size,  num of words, num\\_labels\\]\n\nWhat confuses me is: Wasn't the num of words supposed to be replaced by the embedding\\_dim instead? I would expect embed to have the size \\[batch size, embedding\\_dim\\] instead. The online tutorials which are using the same type of input x are using the above code framework without doing any additional dimension removal or reshaping, and getting the typical 2D results they want. What seems to be wrong here in the above code?", "upvote_ratio": 1.0, "id": "t3_qj1s0y", "created_utc": 1635601465.0}
{"sub": "pytorch", "title": "PyTorch Tutor", "selftext": "Looking for a PyTorch SME to teach and advise me.", "upvote_ratio": 0.6, "id": "t3_qizh3h", "created_utc": 1635592872.0}
{"sub": "pytorch", "title": "Need help with examples of errors", "selftext": "Hello, I\u2019m working on my project related to Pytorch debugging. Could someone help me with examples of problems for the following issues? Thanks so much \ud83d\ude4f\n\nHere are the issues:\n\nAttempting to construct a nn.Module inside TorchScript. This currently errors out because TorchScript would attempt to compile __init__() method of module, which usually contains a call to super(), which isn't supported. Instead, TS should really recognize that a call to constructor of nn.Module is the real problem.\n\nCalling into known torch.* components that are not scriptable. For example, torch.distributions currently is not scriptable. If TS sees a call into torch.distributions methods, it should warn users about it and prompt them to use jit.trace instead.\n\nRegistering new buffers. This isn't supported because it is effectively modifying type of nn.Module. We should also give a better error message here.", "upvote_ratio": 1.0, "id": "t3_qitvff", "created_utc": 1635567825.0}
{"sub": "pytorch", "title": "Ideas for avoiding overfitting in simple logistic regression", "selftext": "I have a simple logistic regression equivalent classifier (I got it from online tutorials):\n\n&amp;#x200B;\n\n    class MyClassifier(nn.Module):\n    \n        def __init__(self, num_labels, vocab_size):\n    \n            super(MyClassifier, self).__init__()\n            self.num_labels = num_labels\n            self.linear = nn.Linear(vocab_size, num_labels)\n    \n        def forward(self, input_):\n            \n            return F.log_softmax(self.linear(input_), dim=1)\n\nSingle there is only one layer, using dropout is not one of the options to reduce overfitting. My parameters and the loss/optimization functions are:\n\n    learning_rate = 0.01\n    num_epochs = 5\n    \n    criterion = nn.CrossEntropyLoss(weight = class_weights)\n    optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n\nI need to mention that my training data is imbalanced, that's why I'm using class\\_weights.\n\nMy training epochs are returning me these (I compute validation performance at every epoch as for the tradition):\n\n&amp;#x200B;\n\n    Total Number of parameters:  98128\n    Epoch 1\n    train_loss : 8.941093041900183 val_loss : 9.984430663749626\n    train_accuracy : 0.6076273690389963 val_accuracy : 0.6575908660222202\n    ==================================================\n    Epoch 2\n    train_loss : 8.115481783001984 val_loss : 11.780701822734605\n    train_accuracy : 0.6991507896001001 val_accuracy : 0.6662275931342518\n    ==================================================\n    Epoch 3\n    train_loss : 8.045773667609911 val_loss : 13.179592760197878\n    train_accuracy : 0.7191923984562909 val_accuracy : 0.6701144928772814\n    ==================================================\n    Epoch 4\n    train_loss : 8.059769958938631 val_loss : 14.473802320314771\n    train_accuracy : 0.731468294135531 val_accuracy : 0.6711249543086926\n    ==================================================\n    Epoch 5\n    train_loss : 8.015543553590438 val_loss : 15.829670974340084\n    train_accuracy : 0.7383795859902959 val_accuracy : 0.6727273308589589\n    ==================================================\n\nPlots are:\n\n&amp;#x200B;\n\nhttps://preview.redd.it/1z09912otew71.png?width=1159&amp;format=png&amp;auto=webp&amp;s=d2a37e322198e7c2337c00210dd990e9994867d4\n\nThe validation loss tells me that we're overfitting, right? How can I prevent that from happening, so I can trust the actual classification results this trained model returns me?", "upvote_ratio": 1.0, "id": "t3_qifpeo", "created_utc": 1635522245.0}
{"sub": "pytorch", "title": "Custom lib or pytorch build with la pack", "selftext": "Hey guys, \n\n&amp;#x200B;\n\nI am trying to transfer a model from python to ios. To do this I used jit and it actually seems to work very easily. The only part was the model fails looking for the LA package built in. I am not sure if this is built into the jit package or built into the libtorch package to compile for ios.   \n\n\nCan these builds be done on M1 or regular macs? I am not totally sure the process for adding the LApack to the build so it is included. Nor am I sure weather original pytorch needs lapack for the jit compile or if it's libtorch. Any insights or step by steps would be very helpful as I haven't done a custom build before so I am not sure what I am doing wrong.", "upvote_ratio": 1.0, "id": "t3_qiato5", "created_utc": 1635507388.0}
{"sub": "pytorch", "title": "Best textbooks for learning Pytorch and NLP", "selftext": "I am moving towards Pytorch from Keras and I like it a lot! Does anyone have any recommendations for textbooks that do a good job of covering NLP in pytorch?", "upvote_ratio": 0.81, "id": "t3_qi4weu", "created_utc": 1635482626.0}
{"sub": "pytorch", "title": "How to debug dimension errors? Pytorch newb here", "selftext": "Hello, I\u2019m new to pytorch. I tried to implement a basic GAN and was running into errors like \u201cstack expects each tensor to be equal size, but got [3, 784, 812] at entry 0 and [3, 960, 784] at entry 1\u201d\n\nIt seems like these are dimension errors as data passes through my layers. Does anyone know how to debug stuff like this? Just print out tensor shapes?\n\n  \n\n\n` ` class Discriminator(nn.Module):\n\u00a0\u00a0\u00a0\u00a0def __init__(self, img_dim):\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0super().__init__()\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.disc = nn.Sequential(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nn.Linear(img_dim, 256),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nn.LeakyReLU(0.2),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nn.Linear(256, 128),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nn.LeakyReLU(0.2),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nn.Linear(128, 64),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nn.LeakyReLU(0.2),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nn.Linear(64, 1),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nn.Sigmoid()\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0)\n\n\u00a0\u00a0\u00a0\u00a0def forward(self, x):\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0return self.disc(x)\n\n\nclass Generator(nn.Module):\n\u00a0\u00a0\u00a0\u00a0def __init__(self, z_dim, img_dim):\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0super().__init__()\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self.gen = nn.Sequential(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nn.Linear(z_dim, 64),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nn.LeakyReLU(0.2),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nn.Linear(64, 128),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nn.LeakyReLU(0.2),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nn.Linear(128, 256),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nn.LeakyReLU(0.2),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nn.Linear(256, img_dim),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0nn.Tanh()\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0)\n\n\u00a0\u00a0\u00a0\u00a0def forward(self, x):\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0return self.gen(x)\n\n` `", "upvote_ratio": 1.0, "id": "t3_qheck7", "created_utc": 1635393322.0}
{"sub": "pytorch", "title": "Getting Latent Values in CNN Autoencoder", "selftext": "Hi all. I've created a CNN Autoencoder in the form of a class as such (I wanted to make it as flexible as possible so I can pass all sorts of configurations to it):\n\n```\nimport sys\nimport os\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\n\nsys.path.append(os.path.join(os.path.dirname(__file__), '.'))\nfrom abstract_dataset import AbstractDataset\n\nclass CnnAutoencoder(nn.Module):\n    def __init__(self, scale=2, channel_maps=[], padding=1, kernel_size=3, num_channels=3, img_width=500, img_height=500, device=torch.device(\"cpu\"), criterion=nn.BCELoss(), h_activation=\"relu\", o_activation=\"sigmoid\"):\n        super().__init__()\n\n        self.scale          = scale\n        self.channel_maps   = channel_maps\n        self.padding        = padding\n        self.kernel_size    = kernel_size\n        self.num_channels   = num_channels\n        self.img_width      = img_width\n        self.img_height     = img_height\n        self.device         = device\n        self.criterion      = criterion\n\n        self.h_activation = h_activation\n        self.o_activation = o_activation\n\n        self.reversed_channel_maps = list(reversed(channel_maps))\n\n        # Build convolutional layers\n        self.convolutional_layers = nn.ModuleList([])\n\n        for i in range(len(self.channel_maps) - 1):\n            self.convolutional_layers.append(\n                nn.Conv2d(\n                    self.channel_maps[i], \n                    self.channel_maps[i+1], \n                    kernel_size=self.kernel_size, \n                    padding=self.padding\n                )\n            )\n\n        # Build deconvolutional layers\n        self.deconvolutional_layers = nn.ModuleList([])\n\n        for i in range(len(self.reversed_channel_maps) - 1):\n            self.deconvolutional_layers.append(\n                nn.ConvTranspose2d(\n                    self.reversed_channel_maps[i], \n                    self.reversed_channel_maps[i+1], \n                    kernel_size=self.kernel_size, \n                    padding=self.padding\n                )\n            )\n\n        self.pool = nn.MaxPool2d(2, 2)\n\n        self.criterion = criterion\n\n        self.errs = []\n\n        # Initialize model to device\n        self.to(self.device)\n\n    def conv(self, x):\n        for i in range(len(self.convolutional_layers)):\n            conv_layer = self.convolutional_layers[i]\n\n            if self.h_activation == \"relu\":\n                x = F.relu(conv_layer(x))\n            else:\n                raise Exception(\"Invalid hidden activation {}\".format(self.h_activation))\n\n            x = self.pool(x)\n\n        return x\n\n    def deconv(self, x):\n        for i in range(len(self.deconvolutional_layers)):\n            deconv_layer = self.deconvolutional_layers[i]\n            x = F.interpolate(x, scale_factor=self.scale, mode='nearest')\n            x = deconv_layer(x)\n\n            if i != len(self.deconvolutional_layers) - 1:\n                if self.h_activation == \"relu\":\n                    x = F.relu(x)\n                else:\n                    raise Exception(\"Invalid hidden activation {}\".format(self.h_activation)) \n            else:\n                if self.o_activation == \"sigmoid\":\n                    x = torch.sigmoid(x)\n                else:\n                    raise Exception(\"Invalid output activation {}\".format(self.o_activation))\n\n        return x\n\n    def encode(self, x):\n        x = self.conv(x)\n        x = x.view(x.size()[0], -1)\n\n        return x\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.deconv(x)\n\n        return x\n\n    def errors(self, x):\n        x_hat = self.forward(x)\n\n        self.criterion.reduction = 'none'\n\n        dim = self.num_channels * self.img_width * self.img_height\n        err = self.criterion(x_hat.view(-1, dim), x_hat.view(-1, dim)).mean(axis=1)\n\n        self.criterion.reduction = 'mean'\n\n        return err.detach().cpu().numpy()\n\n    def save(self, filename):\n        state = {\n            'params': {\n                'o_activation':   self.o_activation,\n                'h_activation':   self.h_activation,\n                'channel_maps':   self.channel_maps,\n                'device':         self.device,\n                'scale':          self.scale,\n                'padding':        self.padding,\n                'kernel_size':    self.kernel_size,\n                'num_channels':   self.num_channels,\n                'img_width':      self.img_width,\n                'img_height':     self.img_height\n            },\n            'state_dict': self.state_dict(),\n            'optimizer':  self.optimizer.state_dict()\n        }\n\n        torch.save(state, filename)\n\n    def load(self, filename):\n        state = torch.load(filename)\n\n        self.load_state_dict(state['state_dict'])\n\n        self.optimizer  = state['optimizer']\n\n        # other parameters\n        params = state['params']\n\n        self.o_activation   = params['o_activation']\n        self.h_activation   = params['h_activation']\n        self.channel_maps   = params['channel_maps']\n        self.device         = params['device']\n        self.scale          = params['scale']\n        self.padding        = params['padding']\n        self.kernel_size    = params['kernel_size']\n        self.num_channels   = params['num_channels']\n        self.img_width      = params['img_width']\n        self.img_height     = params['img_height']\n\n    def fit(self, x, epochs=100, lr=0.001, batch_size=5, optimizer_type=\"adam\"):\n        # Reset errors to empty list\n        self.errs = []\n\n        data        = AbstractDataset(x)\n        dataloader  = DataLoader(dataset=data, batch_size=batch_size, shuffle=True, drop_last=False)\n\n        if optimizer_type == \"adam\":\n            self.optimizer = optim.Adam(self.parameters(), lr=lr)\n        else:\n            raise Exception(\"Invalid optimizer_type: {}\".format(optimizer_type))\n\n        num_iterations = len(x) / batch_size\n\n        for epoch in range(epochs):\n            curr_loss = 0\n\n            for i, (inputs, labels) in enumerate(dataloader):\n                inputs, labels = inputs.to(self.device), labels.to(self.device)\n                self.optimizer.zero_grad()\n\n                output = self.forward(inputs)\n\n                loss = self.criterion(output, labels)\n\n                curr_loss += loss\n                loss.backward()\n                self.optimizer.step()\n\n            curr_loss = curr_loss / num_iterations\n\n            print(\"Epoch: %i\\tLoss: %0.5f\" % (epoch + 1, curr_loss.item()))\n\n            self.errs.append(curr_loss.detach())\n```\n\nMy question is, since I don't use a LinearLayer after convolution, if I wanted to get a vector of features in a latent layer, I would have to perform the `encode` method:\n\n```\ndef encode(self, x):\n        x = self.conv(x)\n        x = x.view(x.size()[0], -1)\n\n        return x\n```\n\nIt seems to be churning out correct values but I can't validate if it really is the correct ones. I just saw the code for `x.view(x.size()[0], -1)` somewhere in the forum but not sure what it does exactly. Or is this the proper way to extract latent layer values off of a CNN Autoencoder? \n\nThanks", "upvote_ratio": 1.0, "id": "t3_qei3ry", "created_utc": 1635038822.0}
{"sub": "pytorch", "title": "Receiving strange results for different batch sizes", "selftext": "I've implemented a simple logistic regression using pytorch as follows:\n\n    class MyClassifier(nn.Module):\n    \n        def __init__(self, num_labels, feat_size):\n    \n            super(MyClassifier, self).__init__()\n            self.linear = nn.Linear(feat_size, num_labels)\n    \n        def forward(self, input_):\n            \n            output = self.linear(input_)\n            return output\n\nwhere my data is quite imbalanced. I didn't add a softmax layer to the end because I use the following:\n\n    criterion = nn.CrossEntropyLoss(weight = class_weights)\n    optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n\nPeople were saying that CrossEntropyLoss would handle the softmax and the class weights I provide it should handle the imbalance during training. To compute the accuracy score, I use balanced\\_accuracy of sklearn API. Under these circumstances, when my batch\\_size is 128, I get the following results:\n\n    Epoch 1\n    train_loss : 6.7878759426243205 val_loss : 7.433685937243139\n    train_accuracy : 0.4824034467755402 val_accuracy : 0.5251206026233478\n    ==================================================\n    Epoch 2\n    train_loss : 5.99113222517245 val_loss : 8.959181152985904\n    train_accuracy : 0.5581671699259839 val_accuracy : 0.5300979308675885\n    ==================================================\n    Epoch 3\n    train_loss : 5.8957389833456055 val_loss : 10.008684014987372\n    train_accuracy : 0.5725333115680177 val_accuracy : 0.5301676217747785\n    ==================================================\n    Epoch 4\n    train_loss : 5.817642052350575 val_loss : 11.025033701397852\n    train_accuracy : 0.5853522987236132 val_accuracy : 0.5307039362469957\n    ==================================================\n    Epoch 5\n    train_loss : 5.831438742645287 val_loss : 11.798206594042883\n    train_accuracy : 0.5896608333837227 val_accuracy : 0.5351319280522936\n    ==================================================\n\n&amp;#x200B;\n\nhttps://preview.redd.it/xfzf0lle2av71.png?width=1169&amp;format=png&amp;auto=webp&amp;s=cf93d2509118c5ee59b54a37ff2b277f7108bc47\n\nAnd when I run the trained model on the validation data after training is done, I get 0.685 balanced\\_accuracy and confusion matrix looks like this:\n\n&amp;#x200B;\n\nhttps://preview.redd.it/jz0xw7en2av71.png?width=820&amp;format=png&amp;auto=webp&amp;s=dc120a7cb676bcf4d58e3fd8d2522d1b05e8ecbf\n\nWhen I decrease the batch\\_size to 72 so my ordinary laptop can handle this process faster on CPU (M1 chips aren't compatible with GPU support of torch unfortunately):\n\n    Epoch 1\n    train_loss : 8.682287086720864 val_loss : 9.941184375842502\n    train_accuracy : 0.8459006809094946 val_accuracy : 0.912647343814439\n    ==================================================\n    Epoch 2\n    train_loss : 8.064770712704517 val_loss : 11.877306490363774\n    train_accuracy : 0.9746633731419807 val_accuracy : 0.9292273581396854\n    ==================================================\n    Epoch 3\n    train_loss : 8.081744464227276 val_loss : 13.33639626492239\n    train_accuracy : 0.9971578027592517 val_accuracy : 0.9340779315955289\n    ==================================================\n    Epoch 4\n    train_loss : 8.06064057003796 val_loss : 14.592282639012128\n    train_accuracy : 1.0152058255637395 val_accuracy : 0.9428616279166708\n    ==================================================\n    Epoch 5\n    train_loss : 7.995261219761772 val_loss : 15.561377628814542\n    train_accuracy : 1.0273957155370776 val_accuracy : 0.9351828058506229\n    ==================================================\n\nhttps://preview.redd.it/vrth9l9ow9v71.png?width=1178&amp;format=png&amp;auto=webp&amp;s=1f1b9115df1c60ef6bc97a63be3ea0456da713f9\n\nWhen I run this one on the validation set after training is over, I get a very similar balanced accuracy (also 0.68) and a similar (not the same but very close) confusion matrix.\n\nThe question is, how come there is such a huge gap between the balanced accuracy results I get during the training epochs for two different batch sizes but similar results after training is done? Also, why do the loss values of validation and training look so off? The results are way too off compared to the reasonable balanced accuracy values I get after training is done, regardless of the batch size. I'm having a hard time explaining or understanding these results and will appreciate any guidance.", "upvote_ratio": 1.0, "id": "t3_qefee1", "created_utc": 1635029291.0}
{"sub": "pytorch", "title": "Introducing PyTorch-DirectML: Train your machine learning models on any GPU", "selftext": "nan", "upvote_ratio": 0.97, "id": "t3_qe4als", "created_utc": 1634993324.0}
{"sub": "pytorch", "title": "Traditional Machine Learning Models on GPU", "selftext": "I've just published a new major revision of a library I've been working on, [PyCave](https://github.com/borchero/pycave).\n\nIt implements K-Means, GMMs and Markov chains using PyTorch and PyTorch Lightning, thus, enabling to train these models on GPUs and multiple nodes (and even TPUs).\n\nAt the same time, the interface of the library is fully compatible with scikit-learn, making it a drop-in replacement.\n\nFor example, you can run the following, to train a GMM on a GPU, gaining a \\~100x speedup over scikit-learn.\n\n    from pycave.bayes import GaussianMixture\n    \n    gmm = GaussianMixture(4, covariance_type=\"diag\", trainer_params=dict(gpus=1))\n    gmm.fit(data)\n\nI'd be delighted if any of you can make use of this work! :)\n\nOf course, I also highly value any feedback ;)", "upvote_ratio": 1.0, "id": "t3_qdm98s", "created_utc": 1634924857.0}
{"sub": "pytorch", "title": "PyTorch Python to mobile", "selftext": "So I need to move some models and modules from Python to objective c to run real-time on a phone: I\u2019ve got the models given to me in pt format which loads a dictionary of the states dictionaries. Just wondering if I am doing this right. I converted the PyTorch save to PyTorch light and did a hit load. But I am not seeing the dictionary like in Python. In Python I just torch.load and can access the dictionary of the generator. What might I be missing?", "upvote_ratio": 0.75, "id": "t3_qdm5e4", "created_utc": 1634924543.0}
{"sub": "pytorch", "title": "Validate models after storing the training model of each epoch?", "selftext": " Can we first store the training model state dict of each epoch, then validate models and plot validation loss curve by importing each epoch state dict? If yes, is this way recommended?\n\nThe issue is that we are training some model variants and we don\u2019t want to waste time on validation until we figure out the best model.", "upvote_ratio": 1.0, "id": "t3_qdd07x", "created_utc": 1634894017.0}
{"sub": "pytorch", "title": "How can I create a loss function that will push the actual NN weights to move?", "selftext": "I'm trying to create a loss function that takes the actual weight values into account:\n\n    loss = 1 - torch.mean(torch.tensor([torch.sum(w_arr) for w_arr in net.parameters()])) \n\nWhere \"net\" is just a simple FC network with any number of layers.\n\nBut I'm getting an error:\n\n    RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn \n\nThe goal here is to get each weight's value closest to 1 (or any other value) as possible.", "upvote_ratio": 1.0, "id": "t3_qd94ww", "created_utc": 1634876428.0}
{"sub": "pytorch", "title": "Text recognition and creation", "selftext": "I'm wondering if there are existing algorithms to perform text recognition based on an existing catalog and a means to feed the algorithm text and have it generate dialogue based on the training data?\n\nIn my example I have dialogue for characters in a movie or TV show. I have training sets for each character, but can't figure out how to train my set to recognize dialogue or classify dialogue to a specific character. I then want to feed an input of dialogue and say return the same sentence as if a particular person spoke it. \n\nDoes this exist in some form?", "upvote_ratio": 1.0, "id": "t3_qd4wr1", "created_utc": 1634861356.0}
{"sub": "pytorch", "title": "PyTorch 1.10 Release, including CUDA Graphs APIs, Frontend and Compiler Improvements", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_qcypb3", "created_utc": 1634842466.0}
{"sub": "pytorch", "title": "Am I understanding CUDA Streams and Events correctly?", "selftext": "I want to exploit some parallelism in the custom model I've built and submit asynchronous kernels to CUDA which execute in parallel.\n\nSay I have a ModuleList full of operations which are fully independent of each other but which require the same input. The results of those operations are later collated into a single object to be passed on in the forward pass.\n\nBelow is a minimally illustrative example of how I *think* streams and events are used.\n\n    from torch.cuda import Stream, stream, Event\n    from torch import empty\n    def func(x, module_list):\n        out = empty(len(module_list), \n                    device=module_list[0].device)\n        event = Event()\n        for i, module in enumerate(module_list):\n            s = Stream()\n            s.wait_event(event)\n            with stream(s):\n                out[i] = module(x)  # this is not executed yet because of the wait_event() call\n        event.record()  # all jobs begin\n        # work actually happens here\n        event.synchronize()  # wait until all jobs end\n        return out  # out is guaranteed to be filled with all results from above\n\nMy understanding here is that the streams which are waiting on an event which has not been recorded will submit their work to the queue but will not run yet. Only once `event.record()` is called are those queued jobs executed. Then `event.synchronize()` tells the parent stream to wait until all those jobs are completed.\n\nWill `func()` do what I want?", "upvote_ratio": 1.0, "id": "t3_qcxkvp", "created_utc": 1634839249.0}
{"sub": "pytorch", "title": "Example models", "selftext": "I am wondering if there is a resource for sample models in pytorch. Basically showing different net architectures and how to use various layers. I am hoping for some more complex ones.\n\nThanks,", "upvote_ratio": 1.0, "id": "t3_qcvk5d", "created_utc": 1634833506.0}
{"sub": "pytorch", "title": "Looking out for Under 18 PyTorch Developers", "selftext": "I am Devvrat from FleishmanHillard, we are one of the communication partners of Facebook.\n\nTogether, we are working on a program in India for which we are identifying young developers under the age of 18 who have worked on PyTorch. If you are/know someone who is under the age of 18, in India and has worked on PyTorch, please recommend or do get in touch.  \n You can contact me at devvrat.more@fleishman.com ", "upvote_ratio": 0.27, "id": "t3_qcmcnb", "created_utc": 1634801477.0}
{"sub": "pytorch", "title": "PyTorch System Requirements", "selftext": "I've looked around a fair bit and I don't see any solid documentation on what the system requirements are to run PyTorch. I'm mainly interested in the amount of RAM it needs to run. I'm speculating that it is at least 8GB of RAM. Any input or links are appreciated. Thank you.", "upvote_ratio": 1.0, "id": "t3_qcdaz6", "created_utc": 1634770579.0}
{"sub": "pytorch", "title": "Vision Transformers in PyTorch", "selftext": "This is a simple explanation of vision transformers and how to incorporate them into your programs via PyTorch:\n\n[https://taying-cheng.medium.com/vision-transformers-in-pytorch-43d13cb7ec7a](https://taying-cheng.medium.com/vision-transformers-in-pytorch-43d13cb7ec7a)", "upvote_ratio": 1.0, "id": "t3_qcd0za", "created_utc": 1634769738.0}
{"sub": "pytorch", "title": "GDAS paper : Searching for A Robust Neural Architecture in Four GPU Hours", "selftext": "nan", "upvote_ratio": 0.8, "id": "t3_qbzik6", "created_utc": 1634732297.0}
{"sub": "pytorch", "title": "Derivatives in the loss function and losing the graph.", "selftext": "Hello all,\n\nI'm a bit of a beginner at Machine Learning and PyTorch and have run into an issue that I have not been able to solve. \n\n## Background\nThis is in the area of solving Bellman Equations from dynamic programming, although I don't think much much background is needed in that, based on the problem I am having.\n\nI am currently trying to use pytorch to approximate a choice/policy function that is implicitly defined by a system of functions, and the derivatives of those functions. \n\n\n## Problem Description\n\nWhen I try to construct a loss function, I lose the `requires_grad` attribute that I need for backprop/training.\n\n\nIn (what might be excessive) detail:\n\nBasically there are 4 fundamental (tensor) functions I need to work with.\n\n - Policy Function: states -&gt; choices\n - Transition Function, G: states, choices -&gt; states\n - Benefit function P: states, choices -&gt; a measure of benefit\n\nI need to use derivatives of the transition G() and benefit P() functions to define the residuals of the model. \n\nWhen I try to take the derivative of P() or G() using `torch.autograd.functional.jacobian` (or `torch.autograd.grad`), even when using `create_graph=True`, I will often lose the computational graph. \n\n## Code example\n\n```python\nimport torch\n\n#Constants\nPAYOFFS = torch.tensor([2.0,3], requires_grad=True)\n\n#states\nstates = torch.tensor([1.0,-2], requires_grad=True)\n\n#NN I am using to approximate the choice function\nchoices = torch.nn.Linear(2,1)\n\n#The benefit function\n\u200bdef P(states, chosen):\n    return PAYOFFS @ states - chosen\n\noutput = choices.forward(states)\n#Returns: tensor([-1.3031], grad_fn=&lt;AddBackward0&gt;)\n\np2 = profit(states, output)\n#Returns: tensor([-2.6969], grad_fn=&lt;SubBackward0&gt;)\n\n#The jacobian gives me what I need, but without the graph\ntorch.autograd.functional.jacobian(profit, (states, output), create_graph=True)\n#Returns: (tensor([[2., 3.]], grad_fn=&lt;ViewBackward&gt;), tensor([[-1.]]))\n```\nNotice how that for the second tensor `tensor([[-1.]]).requires_grad == False`.\nOne of the later conditions included in the loss function is that derivative (for notations sake, `DoP = dP/dOutput`)\n\n```python\nresiduals = DoP - F(states,ouptut)\n```\n\n\n## Various Questions\nThese are some of the questions that I've had while trying to fix this issue.\n\n - Why is the computational graph getting lost in XXX?\n    - Are there good resources that discuss why this is happening?\n - Am I approaching building these derivatives in the wrong way?\n    - I suspect so, I mean facebook uses pytorch.\n - Should I be using some other module for this type of issue?\n    - I've heard about the module `higher` in connection to other issues with derivatives and pytorch, but have not found enough description of what it does to identify if I need to be using it, or even how to.\n\nAnyway, thank you in advance for any help, suggestions, or direction to references I should read.", "upvote_ratio": 1.0, "id": "t3_qbqlde", "created_utc": 1634695695.0}
{"sub": "pytorch", "title": "Question for medical Pytorch", "selftext": "Hello everyone ,\nI am studying pytorch the last month ,\n Also, I am well skilled in frameworks as tf , keras .But I can\u2019t handle the oop setup of Pytorch. How much could it take to get casual?\nAnyone know a blog or YouTube channel for medical pytorch setup for a kickstart?", "upvote_ratio": 0.67, "id": "t3_qaudlc", "created_utc": 1634586305.0}
{"sub": "pytorch", "title": "Making Transfer Learning work right in plain PyTorch", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_qaitvg", "created_utc": 1634547611.0}
{"sub": "pytorch", "title": "Source to learn pytorch?", "selftext": "I used pytorch for a year but am still very unfamiliar with some functions in the library.\n\nIs there any learning resource that can let me walk through each main function with example, say, in the Jupyter notebook?", "upvote_ratio": 1.0, "id": "t3_qaguzk", "created_utc": 1634537879.0}
{"sub": "pytorch", "title": "Handling class imbalance", "selftext": "I've been looking at examples online to see what options I have in the case of a multiclass classification of imbalanced sets. I've read somewhere that I can either provide the class weights to the loss function (CrossEntropyLoss they were recommending) so during the training process, the imbalance is handled, or I can oversample using the WeightedRandomSampler. They were further saying that either option is equivalent to each other.\n\nIs that true? Can I just provide the class weights of training to the loss function and go ahead with that? Any other suggestions?", "upvote_ratio": 1.0, "id": "t3_qa71d1", "created_utc": 1634502661.0}
{"sub": "pytorch", "title": "ResNets PyTorch CIFAR-10", "selftext": "I have trained [ResNet-18](https://github.com/arjun-majumdar/CNN_Classifications/blob/master/ResNet18_PyTorch.ipynb), [ResNet-18 (dropout)](https://github.com/arjun-majumdar/CNN_Classifications/blob/master/ResNet18_Dropout_PyTorch.ipynb), [ResNet-34](https://github.com/arjun-majumdar/CNN_Classifications/blob/master/ResNet34_PyTorch.ipynb) and [ResNet-50](https://github.com/arjun-majumdar/CNN_Classifications/blob/master/ResNet50_PyTorch.ipynb) from scratch using He weights initializations and other SOTA practices and their implementations in Python 3.8 and PyTorch 1.8. ResNet-18/34 has a different architecture as compared to ResNet-50/101/152 due to bottleneck as specified by Kaiming He et al. in their research paper.\n\nAlso, note that when a conv layer is followed by a batch norm layer, the conv layer should not be using a bias layer as it's redundant.\n\nThoughts?", "upvote_ratio": 0.67, "id": "t3_qa3uz7", "created_utc": 1634493309.0}
{"sub": "pytorch", "title": "Query regarding \"torch.save()\"", "selftext": "    #   Code Structure in train.py\n    model.train()\n    #   Model Traing Code\n    model.eval()\n    #   Training accuracy and prediction\n    torch.save()\n\nI am using [train.py](https://train.py) to train the model and then save it. This saved model is then used to train further and then predict on some other data. So I need to know whether or not *torch.save**()* keep the *train()* or *eval()* state of the model? I am a beginner, so if some of this seems obvious, please mention it.", "upvote_ratio": 1.0, "id": "t3_q9zley", "created_utc": 1634480320.0}
{"sub": "pytorch", "title": "can someone help me speed up this very simple code?", "selftext": "I\u2019m taking in latent vecs and generating images,  pretty straight forward, but getting less than half the FPS as my  Tensorflow model. Can someone help me speed this up?\n\n```\ndef convert(model, inputs):\n    z = inputs['z']\n    jzf = [float(i) for i in z]\n    jzft = torch.FloatTensor(jzf)\n    jzftr = jzft.reshape([1, 512])\n    latents = jzftr.cuda()\n    truncation = inputs['truncation']\n\n    img = G(latents, None, truncation)\n    img = (img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n    return {'image': img[0].cpu().numpy()}\n```", "upvote_ratio": 1.0, "id": "t3_q9d2vm", "created_utc": 1634394492.0}
{"sub": "pytorch", "title": "The Ultimate Guide To PyTorch", "selftext": "nan", "upvote_ratio": 0.71, "id": "t3_q8zgx0", "created_utc": 1634338360.0}
{"sub": "pytorch", "title": "Unknown errors with setting up a PyTorch Image classifier", "selftext": "Hello, thanks for your time.\n\n&amp;#x200B;\n\nAs part of a computer science project, I have decided to create an image classifying CNN using PyTorch, which a user can interact with by selecting their own image (using Tkinter as a GUI) then the model will run this image through and output the predictions and confidence scores.\n\n&amp;#x200B;\n\nIn order to speed up the progress, I have decided to use the ResNet18 model from PyTorch's website as well as [their example code](https://pytorch.org/hub/pytorch_vision_resnet/) but adapted it into my own program. From here, I can gradually add and change things until eventually, I have my own hand-trained model.\n\n&amp;#x200B;\n\nHowever, the code is throwing up many errors when trying to pass the image through the model. As PyTorch (and Python programming at this level in general) is quite new to me I was wondering if some guidance could be sought from here. I don't expect someone to code the perfect solution for me, as it needs to be my own work anyway, just an idea as to what the errors messages mean and what I need to look at rewriting. \n\n&amp;#x200B;\n\nThe code is attached below, as well as a transcript of the error messages. \n\n&amp;#x200B;\n\n`import torch, torchvision`\n\n`import torch.nn as nn`\n\n`from torchvision import datasets, transforms`\n\n`import tkinter as tk`\n\n`from tkinter import *`\n\n`from tkinter.filedialog import askopenfile, askopenfilename`\n\n`from PIL import ImageTk, Image`\n\n&amp;#x200B;\n\n`#########################################################################`\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n`def SelectUserImage():`\n\n`userfilewindow = tk.Toplevel()`\n\n`path = askopenfilename(filetypes=[(\"Image File\", '*.jpg')])`\n\n`image =` [`Image.open`](https://Image.open)`(path)`\n\n`MLImage = image.convert('RGB')`\n\n[`MLImage.save`](https://MLImage.save)`('MLImage.jpg')`\n\n`test = ImageClassifier(MLImage)`\n\n`userfilewindow.pack()`\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n`####################################################################################################`\n\n`def ImageClassifier(MLImage):`\n\n`ClassifierWindow = tk.Toplevel()`\n\n`model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)`\n\n`model.eval()`\n\n`preprocess = transforms.Compose([`\n\n`transforms.Resize(256),`\n\n`transforms.CenterCrop(224),`\n\n`transforms.ToTensor(),`\n\n`transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),`\n\n`])`\n\n`input_tensor = preprocess(image)`\n\n`input_batch = input_tensor.unsqueeze(0)`\n\n`if torch.cuda.is_available():`\n\n`input_batch = input_batch.to('cuda')`\n\n[`model.to`](https://model.to)`('cuda')`\n\n`with torch.no_grad():`\n\n`output = model(input_batch)`\n\n`# Tensor of shape 1000, with confidence scores over Imagenet's 1000 classes`\n\n`print(output[0])`\n\n`probabilities = torch.nn.functional.softmax(output[0], dim=0)`\n\n`print(probabilities)`\n\n`with open(\"imagenet_classes.txt\", \"r\") as f:`\n\n`categories = [s.strip() for s in f.readlines()]`\n\n`# Show top categories per image`\n\n`top5_prob, top5_catid = torch.topk(probabilities, 5)`\n\n`for i in range(top5_prob.size(0)):`\n\n`print(categories[top5_catid[i]], top5_prob[i].item())`\n\n`ImageClassifier.Pack()`\n\n&amp;#x200B;\n\n`####################################################################################################`\n\n`window = Tk()`\n\n`window.title(\"CS Machine Learning Project\")  # Title for the main window`\n\n`FileExplorerLogo = ImageTk.PhotoImage(`[`Image.open`](https://Image.open)`(\"Local PC Icon.png\"))`\n\n`Button(window, text='Choose Locally Stored Image', image=FileExplorerLogo, command=SelectUserImage, compound=LEFT).pack(`\n\n`side=TOP)  # Button to load image from PC`\n\n`GDriveLogo = ImageTk.PhotoImage(`[`Image.open`](https://Image.open)`(\"Google Drive Icon.png\"))`\n\n`Button(window, text='Choose Image from Google Drive', image=GDriveLogo, compound=LEFT).pack(`\n\n`side=TOP)  # Button to load image from Google Drive`\n\n&amp;#x200B;\n\n`window.mainloop()`\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n`####################################################################################################`\n\n&amp;#x200B;\n\nErrors:\n\n&amp;#x200B;\n\nException in Tkinter callback\n\nTraceback (most recent call last):\n\n  File \"C:\\\\Users\\\\griff\\\\miniconda3\\\\envs\\\\PyTorch\\\\lib\\\\tkinter\\\\\\_\\_init\\_\\_.py\", line 1892, in \\_\\_call\\_\\_\n\nreturn self.func(\\*args)\n\n  File \"C:\\\\Users\\\\griff\\\\PycharmProjects\\\\Project\\_Interface\\\\[main.py](https://main.py)\", line 28, in SelectUserImage\n\ntest = ImageClassifier(MLImage)\n\n  File \"C:\\\\Users\\\\griff\\\\PycharmProjects\\\\Project\\_Interface\\\\[main.py](https://main.py)\", line 35, in ImageClassifier\n\nmodel = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n\n  File \"C:\\\\Users\\\\griff\\\\miniconda3\\\\envs\\\\PyTorch\\\\lib\\\\site-packages\\\\torch\\\\[hub.py](https://hub.py)\", line 362, in load\n\nrepo\\_or\\_dir = \\_get\\_cache\\_or\\_reload(repo\\_or\\_dir, force\\_reload, verbose)\n\n  File \"C:\\\\Users\\\\griff\\\\miniconda3\\\\envs\\\\PyTorch\\\\lib\\\\site-packages\\\\torch\\\\[hub.py](https://hub.py)\", line 162, in \\_get\\_cache\\_or\\_reload\n\n\\_validate\\_not\\_a\\_forked\\_repo(repo\\_owner, repo\\_name, branch)\n\n  File \"C:\\\\Users\\\\griff\\\\miniconda3\\\\envs\\\\PyTorch\\\\lib\\\\site-packages\\\\torch\\\\[hub.py](https://hub.py)\", line 124, in \\_validate\\_not\\_a\\_forked\\_repo\n\nwith urlopen(url) as r:\n\n  File \"C:\\\\Users\\\\griff\\\\miniconda3\\\\envs\\\\PyTorch\\\\lib\\\\urllib\\\\[request.py](https://request.py)\", line 214, in urlopen\n\nreturn [opener.open](https://opener.open)(url, data, timeout)\n\n  File \"C:\\\\Users\\\\griff\\\\miniconda3\\\\envs\\\\PyTorch\\\\lib\\\\urllib\\\\[request.py](https://request.py)\", line 523, in open\n\nresponse = meth(req, response)\n\n  File \"C:\\\\Users\\\\griff\\\\miniconda3\\\\envs\\\\PyTorch\\\\lib\\\\urllib\\\\[request.py](https://request.py)\", line 632, in http\\_response\n\nresponse = self.parent.error(\n\n  File \"C:\\\\Users\\\\griff\\\\miniconda3\\\\envs\\\\PyTorch\\\\lib\\\\urllib\\\\[request.py](https://request.py)\", line 561, in error\n\nreturn self.\\_call\\_chain(\\*args)\n\n  File \"C:\\\\Users\\\\griff\\\\miniconda3\\\\envs\\\\PyTorch\\\\lib\\\\urllib\\\\[request.py](https://request.py)\", line 494, in \\_call\\_chain\n\nresult = func(\\*args)\n\n  File \"C:\\\\Users\\\\griff\\\\miniconda3\\\\envs\\\\PyTorch\\\\lib\\\\urllib\\\\[request.py](https://request.py)\", line 641, in http\\_error\\_default\n\nraise HTTPError(req.full\\_url, code, msg, hdrs, fp)\n\nurllib.error.HTTPError: HTTP Error 403: rate limit exceeded\n\n&amp;#x200B;\n\n\\############################################################################################\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nThanks for your help everyone, have a nice day\n\n&amp;#x200B;\n\n\\- Christian", "upvote_ratio": 1.0, "id": "t3_q8x24e", "created_utc": 1634330617.0}
{"sub": "pytorch", "title": "Integer embeddings (from scratch)", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_q8rnhl", "created_utc": 1634314187.0}
{"sub": "pytorch", "title": "How can I use biggan ( pytorch implementation) for text to scene generation?", "selftext": "I want to use the flick8r dataset but for the reverse task that it was created for. Biggan doesn't take text input. How do I make text to scene generation happen with it?\n\nIf this post is inappropriate, I'll delete it please ping me. \n\nThank you!", "upvote_ratio": 1.0, "id": "t3_q7sy8m", "created_utc": 1634188101.0}
{"sub": "pytorch", "title": "Need explanations about torchtext steps for using texts in deep learning classification", "selftext": "Many pytorch deep learning examples online where the input is textual data (not those pre-existing ones in the libraries, but original textual data) had this data preparation framework:\n\n1. read the textual data and save it in a csv file (ideally a pandas dataframe where \"text\" column has the texts, \"labels\" column has the labels to be used for classification)\n2. define a tokenizer\n3. call torchtext Fields and provide what preprocessing techniques to be applied, also provide the tokenizer defined above in there, also define the fields object where you specify the dataframe columns and the associated outputs of the Fields in tuple format\n4. call torchtext Tabular and provide the fields, the saved csv file name etc. to obtain the preprocessed data\n5. build vocabulary from the output of the Tabular\n6. call BucketIterator to batch the data into batches\n\nPlease feel free to correct any misunderstandings I might have in the above list, so I can learn and correct them. How it all looks like as a code is:\n\n    from torchtext.legacy import data\n    \n    # 1) I read the texts and piled them up in experimental_data.csv file, where the # first column is something not important, so we ignore that in fields,\n    # next column \"text\" has the training set texts, \"label\" has the labels for \n    # texts in each row\n    \n    torch.manual_seed(SEED)\n    torch.backends.cudnn.deterministic = True\n    \n    # 2) Note that I'm using Field's default tokenizer by not providing it anytokenizers\n    \n    # 3) Fields below\n    TEXT = data.Field(batch_first=True, include_lengths=True)\n    LABEL = data.LabelField(dtype = torch.float, batch_first=True)\n    \n    fields = [(None, None), ('text', TEXT),('label', LABEL)]\n    \n    # 4) Tabular below, loading custom dataset\n    training_data = data.TabularDataset(path = \"experimental_data.csv\", format = 'csv',\n                                          fields = fields, skip_header = True)\n    # 5) Vocabulary building. For some reason, TEXT.vocab.freqs is a dictionary where \n    #keys are unique words. The values per word are not unique, so I suspect it doesn't \n    #assign unique numbers to words, but collects their training data-level frequencies? \n    #Some words have values 1. Why is that possible? I gave min_freq=100, doesn't it \n    #work?\n    TEXT.build_vocab(training_data, min_freq=100)\n    LABEL.build_vocab(training_data)\n    \n    train_data, valid_data = training_data.split(split_ratio=0.75, random_state = random.seed(SEED))\n    \n    BATCH_SIZE = 64\n    \n    # 6) Load an iterator\n    train_iterator, valid_iterator = data.BucketIterator.splits(\n        (train_data, valid_data), \n        batch_size = BATCH_SIZE,\n        sort_key = lambda x: len(x.text),\n        sort_within_batch=True,\n        device = device)\n    \n    # I have no idea how to introduce a test data after this point...\n\nWhat confuses me is, what is actually happening to the data in the above steps?\n\nDo the batches actually have the words in texts, or unique numbers assigned to unique words, or word frequencies?\n\nWhat are the extracted features there?\n\nThere are almost no proper, detailed explanations in none of the examples out there, and torchtext library also did not have enough explanations. I have tried to visualize the outputs of these steps, and they did not make sense to me.\n\nFor example, in the Fields step I provided a min\\_frequency threshold for the words as 100 (so those words that appear less frequent than 100 should be eliminated), but what I obtained after build\\_vocab (.freqs attribute gave me) frequencies of words, and many words had the frequency of 1. How is that possible?\n\nAre there any detailed tutorials anywhere that dissect these steps, so I can learn what is going on?\n\nAlso, let's say I am given a test set that was unseen before, how can I use the training set vocabulary to extract the same features from the test data and use that in evaluation?\n\n&amp;#x200B;\n\nEdit: after reading some online comments on torchtext, alternatively should I skip torchtext altogether and use Dataloader instead? Would writing my own feature extraction functions until the Dataloader step create a huge computational burden for large datasets?", "upvote_ratio": 0.89, "id": "t3_q7948i", "created_utc": 1634124513.0}
{"sub": "pytorch", "title": "[HELP] Not able to detect CUDA on Conda instance", "selftext": "Hello all, I am looking for a some help with my torch installation, I am using a conda implementation on my Ubuntu 21.10 laptop, with a pretty old GPU.\n\n&amp;#x200B;\n\nI don't really know why but whenever I wake the laptop from sleep the CUDA goes missing, sometimes my python code is not even able to detect CUDA even after a fresh reboot, it's really frustrating, bc sometimes it works and other times it won't work. I am really troubled by this, can anyone please help me out in figuring out this issue? Right now I am reinstalling Torch via conda again, to see if I get it working, at this point I also want to mention that I have a cron job that runs the \\`sudo apt update &amp;&amp; sudo apt upgrade\\` every time I login to my machine.\n\n&amp;#x200B;\n\nI am running Xserver on performance mode PRIME profile, is this one the causes of concern?", "upvote_ratio": 1.0, "id": "t3_q76nrs", "created_utc": 1634113348.0}
{"sub": "pytorch", "title": "A PyTorch RL and DL framework I have built (not mine, but u/raharth)", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_q6p685", "created_utc": 1634053810.0}
{"sub": "pytorch", "title": "[R] StyleGAN3: Alias-Free Generative Adversarial Networks", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_q6ow2b", "created_utc": 1634053001.0}
{"sub": "pytorch", "title": "help with ram usage", "selftext": "I am training a stacked gru (2 layer) with a linear output layer.\n\nThe input size (num timesteps, features) is the following: (3,178136) and the output is the following: (1, 155551).\n\nI have access to a supercomputer and am currently trying to run it on one high memory node (1575 gb). But I am getting memory allocation issues.\n\nIs there any tips that I can use to reduce the memory usage with reducing the size or number of layers of the network?\n\n\nSince this computing cluster has other nodes I am currently looking into writing some sort of parallel processing scheme that can take advantage of the memory on other nodes.\n\nCan parallel processing be used to distribute the memory usage across multiple nodes during the initialization of the model?\n\nI tried Googling distributed memory allocation on pytorch but it seems most of the literature out there is for breaking the training and test data into chunks and processing them on separate nodes. I need to distribute the model itself across multiple nodes.\n\nCan yall point me to some resources that speak on this topic?", "upvote_ratio": 0.84, "id": "t3_q6205s", "created_utc": 1633976089.0}
{"sub": "pytorch", "title": "VGG-18 PyTorch", "selftext": "I  have completed some extensive experiments using VGG-18 CNN  network  trained on CIFAR-10 dataset from scratch and have obtained a  validation  accuracy = 92.92%. The codes involves different techniques such as:\n\n1. Image data augmentation\n2. Kaiming He weight initialisations\n3. Learning Rate Scheduler - linear LR warmup over 'k' steps followed by a plateau and step-decay\n4. Manual implementation of early stopping\n5. Comparison between early stopping vs. LR scheduler\n6. Batch Normalization\n7. Dropout - to combat overfitting\n\nYou can access the code [here](https://github.com/arjun-majumdar/CNN_Classifications/blob/master/VGG18_PyTorch.ipynb) and [here](https://github.com/arjun-majumdar/CNN_Classifications/blob/master/VGG18_Dropout_PyTorch.ipynb). According to some research papers, for deep learning architectures, using SGD vs. Adam optimizer leads to faster convergence.\n\nThoughts?", "upvote_ratio": 0.89, "id": "t3_q6049r", "created_utc": 1633970946.0}
{"sub": "pytorch", "title": "How to correctly define the first layer dimensions for textual data batches", "selftext": "I created a simple text classification problem where I'd like to use pytorch to perform a logistic regression operation over the data. Data is texts, converted to batches.\n\nThe classification model is as follows:\n\n    class LogisticRegressionModel(nn.Module):\n        \n        def __init__(self, vocab_size, num_labels):\n            \n            super(LogisticRegressionModel, self).__init__()\n            \n            self.linear = nn.Linear(vocab_size, num_labels)\n            \n        def forward(self, x):\n            \n            print(x.shape)\n            out = self.linear(x)\n            \n            return nn.functional.log_softmax(out, dim=1)\n\nThe batch size is 64. The words in texts are converted to unique numbers, so inside of a batch looks like:\n\n    (tensor([[ 100,   45,   23,  ...,  212, 1560,    2],\n            [   0,    0,    8,  ...,    2,    2,    2],\n            [   3,   51, 1393,  ...,  203,  178,    2],\n            ...,\n            [  38, 3635,  161,  ...,    2,    1,    1],\n            [  97,  162,  285,  ...,    2,    1,    1],\n            [  38,  149,    0,  ...,   30,    1,    1]]), tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n            18, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n            17, 17, 17, 17, 17, 17, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n            16, 16, 16, 16, 16, 16, 16, 16, 16, 16]))\n    torch.Size([64, 18])\n\nEvery batch has a different size, above one has 18 but others have 50 etc. I think that's the maximum number of words a text had in that batch.\n\n&amp;#x200B;\n\nMy question is, why is my classifier not accepting the batches as input? Gives the following error\n\n    RuntimeError: mat1 and mat2 shapes cannot be multiplied (64x18 and 12853x16)\n\n(64x18) is the batch dimensions, and (12853x16) is the layer dimension (12853 is the vocabulary size, 16 is the number of classes). The error seems to be related to how I defined the linear layer in the logistic regression. Examples online had an embedding layer before the linear layer and it seemed like having the same type of batches as I do, embedding layer handled the batches, but the linear layer cannot.\n\n&amp;#x200B;\n\nWhat is the correct way of defining the linear layer, so that it can handle the batches as they are?", "upvote_ratio": 1.0, "id": "t3_q5wbui", "created_utc": 1633960353.0}
{"sub": "pytorch", "title": "Custom C++ extensions vs libtorch", "selftext": "Hello, I am interested in having the model inference in C++ (CPU only). The model is trained in python, I have a few questions regarding libtorch vs Custom C++ extensions of pytorch.\n\n&amp;#x200B;\n\n1. the main difference between libtorch vs Custom C++ extensions of pytorch (can both be used for inference?)\n2. Which one is better in terms of both ease of use and performance in terms of inference.\n3. can you convert any PyTorch model be used for inference in C++ using libtorch and/or C++ extensions.\n4. Finally, which method is recommended for future use.\n\n&amp;#x200B;\n\nP.S: by custom C++ extensions I mean [this](https://pytorch.org/tutorials/advanced/cpp_extension.html#)", "upvote_ratio": 1.0, "id": "t3_q5rs5f", "created_utc": 1633943031.0}
{"sub": "pytorch", "title": "A sneak peek on the features of TorchVision v0.11", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_q57r4v", "created_utc": 1633871407.0}
{"sub": "pytorch", "title": "Any step by step instructions for compiling for Windows?", "selftext": "I'm trying to figure out how to compile pytorch so I can play with it on an old Cuda 3.0 card(a Quadro k420 if that matters), since the compiled binaries aren't compiled with support for older cards. I found lots of detailed instructions for Linux, but I haven't had any luck figuring out how to do it in Windows. Is there anywhere I can find a guide that will hold my hand?", "upvote_ratio": 0.86, "id": "t3_q41avz", "created_utc": 1633710626.0}
{"sub": "pytorch", "title": "Modify convolution kernels ops", "selftext": "Hello, \n\nI try to modify the way convolutions are performed for a conv net (let take ResNet18 as an example) in PyTorch:\n\nTake the first layer. I want to take the FT(Fourier) of the input image F(I); take the FTs of the conv kernels F(k1),...F(k64). Next step, add them up: F(K\\_total) = F(k1)+F(k2)+...+F(k64). Finally do a single do a single multiply: F(I) \\* F(K\\_total). Then do the inverse. \n\nThe FT of the input image is not a big deal. But how can I operate on conv kernels ops in PyTorch ? I am willing to change the framework used, if you recommend it.", "upvote_ratio": 0.72, "id": "t3_q3rvq2", "created_utc": 1633673743.0}
{"sub": "pytorch", "title": "Python server GAN move to on mobile processing. What are the biggest hurdles and issues? It\u2019s a vision model for generating photos.", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_q3m5x6", "created_utc": 1633651991.0}
{"sub": "pytorch", "title": "Can I use a mining rig for training a model on PyTorch, and will it be effective?", "selftext": "So I currently have a mining rig that I am considering using for training a machine learning model, but given the differences between mining rigs and my traditional ML computing setup, I was wondering if there would be any significant bottlenecks. My rig is as follows:\n\n13x RTX 3080 GPUs (connected to the motherboard via PCI-E x16 to x1 risers)\n\n16 GB DDR4 2400 MHz RAM\n\nIntel i5 7600k\n\nWestern Digital SSD\n\nASRock H110 BTC mining pro motherboard\n\nMy main concern is with the use of risers and the relatively old CPU.", "upvote_ratio": 0.9, "id": "t3_q3h00k", "created_utc": 1633635990.0}
{"sub": "pytorch", "title": "Model not training after revisiting in a few weeks", "selftext": "Hey Everyone!\nI created a CNN implementation a few months ago and it worked great on different losses (Dice, CE, Focal). The iou and F1 scores would turn out great in just 3-5 epochs. However, I revisited it now a few weeks later and tried to re-train it on the same data but it's not training. I initially tried to train it using Dice Loss (as it gave the best results) but it gave the no_grad error - something I had never faced before. I set requires_grad to True and it stopped giving the error but now it doesn't really train. The values just hop around a bit but stay the same more or less over 10s of epochs. I tried it with the other losses too and they're not training either. I have not changed the code base in any way. I have however done all this using colab. Would version changes or something along the lines contribute to this?\nWould really appreciate any help. I've been stuck on this for a while now and it's incredibly frustrating.", "upvote_ratio": 1.0, "id": "t3_q307m5", "created_utc": 1633577052.0}
{"sub": "pytorch", "title": "Simple operators comparatively slow on CPU", "selftext": "I've made a [post](https://discuss.pytorch.org/t/speed-up-libtorch-operators-on-cpu/133254) on the Pytorch forums too about this. Libtorch operators such as + * or / are slow compared to other implementations such as C++ vectors or Armadillo. I tested by multiplying two vectors elements-wise 10 million times and got the following durations:\n\nPytorch - 7.2 seconds\nArma - 0.33\nC++ vectors - 0.23\n\nI understand that Pytorch is doing all sorts of things under the hood for backpropagation and is meant to be used on the GPU, but to me this difference is pretty significant and before I invest time in making my own backpropagation algorithm with Arma I'd like to know whether there are any ways of speeding up these types of vector operations.", "upvote_ratio": 1.0, "id": "t3_q2kkea", "created_utc": 1633527214.0}
{"sub": "pytorch", "title": "PyTorch Forecasting lr_find out of bounds - request for help", "selftext": "I am trying to follow the PyTorch Forecasting tutorials to apply to my own data. My data is an irregular time series with multiple grouping variables. I am able to run everything fine until the optimal learning rate step. I get an error:\n\nRuntimeError: index 12 is out of bounds for dimension 0 with size 11. \n\nAs best as I can tell index 12 is the lstm_decoder. I have searched the documentation, stackoverflow, and just googling a bunch but I\u2019m not finding how to narrow down the problem. Could the problem be that my timeseries is irregular (weekly dates with some grouping not present at each time), or is there another issue I\u2019m missing. I have also tried removing all missing values. allow_missing_time steps is set to true in the TimeSeriesDataSet step.", "upvote_ratio": 0.6, "id": "t3_q0v5zj", "created_utc": 1633309869.0}
{"sub": "pytorch", "title": "Increasing GPU RAM Allocation?", "selftext": "Hey\n\nI'm running into an out of memory error while attempting neural style transfer using a pretrained VGG19. I have 2 questions:\n\n\\- I have 6GB GPU memory available on my system, but PyTorch is only allocating around 3GB. Is there any way to increase that?\n\n\\- Is 6GB enough for a single pass through VGG19?", "upvote_ratio": 0.86, "id": "t3_pzeqiu", "created_utc": 1633114949.0}
{"sub": "pytorch", "title": "Primer EZ: Annotated Implementation", "selftext": "Primer is a transformer model found using an &gt;1,000 TPUv2-hour evolutionary architecture search by Google. It can be &gt;4X faster to train than vanilla transformer. Primer EV has the two most robust modifications in Primer: Squared ReLU activations and depth-wise convolution of K, Q, and V projections.\n\n* [Implementation with side-by-side notes](https://nn.labml.ai/transformers/primer_ez/index.html)\n* [Paper](https://papers.labml.ai/paper/2109.08668)\n* [Highlighted PDF](https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/papers/2109.08668.pdf)\n* [Twitter thread](https://twitter.com/labmlai/status/1440273380213551120)", "upvote_ratio": 1.0, "id": "t3_pyk4fx", "created_utc": 1633009609.0}
{"sub": "pytorch", "title": "Ideas on how to create a differentiable loss function with count?", "selftext": "I'm trying to create a loss function for a NN that counts how many values in a tensor are above the value 10.\n\nFor example, if I have the tensors:\n\na = torch.tensor(\\[1, 2, 12, 35, 3\\])\n\nb = torch.tensor(\\[11, 22, 1, 3, 99\\])\n\nThe loss will be 2 + 3 = 5.\n\nHowever, I can't figure out how to make something like this that will be differentiable", "upvote_ratio": 0.86, "id": "t3_pwx7p6", "created_utc": 1632798358.0}
{"sub": "pytorch", "title": "Four short videos on the why, how, and what about PyKale, a library in the PyTorch ecosystem aiming to make machine learning (ML) more accessible to interdisciplinary research. Both ML experts and end users can do better research with our accessible design. Code: https://github.com/pykale/pykale [P]", "selftext": "nan", "upvote_ratio": 0.91, "id": "t3_pvukd7", "created_utc": 1632665372.0}
{"sub": "pytorch", "title": "Cannot access a model's layers in tch-rs", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_pv5c7m", "created_utc": 1632570888.0}
{"sub": "pytorch", "title": "A Guide to Writing CNNs from Scratch in PyTorch", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_puo0nj", "created_utc": 1632502867.0}
{"sub": "pytorch", "title": "Help with Pytorch model deploy", "selftext": "Hi!\n\nI am a backend engineer developing an Rest API to provide access to a PyTorch model. The model was made by another developer and he no longer works in this project, so I have the task of figuring out what to do. However I have zero experience with Machine Learning and would like to know few resource that may help me to understand the basics about a PyTorch Neural Network and deploy it in production.\n\n&amp;#x200B;\n\nThanks!", "upvote_ratio": 1.0, "id": "t3_pu8cwe", "created_utc": 1632442977.0}
{"sub": "pytorch", "title": "Enhancement on splitting tensors", "selftext": "Is there any way to split a tensor into a tensor of sub-tensors in pytorch? Current solutions split it into a list/tuple of sub-tensors and make use of torch.stack() inevitably, which is very slow. Would like to know any suggestions from the community", "upvote_ratio": 1.0, "id": "t3_ptuu82", "created_utc": 1632402347.0}
{"sub": "pytorch", "title": "Host and serve your PyTorch, TensorFlow, and Scikit-learn models quickly and easily", "selftext": " Hi Everyone,\n\nFlashAI.io was born out of a need to find a quick and simple way to host or serve my ML models.\n\nSometimes you want to enable clients, colleagues, or apps to send inference requests to your models.\n\nFlashAI lets you do this via web requests.\n\nServe your models with any hassle, try FlashAI.io\n\nThe workflow is straight forward:\n\n\\* Train your model locally\n\n\\* Upload your model file to FlashAI.io\n\n\\* Send inference requests to your model\n\nCurrently this service supports Scikit-learn, TensorFlow, and PyTorch models.\n\nTry it out at [flashai.io](https://flashai.io/)\n\nSee the tutorial at: [https://youtu.be/0yxUmZ2GnX8](https://youtu.be/0yxUmZ2GnX8)\n\nPlease let me know what you think and if you have any suggestions for other features.", "upvote_ratio": 0.55, "id": "t3_ptf7j1", "created_utc": 1632339406.0}
{"sub": "pytorch", "title": "Best way to append tensors", "selftext": "How can I append multiple tensors to a single one during training? One obvious method is using list comprehension to stack tensors and calling the stack function at the end. But this wouldn't be feasible if I move it into the GPU. Using torch.cat() creates a copy of the tensor and its both time consuming as well as might run out of memory when processing large batches. Which is the optimal way?", "upvote_ratio": 1.0, "id": "t3_pt64bm", "created_utc": 1632312365.0}
{"sub": "pytorch", "title": "Tools and libraries to deploy model in production", "selftext": "Hi All,\n\nQuestion to people who deployed detectron2/pytorch models in production.\n\nHow do you deploy an object detection dl model in production for real-time feed? \n\nCurrently I am running app using plain python inference code but I believe that's not how models are deployed in production.\n\nCan someone guide me on this ? \nI want to deploy a real-time Object detection app on single powerful gpu where it's receiving 2 or more real-time feed and I would like to process them in real-time to trigger some events.\n\nI have looked at some blogs like medium and towards data-science but didn't find anything on real-time video processing in production that detailed as I want.\nI also checked some tools such Mlflow ,torch serve but didn't really understand what to do? \n\nIf someone can help on deploying real-time video processing models in production that will be great !!\n\nLanguage preferred : Python\nPlatform: GPU server - Nvidia but edge also is fine.\n\nIf this question is not valid in this blog or you cannot answer in publicly, please feel free to dm me !!\nThanks in advance !", "upvote_ratio": 0.81, "id": "t3_psulgl", "created_utc": 1632265928.0}
{"sub": "pytorch", "title": "Cloud GPU", "selftext": "What do you use for cloud GPU computing? I\u2019ve tried google colab but it\u2019s limited to 24h. I have a script that\u2019s gonna need a week to run (grid search with leave one out cross validation)", "upvote_ratio": 0.84, "id": "t3_prze03", "created_utc": 1632158447.0}
{"sub": "pytorch", "title": "Image mean and std", "selftext": "I have a dataset with images where sizes varies. The images are polygons with 0 outside the region of interest (ROI). In my network, the input size is always fixed e.g. 224x224. What mean and std should I use in dataloader to normalze the image? The mean and std from the initial image size, the 224x224 mean and std and inside ROI or not?", "upvote_ratio": 1.0, "id": "t3_prs33e", "created_utc": 1632133860.0}
{"sub": "pytorch", "title": "How to interpret pytorchviz visualization graph to debug gradient issue ?", "selftext": "I managed to [visualize the computational graph](https://i.imgur.com/wceJ0Df.png) for my gdas coding using [pytorchviz](https://github.com/szagoruyko/pytorchviz)\n\nHowever, how to use the visualized graph to debug the [grad=None issue](https://github.com/promach/gdas/blob/main/gdas.py#L464) ?  \n\n\nhttps://preview.redd.it/yherfadcblo71.png?width=10121&amp;format=png&amp;auto=webp&amp;s=05af403cf2fee16f5f9e83b5b30702af8d954270", "upvote_ratio": 1.0, "id": "t3_pro1tw", "created_utc": 1632114022.0}
{"sub": "pytorch", "title": "What are the clip gan dataset that was used to train on colab?", "selftext": "I want to train a clip gan on colab, and I was told that they used a imagenet. But I think it is not true.\n\nYou need the text and the image. Do we have the dataset for clip gan or the reproduce clip gan on colab? thank you", "upvote_ratio": 1.0, "id": "t3_pr6jys", "created_utc": 1632052498.0}
{"sub": "pytorch", "title": "Spark2 + pytorch on GPU", "selftext": "Was reading the documentation of sparktorch (https://github.com/dmmiller612/sparktorch) which says you need spark &gt;= 2.4.4. But to the best of my knowledge spark2 doesn't have gpu compute capabilities. Does that mean it can only use cpu compute? Am I missing something?", "upvote_ratio": 0.88, "id": "t3_pqbouq", "created_utc": 1631922764.0}
{"sub": "pytorch", "title": "AWS lambda inference taking 3s even after warmup", "selftext": "I asked the [following from this sub](https://www.reddit.com/r/pytorch/comments/p6duzx/does_torchserve_in_aws_scale_to_have_equal/?utm_medium=android_app&amp;utm_source=share) few weeks ago.\n\n&gt;If I manually launch an ec2 server with pytorch inference, the inference time will depend on the resources I configured and the number of users. When many users request in parallel, inference time will increase (due to limited resources and waiting).\n\n&gt;The requirement is: inference time per image per user should be less than 100 ms. Is there any way I can ensure this is met regardless of number of parallel requests? Is this possible with SageMaker?\n\nMany suggested to move the inference to AWS lambda for the best scaling. So, I open a video file in a server (ec2/beanstalk), stream each frame to the lambda and get predictions back and do further processing.\n\nWhen i do that, each frame takes 1.7 to 3 seconds to process. Although latency isn't an issue for us, we need a throughput of about 10 frames per second.\n\nWhat can I do?", "upvote_ratio": 1.0, "id": "t3_ppvphk", "created_utc": 1631864502.0}
{"sub": "pytorch", "title": "German language sentiment classification - NLP Deep Learning", "selftext": "I am trying to build a sentiment classification (hate speech) for German  language using NLP + Deep Learning. Any code tutorial? I found lots of  research papers but few code implementations.", "upvote_ratio": 0.83, "id": "t3_pp4gd0", "created_utc": 1631758979.0}
{"sub": "pytorch", "title": "Read quantized weights", "selftext": "I managed to quantize a model. I can see the weights are in qint8. But when I call [conv.weight.data](https://conv.weight.data), I get floating-point weights. Isn't there a way to get int8 weights directly, like in tflite?", "upvote_ratio": 1.0, "id": "t3_pp3cdw", "created_utc": 1631754903.0}
{"sub": "pytorch", "title": "Inference with pytorch vs torchscript+libtorch", "selftext": "Hi! I would like to know if there is a big difference between doing inference (in production) with simple pytorch vs exporting pytorch model with torchscript and running it with libtorch. I can't find any benchmark..\n\nMy question is for CPU and GPU.", "upvote_ratio": 0.9, "id": "t3_po0icb", "created_utc": 1631617171.0}
{"sub": "pytorch", "title": "Model bert_score not getting better.", "selftext": "I was developing a Encoder-decoder NLP generative model, and was evaluating it against a Bert_score on each epoch. I found that even when the loss keeps decreasing with ever epoch, the best Bert_score converges early into the training (eg. Epoch3, epoch15 etc) and never changes again. I have set my learning rate to be 0.0001 and my gradient clipping to be 1.0 .  What all could be the possible reasons for this behaviour? I am using GRUs inside my encoder and decoder.", "upvote_ratio": 0.86, "id": "t3_pm7gzn", "created_utc": 1631367040.0}
{"sub": "pytorch", "title": "Pytorch GPU Memory Leak Problem: Cuda Out of Memory Error !!", "selftext": "I am working on implementing UNet for image segmentation using Pytorch. \n\nI think there's a GPU memory leak problem because it raises Cuda out of Memory error.\n\nPytorch Version: 1.9.0 \n\nWhat I've tried so far:\n\n* reducing batch size, even to 1.\n* reducing the number of workers in the data loader.\n* detaching tensors and deleting unneeded tensors after updating the grads.\n* setting the data grads and the model parameters grads to None at the end of the training loop.\n* enforcing GPU Cuda memory freeing using: `torch.cuda.empty_cache()` \n\n&amp;#x200B;\n\nCan anyone help please to check what's wrong here??", "upvote_ratio": 0.75, "id": "t3_pm6t0t", "created_utc": 1631364398.0}
{"sub": "pytorch", "title": "help with installation w.o internet please", "selftext": "dear all, i am setting up my python/conda/pytorch environment on a totally new machine w. 4 GPUs and the machine does not have access to the internet unfortunately (and will not have). I am wondering if there is a way to download the package and build from the source as any commands using pip or conda to install will fail due to no access to the internet. If you have a solution/link to similar thread, please feel free to shine some light on me, much appreciated!!", "upvote_ratio": 0.38, "id": "t3_pkezpv", "created_utc": 1631121852.0}
{"sub": "pytorch", "title": "pytorch hook function does not work", "selftext": "May I know why the [pytorch hook function](https://gist.github.com/promach/b6f526c56e20f029d68e6f9041c3f5c0#file-gdas-py-L316) does not work ?  \n\n\nhttps://preview.redd.it/ymehkpym7xl71.png?width=980&amp;format=png&amp;auto=webp&amp;s=ac49f72563b81a6f8f268af53e3c4dd0342c8355", "upvote_ratio": 1.0, "id": "t3_pj4i55", "created_utc": 1630950595.0}
{"sub": "pytorch", "title": "PonderNet - Paper implementation tutorial", "selftext": "nan", "upvote_ratio": 0.92, "id": "t3_pj1kg2", "created_utc": 1630942050.0}
{"sub": "pytorch", "title": "Beginner Question", "selftext": "Hi, as a part of my early projects, I wanted to find a database I could use to train on letters of the alphabet, just like how I'd done digits with MNIST. Is there a similar dataset I could use? I looked into EMNIST but realized that a lot of the images there were rotated in multiple directions.", "upvote_ratio": 0.84, "id": "t3_piuxyu", "created_utc": 1630914589.0}
{"sub": "pytorch", "title": "handling batch size with custom LSTM", "selftext": " Hi All,  \nI am trying to implement custom LSTM layer with custom cell.  \nIt is working OK when I pass only one sample, but when I want to pass a batch of data a problem appear. \n\nFull question with code implementation is in the main thread\n\n[https://discuss.pytorch.org/t/batch-size-handling-with-lstm/129123](https://discuss.pytorch.org/t/batch-size-handling-with-lstm/129123)", "upvote_ratio": 0.81, "id": "t3_pfths3", "created_utc": 1630501506.0}
{"sub": "pytorch", "title": "Neural Network for Finding Relationships and Correlations between Rows and Different Datasets", "selftext": "Essentially, I have a number of very large datasets with relationships between them (dates, locations, different value columns, etc.). I would like to be able to find relationships and correlations between columns. Is this something that could be done with some sort of unsupervised neural network? If so, does anyone have any examples? Also, does anybody know what the name of this type of research is? Lastly, many of the datasets are very large (100s of millions of rows), which is something to keep in mind. Thanks.", "upvote_ratio": 1.0, "id": "t3_pdj173", "created_utc": 1630188632.0}
{"sub": "pytorch", "title": "Residual Networks in PyTorch", "selftext": "I wrote a tutorial on how to implement simple residual blocks or use pre-trained ResNet models for image classification.\n\nLink: [https://taying-cheng.medium.com/building-a-residual-network-with-pytorch-df2f6937053b](https://taying-cheng.medium.com/building-a-residual-network-with-pytorch-df2f6937053b)", "upvote_ratio": 0.72, "id": "t3_pdd301", "created_utc": 1630168518.0}
{"sub": "pytorch", "title": "Trying to understand the backward function", "selftext": "Hey Guys,\n\nSay I want to call backward on some internal node in the graph (NOT the final loss). Obviously since it is an internal node, it is non-scalar too, and the gradients (ie, x.grad's will be 3-dimensional if we count the batch dim. Here is what the doc says: \"**. If the tensor is non-scalar and requires gradient, the function additionally requires specifying** gradient\".\n\nGradient of what? the loss? I don't care about the loss, why should I provide that? The x.grads should be containing the gradient of that tensor with respect to the leaf nodes, and the gradient is not required at all. Can someone explain please?\n\n&amp;#x200B;\n\nBest", "upvote_ratio": 1.0, "id": "t3_pbijee", "created_utc": 1629919982.0}
{"sub": "pytorch", "title": "Various size image dataset", "selftext": "I have a dataset which consists of various size images (from 155x89 to 437x268) which are polygons (region of interest - ROI) and pixels outside of the polygon/ROI are zero (see [https://prnt.sc/1qia3t5](https://prnt.sc/1qia3t5)). I want to build a deep learning model for a binary classification model. From my experience (e.g. Dogs VS Cats simple dataset), what most people do is resize the images to a fixed size e.g. 224x224 which could be ideal for transfer learning models. However I am not sure if its ideal. Here are my thoughts:\n\n1. Resize all images to a fixed size 224x224 and apply model (the \"texture\" remains the same in my opinion and the model judges based on black and wait composition)\n2. zero pad all images to a \"big\" dimension e.g. 600x600 and apply model (computational expensive, need huge amount of GPU memory, cannot run on my GPU, CPU takes forever)\n3. zero pad all images to a \"big\" dimension e.g. 600x600 and resize back to 224x224 (size matters)", "upvote_ratio": 1.0, "id": "t3_pbg0s3", "created_utc": 1629912590.0}
{"sub": "pytorch", "title": "A sneak peek of the upcoming features of TorchVision", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_p8qunm", "created_utc": 1629550068.0}
{"sub": "pytorch", "title": "Train and test accuracies on my ResNet 50 implementation are 0.1. I don't understand why. Can someone take a look at my model implementation and see where the bug lies?", "selftext": "Hello PyTorch developers,\n\nI tried to implement ResNet 50 (doing Exercise 2 from [d2l.ai book, section 7.6](http://d2l.ai/chapter_convolutional-modern/resnet.html#exercises)). You can find the ResNet architecture described [here](https://arxiv.org/pdf/1512.03385.pdf) (page 5, Table 1). However, when I train it, my train and test accuracies are 0.1. I'd really appreciate it if someone could have a look and see what may be the case here.\n\nHere is my implementation, along with the debug output. I separate every code cell with its own code block. The outputs are also in their own code block.\n\nI worked through the shapes of the matrices on paper and the reason why I use `self.conv4` and `self.conv5` is so that I can adjust the network output and the input so that they can be added together. Maybe I should do all of this in some other way; I'm not sure. Please do have a look at the code below.\n\n```\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom d2l import torch as d2l\n\nclass Residual(nn.Module):  #@save\n    \"\"\"The Residual block of ResNet.\"\"\"\n    def __init__(self, input_channels, num_channels, strides=1): \n        # I removed the use_conv_1x1 attribute since I always adjust both X and Y\n        super().__init__()\n        self.conv1 = nn.Conv2d(input_channels, num_channels, kernel_size=1,\n                               stride=strides)\n        self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3,\n                               padding=1)\n        self.conv3 = nn.Conv2d(num_channels, num_channels * 4, kernel_size=1) # no padding doesn't change the \n                                                                              # image shape\n        self.conv4 = nn.Conv2d(num_channels*4, num_channels, \n                               kernel_size=1) # used to adjust Y\n        self.conv5 = nn.Conv2d(input_channels, num_channels,\n                               kernel_size=1, stride=strides) # used to adjust X\n        self.bn1 = nn.BatchNorm2d(num_channels)\n        self.bn2 = nn.BatchNorm2d(num_channels)\n        self.bn3 = nn.BatchNorm2d(num_channels * 4)\n\n    def forward(self, X):\n        # debug purpose prints are commented out\n        print(\"-----------------------------------\")\n        print(\"X.shape:\")\n        print(X.shape)\n        Y = F.relu(self.bn1(self.conv1(X)))\n        print(\"Y.shape:\")\n        print(Y.shape)\n        Y = F.relu(self.bn2(self.conv2(Y)))\n        print(\"Y.shape:\")\n        print(Y.shape)\n        Y = F.relu(self.bn3(self.conv3(Y)))\n        print(\"Y.shape:\")\n        print(Y.shape)\n        Y = self.conv4(Y)\n        print(\"Y.shape:\")\n        print(Y.shape)\n        print(\"X.shape:\")\n        print(X.shape)\n        X = self.conv5(X)\n        print(\"X.shape:\")\n        print(X.shape)\n        print(\"-----------------------------------\")\n        Y += X\n        return F.relu(Y)\n```\n```\nblk = Residual(3, 3)\nX = torch.rand(4, 3, 6, 6)\nY = blk(X)\nY.shape\n```\n```\n-----------------------------------\nX.shape:\ntorch.Size([4, 3, 6, 6])\nY.shape:\ntorch.Size([4, 3, 6, 6])\nY.shape:\ntorch.Size([4, 3, 6, 6])\nY.shape:\ntorch.Size([4, 12, 6, 6])\nY.shape:\ntorch.Size([4, 3, 6, 6])\nX.shape:\ntorch.Size([4, 3, 6, 6])\nX.shape:\ntorch.Size([4, 3, 6, 6])\n-----------------------------------\ntorch.Size([4, 3, 6, 6])\n```\n```\nblk = Residual(3, 6, strides=2)\nblk(X).shape\n```\n```\n-----------------------------------\nX.shape:\ntorch.Size([4, 3, 6, 6])\nY.shape:\ntorch.Size([4, 6, 3, 3])\nY.shape:\ntorch.Size([4, 6, 3, 3])\nY.shape:\ntorch.Size([4, 24, 3, 3])\nY.shape:\ntorch.Size([4, 6, 3, 3])\nX.shape:\ntorch.Size([4, 3, 6, 6])\nX.shape:\ntorch.Size([4, 6, 3, 3])\n-----------------------------------\n\ntorch.Size([4, 6, 3, 3])\n```\n```\nb1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n                   nn.BatchNorm2d(64), nn.ReLU(),\n                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n```\n```\ndef resnet_block(input_channels, num_channels, num_residuals,\n                 first_block=False):\n    blk = []\n    for i in range(num_residuals):\n        if i == 0 and not first_block:\n            blk.append(\n                Residual(input_channels, num_channels, \n                         strides=2))\n        else:\n            blk.append(Residual(num_channels, num_channels))\n    return blk\n```\n```\nb2 = nn.Sequential(*resnet_block(64, 64, 3, first_block=True))\nb3 = nn.Sequential(*resnet_block(64, 128, 4))\nb4 = nn.Sequential(*resnet_block(128, 256, 6))\nb5 = nn.Sequential(*resnet_block(256, 512, 3))\n```\n```\nnet = nn.Sequential(b1, b2, b3, b4, b5, nn.AdaptiveAvgPool2d((1, 1)),\n                    nn.Flatten(), nn.Linear(512, 10))\n```\n```\nX = torch.rand(size=(1, 1, 224, 224))\nfor layer in net:\n    print(\"X.shape:\")\n    print(X.shape)\n    X = layer(X)\n    print(layer.__class__.__name__, 'output shape:\\t', X.shape)\n```\n```\nX.shape:\ntorch.Size([1, 1, 224, 224])\nSequential output shape:\t torch.Size([1, 64, 56, 56])\nX.shape:\ntorch.Size([1, 64, 56, 56])\n-----------------------------------\nX.shape:\ntorch.Size([1, 64, 56, 56])\nY.shape:\ntorch.Size([1, 64, 56, 56])\nY.shape:\ntorch.Size([1, 64, 56, 56])\nY.shape:\ntorch.Size([1, 256, 56, 56])\nY.shape:\ntorch.Size([1, 64, 56, 56])\nX.shape:\ntorch.Size([1, 64, 56, 56])\nX.shape:\ntorch.Size([1, 64, 56, 56])\n-----------------------------------\n-----------------------------------\nX.shape:\ntorch.Size([1, 64, 56, 56])\nY.shape:\ntorch.Size([1, 64, 56, 56])\nY.shape:\ntorch.Size([1, 64, 56, 56])\nY.shape:\ntorch.Size([1, 256, 56, 56])\nY.shape:\ntorch.Size([1, 64, 56, 56])\nX.shape:\ntorch.Size([1, 64, 56, 56])\nX.shape:\ntorch.Size([1, 64, 56, 56])\n-----------------------------------\n-----------------------------------\nX.shape:\ntorch.Size([1, 64, 56, 56])\nY.shape:\ntorch.Size([1, 64, 56, 56])\nY.shape:\ntorch.Size([1, 64, 56, 56])\nY.shape:\ntorch.Size([1, 256, 56, 56])\nY.shape:\ntorch.Size([1, 64, 56, 56])\nX.shape:\ntorch.Size([1, 64, 56, 56])\nX.shape:\ntorch.Size([1, 64, 56, 56])\n-----------------------------------\nSequential output shape:\t torch.Size([1, 64, 56, 56])\nX.shape:\ntorch.Size([1, 64, 56, 56])\n-----------------------------------\nX.shape:\ntorch.Size([1, 64, 56, 56])\nY.shape:\ntorch.Size([1, 128, 28, 28])\nY.shape:\ntorch.Size([1, 128, 28, 28])\nY.shape:\ntorch.Size([1, 512, 28, 28])\nY.shape:\ntorch.Size([1, 128, 28, 28])\nX.shape:\ntorch.Size([1, 64, 56, 56])\nX.shape:\ntorch.Size([1, 128, 28, 28])\n-----------------------------------\n-----------------------------------\nX.shape:\ntorch.Size([1, 128, 28, 28])\nY.shape:\ntorch.Size([1, 128, 28, 28])\nY.shape:\ntorch.Size([1, 128, 28, 28])\nY.shape:\ntorch.Size([1, 512, 28, 28])\nY.shape:\ntorch.Size([1, 128, 28, 28])\nX.shape:\ntorch.Size([1, 128, 28, 28])\nX.shape:\ntorch.Size([1, 128, 28, 28])\n-----------------------------------\n-----------------------------------\nX.shape:\ntorch.Size([1, 128, 28, 28])\nY.shape:\ntorch.Size([1, 128, 28, 28])\nY.shape:\ntorch.Size([1, 128, 28, 28])\nY.shape:\ntorch.Size([1, 512, 28, 28])\nY.shape:\ntorch.Size([1, 128, 28, 28])\nX.shape:\ntorch.Size([1, 128, 28, 28])\nX.shape:\ntorch.Size([1, 128, 28, 28])\n-----------------------------------\n-----------------------------------\nX.shape:\ntorch.Size([1, 128, 28, 28])\nY.shape:\ntorch.Size([1, 128, 28, 28])\nY.shape:\ntorch.Size([1, 128, 28, 28])\nY.shape:\ntorch.Size([1, 512, 28, 28])\nY.shape:\ntorch.Size([1, 128, 28, 28])\nX.shape:\ntorch.Size([1, 128, 28, 28])\nX.shape:\ntorch.Size([1, 128, 28, 28])\n-----------------------------------\nSequential output shape:\t torch.Size([1, 128, 28, 28])\nX.shape:\ntorch.Size([1, 128, 28, 28])\n-----------------------------------\nX.shape:\ntorch.Size([1, 128, 28, 28])\nY.shape:\ntorch.Size([1, 256, 14, 14])\nY.shape:\ntorch.Size([1, 256, 14, 14])\nY.shape:\ntorch.Size([1, 1024, 14, 14])\nY.shape:\ntorch.Size([1, 256, 14, 14])\nX.shape:\ntorch.Size([1, 128, 28, 28])\nX.shape:\ntorch.Size([1, 256, 14, 14])\n-----------------------------------\n-----------------------------------\nX.shape:\ntorch.Size([1, 256, 14, 14])\nY.shape:\ntorch.Size([1, 256, 14, 14])\nY.shape:\ntorch.Size([1, 256, 14, 14])\nY.shape:\ntorch.Size([1, 1024, 14, 14])\nY.shape:\ntorch.Size([1, 256, 14, 14])\nX.shape:\ntorch.Size([1, 256, 14, 14])\nX.shape:\ntorch.Size([1, 256, 14, 14])\n-----------------------------------\n-----------------------------------\nX.shape:\ntorch.Size([1, 256, 14, 14])\nY.shape:\ntorch.Size([1, 256, 14, 14])\nY.shape:\ntorch.Size([1, 256, 14, 14])\nY.shape:\ntorch.Size([1, 1024, 14, 14])\nY.shape:\ntorch.Size([1, 256, 14, 14])\nX.shape:\ntorch.Size([1, 256, 14, 14])\nX.shape:\ntorch.Size([1, 256, 14, 14])\n-----------------------------------\n-----------------------------------\nX.shape:\ntorch.Size([1, 256, 14, 14])\nY.shape:\ntorch.Size([1, 256, 14, 14])\nY.shape:\ntorch.Size([1, 256, 14, 14])\nY.shape:\ntorch.Size([1, 1024, 14, 14])\nY.shape:\ntorch.Size([1, 256, 14, 14])\nX.shape:\ntorch.Size([1, 256, 14, 14])\nX.shape:\ntorch.Size([1, 256, 14, 14])\n-----------------------------------\n-----------------------------------\nX.shape:\ntorch.Size([1, 256, 14, 14])\nY.shape:\ntorch.Size([1, 256, 14, 14])\nY.shape:\ntorch.Size([1, 256, 14, 14])\nY.shape:\ntorch.Size([1, 1024, 14, 14])\nY.shape:\ntorch.Size([1, 256, 14, 14])\nX.shape:\ntorch.Size([1, 256, 14, 14])\nX.shape:\ntorch.Size([1, 256, 14, 14])\n-----------------------------------\n-----------------------------------\nX.shape:\ntorch.Size([1, 256, 14, 14])\nY.shape:\ntorch.Size([1, 256, 14, 14])\nY.shape:\ntorch.Size([1, 256, 14, 14])\nY.shape:\ntorch.Size([1, 1024, 14, 14])\nY.shape:\ntorch.Size([1, 256, 14, 14])\nX.shape:\ntorch.Size([1, 256, 14, 14])\nX.shape:\ntorch.Size([1, 256, 14, 14])\n-----------------------------------\nSequential output shape:\t torch.Size([1, 256, 14, 14])\nX.shape:\ntorch.Size([1, 256, 14, 14])\n-----------------------------------\nX.shape:\ntorch.Size([1, 256, 14, 14])\nY.shape:\ntorch.Size([1, 512, 7, 7])\nY.shape:\ntorch.Size([1, 512, 7, 7])\nY.shape:\ntorch.Size([1, 2048, 7, 7])\nY.shape:\ntorch.Size([1, 512, 7, 7])\nX.shape:\ntorch.Size([1, 256, 14, 14])\nX.shape:\ntorch.Size([1, 512, 7, 7])\n-----------------------------------\n-----------------------------------\nX.shape:\ntorch.Size([1, 512, 7, 7])\nY.shape:\ntorch.Size([1, 512, 7, 7])\nY.shape:\ntorch.Size([1, 512, 7, 7])\nY.shape:\ntorch.Size([1, 2048, 7, 7])\nY.shape:\ntorch.Size([1, 512, 7, 7])\nX.shape:\ntorch.Size([1, 512, 7, 7])\nX.shape:\ntorch.Size([1, 512, 7, 7])\n-----------------------------------\n-----------------------------------\nX.shape:\ntorch.Size([1, 512, 7, 7])\nY.shape:\ntorch.Size([1, 512, 7, 7])\nY.shape:\ntorch.Size([1, 512, 7, 7])\nY.shape:\ntorch.Size([1, 2048, 7, 7])\nY.shape:\ntorch.Size([1, 512, 7, 7])\nX.shape:\ntorch.Size([1, 512, 7, 7])\nX.shape:\ntorch.Size([1, 512, 7, 7])\n-----------------------------------\nSequential output shape:\t torch.Size([1, 512, 7, 7])\nX.shape:\ntorch.Size([1, 512, 7, 7])\nAdaptiveAvgPool2d output shape:\t torch.Size([1, 512, 1, 1])\nX.shape:\ntorch.Size([1, 512, 1, 1])\nFlatten output shape:\t torch.Size([1, 512])\nX.shape:\ntorch.Size([1, 512])\nLinear output shape:\t torch.Size([1, 10])\n```\n\n**Do you see what is the issue?**\n\nThank you in advance!", "upvote_ratio": 0.57, "id": "t3_p7b464", "created_utc": 1629363372.0}
{"sub": "pytorch", "title": "Does torchserve in AWS scale to have equal inference time for per request for any number of parallel requests?", "selftext": "If I manually launch an ec2 server with pytorch inference, the inference time will depend on the resources I configured and the number of users. When many users request in parallel, inference time will increase (due to limited resources and waiting).\n\nThe requirement is: inference time per image per user should be less than 100 ms. Is there any way I can ensure this is met regardless of number of parallel requests? Is this possible with SageMaker?", "upvote_ratio": 1.0, "id": "t3_p6duzx", "created_utc": 1629238412.0}
{"sub": "pytorch", "title": "can anyone send a ZIP download link for the MNIST dataset?", "selftext": "the fricking download server is down for some reason and im stuck without the dataset", "upvote_ratio": 0.43, "id": "t3_p6296v", "created_utc": 1629201261.0}
{"sub": "pytorch", "title": "What's wrong with my code?", "selftext": "import torch\nimport torchvision\nfrom torchvision import transforms, datasets\n\ntrain = datasets.MNIST(\"\", train=True, download=True,\ntransform=transforms.Compose([transforms.ToTensor()]))\n\ntest = datasets.MNIST(\"\", train=False, download=True,\ntransform=transforms.Compose([transforms.ToTensor()]))", "upvote_ratio": 0.16, "id": "t3_p5czq1", "created_utc": 1629107873.0}
{"sub": "pytorch", "title": "FastAi article, enjoy", "selftext": "nan", "upvote_ratio": 0.38, "id": "t3_p561sz", "created_utc": 1629076756.0}
{"sub": "pytorch", "title": "Can I use pytorch for detecting a hotword? How good will it be?", "selftext": "And how hard will I work?", "upvote_ratio": 0.2, "id": "t3_p4i9oi", "created_utc": 1628982845.0}
{"sub": "pytorch", "title": "Considering getting started with machine Learning and neural networks, is pytorch good for me?", "selftext": "nan", "upvote_ratio": 0.42, "id": "t3_p4i93x", "created_utc": 1628982788.0}
{"sub": "pytorch", "title": "Issue with grad_fn = None", "selftext": "Why `AttributeError: 'ConvEdge' object has no attribute 'grad_fn'` for [https://github.com/promach/gdas/blob/main/gdas.py#L167](https://github.com/promach/gdas/blob/main/gdas.py#L167) ?\n\nis there some pytorch visualization tool that I could use to debug my current situation ?  \n\n\nhttps://preview.redd.it/b4q7cqfgrch71.png?width=980&amp;format=png&amp;auto=webp&amp;s=1852404cadb9eaa65bb153bd4b5409fea8edee00", "upvote_ratio": 0.33, "id": "t3_p3kmzc", "created_utc": 1628850110.0}
{"sub": "pytorch", "title": "Nvidia Releases CUDA Python", "selftext": "nan", "upvote_ratio": 0.95, "id": "t3_p31uwc", "created_utc": 1628781210.0}
{"sub": "pytorch", "title": "PyTorch to CoreML", "selftext": "Hi,\n\nI'm looking for someone to help me with my project.\n\nI have found a project on github that builds a classifier based on audio using pytorch.\n\nMy plan is to convert this model using coreml so it can be used in my ios app.\n\nIt's a simple task and I will pay you $50 if you can do it by the end of this week.\n\nMessage me for details\n\nthanks\n\njk", "upvote_ratio": 0.25, "id": "t3_p2rlvs", "created_utc": 1628739134.0}
{"sub": "pytorch", "title": "Mixup: Implementation + Experiments", "selftext": "nan", "upvote_ratio": 0.86, "id": "t3_p1rtel", "created_utc": 1628610281.0}
{"sub": "pytorch", "title": "How to construct this network in pytorch with onnx on netron", "selftext": "Hi, I am trying to reconstruct this graph using pytorch. I am unable to generate slice and transpose, since the GRU is generating the rest of it by itself.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/ik3putyhecg71.png?width=471&amp;format=png&amp;auto=webp&amp;s=1ec7a3880b893e193ac8bc12c37faf5166d04026\n\nHow do I reconstruct the exact same network?\n\nI am using the given code to construct the graph\n\n    x = torch.randn(batch_size, frames,  161, requires_grad=True)\n    torch_out = model(x)\n    \n    # Export the model\n    torch.onnx.export(model,               # model being run\n                      x,                         # model input (or a tuple for multiple inputs)\n                      \"super_resolution.onnx\",   # where to save the model (can be a file\n    or file-like object)\n                      export_params=True,        # store the trained parameter weights inside the model file\n                      opset_version=10,          # the ONNX version to export the model to\n                      do_constant_folding=True,  # whether to execute constant folding for optimization\n                      input_names = ['input'],   # the model's input names\n                      output_names = ['output'], # the model's output names\n                      dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n                                    'output' : {0 : 'batch_size'}})", "upvote_ratio": 0.9, "id": "t3_p12bt7", "created_utc": 1628518601.0}
{"sub": "pytorch", "title": "PyTorch Dataset class as input to YOLO", "selftext": "I have searched everywhere, but I can't find an example of someone writing their own `Dataset` classes to feed data into a PyTorch YOLO implementation. Everyone just formats a dataset as a directory structure with one bounding box file per image and points the network to that. \n\nBut I want to feed my own dataset in using the `torch.utils.data.Dataset` class. How do I do that? What format do these networks expect the training examples to be in?\n\nOr has someone already done this and somehow I just missed it?\n\nThanks in advance!", "upvote_ratio": 1.0, "id": "t3_p06py7", "created_utc": 1628390758.0}
{"sub": "pytorch", "title": "How can I learn about generative deep learning? VAEs &amp; GANs? Prereqs? DL newbie", "selftext": "\n\nHello all, I\u2019m a statistics student in my university and I\u2019ve had experience with using machine learning algorithms for data mining. My background in ML has come from the approach of introduction to statistical learning, and how to use these machine learning algorithms and apply them to tabular datasets. Learning about machine learning came form my background and exposure to statistical inference/probability theory and regression/glms + Bayesian statistics. \n\nI had never ventured into deep learning and neural networks yet because I had never found a reason to. I really love statistics, but I could never find a niche in deep learning which really clicked \u201cstatistically\u201d for me. But then I found it, and I\u2019ve heard about these things known as \u201cGenerative Networks\u201d and architectures such as GANs and VAEs.\n\nJust reading about it it felt very similar to bayesian concepts and I just thought it was something I wanted to try and get my hands on learning. For programming experience, I know python and R, and I\u2019ve used packages like sklearn/tidymodels for the classical machine learning algorithms for data analysis, but my deep learning experience is quite limited. \n\nThe most I have done was built like one CNN to classify images but it wasn\u2019t really all that great and it was in Tensorflow. However I have planned on learning pytorch, but needed a reason to and I think I will try and learn generative networks using pytorch.\n\nI would like some help on where to go next however\u2026 I guess I should first choose what it is I want to learn within generative models, which I think GANs or VAEs. But what do I need to do to first understand how these work? How the math works? I have some lin alg background, but I feel like there\u2019s a lot I need to do before I can start coding this in pytorch. I have read some medium articles to first learn what generative networks are and the basis of them, but nothing more. Any advice would be appreciated for a deep learning newbie like myself. Thanks!", "upvote_ratio": 0.75, "id": "t3_ozu9w1", "created_utc": 1628346554.0}
{"sub": "pytorch", "title": "Model based on input/label tensor", "selftext": "Complete beginner here:\nBasically I have my two tensors, one for input and one for the output respectively label.\n\nHow do I write a model that takes the input tensor (list of strings that were character for character substituted with numbers as in a labelencoder)\nand output tensor (just one number).\n\nMy plan is to make a text comparison, which compares two or more strings (therefore character substitution instead of word substitution) and compares them on minor differences.", "upvote_ratio": 1.0, "id": "t3_oz1si8", "created_utc": 1628234563.0}
{"sub": "pytorch", "title": "RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [4, 3, 32, 32]], which is output 0 of torch::autograd::CopyBackwards, is at version 5; expected version 1 instead. Hint: enable anomaly detection", "selftext": "For [https://gist.github.com/promach/b6f526c56e20f029d68e6f9041c3f5c0#file-gdas-py-L394](https://gist.github.com/promach/b6f526c56e20f029d68e6f9041c3f5c0#file-gdas-py-L394) , why **RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation:  \\[torch.cuda.FloatTensor \\[4, 3, 32, 32\\]\\], which is output 0 of  torch::autograd::CopyBackwards, is at version 5; expected version 1  instead. Hint: enable anomaly detection to find the operation that  failed to compute its gradient, with  torch.autograd.set\\_detect\\_anomaly(True).** ?\n\n&amp;#x200B;\n\nhttps://preview.redd.it/5n4pxjyjbif71.png?width=1922&amp;format=png&amp;auto=webp&amp;s=88bc54de821a5ef6cd0fca569be555825980dfe5", "upvote_ratio": 1.0, "id": "t3_oydvet", "created_utc": 1628155153.0}
{"sub": "pytorch", "title": "Hands-On Workshop: Accelerate PyTorch Applications Using Intel oneAPI Toolkit", "selftext": "Analytics India Magazine, in association with Intel\u00ae, has put together a hands-on virtual workshop on August 18, 2021, to unpack Intel\u00ae Extension for PyTorch\\*. The participants will learn how to train a model using Intel\u00ae Extension for PyTorch\\* and use the PyTorch extensions for inference. Expert trainers from Intel will also demonstrate how to accelerate AI inference performance with Intel\u00ae Distribution of OpenVINO\u2122 Toolkit through ONNX.  \n\n[https://register.gotowebinar.com/register/5241945191813665294](https://register.gotowebinar.com/register/5241945191813665294)", "upvote_ratio": 1.0, "id": "t3_oycpe9", "created_utc": 1628149384.0}
{"sub": "pytorch", "title": "What is the PyTorch equivalent of this Keras code?", "selftext": "I am using a data generated from a Keras code to train my PyTorch model (using numpy as a bridge). I couldn't find a 1:1 solution in PyTorch as of now. I'm using PyTorch 1.7, that's available by default in Kaggle.\n\n    train_image_generator = ImageDataGenerator( \n        rescale=1./255, \n        featurewise_center=True, \n        featurewise_std_normalization=True, \n        rotation_range=90, width_shift_range=0.15, \n        height_shift_range=0.15, \n        horizontal_flip=True, \n        zoom_range=[0.9, 1.25], \n        brightness_range=[0.5, 1.5] \n    )\n    test_image_generator = ImageDataGenerator(rescale=1./255)", "upvote_ratio": 1.0, "id": "t3_oyclse", "created_utc": 1628148871.0}
{"sub": "pytorch", "title": "How do I train a pretrained JIT model?", "selftext": " I am trying to perform transfer learning using one of the silerio STT models located in the torch hub. The model loads as a JIT model which I am entirely unfamiliar with.\n\nDoes anyone know how I can work with this model as I would a traditional pytorch module? I want to be able to freeze / unfreeze layers, remove layers, etc, as well as use it in a training loop.\n\nThank you for your help!", "upvote_ratio": 1.0, "id": "t3_oxw999", "created_utc": 1628094169.0}
{"sub": "pytorch", "title": "FIVE WAYS TO INCREASE MODEL PERF W/ PYTORCH PROFILER!", "selftext": "nan", "upvote_ratio": 0.75, "id": "t3_oxl8rq", "created_utc": 1628052492.0}
{"sub": "pytorch", "title": "Why grad_fn = None", "selftext": "Why `grad_fn = None` for  [https://github.com/promach/gdas/blob/main/gdas.py#L402](https://github.com/promach/gdas/blob/main/gdas.py#L402) ?  \n\n\nhttps://preview.redd.it/mjaemqb915f71.png?width=1922&amp;format=png&amp;auto=webp&amp;s=604a115065075e2e8733a5242d867cba50ca3bc9", "upvote_ratio": 0.25, "id": "t3_ox2lfo", "created_utc": 1627994280.0}
{"sub": "pytorch", "title": "Why is the PyTorch model doing worse than the same model in Keras even with the same weight initialization?", "selftext": "I made the exact same shallow convolutional network (6 Conv layers, 2 fully connected layers) in  PyTorch and Keras. I used glorot\\_uniform to initialize the weights of Conv2d in PyTorch (as they are by default in Keras). Yet the Keras model reached 97% validation set accuracy while the PyTorch model only reached 85%.", "upvote_ratio": 0.92, "id": "t3_ox0g4e", "created_utc": 1627985680.0}
{"sub": "pytorch", "title": "error w torchvision.io loading video (help greatly appreciated!)", "selftext": "I'm new to pytorch and torchvision, and I keep getting this error whenever I try to run code to read a video.\n\nRuntimeError: Not compiled with video\\_reader support, to enable video\\_reader support, please install ffmpeg (version 4.2 is currently supported) and build torchvision from source.\" \n\nI'm working in an anaconda environment. These are the versions I'm working with:\n\n ffmpeg version 4.2.2 Copyright (c) 2000-2019 the FFmpeg developers  \ntorchvision\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 0.10.0  \nffmpeg\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 4.2.2  \nffmpeg-python\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 0.2.0 \n\n Python 3.9.4 (default, Apr\u00a0 9 2021, 16:34:09 \n\n&amp;#x200B;\n\nI'm running the python file as you regularly would. I'm not sure what they mean by \"build torchvision from source\" or if that entails some extra commands/steps I'm missing. I've looked online but I haven't found anything helpful yet, so I thought I'd ask here. Thanks!", "upvote_ratio": 0.84, "id": "t3_owkij9", "created_utc": 1627928587.0}
{"sub": "pytorch", "title": "Real-Time Training", "selftext": "Hi everyone. I am a rookie on Pytorch Learning and I\u2019m searching about a way to learn about live train my regression models on new entries at the dataset. I\u2019ve got the best checkpoint of the training, but I don\u2019t know the following steps. Could you give me some advice?\n\nCheers", "upvote_ratio": 1.0, "id": "t3_ov9ihj", "created_utc": 1627746566.0}
{"sub": "pytorch", "title": "Graph Attention Networks v2: Annotated implementation", "selftext": "[Implementation with side-by-side notes](https://nn.labml.ai/graphs/gatv2/index.html)\n\nGATv2 is an improvement over Graph Attention Networks (GAT). They show GAT has static attention. i.e., the attention ranks (ordered by the magnitude of attention) for key-nodes are the same for every query-node. They introduce GATv2 that overcomes this limitation by applying the attention scoring linear layer after the activation.\n\n* [Twitter thread](https://twitter.com/labmlai/status/1421350760638418948)\n* [GAT v2 Paper](https://arxiv.org/abs/2105.14491)\n* [GAT annotated implementation](https://nn.labml.ai/graphs/gat/index.html)\n* [GAT Paper](https://arxiv.org/abs/1710.10903)", "upvote_ratio": 1.0, "id": "t3_ov70vc", "created_utc": 1627737599.0}
{"sub": "pytorch", "title": "I'm trying to follow along this beginner machine learning game youtube video using pytorch and unity. How come pytorch exports models as .onnx files instead of .nn? Unity doesn't seem to accept .onnx files.", "selftext": "This is the video:\n\n[https://youtu.be/axF\\_nHHchFQ](https://youtu.be/axF_nHHchFQ)\n\nHere's what it looks like after training\n\nhttps://preview.redd.it/1cfsnagu4fe71.png?width=2568&amp;format=png&amp;auto=webp&amp;s=452e273874c30734299abaeab4e268b54366918b", "upvote_ratio": 0.67, "id": "t3_ouuelm", "created_utc": 1627680730.0}
{"sub": "pytorch", "title": "Implementation of LR-CN", "selftext": "Hi, is there any resources about Long-Term Recurrent Convolutional Network that implemented with PyTorch?", "upvote_ratio": 1.0, "id": "t3_otzu8n", "created_utc": 1627572981.0}
{"sub": "pytorch", "title": "New user here. Some advice or help is needed on getting LSTM working", "selftext": "Hello,\n\nI am new to PyTorch and really need advice.\n\nI can see plenty of tutorials that include hidden states in training `LSTM`. When I try to do the same I get an error. I was wondering if it's required to gather output value from the hidden states and then pass it as a second parameter in another iteration `model(inputs, (h, c))`?\n\nThe model is a trivial sequence \"predictor\", which is going to suggest a next element based on a previous one. Because the input is a sequence, I thought `RNN` and specifically `LSTM` would be a better approach than a regular net.\n\nSo far, I've got the following training loop\n\n    for _ in range(50):\n        for x, c in train_loader:\n            x = x.view(1, 1, -1)\n            optimiser.zero_grad()\n            m_out, h = model(x, h)\n            loss = loss_function(m_out.view(1, -1), c)\n            loss.backward()\n            optimiser.step()\n\nInputs are tensors of `1 x 1 x C`, where `C` a is a one-hot vector with `1` at corresponding index, e.g. `[0, 0, 0, 1, 0, ...]` would indicate 4th class. I use ASCII characters as classes. Hidden states are initialised with zeros\n\n    h = (torch.zeros(1, 1, n_features), torch.zeros(1, 1, n_features))\n\nLoss function, model, and an optimiser are also set\n\n    model = TextPredictor(n_features, n_features)\n    loss_function = nn.NLLLoss()\n    optimiser = torch.optim.SGD(model.parameters(), lr=0.1)\n\nI am trying to test two versions of the model. One where I handle initial states and get the following error\n\n&gt; RuntimeError: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling .backward() or autograd.grad() the first time.\n\nThe other where I remove hidden states\n\n    m_out = model(x)\n\nThe latter version is running through the training loop but it seems that without handling the inner state `(h, c)`, the model is not working after training; it always returns the same class regardless of the input.\n\nI was hoping perhaps someone can give a few clues on how to improve the model.\n\nThanks", "upvote_ratio": 1.0, "id": "t3_otyn0t", "created_utc": 1627569186.0}
{"sub": "pytorch", "title": "Help needed with RNNs!", "selftext": "Hello,\n\nI understand Recurrence Neural Networks are used frequently in natural language processing or similar processes. For example, predicting the next word in the sentence: \"I took my dog for a \\_\\_\\_\" (walk). As an analogy, I'm trying to instead use RNNs for a version of generating what can be seen as a \"coherent\" sentence, given multiple other sentences.\n\nWhat I'm really trying to do is, given multiple sets of points, and how each set of points is partitioned, I'm trying to find how I would partition another set of points in order to match the pattern of the previous sets.\n\nRNNs seemed like a good method to do this, purely because of their high data interdependence (nearby data affecting other data), which I want so that the decision of what cluster a specific point belongs to affects future decisions within the same set. Has anyone found an RNN implementation of this? I am really struggling to find an implementation that has continuous input, classification output, and finite sequence length.\n\nAny help would really, really be appreciated. I believe this deals with many to many RNNs.", "upvote_ratio": 1.0, "id": "t3_otrtel", "created_utc": 1627539065.0}
{"sub": "pytorch", "title": "Long video data loading for RNNs", "selftext": "Hey guys,\n\nHad a lot of trouble with working out how to load long videos into my model, lets say I need to feed in 200 frames to my LSTM layer, but I don't have the memory to pass the entire 200 frame segment into my model. How do I go about doing this? I could use a batch size of 1 and then run 20 forward passes holding 10 frames and passing a segment length of 200 to my LSTM layer? I could run all 200 frames through the convolution layers, return them, then run all those 200 frames through the LSTM layers? I think I'm being a fool and there's probably an easier way. Can anyone confirm if this is an ok way to do this or if there is an easier way?", "upvote_ratio": 0.75, "id": "t3_othmpg", "created_utc": 1627502194.0}
{"sub": "pytorch", "title": "Long-term series forecasting with Query Selector -- efficient model of sparse attention", "selftext": "We would like to share with you our latest development in artificial intelligence - QuerySelector. This is the SOTA (State of the Art) in this field.\n\nOn Papers with Code, you have a link to arXiv and our Github.\n\n[https://paperswithcode.com/paper/long-term-series-forecasting-with-query](https://paperswithcode.com/paper/long-term-series-forecasting-with-query)\n\nFeel free to ask questions", "upvote_ratio": 0.72, "id": "t3_ot7giu", "created_utc": 1627469353.0}
{"sub": "pytorch", "title": "Voice recognition - Speech to Text", "selftext": "I  am looking for implementing Speech to Text system in Python 3.X. Rather  than reinventing the wheel, is there some Amazon Alexa/Google or some  other API which I can easily use to implement this? If yes can you point  me towards it's tutorials?\n\nThanks!", "upvote_ratio": 1.0, "id": "t3_osov52", "created_utc": 1627399082.0}
{"sub": "pytorch", "title": "A collection of some of the best PyTorch courses for beginners to learn PyTorch online", "selftext": "nan", "upvote_ratio": 0.94, "id": "t3_osmzfu", "created_utc": 1627393028.0}
{"sub": "pytorch", "title": "Training loss stays the same", "selftext": "I managed to get [my own GDAS code implementation](https://github.com/promach/gdas) up and running.\n\nHowever, the loss stay the same which indicates the training process is still incorrect.\n\nCould anyone advise ?\n\nhttps://preview.redd.it/k4czo8g58rd71.png?width=980&amp;format=png&amp;auto=webp&amp;s=88db8a643e3282bac66df89f1ee2e87c12edeabb", "upvote_ratio": 1.0, "id": "t3_osmhch", "created_utc": 1627391246.0}
{"sub": "pytorch", "title": "PyTorch batchnorm2d in numpy", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_os2al2", "created_utc": 1627316829.0}
{"sub": "pytorch", "title": "RuntimeError: expand(torch.cuda.FloatTensor{[3, 3, 3, 3]}, size=[]): the number of sizes provided (0) must be greater or equal to the number of dimensions in the tensor (4)", "selftext": "Why  `[3, 3, 3, 3]` for the [variable w](https://github.com/promach/gdas/blob/main/gdas.py#L469) ?  \n\n\nhttps://preview.redd.it/yyztk1dt5ed71.png?width=1922&amp;format=png&amp;auto=webp&amp;s=a0beb6008332b9978bd628eb23900628cc960874", "upvote_ratio": 0.25, "id": "t3_orfi7v", "created_utc": 1627233064.0}
{"sub": "pytorch", "title": "Excessive CPU RAM being used even inside .cuda() mode", "selftext": "I am having issue with excessive CPU RAM usage with [this coding](https://github.com/promach/gdas) even inside `.cuda()` mode\n\nCould anyone advise ?", "upvote_ratio": 0.81, "id": "t3_oqsxm2", "created_utc": 1627142936.0}
{"sub": "pytorch", "title": "ValueError: optimizer got an empty parameter list (nn.parameter is not persistent across parent classes)", "selftext": "how to make `nn.parameter()` persists across [parent classes](https://gist.github.com/promach/b6f526c56e20f029d68e6f9041c3f5c0#file-gdas-py-L291) ?\n\nin my coding: class Graph \u2192 class Cells \u2192 class nodes \u2192 class Connections \u2192 class Edge\n\nthe `nn.parameter()` is located inside class Edge\n\n&amp;#x200B;\n\nhttps://preview.redd.it/1b5wuo6orzc71.png?width=980&amp;format=png&amp;auto=webp&amp;s=bb23c9260b167e6274461774b0b9ded1de74a939", "upvote_ratio": 1.0, "id": "t3_oq6dkm", "created_utc": 1627058865.0}
{"sub": "pytorch", "title": "Need help with environment setup", "selftext": "Hello everyone. I would like to apologize in advance if this is not the right community to post to, but I thought that I should start from somewhere. \n\nTLDR: Couldn't setup the necessary packages for an environment that I want to train using PyTorch (have tried Conda and Docker). Running on Ubuntu 16.04.\n\nI am currently new to PyTorch. Basically I am trying to setup an environment with PyTorch, CUDA, Torchvision, Tensorboard, and OpenCV. Seems like an easy task, but I am currently stuck at this for a few days. I have tried Docker, and one issue with it is not enough shared memory (kernel dying) when I try to run training via a Jupyter notebook.\n\nI've also tried Conda too, but I am currently stuck with actually even installing all the necessary packages that I have listed. I've installed various versions of Anaconda, even installing using the terminal and even the navigator, but I still couldn't install all of the necessary packages. The best I got so far was managing to install some packages separately using the terminal and navigator, but after when I'm done, PyTorch is unable to run on \"cuda:0\" device despite being able to detect it as a 10.2 version.\n\nI am currently stumped right now, and would appreciate it if anyone could give me some pointers on how should I go.\n\nP.S: I am currently using Linux Ubuntu 16.04.", "upvote_ratio": 1.0, "id": "t3_opye1w", "created_utc": 1627030256.0}
{"sub": "pytorch", "title": "RuntimeError: The size of tensor a (5) must match the size of tensor b (32) at non-singleton dimension 3", "selftext": "For the **RuntimeError: The size of tensor a (5) must match the size of tensor b (32) at non-singleton dimension 3** , may I know why tensor b is of size 32 ? and what does it exactly mean by \u201csingleton dimension 3\u201d ?\n\nThe code could be [found here](https://gist.github.com/promach/b6f526c56e20f029d68e6f9041c3f5c0#file-gdas-py-L298-L300).  \n\n\nhttps://preview.redd.it/u0rjkz9uipc71.png?width=960&amp;format=png&amp;auto=webp&amp;s=d2d406c5c5276f1be03f3b4cb4e0ae9dce9aefa5", "upvote_ratio": 1.0, "id": "t3_op860n", "created_utc": 1626934780.0}
{"sub": "pytorch", "title": "Issue with MNIST dataset when applied to Echo State Network", "selftext": "Hello everyone. I'm a beginner in PyTorch  and I'm kinda stuck on a project I'm working on. I've created an Echo State Network from scratch in torch and this is what I have so far. I've tried to do binary classification with it where it has to look at a block in a sequence and tell if it's a square or sine wave. It got 100% acc for that, however, when I try it on MNIST, it doesn't work so well. For the MNIST dataset, I follow the conventional way of interpreting as time-series, which is that each row is an input, and all the rows in chronological order make up a sequence. So like I said, I'm inexperienced in PyTorch and I know all this code could've been done in like numpy or something, but I just used PyTorch to get some practice in. But anyway, below is the code.\n\nThis is main.py\n\n    import model\n    \n    input_size = 28  # row of image\n    hidden_size = 100\n    output_size = 10\n    density = 0.1  # sparse connectivity between reservoir units\n    sigma_bias = 0.01  # if &gt; 0, then spectral radius of w_hh (hidden-to-hidden weights) are &lt; 1\n    \n    sequence_length = 28  # total number of rows in image\n    initial_state_forget_amt = 5  # dismiss initial transient of &lt;initial_state_forget_amt&gt; steps from initial zero hidden state\n    \n    # load training and testing data\n    dataset = model.torchvision.datasets.FashionMNIST(root='data/',\n                                                      train=True,\n                                                      download=True,\n                                                      transform=model.torchvision.transforms.ToTensor())\n    \n    test_dataset = model.torchvision.datasets.FashionMNIST(root='data/',\n                                                           train=False,\n                                                           download=True,\n                                                           transform=model.torchvision.transforms.ToTensor())\n    \n    # number of training sequences to loop through\n    start = 10\n    end = 100\n    \n    # initialize arrays that will record accuracy for every run of the loop\n    train_acc_array = model.np.zeros((end - start,))\n    test_acc_array = model.np.zeros((end - start,))\n    \n    # create model\n    ESN = model.ESN(input_size=input_size,\n                    hidden_size=hidden_size,\n                    output_size=output_size,\n                    density=density,\n                    sigma_bias=sigma_bias)\n    \n    # apply random (seeded) permutation on training and testing data\n    model.np.random.seed(0)\n    idxs = model.np.random.permutation(60000)\n    \n    training_data = dataset.data[idxs, :, :]\n    training_data_targets = dataset.targets[idxs]\n    training_data = model.torch.permute(training_data, [1, 2, 0])\n    \n    model.np.random.seed(10)\n    test_idxs = model.np.random.permutation(10000)\n    \n    test_data = test_dataset.data[test_idxs, :, :]\n    test_data_targets = dataset.targets[test_idxs]\n    test_data = model.torch.permute(test_data, [1, 2, 0])\n    \n    train_input_signal = model.torch.empty(0)\n    train_output_signal = model.torch.empty(0)\n    test_input_signal = model.torch.empty(0)\n    test_output_signal = model.torch.empty(0)\n    \n    valid_states_amt = sequence_length - initial_state_forget_amt\n    \n    for train_num_sequences in range(start, end):\n    \n        test_num_sequences = int(0.3 * train_num_sequences)\n    \n        # isolate data up to &lt;train_num_sequences&gt;\n        # one-hot encoded output but repeated 28 times for every input of sequence (every row of image)\n    \n        train_input_signal = training_data[:, :, :train_num_sequences] / 255\n        e_train = model.torch.zeros(output_size, train_num_sequences)\n        e_train[training_data_targets[:train_num_sequences], range(train_num_sequences)] = 1\n        train_output_signal = e_train.t().repeat(1, sequence_length).view(-1, output_size).t()\n    \n        test_input_signal = test_data[:, :, :test_num_sequences] / 255\n        e_test = model.torch.zeros(output_size, test_num_sequences)\n        e_test[test_data_targets[:test_num_sequences], range(test_num_sequences)] = 1\n        test_output_signal = e_test.t().repeat(1, sequence_length).view(-1, output_size).t()\n    \n        # create hidden matrix whose columns correspond to reservoir states of every row of every sequence\n        # &lt;initial_state_forget_amt&gt; steps during initial transient have been discarded within &lt;create_hidden_states_matrix()&gt;\n    \n        hidden_states_matrix = ESN.create_hidden_states_matrix(input_signal=train_input_signal,\n                                                               sequence_length=sequence_length,\n                                                               initial_state_forget_amt=initial_state_forget_amt)\n    \n        test_hidden_states_matrix = ESN.create_hidden_states_matrix(input_signal=test_input_signal,\n                                                                    sequence_length=sequence_length,\n                                                                    initial_state_forget_amt=initial_state_forget_amt)\n    \n        # create empty tensor \"a,\" perform mode-1 unfolding on training/testing output signals and store in &lt;a/b&gt; respectively.\n        a = model.torch.empty(0)\n        b = model.torch.empty(0)\n    \n        for i in range(train_num_sequences):\n            a = model.torch.hstack(\n                [a, train_output_signal[:, ((i * sequence_length) + initial_state_forget_amt):((i + 1) * sequence_length)]])\n    \n        for j in range(test_num_sequences):\n            b = model.torch.hstack(\n                [b, test_output_signal[:, ((j * sequence_length) + initial_state_forget_amt):((j + 1) * sequence_length)]])\n    \n        # find hidden-to-output weights\n        ESN.fit(hidden_states=hidden_states_matrix, target_tensor=a)\n    \n        # calculate outputs as probabilities, using softmax, based on newly found h2o weights\n        calculated_probs = ESN(hidden_states_matrix)\n        calculated_test_probs = ESN(test_hidden_states_matrix)\n    \n        # --------calculate accruracy----------\n    \n        # find max probability along columns\n        _, max_idxs_train = model.torch.max(calculated_probs, dim=0)\n        _, max_idxs_test = model.torch.max(calculated_test_probs, dim=0)\n        train_output = model.torch.empty(0)\n        test_output = model.torch.empty(0)\n    \n        # an image must be classified but right now, we still have 28 outputs assigned to a single image\n        # take mode of every set of 28 outputs of all images in &lt;max_idxs_train/test&gt;\n    \n        for i in range(int(calculated_probs.shape[1] / valid_states_amt)):\n            v_train, _ = model.torch.mode(max_idxs_train[(valid_states_amt * i):(valid_states_amt * (i + 1))])\n            train_output = model.torch.hstack([train_output, v_train])\n    \n        for j in range(int(calculated_test_probs.shape[1] / valid_states_amt)):\n            v_test, _ = model.torch.mode(max_idxs_test[(valid_states_amt * j):(valid_states_amt * (j + 1))])\n            test_output = model.torch.hstack([test_output, v_test])\n    \n        # find ground truth\n        _, train_ground_truth = model.torch.max(train_output_signal[:, 28 * model.np.array(range(train_num_sequences))],\n                                                dim=0)\n        _, test_ground_truth = model.torch.max(test_output_signal[:, 28 * model.np.array(range(test_num_sequences))], dim=0)\n    \n        # calculate accuracy\n        train_accuracy = model.accuracy(calculated_output=train_output,\n                                        ground_truth=train_ground_truth,\n                                        num_of_elements=train_num_sequences)\n    \n        test_accuracy = model.accuracy(calculated_output=test_output,\n                                       ground_truth=test_ground_truth,\n                                       num_of_elements=test_num_sequences)\n    \n        # store in accuracies in array\n        train_acc_array[train_num_sequences - start] = train_accuracy\n        test_acc_array[train_num_sequences - start] = test_accuracy\n\nAnd here is the file, model.py, containing my classes and functions.\n\n    import torch\n    import torchvision\n    import numpy as np\n    import math\n    import matplotlib.pyplot as plt\n    import random\n    \n    # ESN class\n    class ESN(torch.nn.Module):\n        def __init__(self, input_size, hidden_size, output_size, density, sigma_bias):\n            super().__init__()\n            self.input_size = input_size\n            self.hidden_size = hidden_size\n            self.output_size = output_size\n            self.i2h = torch.nn.Linear(self.input_size, self.hidden_size)\n            self.reservoir = Reservoir(self.hidden_size, density, sigma_bias)\n            self.h2o = torch.nn.Linear(self.hidden_size, self.output_size)\n            self.i2h.bias.data = torch.zeros(tuple(self.i2h.bias.data.shape))\n            self.h2o.bias.data = torch.zeros(tuple(self.h2o.bias.data.shape))\n            self.tanh = torch.nn.Tanh()\n            self.softmax = torch.nn.Softmax(dim=0)\n            self.sigmoid = torch.nn.Sigmoid()\n    \n        # return output based on current hidden state\n        def forward(self, hidden_states_n):\n            output = self.softmax(self.h2o(hidden_states_n.t()).t())              # output connected to current hidden state\n            return output\n    \n        # return next step hidden states\n        # input and hidden tensor form: (# of features, # of units)\n    \n        def compute_next_hidden_state(self, input_tensor, hidden_states_n):\n            i2h_term = self.i2h(input_tensor.t())\n            h2h_term = self.reservoir(hidden_states_n)\n            hidden_states_np1 = self.tanh(i2h_term + h2h_term).t()                  # hidden state at time n + 1\n            return hidden_states_np1\n    \n        # create initial zero hidden state\n        def init_hidden(self):\n            return torch.zeros(self.hidden_size, 1)\n    \n        # least squares solution of w_out (hidden-to-output) matrix\n        def fit(self, hidden_states, target_tensor):\n            # hidden and target tensor form: (# of features, # of units)\n            w_out = torch.matmul(torch.linalg.pinv(hidden_states.t()), target_tensor.t()).t()\n            self.h2o.weight.data = w_out\n    \n        # create hidden state matrix whose column n is the hidden state at time n\n        def create_hidden_states_matrix(self, input_signal, sequence_length, initial_state_forget_amt):\n            hidden_states_matrix = torch.empty(0)\n            for i in range(input_signal.shape[2]):\n                # run on training and testing data\n                current_hidden_state = self.init_hidden()\n                for j in range(sequence_length - 1):\n                    # begin recording hidden states once the initial_state_forget_amt has been passed to leverage ESN's state forgetting property\n                    if j == initial_state_forget_amt:\n                        hidden_states_matrix = torch.hstack([hidden_states_matrix, current_hidden_state])\n                    elif j &lt; initial_state_forget_amt:\n                        current_hidden_state = self.compute_next_hidden_state(input_tensor=coeff_s_u(input_signal[:, j + 1, i], 'u'),\n                                                                              hidden_states_n=current_hidden_state)\n                        continue\n                    hidden_state_np1 = self.compute_next_hidden_state(input_tensor=coeff_s_u(input_signal[:, j + 1, i], 'u'),\n                                                                      hidden_states_n=hidden_states_matrix[:, j - initial_state_forget_amt])\n                    hidden_states_matrix = torch.hstack([hidden_states_matrix, hidden_state_np1])\n    \n            return hidden_states_matrix\n    \n    # Reservoir class\n    class Reservoir(torch.nn.Module):\n        def __init__(self, hidden_size, density, sigma_bias):\n            super().__init__()\n            self.hidden_size = hidden_size\n            self.density = density\n    \n            # create a sparse tensor w_hh (hidden-to-hidden) that satisifes constraints of largest singular/eigenvalue &lt; 1\n            nnz_vals = math.ceil(self.density * self.hidden_size * self.hidden_size)\n            values = torch.normal(0, 1, (nnz_vals,))\n            self.w_hh = make_sparse_square_matrix(values, self.hidden_size)\n    \n            _, s, _ = torch.svd(self.w_hh)\n            s_max = s[0]\n    \n            self.w_hh = self.w_hh / (s_max + sigma_bias)\n    \n        # calculate h2h term in calculation of next step hidden states\n        def forward(self, hidden_states_n):\n            return coeff_s_u(torch.matmul(self.w_hh, hidden_states_n), 's')\n    \n    # squeeze or unsqueeze input_tensor based on certain requirements and calculations\n    def coeff_s_u(input_tensor, s_u):\n        dimension = input_tensor.ndim\n        coeff_in = torch.max(torch.tensor([0, dimension]))\n        if coeff_in != 0:\n            coeff_in = coeff_in / dimension\n        if s_u == 'u':\n            if input_tensor.ndim &lt; 2:\n                input_tensor = torch.unsqueeze(input_tensor, dim=int(coeff_in) * dimension).float()\n        elif s_u == 's':\n            if input_tensor.ndim &gt; 1:\n                input_tensor = torch.squeeze(input_tensor, dim=int(coeff_in) * dimension - 1).float()\n    \n        return input_tensor\n    \n    # create sparse square matrix\n    # will be used to create matrix representing hidden-to-hidden unit connections\n    \n    def make_sparse_square_matrix(values, mode_length):\n        flattened_sparse_tensor = torch.zeros(mode_length * mode_length,)\n    \n        rand_idx = random.sample(range(mode_length * mode_length), values.shape[0])\n        flattened_sparse_tensor[rand_idx] = values\n    \n        return torch.reshape(flattened_sparse_tensor, (mode_length, mode_length))\n    \n    # measures accuracy of calculated output based on ground truth labels\n    def accuracy(calculated_output, ground_truth, num_of_elements):\n        return float(torch.sum(calculated_output == ground_truth) / num_of_elements)\n\nSo the problem here that encountering is this.\n\nhttps://preview.redd.it/1zfzoner4pc71.png?width=640&amp;format=png&amp;auto=webp&amp;s=7c68529416b1f9a254af494017e6d33afbb73d7b\n\nFor the above case, I started at 10 training sequences and ended at 100. I've spent a lot of time trying to debug the code that I have and am unsure why the training acc drops off so quickly (overfitting that early wouldn't make sense, would it?) and why the testing acc is so low. I guess I can attribute the stagnation in the testing acc to me taking 30% of the number of training sequences, which, for such low numbers, will be very close to the previous number of testing sequences. To implement this ESN, I used [this](https://www.researchgate.net/publication/215385037_The_echo_state_approach_to_analysing_and_training_recurrent_neural_networks-with_an_erratum_note%27) paper. Also, I chose not to have any feedback from the outputs to the reservoir layer, which that paper suggests you can do. I'm not sure if this is the right place to post this but I'd appreciate any help. Thank you.\n\n&amp;#x200B;", "upvote_ratio": 0.88, "id": "t3_op7d9n", "created_utc": 1626931142.0}
{"sub": "pytorch", "title": "Why are the embeddings of tokens multiplied by $\\sqrt D$ (note not divided by square root of D) in a transformer?", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_op0z2t", "created_utc": 1626907580.0}
{"sub": "pytorch", "title": "Cloud for Deep Learning training", "selftext": " Hi  Guys, which Cloud platform should I pay for/use in order to perform  deep learning experiments? Google Colab Pro doesn't meet my requirements  since it doesn't let you use the service for more than 24 hours in one  go. My experiments might run for approximately 4 days, give or take.\n\nThanks!", "upvote_ratio": 1.0, "id": "t3_omjwpb", "created_utc": 1626583825.0}
{"sub": "pytorch", "title": "Issue with torch.load() with torch version 1.4", "selftext": "I can't resolve this issue where `torch.load()` can't read a zipfile `model.pth` and shows this:\n```sh\nRuntimeError: version &lt;= kMaxSupportedFileFormatVersion INTERNAL ASSERT FAILED\n```\n\nunfortunately i can't upgrade torch version as I'm bound to python2. Any suggestions will be of great help.", "upvote_ratio": 1.0, "id": "t3_omafr5", "created_utc": 1626548475.0}
{"sub": "pytorch", "title": "Is there a difference between gradient accumulation vs loss accumulation?", "selftext": "I need to minimize the mean/sum loss over a large number of samples, and I can only use a batch\\_size of 1. I decided to accumulate loss because I'm not sure if a batch size of 1 will result in a minimal mean loss.\n\nTo simulate accumulation, is there a difference in *loss target* between the following two snippets?\n\n    model.zero_grad()\n    for _ in range(batch_size):\n       x, target_y = ...\n       y = model(x)\n       loss = loss_function(y, target_y)\n       loss.backward()\n    optimizer.step()\n\nvs\n\n    model.zero_grad()\n    loss = 0\n    for _ in range(batch_size):\n        x, target_y = ...\n        y = model(x)\n        loss = loss + loss_function(y, target_y)\n    loss.backward()\n    optimizer.step()\n\nI think the 2nd snippet is less memory efficient because it needs to keep all gradients for all samples in memory, but I'm mainly interested in trying to understand if these two implementations lead to different solutions when minimizing the loss -even if it's only a subtle difference-.\n\nAny ideas?", "upvote_ratio": 1.0, "id": "t3_om1f7j", "created_utc": 1626516053.0}
{"sub": "pytorch", "title": "Is there a seq2seq model in time series analysis?", "selftext": "Most of the time, I always see machine translation. I did find one but it was on TF2, would love to see and study a seq2seq PyTorch model code. Thank you", "upvote_ratio": 1.0, "id": "t3_ols9l9", "created_utc": 1626476584.0}
{"sub": "pytorch", "title": "JAX Vs TensorFlow Vs PyTorch: A Comparative Analysis", "selftext": "nan", "upvote_ratio": 0.57, "id": "t3_olebus", "created_utc": 1626432062.0}
{"sub": "pytorch", "title": "finite difference method in DARTS code", "selftext": "For my [DARTS coding](https://gist.github.com/promach/b6f526c56e20f029d68e6f9041c3f5c0#file-gdas-py-L373-L374) , how to do the coding for *Ltrain(w+)* and *Ltrain(w-)* inside [equation (8) of DARTS paper](https://arxiv.org/pdf/1806.09055.pdf#page=4) ?\n\n&amp;#x200B;\n\nNote: Upon checking the definition of finite difference method at [https://mythrex.github.io/math\\_behind\\_darts/](https://mythrex.github.io/math_behind_darts/) , it seems that the above 2 lines of code are wrong.  \n\n\nbut the question is how to modify these 2 lines of code ?\n\n&amp;#x200B;\n\nhttps://preview.redd.it/91dieytsh7b71.png?width=1000&amp;format=png&amp;auto=webp&amp;s=0ecd10bb7f13a27413fe836303f6db5edbf40242", "upvote_ratio": 1.0, "id": "t3_ok8bkr", "created_utc": 1626281134.0}
{"sub": "pytorch", "title": "How can I use a CFD loss function with PyTorch3D?", "selftext": "I recently stubbed upon PyTorch3D from facebook and caught my attention this use case in which the model deforms one 3D model into another.\n\n[ Deform a sphere mesh to dolphin](https://i.redd.it/glbgwlsad5b71.gif)\n\n \n\nMy idea is to develop a topology optimization AI for CFD wind tunnel simulations. Basically, starting from a sphere, I want the model to create the most lift/drag efficient shape.\n\nSo can I use as a loss function for example lift divided by drag coefficient? Or should I use a 3D scalar field for the loss function? If that's the case, I don't know how could I pull this off. Any guidelines would be very much appreciated!\n\nThank you!", "upvote_ratio": 0.83, "id": "t3_ok0u2c", "created_utc": 1626255222.0}
{"sub": "pytorch", "title": "[Help] Change models.vgg19(pretrained = True) classification to binary classification", "selftext": "This is what I am looking at:\n\n[https://pytorch.org/tutorials/beginner/transfer\\_learning\\_tutorial.html#finetuning-the-convnet](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html#finetuning-the-convnet)\n\nI want to change the model to vgg19 and binary classification.\n\nThank you!", "upvote_ratio": 0.8, "id": "t3_oilo21", "created_utc": 1626071444.0}
{"sub": "pytorch", "title": "PyTorch Tutorial on Generative Adversarial Networks (GANs)", "selftext": "I have written a tutorial on building and training a GAN on MNIST using PyTorch. Feel free to check it out!\n\nLink: [https://taying-cheng.medium.com/building-a-gan-with-pytorch-237b4b07ca9a](https://taying-cheng.medium.com/building-a-gan-with-pytorch-237b4b07ca9a)", "upvote_ratio": 0.94, "id": "t3_ohm5np", "created_utc": 1625935530.0}
{"sub": "pytorch", "title": "MaskRCNN postprocessing doesn't seem to fit code in git", "selftext": "Hey folks,\n\nI'm currently trying to \"hack\" a MaskRCNN so that I get the full label distributions into the output.\n\nI was looking into the \"[postprocessing\\_detections](https://github.com/pytorch/vision/blob/25bc21dfcbb390f2f215e8f83aa5e028c77a0f24/torchvision/models/detection/roi_heads.py#L664)\" function defined in roi\\_heads - however, when I tried to replace it with my own and slightly altered version by copying it from the git and adding minor changes, it turns out that the function seems to work different than what I find on the git.\n\nMost importantly, it doesn't get 4 inputs - as the code in the git needs - but only 3, which seem to be head outputs, anchors and image shapes. I've found two other definitions of a function with the same name in completely different contexts ([RetinaNet](https://github.com/pytorch/vision/blob/25bc21dfcbb390f2f215e8f83aa5e028c77a0f24/torchvision/models/detection/retinanet.py#L398) and [SSD)](https://github.com/pytorch/vision/blob/25bc21dfcbb390f2f215e8f83aa5e028c77a0f24/torchvision/models/detection/ssd.py#L364)  which take 3 inputs; But it makes 0 sense why they'd play any role here, and also they'd want the arguments to be different types than what I get (those functions want dict, list list; MaskRCNN internally I get inputs torch.Tensor, list, list).\n\nDoes anyone have an idea how and why the methods seem to differ from what I find in the git? I literally can't find a code version that seems to fit what the MaskRCNN actuall does internally.\n\nIf it's important, here's the scheme of how I get to the model (and the error):\n\n    import torchvision\n    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n    \n    def postprocess_detections_new(self,\n                                   class_logits,    # type: Tensor\n                                   box_regression,  # type: Tensor\n                                   proposals,       # type: List[Tensor]\n                                   image_shapes     # type: List[Tuple[int, int]]\n                                   ):\n        --- code 1:1 copypasted from the postprocess_detections function in git ---\n    \n    \n    model.roi_heads.postprocess_detections = postprocess_detections_new\n    \n    output = model(example_input)\n\nWhere example\\_input is some random example image I put in that works perfectly well with the unaltered model. Doing only the insertion of the copypasted code, the model already doesn't work anymore due to mismatching number of expected/given arguments. \n\nThanks a lot in advance, would really appreciate if somebody could point me somewhere!", "upvote_ratio": 0.56, "id": "t3_ogy02j", "created_utc": 1625844705.0}
{"sub": "pytorch", "title": "Can someone explain DQN", "selftext": "Hi!\n\nI've been looking for tutorials on DQN and came across the official one from Pytorch. Can someone explain the math for me?\n\n [Reinforcement Learning (DQN) Tutorial \u2014 PyTorch Tutorials 1.9.0+cu102 documentation](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html#dqn-algorithm)", "upvote_ratio": 0.7, "id": "t3_og0bmv", "created_utc": 1625720219.0}
{"sub": "pytorch", "title": "Extending kerv2d to kerv3d in https://github.com/wang-chen/kervolution/blob/unfold/kervolution.py", "selftext": "Dear friends,\nI am trying to create custom kerv3d layer from the existing custom kerv2d layer given in the repository https://github.com/wang-chen/kervolution/blob/unfold/kervolution.py. Can anyone please suggest me suitable modifications to kerv2d layer for getting kerv3d. I want to replace conv3d layer with kerv3d for experimentation. Thank you.", "upvote_ratio": 0.8, "id": "t3_oeah5n", "created_utc": 1625501236.0}
{"sub": "pytorch", "title": "what type of network should i use?", "selftext": "Im attempting to make a self driving car in a game right now I have a network taking in 5 inputs from the game and I want 4 binary outputs ex\\[0,1,0,1\\] I've tried a bunch of functions/loss calculations and nothing I can get to work. What i  want on the output is a mutli hot vector. The code I'm using right now is below. Any advice is appreciated.\n\n&amp;#x200B;\n\n    import torch\n    import torchvision\n    from torchvision import transforms,datasets\n    import numpy as np\n    import torch.nn as nn\n    import torch.nn.functional as F\n    import torch.optim as optim\n    from torch.utils.data import TensorDataset, DataLoader\n    \n    \n    \n    np_load_old = np.load\n    np.load = lambda *a, **k: np_load_old(*a, allow_pickle=True, **k)\n    train_data = np.load('training_datav2.npy')\n    \n    np.load = np_load_old\n    \n    training_inputs= []\n    training_outputs= []\n    \n    for row in train_data:\n        training_inputs.append(row[0])\n        training_outputs.append(row[1])\n    \n    trainX= training_inputs[:len(training_inputs)//2]\n    testX= training_inputs[len(training_inputs)//2:]\n    trainY= training_outputs[:len(training_outputs)//2]\n    testY= training_outputs[len(training_outputs)//2:]\n    \n    \n    my_x = np.array(trainX) # a list of numpy arrays\n    my_y = np.array(trainY) # another list of numpy arrays (targets)\n    \n    tensor_x = torch.Tensor(my_x) # transform to torch tensor\n    tensor_y = torch.Tensor(my_y)\n    \n    trainDataset = TensorDataset(tensor_x,tensor_y)\n    \n    my_testx = np.array(testX) # a list of numpy arrays\n    my_testy = np.array(testY) # another list of numpy arrays (targets)\n    \n    tensor_testx = torch.Tensor(my_testx) # transform to torch tensor\n    tensor_testy = torch.Tensor(my_testy)\n    \n    testDataset = TensorDataset(tensor_testx,tensor_testy)\n    \n    class Net(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.fc1=nn.Linear(5,10)\n            self.fc2=nn.Linear(10,10)\n            self.fc3=nn.Linear(10,5)\n            self.fc4=nn.Linear(5,4)\n    \n        def forward(self,x):\n            x=F.relu(self.fc1(x))\n            x=F.relu(self.fc2(x))\n            x=F.relu(self.fc3(x))\n            x=(self.fc4(x))\n    \n    \n            return x\n    net = Net()\n    #print(net)\n    \n    optimizer= optim.Adam(net.parameters(),lr=0.001)\n    criterion = torch.nn.BCEWithLogitsLoss()\n    print(\"QUEUE ROCKEY MUSIC\")\n    EPOCHS = 100\n    \n    for epoch in range(EPOCHS):\n    \n        for data in trainDataset:\n            x,y=data\n    \n            net.zero_grad()\n            out=net(x)\n    \n    \n            loss= criterion(out,y)\n            loss.backward()\n            optimizer.step()\n    \n    \n        print(loss)\n    correct =0\n    total=0\n    with torch.no_grad():\n        for data in trainDataset:\n            x, y = data\n            out = net(x)\n            for idx, i in enumerate(out):\n    \n                if torch.argmax(i)==y[idx]:\n                    #print(\"out:\",out)\n                    #print(\"rounded\",torch.round_(out))\n                    # print(\"X:\",x)\n                    # print(\"yidx\",y)\n                    # print(\"i\",i)\n                    # print(\"torch\",torch.argmax(i))\n                    correct+=1\n                total+=1\n    guess=(net(torch.Tensor([11, 0, 1, 0, -0.18750000000000006])))\n    print( (guess))\n    print((guess&gt;0.5).float())\n    guessright=net(torch.tensor([6, 0, 1, 0, 0.3971354166666663]))\n    guessleft=net(torch.tensor([6, 0, 1, 0, -0.3971354166666663]))\n    \n    print(\"right\",(guessright&gt;0.5).float())\n    print(\"left\",(guessleft&gt;0.5).float())\n    print(\"right\",(guessright))\n    print(\"left\",(guessleft))\n    \n    print(\"Accuraccy:\",(correct/total))", "upvote_ratio": 0.8, "id": "t3_odqpll", "created_utc": 1625425094.0}
{"sub": "pytorch", "title": "Please help me with this!!", "selftext": "I am new to pytorch and torchtext module. I haven't used TEnsorflow as of now so the solution i am searching for now must be in PyTorch. \nI am trying to do multilabel classification using torchtext module in stackoverflow tag prediction dataset. I am confused how to do it when converting the csv file to torchtext.legacy.data.tabulardataset( label parameter as there are thousands label for one hot encoding of those labels), i am confused in this step. Can anyone do this project or help me with the syntax or provide me link to tutorial or notebook that has this project done. \nIf this question looks confusing, please tell me i will try to explain further.", "upvote_ratio": 0.67, "id": "t3_odcq9m", "created_utc": 1625370910.0}
{"sub": "pytorch", "title": "Setting up a TPU and Ubuntu VM instance for use with Pytorch on Google Cloud", "selftext": "Hi!\n\nI wrote [a gist, detailing how to setup a TPU and Ubuntu VM instance for use with Pytorch on Google Cloud](https://gist.github.com/visionscaper/91504d755ebf37bf440a24fe4b5ac84f). Since I thought it might be of use to members on this Reddit community I'm sharing it here.\n\nIf you have any comments or thoughts about it, please let me know!", "upvote_ratio": 1.0, "id": "t3_od7qln", "created_utc": 1625351110.0}
{"sub": "pytorch", "title": "Having a hard time understanding Torch serve, any help would be appreciated", "selftext": "so I started programming back in December, learned TF and Keras in April but immediately switched to PyTorch because they said it was more flexible.\n\nI currently want to make and practice producing restful APIs but doing it via Flask always is tedious, so I tried to search a more \"easier\" way and I saw Torch Serve. So as of now, I'm really having hard time understanding it due to lack of resources in the internet. I rarely use AWS Sagemaker because I run most of my models locally so this is pretty new to me.\n\nSo basically I'm pretty decent in making models in Juptyer/PyCharm but when it comes to deployment, I'm really having a hard time.", "upvote_ratio": 1.0, "id": "t3_obin5x", "created_utc": 1625133960.0}
{"sub": "pytorch", "title": "TFLite equivalent in Pytorch?", "selftext": "I know about Torchscript but I'm specifically interested in running exported Pytorch models in a memory-restricted Python environment, to which TFLite lends itself very well but which I don't see in Pytorch. Is there something like a lightweight Torchscript Python implementation that I don't know about (like a Pytorch Lite), or should I just try to export Pytorch -&gt; ONNX -&gt; TFLite? I also saw [Pytorch Mobile](https://pytorch.org/mobile/home/), which said it supports Linux, but it only ever mentions Android and iOS environments.\n\nThanks.", "upvote_ratio": 1.0, "id": "t3_obf99j", "created_utc": 1625118211.0}
{"sub": "pytorch", "title": "Switching Pytorch installation version to save memory", "selftext": "I am trying to run a few large computer vision models (\\~700MB in total) into my app and want to minimize the memory overhead taken up by Pytorch in order to save space. From looking at [the releases page](https://pypi.org/project/torch/#files), I see many different releases with drastically different sizes (specifically [these](https://i.imgur.com/drrFzDx.png) versions).\nWhat differences in these different releases cause the drastically different bundle sizes? Is this actually helpful when running a program in Pytorch (do smaller bundles mean smaller Pytorch memory overhead)?\n\nI'm considering using the `manylinux2014-aarch` version but I don't know if it's missing some important components or something like that, or what platforms it supports (is it arch-only?).\n\nThanks.", "upvote_ratio": 1.0, "id": "t3_oabnde", "created_utc": 1624983720.0}
{"sub": "pytorch", "title": "Everything You Need To Know About Torchvision\u2019s SSDlite Implementation", "selftext": "nan", "upvote_ratio": 0.91, "id": "t3_o9he6a", "created_utc": 1624875473.0}
{"sub": "pytorch", "title": "In-shop clothes retrieval Fine Tuning", "selftext": "I am new to \"In-shop clothes retrieval\" problem statement. One of the dataset can be referenced [here](http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion/InShopRetrieval.html).\n\nIf I want to fine tune a CNN model using a similar dataset, how can I go about it?\n\nThanks!", "upvote_ratio": 0.5, "id": "t3_o8t7di", "created_utc": 1624783281.0}
{"sub": "pytorch", "title": "REDUCE MEMORY LOADED INTO GPU WHEN TRAINING MODEL", "selftext": "Hello everyone,\n\nI'm a Pytorch beginner. When I try to train a network (not written by me) using RTX 2060, it triggers \"RuntimeError: CUDA out of memory...\".\n\nThe model was built using Python 2.7 &amp; Pytorch 0.4.0 (trained by GTX 1080Ti). I tried to adapt to Python 3.9.5 &amp; Pytorch 1.8.1 and successfully run inference. I think that this is due to the memory limit of my GPU (6 GBs). Is there anyway that I can solve this issue given that I am not the author of the code?\n\nHere is the github repository if you guys want to have a look: [https://github.com/zijundeng/BDRAR](https://github.com/zijundeng/BDRAR)\n\nCheers.", "upvote_ratio": 0.5, "id": "t3_o8b51t", "created_utc": 1624715341.0}
{"sub": "pytorch", "title": "Pie &amp; AI: Bangalore - Getting started with PyTorch Lightning", "selftext": "nan", "upvote_ratio": 0.67, "id": "t3_o82vuc", "created_utc": 1624678110.0}
{"sub": "pytorch", "title": "Stoping running training on Jupyter without running out of memory", "selftext": "Hey Guys,\n\nYou all have probably had this experience that you needed to stop training a model in Jupyter to test something and then using the same model again. The issue is that when I stop the run, the gpu memory is not released so any further use of the model leads to a CUDA memory error. Anyone knows a solution to this problem ?\n\nThanks", "upvote_ratio": 0.67, "id": "t3_o7r1f0", "created_utc": 1624638148.0}
{"sub": "pytorch", "title": "DINO - Emerging Properties in Self-Supervised Vision Transformers | Implementation", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_o7qvap", "created_utc": 1624637637.0}
{"sub": "pytorch", "title": "Concatenating ResNet-50 predictions PyTorch", "selftext": "I am using a pre-trained ResNet-50 model where the last dense is removed and the output from the average pooling layer is flattened. This is done for feature extraction purposes. The images are read from folder after being resized to (300, 300); it's RGB images.\n\ntorch version: 1.8.1 &amp; torchvision version: 0.9.1 with Python 3.8.\n\nThe code is as follows:\n\n        model_resnet50 = torchvision.models.resnet50(pretrained = True)\n        \n        # To remove last dense layer from pre-trained model, Use code-\n        model_resnet50_modified = torch.nn.Sequential(*list(model_resnet50.children())[:-1])\n        \n        # Using 'AdaptiveAvgPool2d' layer, the predictions have shape-\n        model_resnet50_modified(images).shape\n        # torch.Size([32, 2048, 1, 1])\n        \n        # Add a flatten layer after 'AdaptiveAvgPool2d(output_size=(1, 1))' layer at the end-\n        model_resnet50_modified.flatten = nn.Flatten()\n        \n        # Sanity check- make predictions using a batch of images-\n        predictions = model_resnet50_modified(images)\n        \n        predictions.shape\n        # torch.Size([32, 2048])\n\nI want to now feed batches of images to this model and concatenate the predictions made by the model (32, 2048) vertically.\n\n        # number of images in training and validation sets-\n        len(dataset_train), len(dataset_val)\n        # (22500, 2500)\n\nThere are a total of 22500 + 2500 = 25000 images. So the final table/matrix should have the shape: (25000, 2048) -&gt; number of images = 25000 and number of extracted features = 2048.\n\nI tried running a toy code using np.vstack() as follows:\n\n        x = np.random.random_sample(size = (1, 3))\n        x.shape\n        # (1, 3)\n        \n        x\n        # array([[0.52381798, 0.12345404, 0.1556422 ]])\n        \n        for i in range(5):\n            y = np.random.random_sample(size = (1, 3))\n            np.vstack((x, y))\n            \n        x\n        # array([[0.52381798, 0.12345404, 0.1556422 ]])\n    \n\nSolution(s)?\n\n&amp;#x200B;\n\nThanks!", "upvote_ratio": 1.0, "id": "t3_o7i5g3", "created_utc": 1624602952.0}
{"sub": "pytorch", "title": "Google Colab Cuda RuntimeError", "selftext": "Why do I keep getting this error? RuntimeError: CUDA error: device-side assert triggered CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect. For debugging consider passing CUDA\\_LAUNCH\\_BLOCKING=1.\n\nI first got this while training my model. And now it appears even when I just want to load another co-model weights I have saved. I try to look it up, but all the answers seem to be having mistakes within the loss function with the wrong ground truth data form. But I got this in the middle of forwarding. How do I \"pass CUDA\\_LAUNCH\\_BLOCKING=1\" to fix this?", "upvote_ratio": 1.0, "id": "t3_o7bm61", "created_utc": 1624577420.0}
{"sub": "pytorch", "title": "Same results over and over again", "selftext": "I have a custom dataset of 100 images that i prepare some models for a next challenge. When I run a model for 10 epochs, the validation accuracy remains always the same (e.g. 40%). If I increase the epochs to 1000, the validation accuracy also remains the same for all the epochs (e.g. 35%). The train loader is heavy augmented so I did not expect this. If I run the same code 10 mins later, the validation accuracy does not remain the same but varies. There are times that the training accuracy also remains the same. How is this possible?", "upvote_ratio": 1.0, "id": "t3_o78vx2", "created_utc": 1624565211.0}
{"sub": "pytorch", "title": "Get file names and file path using PyTorch dataloader", "selftext": "I am using PyTorch 1.8 and Python 3.8 to read images from a folder using the following code:\n\n&amp;#x200B;\n\n        print(f\"PyTorch version: {torch.__version__}\")\n        # PyTorch version: 1.8.1\n        \n        # Device configuration-\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        print(f\"currently available device: {device}\")\n        # currently available device: cpu\n        \n        # Define transformations for training and test sets-\n        transform_train = transforms.Compose(\n            [\n              # transforms.RandomCrop(32, padding = 4),\n              # transforms.RandomHorizontalFlip(),\n              transforms.ToTensor(),\n              # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n             ]\n             )\n        \n        transform_test = transforms.Compose(\n            [\n              transforms.ToTensor(),\n              # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n             ]\n             )\n    \n        # Define directory containing images-\n        data_dir = 'My_Datasets/Cat_Dog_data/'\n        \n        # Define datasets-\n        train_data = datasets.ImageFolder(data_dir + '/train', \n                                          transform = train_transforms)\n        test_data = datasets.ImageFolder(data_dir + '/test', \n                                         transform = test_transforms)\n        \n        print(f\"number of train images = {len(train_data)} &amp; number of validation images = {len(test_data)}\")\n        # number of train images = 22500 &amp; number of validation images = 2500\n        \n        print(f\"number of training classes = {len(train_data.classes)} &amp; number of validation classes = {len(test_data.classes)}\")\n        # number of training classes = 2 &amp; number of validation classes = 2\n    \n        # Define data loaders-\n        trainloader = torch.utils.data.DataLoader(train_data, batch_size = 32)\n        testloader = torch.utils.data.DataLoader(test_data, batch_size = 32)\n        \n        len(trainloader), len(testloader)\n        # (704, 79)\n        \n        # Sanity check-\n        len(train_data) / 32, len(test_data) / 32\n    \n\nYou can iterate through the train data using 'train\\_loader' as follows:\n\n&amp;#x200B;\n\n        for img, lab in train_loader:\n           print(img.shape, lab.shape)\n           pass\n\nHowever, I am interested in getting the file name along with the file path from which the file was read. How can I achieve this?\n\nThanks!\n\n&amp;#x200B;", "upvote_ratio": 0.75, "id": "t3_o6wxug", "created_utc": 1624524536.0}
{"sub": "pytorch", "title": "Convolutional Autoencoder CIFAR10 PyTorch - RuntimeError", "selftext": "I am using PyTorch version: 1.9.0+cu102 with Convolutional Autoencoder for CIFAR-10 dataset as follows:\n\n        # Define transformations for training and test sets-\n        transform_train = transforms.Compose(\n            [\n              # transforms.RandomCrop(32, padding = 4),\n              # transforms.RandomHorizontalFlip(),\n              transforms.ToTensor(),\n              # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n             ]\n             )\n        \n        transform_test = transforms.Compose(\n            [\n              transforms.ToTensor(),\n              # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n             ]\n             )\n        \n        # Load dataset-\n        train_dataset = torchvision.datasets.CIFAR10(\n                root = './data', train = True,\n                download = True, transform = transform_train\n                )\n        \n        test_dataset = torchvision.datasets.CIFAR10(\n                root = './data', train = False,\n                download = True, transform = transform_test\n                )\n        \n        print(f\"len(train_dataset) = {len(train_dataset)} &amp; len(test_dataset) = {len(test_dataset)}\")\n        # len(train_dataset) = 50000 &amp; len(test_dataset) = 10000\n        \n        \n        batch_size = 64\n        \n        # Create training and testing loaders-\n        train_loader = torch.utils.data.DataLoader(\n                train_dataset, batch_size = batch_size,\n                shuffle = True\n                )\n        \n        test_loader = torch.utils.data.DataLoader(\n                test_dataset, batch_size = batch_size,\n                shuffle = False\n                )\n        \n        print(f\"len(train_loader) = {len(train_loader)} &amp; len(test_loader) = {len(test_loader)}\")\n        # len(train_loader) = 782 &amp; len(test_loader) = 157\n        \n        # Sanity check-\n        len(train_dataset) / batch_size, len(test_dataset) / batch_size\n        # (781.25, 156.25)\n        \n        # Get some random training images-\n        images, labels = next(iter(train_loader))\n        print(f\"images.shape: {images.shape} &amp; labels.shape: {labels.shape}\")\n        # images.shape: torch.Size([64, 3, 32, 32]) &amp; labels.shape: torch.Size([64])\n        \n        LEARNING_RATE = 0.001\n        num_epochs = 20\n        \n        \n        class Reshape(nn.Module):\n            def __init__(self, *args):\n                super().__init__()\n                self.shape = args\n            \n            def forward(self, x):\n                return x.view(self.shape)\n        \n        \n        class Trim(nn.Module):\n            def __init__(self, *args):\n                super().__init__()\n            \n            def forward(self, x):\n                return x[:, :, :32, :32]\n                \n        encoder = nn.Sequential(\n            nn.Conv2d(\n                in_channels = 3, out_channels = 32,\n                kernel_size = 3, padding = 1,\n                stride = 1, bias = True\n                ),\n            nn.LeakyReLU(negative_slope = 0.01),\n             \n            nn.Conv2d(\n                in_channels = 32, out_channels = 64,\n                kernel_size = 3, padding = 1,\n                stride = 2, bias = True\n                ),\n            nn.LeakyReLU(negative_slope = 0.01),\n            \n            nn.Conv2d(\n                in_channels = 64, out_channels = 64,\n                kernel_size = 3, padding = 1,\n                stride = 2, bias = True\n                ),\n            nn.LeakyReLU(negative_slope = 0.01),\n            \n            nn.Conv2d(\n                in_channels = 64, out_channels = 64,\n                kernel_size = 3, padding = 1,\n                stride = 1, bias = True\n                ),\n            nn.LeakyReLU(negative_slope = 0.01),\n        \n            nn.Flatten(),\n            \n            nn.Linear(\n                in_features = 4096, out_features = 1500,\n                bias = True\n            ),\n            nn.Linear(\n                in_features = 1500, out_features = 500,\n                bias = True\n            ),\n            nn.Linear(\n                in_features = 500, out_features = 100,\n                bias = True\n            )\n        )\n        \n        # Sanity check-\n        x = torch.rand(size = (32, 3, 32, 32))\n        print(f\"x.shape = {x.shape}\")\n        \n        encoder_op = encoder(x)\n        print(f\"encoder_op.shape = {encoder_op.shape}\")\n        # x.shape = torch.Size([32, 3, 32, 32])\n        # encoder_op.shape = torch.Size([32, 100])\n        \n        decoder = nn.Sequential(\n            nn.Linear(\n                in_features = 100, out_features = 500,\n                bias = True),\n            nn.Linear(\n                in_features = 500, out_features = 1500,\n                bias = True),\n            nn.Linear(\n                in_features = 1500, out_features = 4096,\n                bias = True),\n            \n            Reshape(-1, 64, 8, 8),\n            \n            nn.ConvTranspose2d(\n                in_channels = 64, out_channels = 64,\n                kernel_size = 3, stride = 1,\n                padding = 1, bias = True),\n            # output: torch.Size([32, 64, 8, 8])\n            nn.ConvTranspose2d(\n                in_channels = 64, out_channels = 64,\n                kernel_size = 3, stride = 2,\n                padding = 1, bias = True),\n            # output: torch.Size([32, 64, 15, 15])\n            nn.ConvTranspose2d(\n                in_channels = 64, out_channels = 32,\n                kernel_size = 3, stride = 2,\n                padding = 0, bias = True),\n            # torch.Size([32, 32, 31, 31])\n            nn.ConvTranspose2d(\n                in_channels = 32, out_channels = 3,\n                kernel_size = 3, stride = 1,\n                padding = 0, bias = True),\n            # output: torch.Size([32, 3, 33, 33])\n            \n            Trim(),\n            # (3, 33, 33) -&gt; (3, 32, 32)\n            nn.Sigmoid()\n            \n        )\n        \n        # Sanity check-\n        decoder(encoder_op).shape\n        # torch.Size([32, 3, 32, 32])\n        \n        \n        class AutoEncoder(nn.Module):\n            def __init__(self):\n                super().__init__()\n                \n                self.encoder = encoder\n                self.decoder = decoder\n            \n            def forward(self, x):\n                x = self.encoder(x)\n                x = self.decoder(x)\n                return x\n        \n        # Initialize an autoencoder instance-\n        model = AutoEncoder()\n        \n        # Move model to (GPU) device-\n        model.to(device)\n        \n        # Specify optimizer and loss function-\n        optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n        loss_fn = F.mse_loss\n        \n        num_epochs = 15\n        \n        # Python3 lists to hold training metrics-\n        trainining_loss = []\n        validation_loss = []\n        \n        def compute_epoch_loss_autoencoder(model, data_loader, loss_fn, device):\n            model.eval()\n            curr_loss, num_examples = 0., 0\n            \n            with torch.no_grad():\n                for features, _ in data_loader:\n                    features = features.to(device)\n                    logits = model(features)\n                    loss = loss_fn(logits, features, reduction='sum')\n                    num_examples += features.size(0)\n                    curr_loss += loss\n        \n                curr_loss = curr_loss / num_examples\n                return curr_loss\n        \n        start_time = time.time()\n        \n        for epoch in range(num_epochs):\n            running_loss = 0.0\n        \n            model.train()\n            \n            for batch_idx, (features, _) in enumerate(train_loader):\n                \n                features = features.to(device)\n                \n                # forward and back prop-\n                logits = model(features)  # make predictions using model\n                loss = loss_fn(logits, features)\n                optimizer.zero_grad()\n                \n                # Perform backprop-\n                loss.backward()\n                \n                # Update model parameters-\n                optimizer.step()\n                \n                # Compute model's performance-\n                running_loss += loss.item() * features.size(0)\n            \n            # Compute loss using training dataset-\n            epoch_loss = running_loss / len(train_dataset)\n            trainining_loss.append(epoch_loss)\n        \n            # Compute loss using validation dataset-\n            val_loss = compute_epoch_loss_autoencoder(\n                model, test_loader,\n                loss_fn, device\n                )\n            validation_loss.append(val_loss)\n        \n            print(f\"Epoch = {epoch + 1}: Autoencoder train loss = {epoch_loss:.4f} &amp; val loss = {val_loss:.4f}\")\n        \n        \n        end_time = time.time()\n        \n        # Get some validation images-\n        for img, label in test_loader:\n            break\n        \n        img.shape, label.shape\n        # (torch.Size([64, 3, 32, 32]), torch.Size([64]))\n        \n        img.to(device)\n        \n        # Pass batch size = 64 images through encoder to get latent space representations-\n        model.encoder(img)\n\nThis line gives me the error:\n\n&gt;RuntimeError                              Traceback (most recent call last) &lt;ipython-input-69-14d47c831d37&gt; in &lt;module&gt;()  \n&gt;  \n&gt;\\----&gt; 1 model.encoder(img)  \n&gt;  \n&gt;4 frames  \n&gt;  \n&gt;/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py in \\_conv\\_forward(self, input, weight, bias)  \n&gt;  \n&gt;**438**                             \\_pair(0), self.dilation, self.groups)  \n&gt;  \n&gt;**439**         return F.conv2d(input, weight, bias, self.stride,  \n&gt;  \n&gt;\\--&gt; 440                         self.padding, self.dilation, self.groups)  \n&gt;  \n&gt;**441**  \n&gt;  \n&gt;**442** def forward(self, input: Tensor) -&gt; Tensor:  \n&gt;  \n&gt;RuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n\nWhat's going wrong?\n\nThanks!", "upvote_ratio": 1.0, "id": "t3_o6w3zg", "created_utc": 1624520498.0}
{"sub": "pytorch", "title": "Can we used matrices and lists for custom Dataset?", "selftext": "I am trying to create a dataset where the inputs are matrices and then also a list containing tuples. The label to these matrices + lists is an integer value.\n\nIs this possible?", "upvote_ratio": 1.0, "id": "t3_o63ozl", "created_utc": 1624415704.0}
{"sub": "pytorch", "title": "Can pytorch handle string processing?", "selftext": "Is there any way for me to preform a regular expression search on a string using pytorch? I would very much enjoy any resources surrounding such.", "upvote_ratio": 1.0, "id": "t3_o61m2r", "created_utc": 1624409035.0}
{"sub": "pytorch", "title": "2 transforms on 1 dataset", "selftext": "I have a custom dataset which I initialize as `dataset=CustomDataset(root_dir=..., transform=None)`. Then I split it to training and testing set  with `train_set, test_set = torch.utils.data.random_split(dataset, [num_training, num_testing])`. I want to apply 2 different transormations (from `torchvision.transforms`) to each of the `train_set` and `test_set`. Is it possible? If yes, how can i do that?", "upvote_ratio": 1.0, "id": "t3_o5lxet", "created_utc": 1624364781.0}
{"sub": "pytorch", "title": "How to do Linear Algebra with Probability Distributions", "selftext": "I would like to use probability distributions instead of variables in python. I have tried a few libraries (PyMC3, Tensorflow+Tensorflow\\_probability, edward2, numpyro, torch with pyro). So far the things I want to do, add, multiply with a scalar and with another probability distribution, apply sin/cos to a probability distribution (not to a sample from the probability distribution) I could only achieve in PyMC3.\n\nThe following code returns errors at both sums and products.\n\n    import torch\n    \n    loc = 200.  \n    scale = 1\n    d = torch.distributions.Normal(loc, scale)\n    scalar = torch.tensor(5.)\n    sum1 = d + d\n    sum2 = torch.add(d, d)\n    prod1 = d * scalar\n    prod2 = torch.mul(d, scalar)\n\nThe same concept can be implemented in PyMC3 without any problems and the resulting probability distributions are scaled and/or shifted. Can this be implemented somehow in pytorch?", "upvote_ratio": 1.0, "id": "t3_o4sg7d", "created_utc": 1624272836.0}
{"sub": "pytorch", "title": "Pre-processing audio data on GPU", "selftext": "In my new tutorial, you can learn how to pre-process audio data directly on GPU using Pytorch and torchaudio.\n\nThis video is part of the \u201cPyTorch for Audio and Music Processing\u201d series, which aims to teach you how to use PyTorch and torchaudio for audio-based Deep Learning projects. \n\nVideo:\n\n[https://www.youtube.com/watch?v=3wD\\_eocmeXA&amp;list=PL-wATfeyAMNoirN4idjev6aRu8ISZYVWm&amp;index=7](https://www.youtube.com/watch?v=3wD_eocmeXA&amp;list=PL-wATfeyAMNoirN4idjev6aRu8ISZYVWm&amp;index=7)", "upvote_ratio": 1.0, "id": "t3_o4rjei", "created_utc": 1624268672.0}
{"sub": "pytorch", "title": "Neural Style Transfer in PyTorch", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_o4mtnc", "created_utc": 1624248972.0}
{"sub": "pytorch", "title": "Using hooks", "selftext": "I'm learning about hooks and wanted to practice them. I'm basically trying to create a hook that whenever theres a gradient updates to each weights, will just take the weight value and the loss, and multiply that value by 1e-1. So if the update should be:\n\nw1 -= lr \\* loss\\_value = 1e-5 \\* 50\n\nI want it to go through the hook before the update and make it 1e-5 \\* 50 \\* 1e-1\n\nHow can I go about this? The concept of hooks is a bit confusing to me", "upvote_ratio": 1.0, "id": "t3_o4m21c", "created_utc": 1624246255.0}
{"sub": "pytorch", "title": "Skip connection implementation", "selftext": "how to implement skip connection for this [coding](https://gist.github.com/promach/b6f526c56e20f029d68e6f9041c3f5c0#file-gdas-py-L124) ?\n\n    class SkipEdge(Edge):\n        def __init__(self):\n            super().__init__()\n            self.f =\n\n&amp;#x200B;", "upvote_ratio": 0.71, "id": "t3_o3wdro", "created_utc": 1624159145.0}
{"sub": "pytorch", "title": "How to use a custom dataset in PyTorch?", "selftext": " I'm trying to create a dataset for training a robot in a 2D world. The World is going to have labels like \"free\", \"unknown\", \"obstacle\" to identify different components of the map. This will most likely be with matrices using integers for the labels e.g. free=1, unknown=2 etc.\n\nI want to use this dataset to train a CNN to learn the value of each  state in the map. The current plan is to break this world down into multiple binary  matrices to separate the different labels. So, eventually the neural net will have input like feeding in RGB images  (but the different matrix layers here will represent the different  labels).\n\nI was looking at the DataLoader class in Pytorch and it allows us to  create custom datasets. However, in my dataset I don't have .jpg files, but separate matrix  layers to represent the different labels in the map at each state. Does anyone know how I can create a dataset like the one I want in  PyTorch?", "upvote_ratio": 1.0, "id": "t3_o346c2", "created_utc": 1624062443.0}
{"sub": "pytorch", "title": "Derivative of neural network with respect to inputs?", "selftext": "I have a simple MLP that takes input X (shape 1,100) and outputs Y\\_predict (shape 1,100). I would like to take the derivative of Y\\_predict with rspct to each input:\n\ne.g.\n\nd/dx\\_i (Y\\_predict,i)\n\n&amp;#x200B;\n\nAny advice on doing this? Thanks!", "upvote_ratio": 1.0, "id": "t3_o2830z", "created_utc": 1623964539.0}
{"sub": "pytorch", "title": "Creating random linearly independent vectors", "selftext": "I can generate a random vector with\n\na = torch.rand(10) \n\nHowever, how can I generate N more vectors (e.g. N=5: b,c,d,e,f) that will be linearly independent of the other vectors (i.e a is independent of any of b,c,d,e,f, b is independent of any of a,c,d,e,f, etc.)?", "upvote_ratio": 1.0, "id": "t3_o27bl3", "created_utc": 1623962538.0}
{"sub": "pytorch", "title": "Pre-processing audio data with different durations", "selftext": "In real-world audio datasets, not all files have the same duration / num. of samples. This can be a problem for the majority of Deep Learning models (e.g., CNN), which expect training data with a fixed shape.\n\nIn computer vision, there\u2019s a simple workaround when there are images with different sizes: resizing. What about audio data? The solution is more complex.\n\nFirst, you should decide the number of samples you want to consider for your experiments (e.g., 22050 samples)\n\nThen, when loading waveforms you should ensure that they have as many samples as the expected ones. To ensure this, you can do two things:\n\n1. cut the waveforms which have more samples than the expected ones;\n2. zero pad the waveforms which have less samples than the expected ones.\n\nDoes this feel too abstract? \n\nNo worries, in my new video I demonstrate how you can use cutting/padding with audio data in Pytorch and torchaudio.\n\nThis video is part of the \u201cPyTorch for Audio and Music Processing\u201d series, which aims to teach you how to use PyTorch and torchaudio for audio-based Deep Learning projects. \n\nVideo:\n\n[https://www.youtube.com/watch?v=WyJvrzVNkOc&amp;list=PL-wATfeyAMNoirN4idjev6aRu8ISZYVWm&amp;index=6](https://www.youtube.com/watch?v=WyJvrzVNkOc&amp;list=PL-wATfeyAMNoirN4idjev6aRu8ISZYVWm&amp;index=6)", "upvote_ratio": 1.0, "id": "t3_o1ufte", "created_utc": 1623927361.0}
{"sub": "pytorch", "title": "cuda not allocating any memory but caching", "selftext": "I have added these lines to see if the GPU is being used:\n\nif device.type == 'cuda':   \nprint('Memory Usage:')  \ntorch.rand(20000,20000).cuda()  \nprint('Allocated:', round(torch.cuda.memory\\_allocated(0)/1024\\*\\*3,10), 'GB')  \nprint('Cached:   ', round(torch.cuda.memory\\_reserved(0)/1024\\*\\*3,10), 'GB')\n\nThe result was:\n\nMemory Usage:  \nAllocated: 0.0 GB  \nCached:    1.490234375 GB\n\nMy supervisor pointed out to me that my code isn't using the GPU while running on our cluster, and I am not sure why.", "upvote_ratio": 1.0, "id": "t3_o1t5az", "created_utc": 1623922324.0}
{"sub": "pytorch", "title": "Everything You Need To Know About Torchvision\u2019s SSD Implementation", "selftext": "nan", "upvote_ratio": 0.87, "id": "t3_o1sxqg", "created_utc": 1623921472.0}
{"sub": "pytorch", "title": "Installing PyTorch fails on MacOS with brand new conda env", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_o1hwgv", "created_utc": 1623883912.0}
{"sub": "pytorch", "title": "Any way to train models on phone using Pytorch?", "selftext": "I'm currently doing a research in federated learning which requires training a lightweight model on a mobile device.\n\nI read about Pytorch Mobile, but it apparently cannot be used to perform backprop on the phone itself (correct me if I'm wrong).\n\nIs there any workaround to do this task?", "upvote_ratio": 0.84, "id": "t3_o0zmpa", "created_utc": 1623830241.0}
{"sub": "pytorch", "title": "PyTorch 1.9 Release, including torch.linalg and Mobile Interpreter", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_o0wmry", "created_utc": 1623817666.0}
{"sub": "pytorch", "title": "Preforming well in training but very poor performance in testing.", "selftext": "I\u2019m training a semantic segmentation model based on the resent50 framework. I\u2019ve trained the data on 5000 images over 50 epochs and it\u2019s predicting well with the training data, however when I provide new data for evaluation the model is failing to classify. Any suggestions on how to correct this? Larger training dataset, more data (as in depth of the data)?\n\nEdit: fixed 500 to 5000", "upvote_ratio": 1.0, "id": "t3_nzxfd8", "created_utc": 1623705818.0}
{"sub": "pytorch", "title": "Performance issues with torch.norm", "selftext": "Has anyone else experienced performance issues using \\`torch.norm\\` and how did you work around this?", "upvote_ratio": 0.33, "id": "t3_nzke5l", "created_utc": 1623669083.0}
{"sub": "pytorch", "title": "Extract mel spectrograms with Pytorch + torchaudio", "selftext": "I published a new tutorial where you can learn how to extract Mel Spectrograms and resampling audio with torchaudio. I also review the most common torchaudio transforms and explain how you can use them.\n\nThis video is part of the \u201cPyTorch for Audio and Music Processing\u201d series, which aims to teach you how to use PyTorch and torchaudio for audio-based Deep Learning projects. \n\nHere's the video:\n\n[https://www.youtube.com/watch?v=lhF\\_RVa7DLE&amp;list=PL-wATfeyAMNoirN4idjev6aRu8ISZYVWm&amp;index=5](https://www.youtube.com/watch?v=lhF_RVa7DLE&amp;list=PL-wATfeyAMNoirN4idjev6aRu8ISZYVWm&amp;index=5)", "upvote_ratio": 1.0, "id": "t3_nzjzse", "created_utc": 1623667519.0}
{"sub": "pytorch", "title": "Converting a trained model to a callable function object with different function signature (and datatype)", "selftext": "I\u2019m usually not the one to post \u201csyntax, please help!\u201d questions on here but this one has me stumped. I parametrized a derivative function with a PyTorch nn.Module object and now I want to pass it to a scipy function (\u2018solve_ivp\u2019). The problem is that \u2018solve_ivp\u2019 takes two parameters (y and t) while I only use one, and it passes numpy arrays and expects them when returned.\n\nMy first thought was an anonymous function, like this:\n    ode_scipy = lambda t, y: func(torch.from_numpy(y).to(device)).cpu().detach().numpy()\n\nWhere func is my trained network. This gave the error \u2018\u2019TypeError: forward() missing 1 required positional argument: \u2018y\u2019 \u2018\u2019 which doesn\u2019t make sense to me because I am definitely passing a parameter. As far as I know, the \u2018forward\u2019 function only takes one parameter. Any thoughts? I\u2019m not against ditching the anonymous function altogether if someone has a smarter way to do this", "upvote_ratio": 1.0, "id": "t3_nxuo0m", "created_utc": 1623458954.0}
{"sub": "pytorch", "title": "How PyTorch Is Challenging TensorFlow Lately", "selftext": "nan", "upvote_ratio": 0.91, "id": "t3_nx9vkx", "created_utc": 1623395683.0}
{"sub": "pytorch", "title": "CNN - Apple Classification", "selftext": " I  have the following problem statement in which I only need to predict  whether a given image is an apple or not. For training only 8 images are  provided with the following details:\n\n1. apple\\_1 image - 2400x1889 PNG\n2. apple\\_2 image - 641x618 PNG\n3. apple\\_3 image - 1000x1001 PNG\n4. apple\\_4 image - 500x500 PNG\t\tcontains a sticker on top of fruit\n5. apple\\_5 image - 2400x1889 PNG\n6. apple\\_6 image - 1000x1000 PNG\n7. apple\\_7 image - 253x199 JPG\n8. apple\\_8 image - 253x199 JPG\n\nI  am thinking about using Transfer learning: either VGG or  ResNet-18/34/50. Maybe ResNet is an overkill for this problem statement?  How do I deal with such varying image sizes and of different file  extensions (PNG, JPG)?\n\nAny online code tutorial will be helpful.\n\nThanks!", "upvote_ratio": 0.75, "id": "t3_nx8a95", "created_utc": 1623389359.0}
{"sub": "pytorch", "title": "How do you save/load models for later use in your PyTorch workflow?", "selftext": "A persistent problem for me is that when I am still developing a model, I will often need to do lots of tweaking of activations, dense layer size/number, etc, and it\u2019s a common headache for me to try to load a model from a few days back and try to evaluate it only to see that I tweaked something and now it won\u2019t load right.\n\nI use torch.save(self.state_dict(), \u2026) and self.load_state_dict(torch.load(\u2026)) for this since sometimes I\u2019ll run little tests on CPU, sometimes GPU(s) and this has worked the best for me across all devices\n\nI end up having to either have 1) tons of command line arguments or 2) tons of separate files calling different model parameters if I want the same training architecture for all models (which I really do)\n\nAny advice on how to manage using a large set of varied models? Is there a smart way I\u2019m not seeing for managing how to load or save models where I don\u2019t need to worry about model tweaks during testing?", "upvote_ratio": 1.0, "id": "t3_nvg52f", "created_utc": 1623190480.0}
{"sub": "pytorch", "title": "Minimum requirements for loading gpt2-xl", "selftext": "Hey! Quick question, First of all, sorry if this is not the right place for this question. I'm a beginner.\n\nSo I'm wondering which would be the minimum requirements for loading the [gpt2-xl](https://huggingface.co/gpt2-xl) (6 GB) PyTorch model, RAM is the key thing here I guess. Note that I am not talking about Fine-Tuning, but rather just load it.  \n\nI am using [this](https://github.com/graykode/gpt-2-Pytorch) repo to load the [large](https://huggingface.co/gpt2-large/tree/main) (3 GB) version via Google Colab and it works by changing a few parameters (Google Colab has 13 GB RAM). But when trying to load the xl in the RAM the python process is just directly killed.\n\nFor example, would this VPS configuration be enough?\n\n**6 vCPU Cores**\n\n**16 GB RAM**\n\n**400 GB SSD**\n\n**400 Mbit/s Port**\n\n**thanks!!**", "upvote_ratio": 1.0, "id": "t3_nv2ta4", "created_utc": 1623154968.0}
{"sub": "pytorch", "title": "Variant Size Input Image to Convolution Layers", "selftext": "Many problems in computer vision have images as inputs but the region of interest (ROI) is not squared or orthogonal, its a polygon. As a results, the image the computer \"sees\", is an orthogonal image with \"0\" outside the ROI. An example can be a plaque of the carotid ultrasound. Moreover, the dimensions of these images are not standard. How do you guys approach those problems? I will start with some of my ideas:\n\n* zero padding to a maximum height N1 and maximum width N2 and the input to the convolution layer is N1 x N2 (too many \"0\"s)\n* Resize image to standard size M1 x M2 (loose information)\n* apply abose but with N1=N2 or M1=M2\n\nNote that especially in medial application, any loss of information might be crucial.", "upvote_ratio": 1.0, "id": "t3_num4c2", "created_utc": 1623096299.0}
{"sub": "pytorch", "title": "Pay Attention to MLPs - Annotated implementation", "selftext": "[https://nn.labml.ai/transformers/gmlp/index.html](https://nn.labml.ai/transformers/gmlp/index.html)\n\ngMLP uses Multilayer Perceptrons (MLP) with gating instead of attention. It does pretty well compared to BERT on NLP and achieves same accuracy as ViT in vision tasks.\n\n* [Github](https://github.com/labmlai/annotated_deep_learning_paper_implementations/tree/master/labml_nn/transformers/gmlp)\n* [Paper](https://arxiv.org/abs/2105.08050)", "upvote_ratio": 1.0, "id": "t3_nud24p", "created_utc": 1623073926.0}
{"sub": "pytorch", "title": "Create a custom audio PyTorch dataset using torchaudio", "selftext": "I published a new tutorial in my \"Pytorch for Audio + Music Processing\" series called \"Custom audio PyTorch dataset with torchaudio\"\n\nIn the video, you can learn how to create a custom audio dataset with PyTorch loading audio files with torchaudio.\n\nIn the process, you\u2019ll also learn basic I/O functions in torchaudio.\n\nThis video is part of the \u201cPyTorch for Audio and Music Processing\u201d series, which aims to teach you how to use PyTorch and torchaudio for audio-based Deep Learning projects. \n\nVideo:\n\n[https://www.youtube.com/watch?v=88FFnqt5MNI&amp;list=PL-wATfeyAMNoirN4idjev6aRu8ISZYVWm&amp;index=4](https://www.youtube.com/watch?v=88FFnqt5MNI&amp;list=PL-wATfeyAMNoirN4idjev6aRu8ISZYVWm&amp;index=4)", "upvote_ratio": 1.0, "id": "t3_nuam0c", "created_utc": 1623067044.0}
{"sub": "pytorch", "title": "Forward pass computation for GDAS NAS coding", "selftext": "  \nFor [https://gist.github.com/promach/b6f526c56e20f029d68e6f9041c3f5c0#file-gdas-py-L143-L169](https://gist.github.com/promach/b6f526c56e20f029d68e6f9041c3f5c0#file-gdas-py-L143-L169) , how to do forward pass training computation for architecture \\`weights\\` of each and every edges ?  \n\n\n[https://github.com/D-X-Y/AutoDL-Projects/issues/99#issuecomment-835802887](https://github.com/D-X-Y/AutoDL-Projects/issues/99#issuecomment-835802887)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/ei8qje4cqr371.png?width=1920&amp;format=png&amp;auto=webp&amp;s=efc30910d6664081e05a6fc52e7ae7e8862bdfa6", "upvote_ratio": 1.0, "id": "t3_nu3jl6", "created_utc": 1623038975.0}
{"sub": "pytorch", "title": "Convergence Issues with autograd", "selftext": "I am converting a tensorflow custom training loop to PyTorch, but autograd is having issues with convergence, while GradientTape does not.\n\nWhat am I doing wrong?  \n\n### with PyTorch\n```\ndef train_fn(X, Y, epsilon=5, epochs=1000):\n    b = ((-epsilon - epsilon) * torch.rand(X.shape) + epsilon).clone().detach().requires_grad_(True)\n    opt = torch.optim.Adam([b], lr=5, betas=(0.1,0.1))\n    for epoch in range(epochs):\n        x = box(X+b)\n        loss = loss_fn(x, Y)\n        loss.backward()\n        print(f'{epoch} :: {loss}')\n        opt.step()\n    return x, b\n```\n\n### with TensorFlow\n```\ndef train_fn(X, Y, epsilon=5, epochs=1000):\n    b = tf.Variable(np.random.uniform(-epsilon, epsilon, X.shape).astype('float32'))\n    opt = tf.keras.optimizers.Adam(learning_rate=5, beta_1=0.1, beta_2=0.1)\n    for epoch in range(epochs):\n        with tf.GradientTape(persistent=False, watch_accessed_variables=True) as grad:\n            x = box(X+b)\n            loss = loss_fn(x, Y)\n            print(f'{epoch} :: {loss}')\n        opt.minimize(loss, [b], tape=grad)\n    return x, b\n```", "upvote_ratio": 1.0, "id": "t3_ntt98s", "created_utc": 1623007110.0}
{"sub": "pytorch", "title": "[DL] Validation step: metrics remain unchanged after each epoch (PyTorch Lightning)", "selftext": "I\u2019m running a DL model with PyTorch Lightning to try and classify some data (2 categories:  1/0).\n\nI don\u2019t understand why the validation score remains identical after each epoch.\n\nBatch size = 1024\n\nTrain data = 900\\_000 rows\n\nVal data = 100\\_000 rows\n\n                ...\n                self.layers = nn.Sequential(\n                    nn.Linear(100, 1024*16),\n                    nn.LeakyReLU(),\n                    nn.Linear(1024*16, 1024*8),\n                    nn.LeakyReLU(),\n                    nn.Linear(1024*8, 1024*8),\n                    nn.LeakyReLU(),\n                    nn.Linear(1024*8, 1024*8),\n                    nn.LeakyReLU(),\n                    nn.Linear(1024*8, 1024*4),\n                    nn.LeakyReLU(),\n                    nn.Linear(1024*4, 1024*4),\n                    nn.LeakyReLU(),\n                    nn.Linear(1024*4, 256),\n                    nn.LeakyReLU(),\n                    nn.Linear(256, 1),\n                    nn.Sigmoid(),\n                )\n        \n            def forward(self, x):\n                return self.layers(x.float())\n        \n            def training_step(self, batch, batch_idx):\n                x, y = batch\n                preds = self.layers(x.float())\n                loss = self.criterion(preds, y.float())    # nn.BCELoss()\n                acc = FM.accuracy(preds &gt; 0.5, y)\n                metrics = {'train_acc': acc.item(), 'train_loss': loss.item()}\n                self.log_dict(metrics)\n                return loss\n        \n            def validation_step(self, batch, batch_idx):\n                x, y = batch\n                preds = self(x.float())\n                loss = self.criterion(preds, y.float())    # nn.BCELoss()\n                acc = FM.accuracy(preds &gt; 0.5, y)\n                metrics = {'val_acc': acc.item(), 'val_loss': loss.item()}\n                self.log_dict(metrics)\n                return metrics\n\nThe val\\_loss  remains stable at 48.79  after each and every epoch (tested for up to 10 epochs; same true for val\\_acc  which doesn\u2019t change), which is weird. I would expect some slight  variation even if the model doesn\u2019t have much to learn from the data. At  least some ovefitting should be possible (model has 300 million+  parameters in total).\n\nHowever, the train\\_loss  does vary from batch to batch:\n\nhttps://preview.redd.it/zdbf4x4qio371.png?width=355&amp;format=png&amp;auto=webp&amp;s=02a35bd2f3061cc6ab9397469272706e932759cc\n\nAm I missing something? Why doesn't the validation loss change?\n\n\\*Edit\\* if anyone wants to try this, here is the Notebook and data file (.pt) with 110k rows in \"x\" and \"y\":  [https://we.tl/t-VfV34UqXK5](https://we.tl/t-VfV34UqXK5) ", "upvote_ratio": 0.75, "id": "t3_ntqrju", "created_utc": 1623000288.0}
{"sub": "pytorch", "title": "Transforming an AI Project Into an Application", "selftext": "Hello everyone. I have completed a reinforcement learning project by using Python and PyTorch for neural networks. Basically, I have an AI agent that uses MCTS and PyTorch neural networks to play a game (Mancala), but it only works in my terminal. How can I transform it into a mobile application or a website? I would like people to play against my AI agent on a website or on a mobile app. Any help is appreciated. I just want to know if it is possible and what are the ways to do that.", "upvote_ratio": 1.0, "id": "t3_nsd65m", "created_utc": 1622834582.0}
{"sub": "pytorch", "title": "I made a bunch of Vision Models with PyTorch and deployed it over Heroku :)", "selftext": "nan", "upvote_ratio": 0.85, "id": "t3_nsbihc", "created_utc": 1622830156.0}
{"sub": "pytorch", "title": "Why Pytorch chose C++ backend instead of Rust?", "selftext": "nan", "upvote_ratio": 0.29, "id": "t3_ns53d3", "created_utc": 1622813195.0}
{"sub": "pytorch", "title": "How to reduce pytorch download size?", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_nroboj", "created_utc": 1622755232.0}
{"sub": "pytorch", "title": "Checking errors in Multi-class 3D segmentation.", "selftext": "I am performing 3D multi-class segmentation of medical images(T1w MR). But U-Net with MSDL(multi-sourced Dice Loss) never reaches beyond 74% dice. And the desired performance is not satisfactory.\n\n1. To check there are any bugs/errors in the code, I ran training with one single data point and the same validation set to overfit the U-Net. The validation set is also the same data point. After training for 200 epochs both train and validation, the dice score seems to reach 80%(*I expected it to be 100%*). And validation loss curve follows the training curve very closely(first image).\n2. With the same loss, learning rate, and weight initialization, I changed the validation set with completely different image patches. The validation curves seem to behave in a random way(*as it should*) and for some reason, the dice go beyond 1 in train samples(all samples are the same image/mask). (second image)The dice overshoot may be due to the MSDL(multi-sourced dice loss) I have used(*which doesn\u2019t take background into account*).\n\n&amp;#x200B;\n\n[Fig. 1. ](https://preview.redd.it/9v3sa9p1p3371.png?width=4000&amp;format=png&amp;auto=webp&amp;s=4c2675e467aa08ae6cb85ecd0023a61b74df82aa)\n\n&amp;#x200B;\n\n[Fig. 2. ](https://preview.redd.it/p6eww0d4p3371.png?width=4000&amp;format=png&amp;auto=webp&amp;s=ae5178e0aa7a1a54070d606159bfd23df09cf3a7)\n\n&amp;#x200B;\n\nFrom these tests\u2019 results, is it confirmed that the current framework works? Are there any other checks that need to be performed?If the setup is good, shouldn\u2019t it overfit in the first case or dice metric reaching nearly 1?", "upvote_ratio": 1.0, "id": "t3_nrlgo4", "created_utc": 1622747827.0}
{"sub": "pytorch", "title": "Attention Weights in the Encoder Layer", "selftext": "I use multiple [TransformerEncoderLayers](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html) on input sequences for self-attention. Since the size of the sequences differs, so I use src\\_key\\_padding\\_mask:\n\n&amp;#x200B;\n\n    x = some input\n    mask = give_mask(x)\n    for encoderlayer in self.encoderlayers:\n        x = encoderlayer(x, src_key_padding_mask=mask)\n\nAfter training, I extracted the attention weights of each layer. Here I have two questions:\n\n1. Do my attention weights look right? My picture shows the weights of one layer (the others look similar). The sequence length here is nine, so I expected a squared shape of 9x9..\n2. How can I put the weights of multiple, stacked layers together? Just add them up to one weight matrix?\n\nthanks in advance\n\n&amp;#x200B;\n\nhttps://preview.redd.it/bd54f1lfo2371.png?width=380&amp;format=png&amp;auto=webp&amp;s=e72f3c9193a8f69e47a427bfd88b7a95fc66a11d", "upvote_ratio": 0.83, "id": "t3_nrgjtp", "created_utc": 1622735083.0}
{"sub": "pytorch", "title": "PyTorch and Binary Classification", "selftext": "I recently implemented some PyTorch models (CNN) for a binary classification problem. And then I asked myself if the outputs should be 1 (True/False thresholded at 0.5) or 2 (Class 1/Class 2). I found both in literature but I have to ask: What do you use and why? \n\nLet me start first: I use 2 outputs because I want the probabilities of each class after a softmax function application to the output. Moreover, I find a 2 output model more generalised in order to apply a similar model to another problem. Note that I write my models as this: `model = &lt;model&gt;(img_width=28, img_height=28, in_channels=1, num_classes=2)`", "upvote_ratio": 1.0, "id": "t3_nrc0ta", "created_utc": 1622722485.0}
{"sub": "pytorch", "title": "Annotated implementation of Attention Free Transformer (AFT)", "selftext": "This is a PyTorch implementation of paper \"An Attention Free Transformer\" with side-by-side notes.\n\n[https://nn.labml.ai/transformers/aft/index.html](https://nn.labml.ai/transformers/aft/index.html)\n\nAttention Free Transformer (AFT) replaces dot product self-attention with a new operation that has lower memory complexity. They also introduce AFT-local and AFT-conv.\n\n* [Github](https://github.com/labmlai/annotated_deep_learning_paper_implementations/tree/master/labml_nn/transformers/aft)\n* [Paper](https://arxiv.org/abs/2105.14103)\n* [Twitter Thread](https://twitter.com/labmlai/status/1400126844997750785)", "upvote_ratio": 0.95, "id": "t3_nqrsyh", "created_utc": 1622656376.0}
{"sub": "pytorch", "title": "Jetson Nano: TensorFlow model. Possibly I should use PyTorch instead?", "selftext": "Hello - \n\nI built a model (for ANPR) using TensorFlow and EasyOCR. Over the past week or so, getting TensorFlow to install on the Jetson Nano has been next to impossible. Tons of issues with it (some are documented) and overall I found one person that was able to get it running well which took over 50hrs to install on the Jetson Nano.\n\nMaybe I should try PyTorch instead of TensorFlow to work on the Jetson Nano? I read on the NVIDIA forums that it works better with the Jetson Nano, but I am not completely sure why (I can explore, but thought someone here might know)?\n\nThis all said, there has to be a better way to get a TensorFlow model to run on a Jetson Nano. Is this where ONNX comes in (if so, any ideas/resources you might be able to point me to that shows how?)\n\nAny/all thoughts/help is much appreciated. \n\nThanks!", "upvote_ratio": 1.0, "id": "t3_nqmo3h", "created_utc": 1622643009.0}
{"sub": "pytorch", "title": "Quantization in pytorch", "selftext": "I have tried to load the model to cpu(map location has been set to cpu)  which has been trained on GPU and then I got error stating the parameter size mismatch and couldn't get rid of it for long time. I've been able to remove it by adding torch.quantization.prepare_qat(net, inplace= True)\nmodel = torch quantization.convert(model.eval(), inplace= False)\n\nAnd then the model has been loaded successfully on to cpu and works.\n\nI've been through pytorch documentation but couldn't understand what exactly was happening.  Can some make me understand it please?", "upvote_ratio": 1.0, "id": "t3_nqi05c", "created_utc": 1622626946.0}
{"sub": "pytorch", "title": "Anyone use the torchdiffeq neural ODE library?", "selftext": "I\u2019ve started using this library recently, and it took me a while to learn the tricks of working with it. If anyone else has experience with this library or uses it now, I\u2019d love to do a knowledge/code swap or keep in touch to have someone to bounce ideas off of.\n\nIn my research group, I\u2019m the only one who uses this library, and only a couple others do anything related to deep learning. I\u2019d love to share some ideas with someone!", "upvote_ratio": 0.84, "id": "t3_npx0to", "created_utc": 1622562362.0}
{"sub": "pytorch", "title": "GPU out of memory", "selftext": "I am using PyTorch to build some CNN models. My dataset is some custom medical images around 200 x 200. However, my 3070 8GB GPU runs out of memory every time. I tried to use `.detach()` after each batch but the problem still appears.  I attach my code:\n\n        def training(epoch,model,data_loader):\n            model.train()\n            running_loss = 0.0\n            running_correct = 0\n            for batch_idx , (data,target) in enumerate(data_loader):\n                if IS_CUDA:\n                    data,target = data.cuda(),target.cuda()\n                data , target = Variable(data,False),Variable(target)\n                optimizer.zero_grad()\n                output = model(data.float())\n                loss = criterion(output,target) \n                running_loss += criterion(output,target)\n                preds = output.data.max(dim=1,keepdim=True)[1]\n                running_correct += preds.eq(target.data.view_as(preds)).cpu().sum()\n                #data.detach()\n                #target.detach()\n                loss.backward()\n                optimizer.step()\n            accuracy = 100. * running_correct/len(data_loader.dataset)\n            print(f'&gt; training loss is {loss:{5}.{4}} and training accuracy is {accuracy:{5}.{4}} %')\n            return loss,accuracy\n        \n        #%% Testing\n        def validation(epoch,model,data_loader):\n            model.eval()  \n            running_loss = 0.0\n            running_correct = 0\n            for batch_idx , (data,target) in enumerate(data_loader):\n                if IS_CUDA:\n                    data,target = data.cuda(),target.cuda()\n                data , target = Variable(data,True),Variable(target)\n                output = model(data.float())\n                loss = criterion(output,target) \n                running_loss += criterion(output,target)\n                preds = output.data.max(dim=1,keepdim=True)[1]\n                running_correct += preds.eq(target.data.view_as(preds)).cpu().sum()\n                #data.detach()\n                #target.detach()\n            loss = running_loss/len(data_loader.dataset)\n            accuracy = 100. * running_correct/len(data_loader.dataset)\n            print(f'&gt; validation loss is {loss:{5}.{4}} and validation accuracy is {accuracy:{5}.{4}} %')\n            return loss,accuracy  \n\nWhat should I do to train my model in the GPU for faster computations? Images shall NOT be resized to lower height/width due to loss of information. Batch size is kept to minimum. Epochs will be much larger in main training. That is just a test for 40 images out of 150 which will be 2000+ after augmentation.", "upvote_ratio": 1.0, "id": "t3_npu26a", "created_utc": 1622554114.0}
{"sub": "pytorch", "title": "Hey guys could you help me solve this problem ?", "selftext": "I have a dataset with over 1000 classes with just 10 images in each classes. In the end, I have to predict if any new image is part of the dataset or not.I tried implementing with my own neural network, tried Siamese networks. My model goes up to 72% val accuracy but shows wrong outputs.I've been on this for days.Could you guys help me out ?", "upvote_ratio": 0.33, "id": "t3_np28qh", "created_utc": 1622463981.0}
{"sub": "pytorch", "title": "Text to Image generation using path file", "selftext": "I trained a text to image generation model based on [https://github.com/aelnouby/Text-to-Image-Synthesis](https://github.com/aelnouby/Text-to-Image-Synthesis). Now I have 2 path files (one for generator , another for discriminator) . How to generate images using this path files?", "upvote_ratio": 1.0, "id": "t3_np14e6", "created_utc": 1622460163.0}
{"sub": "pytorch", "title": "Best book to learn Pytorch?", "selftext": "Hello,\n\nI know a bit of Pytorch but I would like to become an expert. Is there any book that dives very deep into Pytorch?\n\nTopics I would like to know more:\n\n\\+ A bit of review of the basics e.g. data types on Pytorch.\n\n\\+ Building trivial neural networks review (experience on MLP and CNN, not so much in RNN such as LSTM).\n\n\\+ Building nontrivial neural networks. For example strange connections.\n\n\\+ Autograd. How does Autograd work on non-trivial neural networks?\n\n\\+ Best practices.\n\n\\+ Pytorch distributed.\n\n\\+ A bit explanation of CUDA and how it is used in Pytorch.\n\n\\+ Emphasis on code.\n\nThanks.", "upvote_ratio": 1.0, "id": "t3_nozzl2", "created_utc": 1622455915.0}
{"sub": "pytorch", "title": "Pytorch machine learning deployment on Heroku getting R14 Memory Quota Exceeded warnings", "selftext": "I created a machine learning and object recognition web application. The Heroku memory quota is 512mb, my application fluctuates between 700mb and 950mb.  \n\n\nWondering what the best approach to reducing application memory usage is? Dependencies listed below.\n\n    absl-py==0.12.0\n    astroid==2.5.3\n    attrs==20.3.0\n    backcall==0.2.0\n    cachetools==4.2.1\n    certifi==2020.12.5\n    chardet==4.0.0\n    click==7.1.2\n    clickclick==20.10.2\n    cycler==0.10.0\n    Cython==0.29.23\n    decorator==5.0.5\n    Flask==1.1.2\n    google-auth==1.28.1\n    google-auth-oauthlib==0.4.4\n    grpcio==1.37.0\n    idna==2.10\n    inflection==0.5.1\n    ipython==7.22.0\n    ipython-genutils==0.2.0\n    isodate==0.6.0\n    isort==5.8.0\n    itsdangerous==1.1.0\n    jedi==0.18.0\n    Jinja2==2.11.3\n    jsonschema==3.2.0\n    kiwisolver==1.3.1\n    lazy-object-proxy==1.6.0\n    Markdown==3.3.4\n    MarkupSafe==1.1.1\n    matplotlib==3.4.1\n    mccabe==0.6.1\n    numpy==1.20.2\n    oauthlib==3.1.0\n    openapi-schema-validator==0.1.5\n    openapi-spec-validator==0.3.0\n    opencv-python==4.5.1.48\n    pandas==1.2.4\n    parso==0.8.2\n    pexpect==4.8.0\n    pickleshare==0.7.5\n    Pillow==8.2.0\n    prompt-toolkit==3.0.18\n    protobuf==3.15.8\n    ptyprocess==0.7.0\n    pyasn1==0.4.8\n    pyasn1-modules==0.2.8\n    Pygments==2.8.1\n    pylint==2.7.4\n    pyparsing==2.4.7\n    pyrsistent==0.17.3\n    python-dateutil==2.8.1\n    pytz==2021.1\n    PyYAML==5.4.1\n    requests==2.25.1\n    requests-oauthlib==1.3.0\n    rsa==4.7.2\n    scipy==1.6.2\n    seaborn==0.11.1\n    six==1.15.0\n    tensorboard==2.4.1\n    tensorboard-plugin-wit==1.8.0\n    thop==0.0.31.post2005241907\n    toml==0.10.2\n    torch==1.8.1\n    torchaudio==0.8.1\n    torchvision==0.9.1\n    tqdm==4.60.0\n    traitlets==5.0.5\n    typing-extensions==3.7.4.3\n    urllib3==1.26.4\n    watchdog==2.0.3\n    wcwidth==0.2.5\n    Werkzeug==1.0.1\n    wrapt==1.12.1", "upvote_ratio": 1.0, "id": "t3_nofx6u", "created_utc": 1622395537.0}
{"sub": "pytorch", "title": "How to make good looking network diagrams for publication?", "selftext": "I\u2019ve found a few tools online that make pretty sharp looking flow charts, but they are typically geared towards convolutional models. I just have a few dense layers and I want to make a good looking diagram showing what the model looks like, it feels incomplete just describing it. Any tips/packages that are useful?", "upvote_ratio": 1.0, "id": "t3_nnxbaj", "created_utc": 1622325735.0}
{"sub": "pytorch", "title": "Tutorials/walkthroughs of torchtext 0.9 anywhere?", "selftext": "Saw a bunch of stuff from torchtext that are now in legacy and the documentation for 0.9 is pretty poor, especially with the example code. \n\nAny help with resources would be great.", "upvote_ratio": 0.84, "id": "t3_nnvyrt", "created_utc": 1622321170.0}
{"sub": "pytorch", "title": "Can someone please help me navigate through the following error message I am getting trying to install pytorch.", "selftext": "I got the following command from PyTorch's website -\n\n    conda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch -c nvidia \n\nI get the following error -\n\n    Collecting package metadata (current_repodata.json): done\n    Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n    Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.\n    Collecting package metadata (repodata.json): done\n    Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n    Solving environment: | \n    Found conflicts! Looking for incompatible packages.\n    This can take several minutes.  Press CTRL-C to abort.\n    failed                                                                                                                                                                                                                         \n    \n    UnsatisfiableError: The following specifications were found to be incompatible with each other:\n    \n    Output in format: Requested package -&gt; Available versions\n    \n    Package cudatoolkit conflicts for:\n    cudatoolkit=11.1\n    pytorch -&gt; cudatoolkit[version='10.0.*|8.*|&gt;=10.0,&lt;10.1|&gt;=10.1,&lt;10.2|&gt;=11.1,&lt;11.2|&gt;=10.2,&lt;10.3|&gt;=11.0,&lt;11.1|&gt;=9.2,&lt;9.3|&gt;=9.0,&lt;9.1|&gt;=8.0,&lt;8.1|9.*|&gt;=10.1.243,&lt;10.2.0a0|&gt;=9.2,&lt;9.3.0a0|&gt;=10.0.130,&lt;10.1.0a0|9.2.*|&gt;=9.0,&lt;9.1.0a0|&gt;=8.0,&lt;8.1.0a0|9.0.*|8.0.*|7.5.*']\n    torchvision -&gt; pytorch==1.4.0 -&gt; cudatoolkit[version='10.0.*|&gt;=10.1.243,&lt;10.2.0a0|9.2.*|&gt;=8.0,&lt;8.1|&gt;=8.0,&lt;8.1.0a0|8.*|9.*|9.0.*|8.0.*|7.5.*']\n    torchvision -&gt; cudatoolkit[version='&gt;=10.0,&lt;10.1|&gt;=10.1,&lt;10.2|&gt;=11.1,&lt;11.2|&gt;=10.2,&lt;10.3|&gt;=11.0,&lt;11.1|&gt;=9.2,&lt;9.3|&gt;=9.0,&lt;9.1|&gt;=10.0.130,&lt;10.1.0a0|&gt;=9.2,&lt;9.3.0a0|&gt;=9.0,&lt;9.1.0a0']\n    torchaudio -&gt; pytorch==1.8.1 -&gt; cudatoolkit[version='10.0.*|&gt;=10.0,&lt;10.1|&gt;=10.1,&lt;10.2|&gt;=11.1,&lt;11.2|&gt;=10.2,&lt;10.3|&gt;=11.0,&lt;11.1|&gt;=9.2,&lt;9.3|&gt;=10.1.243,&lt;10.2.0a0|&gt;=9.2,&lt;9.3.0a0|&gt;=10.0.130,&lt;10.1.0a0|9.2.*|&gt;=9.0,&lt;9.1|&gt;=9.0,&lt;9.1.0a0']\n    \n    Package libstdcxx-ng conflicts for:\n    torchvision -&gt; cudatoolkit[version='&gt;=11.1,&lt;11.2'] -&gt; libstdcxx-ng[version='&gt;=7.2.0|&gt;=9.3.0']\n    torchaudio -&gt; python[version='&gt;=3.9,&lt;3.10.0a0'] -&gt; libstdcxx-ng[version='&gt;=5.4.0|&gt;=7.2.0|&gt;=7.3.0']\n    cudatoolkit=11.1 -&gt; libstdcxx-ng[version='&gt;=7.3.0|&gt;=9.3.0']\n    pytorch -&gt; libstdcxx-ng[version='&gt;=5.4.0|&gt;=7.3.0']\n    pytorch -&gt; cudatoolkit[version='&gt;=11.1,&lt;11.2'] -&gt; libstdcxx-ng[version='&gt;=7.2.0|&gt;=9.3.0']\n    python=3.9 -&gt; libstdcxx-ng[version='&gt;=7.3.0']\n    torchvision -&gt; libstdcxx-ng[version='&gt;=5.4.0|&gt;=7.3.0']\n    \n    Package pytorch conflicts for:\n    pytorch\n    torchvision -&gt; pytorch[version='1.1.*|1.2.0+cu92|1.2.0|1.3.0|1.3.1|1.4.0|1.5.0|1.5.1|1.6.0|1.7.0|1.7.1|1.8.0|1.8.1|&gt;=1.1.0|&gt;=1.0.0|&gt;=0.4|&gt;=0.3|1.3.1.*|1.2.0.*']\n    torchaudio -&gt; pytorch[version='1.2.0|1.3.0|1.3.1|1.4.0|1.5.0|1.5.1|1.6.0|1.7.0|1.7.1|1.8.0|1.8.1|&gt;=1.1.0']\n    \n    Package _libgcc_mutex conflicts for:\n    cudatoolkit=11.1 -&gt; libgcc-ng[version='&gt;=7.3.0'] -&gt; _libgcc_mutex=[build=main]\n    torchvision -&gt; libgcc-ng[version='&gt;=7.3.0'] -&gt; _libgcc_mutex=[build=main]\n    python=3.9 -&gt; libgcc-ng[version='&gt;=7.3.0'] -&gt; _libgcc_mutex=[build=main]\n    pytorch -&gt; libgcc-ng[version='&gt;=7.3.0'] -&gt; _libgcc_mutex=[build=main]\n    \n    Package libgcc-ng conflicts for:\n    python=3.9 -&gt; libgcc-ng[version='&gt;=7.3.0']\n    python=3.9 -&gt; zlib[version='&gt;=1.2.11,&lt;1.3.0a0'] -&gt; libgcc-ng[version='&gt;=7.2.0']\n    \n    Package six conflicts for:\n    pytorch -&gt; mkl-service[version='&gt;=2,&lt;3.0a0'] -&gt; six\n    torchvision -&gt; sixThe following specifications were found to be incompatible with your system:\n    \n      - feature:/linux-64::__glibc==2.33=0\n      - feature:|@/linux-64::__glibc==2.33=0\n      - cudatoolkit=11.1 -&gt; __glibc[version='&gt;=2.17,&lt;3.0.a0']\n      - pytorch -&gt; cudatoolkit[version='&gt;=11.1,&lt;11.2'] -&gt; __glibc[version='&gt;=2.17,&lt;3.0.a0']\n      - torchvision -&gt; cudatoolkit[version='&gt;=11.1,&lt;11.2'] -&gt; __glibc[version='&gt;=2.17,&lt;3.0.a0']\n    \n    Your installed version is: 2.33\n    \n    \n    \n\nI am not being able to understand exactly which package is creating the issue. I am trying to install this in a virtual env through conda.", "upvote_ratio": 0.67, "id": "t3_nmswap", "created_utc": 1622189367.0}
{"sub": "pytorch", "title": "Digging into TorchVision's MobileNetV3 implementation", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_nmjge0", "created_utc": 1622154711.0}
{"sub": "pytorch", "title": "Lego generator. You could use it to generate any lego-based images or videos", "selftext": "nan", "upvote_ratio": 0.48, "id": "t3_nm69pr", "created_utc": 1622118319.0}
{"sub": "pytorch", "title": "I published a new tutorial in \"PyTorch for Audio + Music Processing\": \"Making Predictions with PyTorch Deep Learning Models\"", "selftext": "In my new tutorial, you can learn how to make inferences with an already trained PyTorch model.\n\nThis video is part of the \u201cPyTorch for Audio and Music Processing\u201d series, which aims to teach you how to use PyTorch and torchaudio for audio-based Deep Learning projects. \n\nHave fun! \n\n[https://www.youtube.com/watch?v=0Q5KTt2R5w4&amp;list=PL-wATfeyAMNoirN4idjev6aRu8ISZYVWm&amp;index=3](https://www.youtube.com/watch?v=0Q5KTt2R5w4&amp;list=PL-wATfeyAMNoirN4idjev6aRu8ISZYVWm&amp;index=3)", "upvote_ratio": 0.94, "id": "t3_nm4o2q", "created_utc": 1622112352.0}
{"sub": "pytorch", "title": "Tutorial: Pruning and Quantizing PyTorch YOLOv3 for Real-time Laptop Performance", "selftext": "nan", "upvote_ratio": 0.97, "id": "t3_nll7h1", "created_utc": 1622046735.0}
{"sub": "pytorch", "title": "Data Structure coding for GDAS NAS", "selftext": "[https://github.com/D-X-Y/AutoDL-Projects/issues/99#issuecomment-835713058](https://github.com/D-X-Y/AutoDL-Projects/issues/99#issuecomment-835713058)\n\nhow to code the following graph structure ? which kind of data structure is suitable in this case especially for both forward inference and backward propagation ?  would [https://networkx.org/documentation/stable/tutorial.html#multigraphs](https://networkx.org/documentation/stable/tutorial.html#multigraphs) be suitable  ?\n\n[https://gist.github.com/promach/b6f526c56e20f029d68e6f9041c3f5c0#file-gdas-py-L106-L107](https://gist.github.com/promach/b6f526c56e20f029d68e6f9041c3f5c0#file-gdas-py-L106-L107)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/kivfxaw68a171.png?width=976&amp;format=png&amp;auto=webp&amp;s=70d37159a15ecdab923487bad05cc4fba4e15425", "upvote_ratio": 0.66, "id": "t3_nkrydk", "created_utc": 1621955286.0}
{"sub": "pytorch", "title": "Unable to run torch on GPU", "selftext": "Hi,\nI am new to pytorch. Everytime i run:\n\n1) torch. Cuda. Is available  i get FALSE\n2)  torch. Version shows 1.8.1+cpu\n3) cuda. Gpu shows mx130 [supported] \n\nI Have Windows,py 3.7 Idle, Nvidia MX 130 whose Compute capability is 5.0.\n\nHere are the steps i performed :\n\n1) I installed cuda 11.0.2 from nvdia website\n2) i extracted cudnn 8.0 and replaced cuda toolkit lib, include and bin file with cudnn's file. \n\n3) i did pip install Torch ==1.8.1\n\n\nnvidia - smi displays\nNVIDIA SMI 462.3\nCUDA VERSION : 11.2", "upvote_ratio": 0.67, "id": "t3_nkmyjw", "created_utc": 1621940343.0}
{"sub": "pytorch", "title": "Trouble importing pytorch once installed", "selftext": "I have successfully installed torch-1.8.1+cpu on windows 10 using pip3, but when I try \"import torch as T\" in cmd I get the error: 'Error loading \"(rest of path) cudnn_adv_infer64_8.dll\" or one of its dependencies.'.\n\nAny help is greatly appreciated! If there is a better place to ask this please let me know, I haven't found this exact issue anywhere else online.", "upvote_ratio": 0.6, "id": "t3_nklixy", "created_utc": 1621934697.0}
{"sub": "pytorch", "title": "I published a new tutorial in my \"Pytorch for Audio + Music\" series: \"Implementing and Training a Neural Network with PyTorch\"", "selftext": "I\u2019m excited to publish the first tutorial in the \u201cPytorch for Audio + Music\u201d series. \n\nIn this installment, I start from the basics. What\u2019s better than getting our hands dirty training a simple neural network on a toy dataset?\n\nIn this video, you can get a first approach to the PyTorch framework, learn its fundamental components, while working on a self-contained project. \n\nHave fun! \n\n  \n[https://www.youtube.com/watch?v=4p0G6tgNLis&amp;list=PL-wATfeyAMNoirN4idjev6aRu8ISZYVWm&amp;index=2&amp;ab\\_channel=ValerioVelardo-TheSoundofAIValerioVelardo-TheSoundofAI](https://www.youtube.com/watch?v=4p0G6tgNLis&amp;list=PL-wATfeyAMNoirN4idjev6aRu8ISZYVWm&amp;index=2&amp;ab_channel=ValerioVelardo-TheSoundofAIValerioVelardo-TheSoundofAI)", "upvote_ratio": 0.97, "id": "t3_nk0pgo", "created_utc": 1621870450.0}
{"sub": "pytorch", "title": "Re-evaluate gradient in step method on new data point?", "selftext": "Hi folks,\n\nI am trying to implement an GD-based optimizer where I need to re-evaluate the gradient at the step that just got calculated. I.e. imagine we have the basic\n\nx\\_next = x\\_prev - learning\\_rate\\*grad(x\\_prev)\n\nonce I have x\\_ next I need to do a calculation on the next line of code based on grad(x\\_next)  (basically in the same step of the optimizer).\n\nAny ideas on how one can achieve this using pytorch?\n\n&amp;#x200B;\n\nEDIT: never mind I think I need to use \"closure\" here no?", "upvote_ratio": 1.0, "id": "t3_nk0cym", "created_utc": 1621869523.0}
{"sub": "pytorch", "title": "How to calculate Flops for pixel shuffle in Pytorch?", "selftext": "I have tried the ptflops, thops and touch scan library to calculate the flops of my model (contain pixelshuffle operation)\n\nHowever, these library don't support the pixel shuffle operation and treat it as zero flops.\n\nAlso, I cannot find any equation to calculate flops for pixel shuffles.\n\nAnyone has idea for that?\n\nThanks.", "upvote_ratio": 1.0, "id": "t3_njtvoq", "created_utc": 1621848560.0}
{"sub": "pytorch", "title": "ResNet-18 magnitude based pruning", "selftext": "ResNet-18 global, unstrctured, magnitude based and iterative pruning with CIFAR-10 dataset. The pruning goes on till 99.08% sparsity.\n\nThis is based on the research papers:\n\n1. \"Learning both Weights and Connections for Efficient Neural Networks\" by Song Han et al.\n2. \"Deep Compression: by Song Han et al.\n3. \"The Lottery Ticket Hypothesis\" by Frankle et al.\n4. \"What is the State of Neural Network Pruning?\" by Blalock et al.\n\nOriginal and unpruned model has  val\\_accuracy = 88.990% . Original model size = 42.7 MB, zipped model size = 40 MB.\n\nPruned model with sparsity = 99.063% has  val\\_accuracy = 91.260%. Pruned, trained and zipped model size = 3.5 MB. This results into a compression ratio = 11.43%.\n\nYou can refer to the code [here](https://github.com/arjun-majumdar/Neural_Network_Pruning/blob/main/ResNet18_Global_Magnitude_Custom_Pruning.ipynb).\n\n*NOTE:* Post pruning PyTorch doesn't cast tensors to sparse format. Therefore, the tensors are of the same dimensions as before but with 0s in it to denote pruned connections.\n\nThoughts?", "upvote_ratio": 1.0, "id": "t3_nj9pvc", "created_utc": 1621782807.0}
{"sub": "pytorch", "title": "Difficulty in using LSTMs for text generation", "selftext": "I am currently trying quote generation (character level) with LSTMs using Pytorch. I am currently facing some issues understanding exactly how the hidden state is implemented in Pytorch.\n\n**Some details:**\n\nI have a list of quotes from a character in a TV series. I am converting those to a sequence of integers with each character corresponding to a certain integer by using a dictionary *char2idx*. I also have the inverse of this *idx2char* where the mapping is reversed.\n\nAfter that, I am using a sliding window, say of size *window\\_size*, and a step of size *step* to prepare the data.\n\nAs an example, let's say the sequence is *\\[1, 2, 3, 4, 5, 0\\]* where 0 stands for the EOS character. Then using window\\_size = 3 and step = 2, I get the sequence for x and y as:\n\n    x1 = [1, 2, 3], y1 = [2, 3, 4]\n    x2 = [3, 4, 5], y1 = [4, 5, 0]\n    \n    x = [x1, x2], y = [y1, y2]\n\nThe next step is to train the model. I have attached the code I am using to train the model.\n\n**NOTE:** I am not passing hidden states from one batch to the other as the ith sequence of the (j+1)th batch is probably not the next step to the ith sequence from the jth batch. (This is why I am using a sliding window to help the model remember). Is there a better way to do this?\n\nMy main question occurs during testing time. There are two methods by which I am testing.\n\n**Method 1:**\n\nI take the initial seed string, pass it into the model and get the next character as the prediction. Now, I add that to the starting string and pass this whole sequence into the model, without passing the hidden state. That is, I input the whole sequence to the model, with the LSTM having the initial hidden state as 0, get the output, append the output to the sequence and repeat till I encounter the EOS character.\n\n**Method 2:**\n\nI take the initial seed string, pass it into the model and get the next character as the prediction. Now, I just pass the character and the previous hidden state as the next input and continue doing so until an EOS character is encountered.\n\n**Question**\n\n1. According to my current understanding, the outputs of both methods should be the same because the same thing should be happening in both.\n2. What's actually happening is that both methods are giving completely different results. Why is this happening?\n3. The second one gets stuck in an infinite loop for most inputs (e.g. it gives \"back to back to back to ....\") and on some inputs, the first one also gets stuck. How to prevent and avoid this?\n4. Is this related in some way to the training?\n\n&amp;#x200B;\n\nI have tried multiple different ways (using bidirectional LSTMs, using one hot encoding (instead of embedding), changing the batch sizes, not using a sliding window approach (using padding and feeding the whole quote at once).\n\nI cannot figure out how to solve this issue. Any help would be greatly appreciated.\n\n**CODE**\n\nCode for the Model Class:\n\n    class RNN(nn.Module):\n        def __init__(self, vocab_size, hidden_size, num_layers, dropout=0.15):\n            super(RNN, self).__init__()\n            self.vocab_size = vocab_size\n            self.hidden_size = hidden_size\n            self.num_layers = num_layers\n            self.embedding = nn.Embedding(vocab_size, hidden_size)\n            self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, dropout=dropout, batch_first=True)\n            self.dense1 = nn.Linear(hidden_size, hidden_size*4)\n            self.dense2 = nn.Linear(hidden_size*4, hidden_size*2)\n            self.dense3 = nn.Linear(hidden_size*2, vocab_size)\n            self.drop = nn.Dropout(dropout)\n        def forward(self, X, h=None, c=None):\n            if h is None:\n                h, c = self.init_hidden(X.size(0))\n            out = self.embedding(X)\n            out, (h, c) = self.lstm(out, (h, c))\n            out = self.drop(out)\n            out = self.dense1(out.reshape(-1, self.hidden_size)) # Reshaping it into (batch_size*seq_len, hidden_size)\n            out = self.dense2(out)\n            out = self.dense3(out)\n            return out, h, c\n        def init_hidden(self, batch_size):\n            num_l = self.num_layers\n            hidden = torch.zeros(num_l, batch_size, self.hidden_size).to(DEVICE)\n            cell = torch.zeros(num_l, batch_size, self.hidden_size).to(DEVICE)\n            return hidden, cell\n\nCode for training:\n\n    rnn = RNN(VOCAB_SIZE, HIDDEN_SIZE, NUM_LAYERS).to(DEVICE)\n    optimizer = torch.optim.Adam(rnn.parameters(), lr=1e-3)\n    criterion = nn.CrossEntropyLoss()\n    \n    rnn.train()\n    history = {}\n    best_loss = 100\n    \n    for epoch in range(EPOCHS): #EPOCH LOOP\n        counter = 0\n        epoch_loss = 0\n        for x, y in train_loader: #BATCH LOOP\n            optimizer.zero_grad()\n            counter += 1\n            \n            o, h, c = rnn(x)\n            loss = criterion(o, y.reshape(-1))\n            epoch_loss += loss.item()\n            loss.backward()\n            nn.utils.clip_grad_norm_(rnn.parameters(), 5) # Clipping Gradients\n            optimizer.step()\n        \n        if counter%print_every == 0:\n        print(f\"[INFO] EPOCH: {epoch+1}, BATCH: {counter}, TRAINING LOSS: {loss.item()}\")\n        epoch_loss = epoch_loss/counter\n        history[\"train_loss\"] = history.get(\"train_loss\", []) + [epoch_loss]\n        print(f\"\\nEPOCH: {epoch+1} COMPLETED!\\nTRAINING LOSS: {epoch_loss}\\n\")\n\nMethod 1 Code:\n\n    with torch.no_grad():\n        w = None\n        start_str = \"Hey, \"\n        x1 = quote2seq(start_str)[:-1]\n        \n        while w != EOS_TOKEN:\n            x1 = torch.tensor(x1, device=DEVICE).unsqueeze(0)\n            o1, h1, c1 = rnn(x1)\n            p1 = F.softmax(o1, dim=1).detach()\n            q1 = np.argmax(p1.cpu(), axis=1)[-1].item()\n            w = idx2char[q1]\n            start_str += w\n            x1 = x1.tolist()[0]+ [q1]\n    quote = start_str.replace(\"&lt;EOS&gt;\", \"\")\n\nMethod 2 Code:\n\n    with torch.no_grad():\n        w = None\n        start_str = \"Are we back\"\n        x1 = quote2seq(start_str)[:-1]\n        h1, c1 = rnn.init_hidden(1)\n        \n        while w != EOS_TOKEN:\n            x1 = torch.tensor(x1, device=DEVICE).unsqueeze(0)\n            h1, c1 = h1.data, c1.data\n            o1, h1, c1 = rnn(x1, h1, c1)\n            p1 = F.softmax(o1, dim=1).detach()\n            q1 = np.argmax(p1.cpu(), axis=1)[-1].item()\n            w = idx2char[q1]\n            start_str += w\n            x1 = [q1]\n    quote = start_str.replace(\"&lt;EOS&gt;\", \"\")\n\n&amp;#x200B;", "upvote_ratio": 1.0, "id": "t3_nj6xbl", "created_utc": 1621773465.0}
{"sub": "pytorch", "title": "PyTorch, Tensorflow Structure", "selftext": "Hi all,\n\nGoing through PyTorch and Tensorflow implementations, every author has its own structure and way of writing the code. It is very easy to write crappy and confusing code since there isn't a guideline behind writing models in PyTorch. In comparison when working on web development, a standard approach is to use the Model, View, Controller structure. \n\nWould a framework that creates a basic structure and the appropriate dataloaders be useful? There is [this](https://github.com/victoresque/pytorch-template) project that tries to achieve that but it is not universal and has limited options.  \n\n\nOne example of the framework would be writing a command to start a project with a basic structure,  downloading models, or building data loaders depending on the task. Explain with commends on the templates how things should be done such as building data loaders that return dictionaries.\n\nDo you think a project like that would be helpful? Do you know maybe of any projects like that?", "upvote_ratio": 1.0, "id": "t3_nj5qcl", "created_utc": 1621768634.0}
{"sub": "pytorch", "title": "StyleGAN2 implementation in PyTorch with side-by-side notes", "selftext": "Implemented StyleGAN2 model and training loop from paper \"Analyzing and Improving the Image Quality of StyleGAN\".\n\nCode with annotations: [https://nn.labml.ai/gan/stylegan/index.html](https://nn.labml.ai/gan/stylegan/index.html)\n\nThis is a minimalistic implementation with only 425 lines of code and lots of documentations and diagrams explaining the model.\n\n* [Github](https://github.com/lab-ml/annotated_deep_learning_paper_implementations/tree/master/labml_nn/gan/stylegan)\n* [Paper on arXiv](https://arxiv.org/abs/1912.04958)\n* [Twitter Thread](https://twitter.com/labmlai/status/1396298504872423425)", "upvote_ratio": 0.95, "id": "t3_nj3z5v", "created_utc": 1621761042.0}
{"sub": "pytorch", "title": "How can I create a Pytorch Dataloader from a hdf5 file with multiple groups/datasets?", "selftext": "Say that from an image folder with 9k images I have 4k images of size  (100,400) , 2k images of size(150 ,350) and the rest have a size of (200  , 500) I can use a single hdf5 file to store all three types of data  subsets using \n\n    create_group/_create_dataset\n\nWhat I want to learn is how would i modify \n\n    collate_fn ,BatchSampler ,Dataset\n\n class when I have, suppose a single hdf5 file that has three groups ,  each with a dataset of images of same sizes .... now with a single \n\n    dataset/group\n\n in hdf5 file it's simple and straightforward to get each item with an  index ... but with more groups/datasets I can not understand how can i  get batches from the data.", "upvote_ratio": 1.0, "id": "t3_nj134s", "created_utc": 1621748479.0}
{"sub": "pytorch", "title": "Why is pytorch take up 2gb during pip install", "selftext": "i install pytorch using 1.7.1+cu110. is there a way to lessen the install size?", "upvote_ratio": 0.4, "id": "t3_niip01", "created_utc": 1621690270.0}
{"sub": "pytorch", "title": "Torchvision Object Detection", "selftext": "Is someone using Torchvision Object Detection API for Pascal VOC using Faster-RCNN and have some tricks how to reach the 70% mAP that is SOTA using this architecture? \n\nAnd does anyone know if the Torchvision detection API is feasible to compare against available Pytorch Implementations of Faster-RCNN", "upvote_ratio": 1.0, "id": "t3_ngylaw", "created_utc": 1621512807.0}
{"sub": "pytorch", "title": "Is there a way to build Pipeline like Scikit-learn that bounds data transformation and model?", "selftext": "I believe it's a common requirement for many machine learning project: After model is fine-tuned, create a pipeline that could do data preprocessing and model inference sequentially.\n\n&amp;#x200B;\n\nIn Scikit-learn, they have Pipeline module that could do the task, but I couldn't find that kind of function in pytorch. The Pipeline in pytorch seems to serve for multiprocessing, not combine different workflows.\n\n&amp;#x200B;\n\nI'd found online that use Scikit-learn combined with Pytorch model, but now I need to convert the whole workflow into ONNX, using more than one framework might be something I want to avoid.\n\n&amp;#x200B;\n\nIs scikit-learn the only way to build a pipeline? Thank you!", "upvote_ratio": 1.0, "id": "t3_ngtb17", "created_utc": 1621494053.0}
{"sub": "pytorch", "title": "PyTorch YOLOv5 - Microsoft C++ Build Tools", "selftext": "I am trying to install PyTorch YOLOv5 from ultralytics from [here](https://pytorch.org/hub/ultralytics_yolov5/) in Windows 10 x86\\_64 system. The instructions seem pretty straightforward and I after having installed PyTorch for GPU, I am attempting to install the required requirements by using the command:\n\npip install -qr [https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt](https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt)\n\n&amp;#x200B;\n\nto which I get the following ERROR log:\n\n&amp;#x200B;\n\n&gt;ERROR: Command errored out with exit status 1:    command:  \n&gt;  \n&gt;'C:\\\\Users\\\\arjun\\\\anaconda3\\\\envs\\\\pytorch\\_object\\_detection\\\\python.exe' -u  \n&gt;  \n&gt;\\-c 'import sys, setuptools, tokenize; sys.argv\\[0\\] = '\"'\"'C:\\\\\\\\Users\\\\\\\\arjun\\\\\\\\AppData\\\\\\\\Local\\\\\\\\Temp\\\\\\\\pip-install-7kbo300l\\\\\\\\pycocotools\\_e5774d8d59d14fa9b3baece40c2b7248\\\\\\\\[setup.py](https://setup.py)'\"'\"';  \n&gt;  \n&gt;\\_\\_file\\_\\_='\"'\"'C:\\\\\\\\Users\\\\\\\\arjun\\\\\\\\AppData\\\\\\\\Local\\\\\\\\Temp\\\\\\\\pip-install-7kbo300l\\\\\\\\pycocotools\\_e5774d8d59d14fa9b3baece40c2b7248\\\\\\\\[setup.py](https://setup.py)'\"'\"';f=getattr(tokenize,  \n&gt;  \n&gt;'\"'\"'open'\"'\"', open)(\\_\\_file\\_\\_);[code=f.read](https://code=f.read)().replace('\"'\"'\\\\r\\\\n'\"'\"',  \n&gt;  \n&gt;'\"'\"'\\\\n'\"'\"');f.close();exec(compile(code, \\_\\_file\\_\\_, '\"'\"'exec'\"'\"'))'  \n&gt;  \n&gt;bdist\\_wheel -d 'C:\\\\Users\\\\arjun\\\\AppData\\\\Local\\\\Temp\\\\pip-wheel-kc1jnk9w'  \n&gt;  \n&gt;cwd: C:\\\\Users\\\\arjun\\\\AppData\\\\Local\\\\Temp\\\\pip-install-7kbo300l\\\\pycocotools\\_e5774d8d59d14fa9b3baece40c2b7248\\\\  \n&gt;  \n&gt;Complete output (16 lines):   running bdist\\_wheel   running build    \n&gt;  \n&gt;running build\\_py   creating build   creating build\\\\lib.win-amd64-3.8    \n&gt;  \n&gt;creating build\\\\lib.win-amd64-3.8\\\\pycocotools   copying  \n&gt;  \n&gt;pycocotools\\\\[coco.py](https://coco.py) \\-&gt; build\\\\lib.win-amd64-3.8\\\\pycocotools   copying  \n&gt;  \n&gt;pycocotools\\\\[cocoeval.py](https://cocoeval.py) \\-&gt; build\\\\lib.win-amd64-3.8\\\\pycocotools    \n&gt;  \n&gt;copying pycocotools\\\\[mask.py](https://mask.py) \\-&gt; build\\\\lib.win-amd64-3.8\\\\pycocotools    \n&gt;  \n&gt;copying pycocotools\\\\\\_\\_init\\_\\_.py -&gt; build\\\\lib.win-amd64-3.8\\\\pycocotools  \n&gt;  \n&gt;running build\\_ext   cythoning pycocotools/\\_mask.pyx to  \n&gt;  \n&gt;pycocotools\\\\\\_mask.c    \n&gt;  \n&gt;C:\\\\Users\\\\arjun\\\\anaconda3\\\\envs\\\\pytorch\\_object\\_detection\\\\lib\\\\site-packages\\\\Cython\\\\Compiler\\\\[Main.py:369](https://Main.py:369):  \n&gt;  \n&gt;FutureWarning: Cython directive 'language\\_level' not set, using 2 for  \n&gt;  \n&gt;now (Py2). This will change in a later release! File:  \n&gt;  \n&gt;C:\\\\Users\\\\arjun\\\\AppData\\\\Local\\\\Temp\\\\pip-install-7kbo300l\\\\pycocotools\\_e5774d8d59d14fa9b3baece40c2b7248\\\\pycocotools\\\\\\_mask.pyx  \n&gt;  \n&gt;tree = Parsing.p\\_module(s, pxd, full\\_module\\_name)   building 'pycocotools.\\_mask' extension   error: Microsoft Visual C++ 14.0 or  \n&gt;  \n&gt;greater is required. Get it with \"Microsoft C++ Build Tools\":  \n&gt;  \n&gt;[https://visualstudio.microsoft.com/visual-cpp-build-tools/](https://visualstudio.microsoft.com/visual-cpp-build-tools/)    \n&gt;  \n&gt;\\----------------------------------------   ERROR: Failed building wheel for pycocotools  \n&gt;  \n&gt;ERROR: Command errored out with exit status 1:  \n&gt;  \n&gt;command: 'C:\\\\Users\\\\arjun\\\\anaconda3\\\\envs\\\\pytorch\\_object\\_detection\\\\python.exe' -u  \n&gt;  \n&gt;\\-c 'import sys, setuptools, tokenize; sys.argv\\[0\\] = '\"'\"'C:\\\\\\\\Users\\\\\\\\arjun\\\\\\\\AppData\\\\\\\\Local\\\\\\\\Temp\\\\\\\\pip-install-7kbo300l\\\\\\\\pycocotools\\_e5774d8d59d14fa9b3baece40c2b7248\\\\\\\\[setup.py](https://setup.py)'\"'\"';  \n&gt;  \n&gt;\\_\\_file\\_\\_='\"'\"'C:\\\\\\\\Users\\\\\\\\arjun\\\\\\\\AppData\\\\\\\\Local\\\\\\\\Temp\\\\\\\\pip-install-7kbo300l\\\\\\\\pycocotools\\_e5774d8d59d14fa9b3baece40c2b7248\\\\\\\\[setup.py](https://setup.py)'\"'\"';f=getattr(tokenize,  \n&gt;  \n&gt;'\"'\"'open'\"'\"', open)(\\_\\_file\\_\\_);[code=f.read](https://code=f.read)().replace('\"'\"'\\\\r\\\\n'\"'\"',  \n&gt;  \n&gt;'\"'\"'\\\\n'\"'\"');f.close();exec(compile(code, \\_\\_file\\_\\_, '\"'\"'exec'\"'\"'))'  \n&gt;  \n&gt;install --record  \n&gt;  \n&gt;'C:\\\\Users\\\\arjun\\\\AppData\\\\Local\\\\Temp\\\\pip-record-l60dglwi\\\\install-record.txt'  \n&gt;  \n&gt;\\--single-version-externally-managed --compile --install-headers 'C:\\\\Users\\\\arjun\\\\anaconda3\\\\envs\\\\pytorch\\_object\\_detection\\\\Include\\\\pycocotools'  \n&gt;  \n&gt;cwd: C:\\\\Users\\\\arjun\\\\AppData\\\\Local\\\\Temp\\\\pip-install-7kbo300l\\\\pycocotools\\_e5774d8d59d14fa9b3baece40c2b7248\\\\  \n&gt;  \n&gt;Complete output (14 lines):  \n&gt;  \n&gt;running install  \n&gt;  \n&gt;running build  \n&gt;  \n&gt;running build\\_py  \n&gt;  \n&gt;creating build  \n&gt;  \n&gt;creating build\\\\lib.win-amd64-3.8  \n&gt;  \n&gt;creating build\\\\lib.win-amd64-3.8\\\\pycocotools  \n&gt;  \n&gt;   copying pycocotools\\\\[coco.py](https://coco.py) \\-&gt; build\\\\lib.win-amd64-3.8\\\\pycocotools  \n&gt;  \n&gt;   copying pycocotools\\\\[cocoeval.py](https://cocoeval.py) \\-&gt; build\\\\lib.win-amd64-3.8\\\\pycocotools  \n&gt;  \n&gt;copying pycocotools\\\\[mask.py](https://mask.py) \\-&gt; build\\\\lib.win-amd64-3.8\\\\pycocotools  \n&gt;  \n&gt;copying pycocotools\\\\\\_\\_init\\_\\_.py -&gt; build\\\\lib.win-amd64-3.8\\\\pycocotools  \n&gt;  \n&gt;running build\\_ext  \n&gt;  \n&gt;skipping 'pycocotools\\\\\\_mask.c' Cython extension (up-to-date)  \n&gt;  \n&gt;building 'pycocotools.\\_mask' extension  \n&gt;  \n&gt;error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\":  \n&gt;  \n&gt;[https://visualstudio.microsoft.com/visual-cpp-build-tools/](https://visualstudio.microsoft.com/visual-cpp-build-tools/)  \n&gt;  \n&gt;\\---------------------------------------- ERROR: Command errored out with exit status 1:  \n&gt;  \n&gt;'C:\\\\Users\\\\arjun\\\\anaconda3\\\\envs\\\\pytorch\\_object\\_detection\\\\python.exe' -u  \n&gt;  \n&gt;\\-c 'import sys, setuptools, tokenize; sys.argv\\[0\\] = '\"'\"'C:\\\\\\\\Users\\\\\\\\arjun\\\\\\\\AppData\\\\\\\\Local\\\\\\\\Temp\\\\\\\\pip-install-7kbo300l\\\\\\\\pycocotools\\_e5774d8d59d14fa9b3baece40c2b7248\\\\\\\\[setup.py](https://setup.py)'\"'\"';  \n&gt;  \n&gt;\\_\\_file\\_\\_='\"'\"'C:\\\\\\\\Users\\\\\\\\arjun\\\\\\\\AppData\\\\\\\\Local\\\\\\\\Temp\\\\\\\\pip-install-7kbo300l\\\\\\\\pycocotools\\_e5774d8d59d14fa9b3baece40c2b7248\\\\\\\\[setup.py](https://setup.py)'\"'\"';f=getattr(tokenize,  \n&gt;  \n&gt;'\"'\"'open'\"'\"', open)(\\_\\_file\\_\\_);[code=f.read](https://code=f.read)().replace('\"'\"'\\\\r\\\\n'\"'\"',  \n&gt;  \n&gt;'\"'\"'\\\\n'\"'\"');f.close();exec(compile(code, \\_\\_file\\_\\_, '\"'\"'exec'\"'\"'))'  \n&gt;  \n&gt;install --record  \n&gt;  \n&gt;'C:\\\\Users\\\\arjun\\\\AppData\\\\Local\\\\Temp\\\\pip-record-l60dglwi\\\\install-record.txt'  \n&gt;  \n&gt;\\--single-version-externally-managed --compile --install-headers 'C:\\\\Users\\\\arjun\\\\anaconda3\\\\envs\\\\pytorch\\_object\\_detection\\\\Include\\\\pycocotools'  \n&gt;  \n&gt;Check the logs for full command output.\n\n&amp;#x200B;\n\nI have installed Microsoft C++ Build Tools and get the following output in CMD:\n\n&gt; \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*  \n&gt;  \n&gt; \\*\\* Visual Studio 2019 Developer Command Prompt v16.9.6  \n&gt;  \n&gt; \\*\\* Copyright (c) 2021 Microsoft Corporation  \n&gt;  \n&gt; \\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\n\nI am trying to reinstall the requirements.txt but the error for Microsoft C++ Build Tools still exist.\n\n&amp;#x200B;\n\nWhat should I do?", "upvote_ratio": 1.0, "id": "t3_ngrhrx", "created_utc": 1621487492.0}
{"sub": "pytorch", "title": "Aggregating batched multi-label predictions", "selftext": "Hi everybody,\n\nI have a pytorch question and I hope it\u2019s a simple answer. Thanks in advance for your help!\n\nI have a model which makes multi-label predictions. So on a batch input, the output is a tensor of shape B x L (where B is the batch size, L is the number of labels). I\u2019m trying to use Monte Carlo dropout to generate a distribution of N predictions for each instance (ie basically just activate the dropout layers at prediction time to get a population of N probabilistic predictions). So I use a loop to get N B x L tensors, and I want to convert these into 1 B x L x N tensor. I can\u2019t figure out this operation though. Any help with useful resources or if you know the syntax to do so, please let me know. Thanks!", "upvote_ratio": 1.0, "id": "t3_nga60s", "created_utc": 1621441299.0}
{"sub": "pytorch", "title": "loss_fn expected scalar type Long but found Float", "selftext": "Apologies in advance, I'm an absolute beginner at neural networks.\n\nI'm trying to adapt [this](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html) tutorial to work with some csv data in the following format:\n\n\"xlabel1\"         \"xlabel2\"         \"ylabel'\n\n88.00788879\t54.17111206\t90.29861259\n\n88.00788879\t54.17111206\t89.44630686\n\n88.00788879\t54.17111206\t89.73772812\n\n   .                        .                            .\n\n   .                        .                            .\n\n   .                        .                            .\n\n&amp;#x200B;\n\n    import torch\n    from torch import nn\n    from torch.utils.data import DataLoader\n    from torchvision import datasets\n    from torchvision.transforms import ToTensor, Lambda, Compose\n    import matplotlib.pyplot as plt\n    import pandas as pd\n    import numpy as np\n    import torch as T\n    \n    \n    # Get cpu or gpu device for training.\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(\"Using {} device\".format(device))\n    \n    \n    class PeopleDataset(T.utils.data.Dataset):\n    \n        def __init__(self, src_file, num_rows=None):\n            x_tmp = np.loadtxt(src_file, max_rows=num_rows,\n                               usecols=range(0, 2), delimiter=\",\",\n                               skiprows=1, dtype=np.float32)\n            y_tmp = np.loadtxt(src_file, max_rows=num_rows,\n                               usecols=2, delimiter=\",\", skiprows=1,\n                               dtype=np.float32)\n    \n            self.x_data = T.tensor(x_tmp,\n                                   dtype=T.float32).to(device)\n            self.y_data = T.tensor(y_tmp,\n                                   dtype=T.float32).to(device)\n    \n        def __len__(self):\n            return len(self.x_data)  # required\n    \n        def __getitem__(self, idx):\n            if T.is_tensor(idx):\n                idx = idx.tolist()\n            preds = self.x_data[idx, 0:2]\n            pol = self.y_data[idx]\n            sample = {'predictors': preds, 'airflow': pol}\n            # print(sample)\n            return sample\n    \n    \n    train_ds = PeopleDataset('airflow-airflow_setpoint.csv', num_rows=43224)\n    train_dataloader = DataLoader(train_ds, batch_size=64)\n    test_dataloader = DataLoader(train_ds, batch_size=64)\n    \n    \n    # Define model\n    class NeuralNetwork(nn.Module):\n        def __init__(self):\n            super(NeuralNetwork, self).__init__()\n            self.flatten = nn.Flatten()\n            self.linear_relu_stack = nn.Sequential(\n                nn.Linear(1*2, 2),\n                nn.ReLU(),\n                nn.Linear(2, 2),\n                nn.ReLU(),\n                nn.Linear(2, 1),\n                nn.ReLU()\n            )\n    \n        def forward(self, x):\n            x = self.flatten(x)\n            logits = self.linear_relu_stack(x)\n            return logits\n    \n    \n    model = NeuralNetwork().to(device)\n    print(model)\n    \n    loss_fn = nn.CrossEntropyLoss()\n    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n    \n    \n    def train(dataloader, model, loss_fn, optimizer):\n        size = len(dataloader.dataset)\n        for batch, myDic in enumerate(dataloader):\n    \n            # X, y = X.to(device), y.to(device)\n            X = myDic.get('predictors').to(device)\n            y = myDic.get('airflow').to(device)\n            print(X)\n            print(y)\n    \n            # Compute prediction error\n            pred = model(X)\n            loss = loss_fn(pred, y)\n    \n            # Backpropagation\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n    \n            if batch % 100 == 0:\n                loss, current = loss.item(), batch * len(X)\n                print(f\"loss: {loss:&gt;7f}  [{current:&gt;5d}/{size:&gt;5d}]\")\n    \n    \n    def test(dataloader, model):\n        size = len(dataloader.dataset)\n        model.eval()\n        test_loss, correct = 0, 0\n        with torch.no_grad():\n            for X, y in dataloader:\n                X, y = X.to(device), y.to(device)\n                pred = model(X)\n                test_loss += loss_fn(pred, y).item()\n                correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n        test_loss /= size\n        correct /= size\n        print(f\"Test Error: \\n Accuracy: {(100 * correct):&gt;0.1f}%, Avg loss: {test_loss:&gt;8f} \\n\")\n    \n    \n    epochs = 5\n    for t in range(epochs):\n        print(f\"Epoch {t + 1}\\n-------------------------------\")\n        train(train_dataloader, model, loss_fn, optimizer)\n        test(test_dataloader, model)\n    print(\"Done!\")\n    \n\nAny help/advice/direction would be appreciated", "upvote_ratio": 1.0, "id": "t3_nfvm42", "created_utc": 1621397490.0}
{"sub": "pytorch", "title": "Flatten 3D tensor", "selftext": "I have a tensor of the shape T x B x N (training data for a RNN, T is max seq length, B is number of batches, and N number of features) and I'd like to flatten all the features across timesteps, such that I get a tensor of the shape B x TN. Any ideas? Thanks!", "upvote_ratio": 1.0, "id": "t3_nfppv8", "created_utc": 1621380747.0}
{"sub": "pytorch", "title": "Self-published book on PyTorch", "selftext": "Hi,\n\nI'm Daniel, and I've been teaching machine learning at a bootcamp in Berlin for more than three years.\n\nI've just finished self-publishing my book, \"Deep Learning with PyTorch Step-by-Step: A Beginner's Guide\" ([https://leanpub.com/pytorch](https://leanpub.com/pytorch)).\n\nIf you're looking for a book where you can learn about Deep Learning and PyTorch without having to spend hours deciphering cryptic text and code, and that's easy and enjoyable to read, this is it :-)\n\nIt is a very comprehensive work, and it covers from the basics of gradient descent all the way up to fine-tuning large NLP models (BERT and GPT-2) using HuggingFace. The book is divided into four parts:\n\n\\- Part I: Fundamentals (gradient descent, training linear and logistic regressions in PyTorch)\n\n\\- Part II: Computer Vision (deeper models and activation functions, convolutions, transfer learning, initialization schemes)\n\n\\- Part III: Sequences (RNN, GRU, LSTM, seq2seq models, attention, self-attention, transformers)\n\n\\- Part IV: Natural Language Processing (tokenization, embeddings, contextual word embeddings, ELMo, BERT, GPT-2)\n\nMy writing style is very informal, and I write as if I were having a conversation with you, the reader. I'd imagine which questions my students would ask if I were teaching them a given topic, and then I'd raise (and answer) these questions in the text.\n\nAnd, in case you're wondering \"who is this guy, and why should I care about his book\", here's a bit of background on me:\n\nAs a teacher, I've helped more than 150 students advance their careers. My professional background includes 20 years of experience working for companies in several industries: banking, government, fintech, retail, and mobility.\n\nI've written some popular blog posts, like:\n\n\\- Understanding binary cross-entropy / log loss: a visual explanation ([https://bit.ly/2S0VSok](https://bit.ly/2S0VSok)) - more than 400k views\n\n\\- Understanding PyTorch with an example: a step-by-step tutorial ([https://bit.ly/3uRfbPn](https://bit.ly/3uRfbPn)) - more than 220k views\n\nI've also been invited to give my talk \"PyTorch 101: Building a Model Step-by-Step\" at the Open Data Science Conference (ODSC) in 2019, 2020, and 2021.\n\nAnd I've just finished publishing my first book, focusing on Deep Learning and PyTorch :-)", "upvote_ratio": 1.0, "id": "t3_nfdx4p", "created_utc": 1621352405.0}
{"sub": "pytorch", "title": "Is there an easy way to visualize pytorch vision transforms", "selftext": "I am looking for a library or something hosted by streamlit- that takes an image I can upload or give a path, takes the transforms I want to apply as input and outputs how the images look like. \n\nThis helps in quickly ideating what transforms are required", "upvote_ratio": 1.0, "id": "t3_nf8eet", "created_utc": 1621337801.0}
{"sub": "pytorch", "title": "MLP-Mixer in Flax and PyTorch", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_neki4a", "created_utc": 1621268305.0}
{"sub": "pytorch", "title": "Weird problem with the loss- looking for suggestions", "selftext": "The weirdest thing is happening in my model and I can't figure out why.So, TLDR, if my input has 100 points (in a batch), my loss at every output of my network will be 50 (half of that):\n\n    y-y_predict =  tensor([50.1337, 49.6812, 50.0728, 50.3125, 49.8369, 50.1456, 50.0340, 50.0508,         50.1231, 50.2237, 49.4339, 49.7306, 49.8324, 49.6193, 49.6949, 49.8605,         50.0395, 50.1412, 50.3325, 50.4694, 50.2878, 50.2262, 50.1926, 50.1933,         50.2082, 50.2194, 50.1791, 50.1530, 50.1509, 50.1562, 50.1650, 50.1777,         50.1900, 50.2044, 50.2156, 50.2246, 50.2187, 50.2143, 50.2464, 50.2789,         50.3046, 50.3081, 50.3118, 50.3157, 50.3172, 50.3146, 50.3121, 50.3128,         50.3165, 50.3203, 50.3240, 50.3278, 50.3316, 50.3353, 50.3391, 50.3429,         50.3468, 50.3506, 50.3545, 50.3584, 50.3622, 50.3660, 50.3699, 50.3738,         50.3777, 50.3815, 50.3854, 50.3892, 50.3931, 50.3970, 50.4009, 50.4047,         50.4086, 50.4125, 50.4163, 50.4183, 50.4201, 50.4219, 50.4236, 50.4253,         50.4270, 50.4288, 50.4306, 50.4323, 50.4341, 50.4358, 50.4375, 50.4471,         50.4729, 50.4986, 50.5243, 50.5499, 50.5756, 50.6001, 50.6285, 50.6592,         50.6900, 50.7367, 50.8411, 51.0187], dtype=torch.float64,        grad_fn=&lt;SubBackward0&gt;)\n\nIf my input size is 50, the loss at every output will be 25. And if the input size is 25, the loss is 12.5.I have no idea why it's happening, but I know it has something to do with the number of points I'm using (as the difference between the actual and prediction seems to always be half of that).\n\nI'm trying to figure out if it has anything to do with the backwards pass/gradients, but so far I'm lost.My input is in a CSV file and I'm changing it to a batch as follows:\n\n    x = torch.tensor(x, requires_grad=True)\n    x_float = x.float() \n    x_batch = torch.reshape(x_float, (len(x),1)) #reshape for batch \n    x_batch.shape #torch.Size([100, 1])\n\n\\`x\\_batch\\` goes into the model, and the output is 100 values (see above).\n\nMy code is a bit convoluted &amp; long otherwise I would share it. Any suggestions on what I can check/ideas will be gladly appreciated.\n\n**Update:**\n\nAttaching loss function:\n\n    loss = (torch.mean(torch.square(self.y_actual - y_predict)))\n\nAdding a plot to show that it learns the function perfectly, except for the difference:\n\n&amp;#x200B;\n\nhttps://preview.redd.it/7gt3vazqqpz61.png?width=1264&amp;format=png&amp;auto=webp&amp;s=9f9dfca1d43c740311ce5119f673d2a043275cd8\n\nLast update:  \nCan't believe I missed it, but the optimizer.zero\\_grad was misplaced by 1 line. I spent most of my time trying to figure out why the strange ratio of input\\_size/2 and missed that silly thing. Thanks everyone for all the help", "upvote_ratio": 1.0, "id": "t3_negegz", "created_utc": 1621258445.0}
{"sub": "pytorch", "title": "Is relative error feasible as a loss function? Also is there any benefit?", "selftext": "I\u2019m performing regression where my outputs can widely vary, often between 50,000 and 0.01. Is there any sense to using relative error as a loss function, meaning using the percentage that the estimate differs from the target?\n\nMy issue is that things like MSE cause my large values to dominate the loss, resulting in an overestimation of small values by several orders of magnitude. Root scaling to spread out my outputs helps but with diminishing returns, and eventually hampers prediction of large values\n\nDoes a relative error loss make sense in this case?", "upvote_ratio": 0.84, "id": "t3_nd9tzx", "created_utc": 1621116770.0}
{"sub": "pytorch", "title": "ResNet-50 PyTorch Pruning", "selftext": "Used **Global**, **Absolute Magnitude Weight**, **Unstructured** and **Iterative** pruning using ResNet-50 with *Transfer Learning* on CIFAR-10 dataset. Surprisingly, a **sparsity of 99.078%** has been achieved with an increase of performance! The code can be referred [here](https://github.com/arjun-majumdar/Neural_Network_Pruning/blob/main/ResNet50_Global_Absolute_Magnitude_Pruning.ipynb).\n\nOriginal and unpruned model's val\\_accuracy = 92.58%, original model size = 90 MB, zipped model size = 83.5 MB.\n\nPruned model's (sparsity = 99.078%) val\\_accuracy = 92.94%, original model size = 90 MB, zipped model size = 7.1 MB.\n\n**This results into a compression ratio of 11.76x.**\n\nThoughts?", "upvote_ratio": 0.78, "id": "t3_nczcbi", "created_utc": 1621086668.0}
{"sub": "pytorch", "title": "Beginner : Object (shape) detection in binary images", "selftext": "(by binary I mean white on black, no greyscale, no RGB)\n\nI appreciate that this may be a stupid question, but I have been struggling for a while so would appreciate any general feedback.\n\nI wanted to build a model that could identify and label multiple simple shapes (ie square, triangle) in an image (similar to cv matchshapes etc). I wanted to try with machine learning, so I could introduce some distortion/noise in my training data, hoping I could be a bit more flexible in detection.\n\nI thought the simplicity of the images, may make this easier, but in reality it seems to be harder (since most tutorials are focused on photos). The closest tutorial I could find was on MNIST data (ie simple shapes), here : https://pythonprogramming.net/training-deep-learning-neural-network-pytorch/ but my results have been unusable.\n\nI have also experimented with SSD300 models from this example : \nhttps://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection but again, I think the lack of RGB/greyscale data makes this largely useless ?\n\nMy broad question is : Is there a model architecture I can read about that may be suited to this type of application ? ie draw an approximate square shape [255,255,255] on black background [0,0,0] and have this correctly identified and labelled in the source image.", "upvote_ratio": 1.0, "id": "t3_ncx6sk", "created_utc": 1621079695.0}
{"sub": "pytorch", "title": "Gumbel-Max implementation", "selftext": "Could anyone explain how this [Gumbel-Max](https://medium.com/swlh/on-the-gumbel-max-trick-5e340edd1e01) pytorch [code implementation](https://github.com/D-X-Y/AutoDL-Projects/blob/main/lib/models/cell_searchs/search_model_gdas.py#L115-L129) works ?\n\nhttps://preview.redd.it/5qhqajfos9z61.png?width=936&amp;format=png&amp;auto=webp&amp;s=c3810642488bd6800ad414446efc4fa6a8d4b550", "upvote_ratio": 1.0, "id": "t3_ncwtkv", "created_utc": 1621078345.0}
{"sub": "pytorch", "title": "Is the GPU accelerated version for Mac M1 released?", "selftext": "Hey r/pytorch\n\nIs the GPU accelerated version for Mac M1 released?\n\nIf so it will be very helpful if someone could share the link or help me with installation\n\nThanks.", "upvote_ratio": 0.89, "id": "t3_nc9s03", "created_utc": 1621002134.0}
{"sub": "pytorch", "title": "How to slice a tensor using segments of indices", "selftext": "Hello,\n\nLet's assume that we have a tensor x = [1,2,3,4,5,6,7,8,9] and another tensor s = [(0,2),(4,8)].\n\n0,2,4,8 are indices from x. And the goal is to slice the 2 segments from x using that indices means from 0 to 2 and from 4 to 8.\n\nWhat's the most efficient way to do it?", "upvote_ratio": 1.0, "id": "t3_nc5ude", "created_utc": 1620989792.0}
{"sub": "pytorch", "title": "Basic Auto Encoder project - Generating poorly written digits [PyTorch]", "selftext": "Hi! If anyone is looking to play in latent domain, checkout this side project that I hosted on Streamlit. The aim was to transform an input image to something that looks somewhere between 2 digits. The repository below will give you a practical exposure to Auto Encoders, Latent Domain, PyTorch, Hosting on Streamlit.\n\nGitHub Repository - [LINK](https://github.com/vdivakar/mnistMuddle)|    Streamlit App Demo - [PAGE](https://share.streamlit.io/vdivakar/mnistmuddle/br_streamlit/app.py)\n\nhttps://i.redd.it/2ghrmj6rj1z61.gif", "upvote_ratio": 0.81, "id": "t3_nc33cf", "created_utc": 1620978495.0}
{"sub": "pytorch", "title": "How to use \"backward\" with a tensor?", "selftext": "I'm following [this](https://stackoverflow.com/questions/56111340/how-to-calculate-gradients-on-a-tensor-in-pytorch) question and trying to modify it.The following code works:\n\n    import torch\n    x = torch.full((5,4), 2.0, requires_grad=True) \n    y = (2*x**2+3) \n    print(x.shape) \n    print(y.shape) \n    y.backward(torch.ones_like(x)) \n    x.grad \n    &gt;&gt;&gt;torch.Size([5, 4]) \n    torch.Size([5, 4]) \n    tensor([[8., 8., 8., 8.], [8., 8., 8., 8.], [8., 8., 8., 8.], [8., 8., 8., 8.], [8., 8., 8., 8.]])\n\nBut when I try to do the same thing with a neural network:\n\n    import torch\n    from torch.autograd import grad \n    import torch.nn as nn\n    \n    class model(nn.Module): \n        def init(self): \n            super(model, self).init() \n            self.fc1=nn.Linear(1, 20) \n            self.fc2=nn.Linear(20, 20) \n            self.out=nn.Linear(20, 4)\n        def forward(self, x): \n            x=self.fc1(x) \n            x=self.fc2(x) \n            x=self.out(x) \n            return x\n            \n    net = model()\n    batch = 10\n    x = torch.rand(batch, requires_grad = True) \n    x = torch.reshape(x, (batch,1)) \n    y = net(x)\n    y.backward(torch.ones_like(x))\n    x.grad\n\nI get the following error:\n\n\"RuntimeError: Mismatch in shape: grad\\_output\\[0\\] has a shape of torch.Size(\\[10, 1\\]) and output\\[0\\] has a shape of torch.Size(\\[10, 4\\]).\"\n\n**Update 1 :**\n\nTrying to make y and x be the same shape as u/ablindelephant was saying:  \nI just changed this line:\n\n    y[:,0].unsqueeze(1).backward(torch.ones_like(x))\n\nSo I only use the first dimension of y.  \nHowever this gives me the following error:  \n\"ipykernel\\_launcher:7: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain\\_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. \"", "upvote_ratio": 0.67, "id": "t3_nbldyk", "created_utc": 1620925103.0}
{"sub": "pytorch", "title": "How to solve the error: The size of tensor a (16) must match the size of tensor b (128) at non-singleton dimension 2", "selftext": "Currently, I'm working on an image motion deblurring problem with PyTorch. I have two kinds of images: Blurry images (variable = blur\\_image) that are the input image and the sharp version of the same images (variable = shar\\_image), which should be the output. Now I wanted to try out transfer learning, but I can't get it to work.\n\n&amp;#x200B;\n\nHere is the code for my dataloaders:\n\n&amp;#x200B;\n\n        train_loader = torch.utils.data.DataLoader(train_dataset, \n                                                   batch_size=batch_size, \n                                                   shuffle = True)\n        validation_loader = torch.utils.data.DataLoader(valid_dataset, \n                                                        batch_size=batch_size,\n                                                        shuffle = False)\n        test_loader = torch.utils.data.DataLoader(test_dataset, \n                                                  batch_size=batch_size,\n                                                  shuffle = False)\n\nTheir shape:\n\n&amp;#x200B;\n\n    Trainloader - Shape of blur_image [N, C, H, W]:  torch.Size([16, 3, 128, 128])\n    Trainloader - Shape of sharp_image [N, C, H, W]:  torch.Size([16, 3, 128, 128])torch.float32\n    Validationloader - Shape of blur_image [N, C, H, W]:  torch.Size([16, 3, 128, 128])\n    Validationloader - Shape of sharp_image [N, C, H, W]:  torch.Size([16, 3, 128, 128])torch.float32\n    Testloader- Shape of blur_image [N, C, H, W]:  torch.Size([16, 3, 128, 128])\n    Testloader- Shape of sharp_image [N, C, H, W]:  torch.Size([16, 3, 128, 128])torch.float32\n\n&amp;#x200B;\n\nThe way I use transfer learning (I thought that for the 'in\\_features' I have to put in the number of pixels):\n\n&amp;#x200B;\n\n        model = models.alexnet(pretrained=True)\n        model.classifier[6] = torch.nn.Linear(model.classifier[6].in_features, 128)\n        device_string = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        device = torch.device(device_string)\n        model = model.to(device)\n\n&amp;#x200B;\n\nThe way I define my training process:\n\n&amp;#x200B;\n\n        # Define the loss function (MSE was chosen due to the comparison of pixels\n        # between blurred and sharp images\n        criterion = nn.MSELoss()\n        \n        # Define the optimizer and learning rate\n        optimizer = optim.Adam(model.parameters(), lr=0.001)\n        \n        # Learning rate schedule - If the loss value does not improve after 5 epochs\n        # back-to-back then the new learning rate will be:  previous_rate*0.5\n        \n        #scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n        \n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau( \n                optimizer,\n                mode='min',\n                patience=5,\n                factor=0.5,\n                verbose=True\n            )\n        \n        def training(model, trainDataloader, epoch):\n          \"\"\" Function to define the model training\n          \n            Args:\n                model (Model object): The model that is going to be trained.\n                trainDataloader (Dataloader object): Dataloader object of the trainset.\n                epoch (Integer): Number of training epochs.\n          \n          \"\"\"\n          # Changing model into trainings mode\n          model.train()\n          # Supporting variable to display the loss for each epoch\n          running_loss = 0.0\n          running_psnr = 0.0\n          for i, data in tqdm(enumerate(trainDataloader), \n                              total=int(len(train_dataset)/trainDataloader.batch_size)):\n            blur_image = data[0]\n            sharp_image = data[1]\n                \n            # Transfer the blurred and sharp image instance to the device\n            blur_image = blur_image.to(device)\n            sharp_image = sharp_image.to(device)\n        \n            # Sets the gradient of tensors to zero\n            optimizer.zero_grad()\n            outputs = model(blur_image)\n            loss = criterion(outputs, sharp_image)\n        \n            # Perform backpropagation\n            loss.backward()\n            # Update the weights \n            optimizer.step()\n        \n            # Add the loss that was calculated during the trainigs run\n            running_loss += loss.item()\n        \n            # calculate batch psnr (once every `batch_size` iterations)\n            batch_psnr =  psnr(sharp_image, blur_image)\n            running_psnr += batch_psnr\n        \n          # Display trainings loss\n          trainings_loss = running_loss/len(trainDataloader.dataset)\n          final_psnr = running_psnr/int(len(train_dataset)/trainDataloader.batch_size)\n          final_ssim = ssim(sharp_image, blur_image, data_range=1, size_average=True)\n          print(f\"Trainings loss: {trainings_loss:.5f}\")\n          print(f\"Train PSNR: {final_psnr:.5f}\")\n          print(f\"Train SSIM: {final_ssim:.5f}\")\n        \n          return trainings_loss, final_psnr, final_ssim\n\nAnd here is my way to start the training:\n\n        train_loss  = []\n        val_loss = []\n        train_PSNR_score  = []\n        train_SSIM_score  = []\n        val_PSNR_score  = []\n        val_SSIM_score  = []\n        \n        start = time.time()\n        for epoch in range(nb_epochs):\n            print(f\"Epoch {epoch+1}\\n-------------------------------\")\n            train_epoch_loss = training(model, train_loader, nb_epochs)\n            val_epoch_loss = validation(model, validation_loader, nb_epochs)\n            train_loss.append(train_epoch_loss[0])\n            val_loss.append(val_epoch_loss[0])\n        \n            train_PSNR_score.append(train_epoch_loss[1])\n            train_SSIM_score.append(train_epoch_loss[2])\n        \n            val_PSNR_score.append(val_epoch_loss[1])\n            val_SSIM_score.append(val_epoch_loss[2])\n        \n            scheduler.step(train_epoch_loss[0])\n            scheduler.step(val_epoch_loss[0])\n        end = time.time()\n        print(f\"Took {((end-start)/60):.3f} minutes to train\")\n\n&amp;#x200B;\n\nBut every time when I want to perform the training I receive the following error:\n\n&amp;#x200B;\n\n       0%|          | 0/249 [00:00&lt;?, ?it/s]Epoch 1\n    -------------------------------\n    /usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([16, 3, 128, 128])) that is different to the input size (torch.Size([16, 128])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n      return F.mse_loss(input, target, reduction=self.reduction)\n    ---------------------------------------------------------------------------\n    RuntimeError                              Traceback (most recent call last)\n    &lt;ipython-input-195-ff0214e227cd&gt; in &lt;module&gt;()\n          9 for epoch in range(nb_epochs):\n         10     print(f\"Epoch {epoch+1}\\n-------------------------------\")\n    ---&gt; 11     train_epoch_loss = training(model, train_loader, nb_epochs)\n         12     val_epoch_loss = validation(model, validation_loader, nb_epochs)\n         13     train_loss.append(train_epoch_loss[0])\n    \n    &lt;ipython-input-170-dfa2c212ad23&gt; in training(model, trainDataloader, epoch)\n         25     optimizer.zero_grad()\n         26     outputs = model(blur_image)\n    ---&gt; 27     loss = criterion(outputs, sharp_image)\n         28 \n         29     # Perform backpropagation\n    \n    /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\n        887             result = self._slow_forward(*input, **kwargs)\n        888         else:\n    --&gt; 889             result = self.forward(*input, **kwargs)\n        890         for hook in itertools.chain(\n        891                 _global_forward_hooks.values(),\n    \n    /usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py in forward(self, input, target)\n        526 \n        527     def forward(self, input: Tensor, target: Tensor) -&gt; Tensor:\n    --&gt; 528         return F.mse_loss(input, target, reduction=self.reduction)\n        529 \n        530 \n    \n    /usr/local/lib/python3.7/dist-packages/torch/nn/functional.py in mse_loss(input, target, size_average, reduce, reduction)\n       2926         reduction = _Reduction.legacy_get_string(size_average, reduce)\n       2927 \n    -&gt; 2928     expanded_input, expanded_target = torch.broadcast_tensors(input, target)\n       2929     return torch._C._nn.mse_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction))\n       2930 \n    \n    /usr/local/lib/python3.7/dist-packages/torch/functional.py in broadcast_tensors(*tensors)\n         72     if has_torch_function(tensors):\n         73         return handle_torch_function(broadcast_tensors, tensors, *tensors)\n    ---&gt; 74     return _VF.broadcast_tensors(tensors)  # type: ignore\n         75 \n         76 \n    \n    RuntimeError: The size of tensor a (16) must match the size of tensor b (128) at non-singleton dimension 2\n\n&amp;#x200B;\n\nI'm a newbie in terms of using Pytorch (and image deblurring in general) and so I rather confused about the meaning of the error message and how to fix it. I tried to change my parameters and nothing worked. Does anyone have any advice for me on how to solve this problem?\n\nI would appreciate every input :)", "upvote_ratio": 1.0, "id": "t3_nbg9q0", "created_utc": 1620911402.0}
{"sub": "pytorch", "title": "How you do efficiently parameterize a batch of multivariate Gaussians in PyTorch", "selftext": "Hi all.\n\nThis is my first post here, and I am sorry if this isn't for questions (But didn't find any rule against).\n\nI want to output a batch of multivariable Gaussians (For likelihood based learning). But I don't know how I can do that. I have a neural network which will output a mean vector of batch\\_size\\*obs\\_dim. and    I have an output which returns a tensor of size batch\\_size\\*(obs\\_dim\\*obs\\_dim). \n\nBut my problem is that I want to ideally return a batch of multivariable Gaussian distributions. But for this I need to make the covariance positive definite which I don't know how I can do. If I didn't have the batch dimension I would have probably tried torch.matmul(a,a.T).\n\nI was also thinking of parametrizing a lower triangular matrix, but again, I have the problem on how to ensure that it has positive diagonal + it is lower triangular. I can imagine multiplying by a mask for example, but was hoping for a more straightforward way.\n\n&amp;#x200B;\n\nThanks", "upvote_ratio": 1.0, "id": "t3_nb1tvr", "created_utc": 1620859968.0}
{"sub": "pytorch", "title": "ResNet for big size images", "selftext": "I am using ResNet architecture for images size of (1280,720) which are pretty big for standard input size of (224,224).  So I am thinking should I add more \"layers\" (actually combination of residual blocks) to down sample? \n\nBtw I am using ResNet for Encoder, so do you think I should leave the AdaptiveAveragePool2D layer there? If so what is the Transpose layer for this to up sample?\n\nThanks", "upvote_ratio": 1.0, "id": "t3_na43yf", "created_utc": 1620758234.0}
{"sub": "pytorch", "title": "Anyone aware of Pytorch implementation of Resnet which lists all modules in forward method?", "selftext": "The official code of pytorch Resnet has the following forward function in it's implementation.\n\nIt's short and sweet but I would like to have the forward method contain all the convolution classes listed out instead of some function(layer1/layer2 in the pic) doing it. It would certainly make the code long and difficult to read but my research demands it. In case anyone knows about any such implementation or something similar for resnet, kindly let me know. Thanks!\n\nhttps://preview.redd.it/3tzwjtz6xiy61.png?width=286&amp;format=png&amp;auto=webp&amp;s=073ccdc27af205261a785c2ef15026224284ead2", "upvote_ratio": 1.0, "id": "t3_na23bw", "created_utc": 1620753242.0}
{"sub": "pytorch", "title": "Importing the numpy C-extensions failed", "selftext": "Hello,\n\nI'm new to pytorch and the installation process is driving me crazy.\n\nI am using anaconda3, cuda 11.1.0, and used  `conda install pytorch torchvision torchaudio cudatoolkit=11.0 -c pytorch` (the 11.1 nvidia does not work)to install pytorch under conda environment python=3.8.5(the default python comes with conda).\n\nAnd it's fine importing torch in cmd but not in vscode. It's giving me this error when importing either numpy or torch:\n\n`Exception has occurred: ImportError`\n\n`IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE! Importing the numpy C-extensions failed. This error can happen for many reasons, often due to issues with your setup or how NumPy was installed. We have compiled some common reasons and troubleshooting tips at: https://numpy.org/devdocs/user/troubleshooting-importerror.html Please note and check the following: * The Python version is: Python3.8 from \"C:\\Users\\22468\\anaconda3\\envs\\pytorch\\python.exe\" * The NumPy version is: \"1.20.1\" and make sure that they are the versions you expect. Please carefully study the documentation linked above for further help. Original error was: DLL load failed while importing _multiarray_umath: The specified module could not be found. During handling of the above exception, another exception occurred: File \"D:\\Workspace\\pytorch\\notes\\notes.py\", line 1, in &lt;module&gt; import numpy`\n\nI've got the paths since I installed conda with the adding path tickbox ticked, and I've also tried modifying settings.json within the vscode project:  \"python.terminal.activateEnvironment\":\u00a0true and some other things on SO but nothing works, should I use pip instead?", "upvote_ratio": 1.0, "id": "t3_n9s3qr", "created_utc": 1620721314.0}
{"sub": "pytorch", "title": "PyTorch Transfer Learning", "selftext": "Used Transfer Learning with ResNet-50 on CIFAR-10 in PyTorch to achieve val\\_accuracy = 92.58%. You can see the code [here](https://github.com/arjun-majumdar/CNN_Classifications/blob/master/ResNet50_Transfer_Learning_CIFAR10_Finetuning_entire_model.ipynb).\n\n**Key takeaway:** Change the first conv layer to have the hyper-parameters kernel\\_size = (3, 3), stride = (1, 1) and padding = (1, 1) instead of the original ones since CIFAR-10 dataset has much smaller images and using the original conv layer hyper-parameters reduces the image size due to which the resulting model performs not so good, according to my experiments.\n\nThoughts?", "upvote_ratio": 0.81, "id": "t3_n9p2j6", "created_utc": 1620708384.0}
{"sub": "pytorch", "title": "Remove pruned connections", "selftext": "One of the most common pruning techniques is \"unstructured, iterative, global magnitude pruning\" which prunes smallest magnitude p% of weights in each iterative pruning round. 'p' is typically between (10-20)%. However, after the desired sparsity is reached, say 96% (meaning that 96% of the weights in the neural network is 0), how can I remove these 0s to essentially remove say filters/neurons?\n\nBecause this pruning technique produces a lot of 0s which still participate in forward propagation using out = W.out\\_prev + b. Therefore, this pruning technique will help in compression but not in the reduction of inference time.\n\nThanks!", "upvote_ratio": 1.0, "id": "t3_n9mdq8", "created_utc": 1620698958.0}
{"sub": "pytorch", "title": "7 PyTorch Tips You Should Know", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_n9fao8", "created_utc": 1620678732.0}
{"sub": "pytorch", "title": "Using PyTorch's autograd efficiently with tensors by calculating the Jacobian", "selftext": "In my previous [question](https://stackoverflow.com/questions/67320792/how-to-use-pytorchs-autograd-efficiently-with-tensors/67334809#67334809) I found how to use PyTorch's autograd with tensors:\n\n    import torch\n    from torch.autograd import grad\n    import torch.nn as nn\n    import torch.optim as optim\n    class net_x(nn.Module): \n        def __init__(self):\n            super(net_x, self).__init__()\n            self.fc1=nn.Linear(1, 20) \n            self.fc2=nn.Linear(20, 20)\n            self.out=nn.Linear(20, 4) #a,b,c,d\n        def forward(self, x):\n            x=torch.tanh(self.fc1(x))\n            x=torch.tanh(self.fc2(x))\n            x=self.out(x)\n            return x\n    \n    nx = net_x()\n    \n    #input\n    t = torch.tensor([1.0, 2.0, 3.2], requires_grad = True) #input vector\n    t = torch.reshape(t, (3,1)) #reshape for batch\n    \n    #method \n    dx = torch.autograd.functional.jacobian(lambda t_: nx(t_), t)\n    dx = torch.diagonal(torch.diagonal(dx, 0, -1), 0)[0] #first vector\n    #dx = torch.diagonal(torch.diagonal(dx, 1, -1), 0)[0] #2nd vector\n    #dx = torch.diagonal(torch.diagonal(dx, 2, -1), 0)[0] #3rd vector\n    #dx = torch.diagonal(torch.diagonal(dx, 3, -1), 0)[0] #4th vector\n    dx \n    &gt;&gt;&gt; \n    tensor([-0.0142, -0.0517, -0.0634])\n\nThe issue is that \\`grad\\` only knows how to propagate gradients from a scalar tensor (which my network's output is not), which is why I had to calculate the Jacobian.\n\nHowever, this is not very efficient and a bit slow as my matrix is large and calculating the entire Jacobian takes a while (and I'm also not using the entire Jacobian matrix). Is there a way to calculate only the diagonals of the Jacobian (to get the 4 vectors in this example)?  \nThere appears to be an [open feature request](https://github.com/pytorch/pytorch/issues/41530) but it doesn't appear to have gotten much attention.\n\nI tried setting \n\n    torch.autograd.functional.jacobian(vectorize=True)\n\nHowever, this seems to be slower.", "upvote_ratio": 1.0, "id": "t3_n98j4a", "created_utc": 1620662217.0}
{"sub": "pytorch", "title": "I'm publishing a free course that teaches PyTorch for audio/music processing.", "selftext": "I\u2019ve received numerous requests from The Sound of AI community to cover PyTorch in my tutorials.\n\nFor this reason, I\u2019m starting a new series which will teach you PyTorch with an eye on audio and music processing!\n\nAmong other aspects of PyTorch, I\u2019ll be covering torchaudio, the GPU-accelerated audio processing library for PyTorch.\n\nReady to start this cool journey with me?\n\nCheck out the course overview:\n\n[https://www.youtube.com/watch?v=gp2wZqDoJ1Y](https://www.youtube.com/watch?v=gp2wZqDoJ1Y)", "upvote_ratio": 1.0, "id": "t3_n92mw8", "created_utc": 1620647614.0}
{"sub": "pytorch", "title": "Changing learning rate after loading scheduler's state dict", "selftext": "I have my model:\n\n    import torch\n    import torch.nn as nn\n    import torch.optim as optim\n    class net_x(nn.Module): \n        def __init__(self):\n            super(net_x, self).__init__()\n            self.fc1=nn.Linear(2, 20) \n            self.fc2=nn.Linear(20, 20)\n            self.out=nn.Linear(20, 4) \n        def forward(self, x):\n            x=self.fc1(x)\n            x=self.fc2(x)\n            x=self.out(x)\n            return x\n    \n    nx = net_x()\n    r = torch.tensor([1.0,2.0])\n    optimizer = optim.Adam(nx.parameters(), lr = 0.1)\n    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-2, max_lr=0.1, step_size_up=1, mode=\"triangular2\", cycle_momentum=False)\n    \n    path = 'opt.pt'\n    for epoch in range(10):\n        optimizer.zero_grad()\n        net_predictions = nx(r)\n        loss = torch.sum(torch.randint(0,10,(4,)) - net_predictions)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        print('loss:' , loss)\n        torch.save({ 'epoch': epoch,\n        'net_x_state_dict': nx.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'scheduler': scheduler.state_dict(),  \n        }, path)\n    \n    checkpoint = torch.load(path)  \n    nx.load_state_dict(checkpoint['net_x_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    scheduler.load_state_dict(checkpoint['scheduler'])\n\nIt trains and loads just fine, but I'm interested in lowering the \n\n    base_lr\n\nof the scheduler from \n\n    10^-2\n\nto \n\n    10^-3\n\nafter training. Is there a way to do this without creating a completely new scheduler? From my understanding if I do that I will lose the parameters that the current scheduler is using and training will get worse/start from sort of scratch", "upvote_ratio": 1.0, "id": "t3_n8inio", "created_utc": 1620580398.0}
{"sub": "pytorch", "title": "One liner question", "selftext": "I have two tensors:\n\n    a = torch.nn.Parameter(torch.rand(7, requires_grad=True))\n    b = torch.randint(0,60, (20,))\n\nIs there a one liner (or a quick &amp; short way) that can create a tensor (call it \"x\") of size 20 (similar to \"b\") with conditions?i.e.\\[b&lt;4 use a\\[0\\], 4 &lt;=b&lt;12 use a\\[1\\], 12&lt;=b&lt;22 use a\\[2\\], &lt;28, &lt;38, &lt;50, &gt;50\\] for every b\n\nSo if:\n\n    b = [12, 93, 54, 0...]\n    I want my new tensor \"x\" to be:\n    x = [a[2],a[6], a[6]...]\n\nI'm going to use this \"x\" tensor to train and need the values the be backproped and learnable\n\ni.e.\n\nloss = torch.rand(20) \\* x\n\nloss.backward() ...\n\nSo if one of the a's is not in x I want it to not change.\n\n**Update**\n\nApologies, I thought I hid the post so I won't get answers but for some reason it didn't seem to work. Anyway, I found an answer:\n\n    x = a[0](b&lt;4) + a[1]((4&lt;=b)&amp;(b&lt;12)) + a[2]((12&lt;=b)&amp;(b&lt;22)) + a[3]((22&lt;=b)&amp;(b&lt;28)) + a[4]((28&lt;=b)&amp;(b&lt;30)) + a[5]((30&lt;=b)&amp;(b&lt;50)) + a[6]*(b&gt;=50)\n\n&amp;#x200B;", "upvote_ratio": 1.0, "id": "t3_n7qiyr", "created_utc": 1620484548.0}
{"sub": "pytorch", "title": "Is there anyway I can install pytorch without using command print or compiling myself?", "selftext": "I have a limited internet plan so I\u2019d like to use my phone\u2019s unlimited data to just download a .exe or .msi file to transfer to my pc. Is this possible? I assume not but want to make sure.", "upvote_ratio": 0.71, "id": "t3_n7dlp5", "created_utc": 1620435243.0}
{"sub": "pytorch", "title": "[N] PyTorch Lightning 1.3 - Lightning CLI, PyTorch Profiler, Improved Early Stopping", "selftext": "Announcing the release of Lightning 1.3!\n\nThis new release includes:\n\n* a new Lightning CLI\n* PyTorch profiler integration\n* improved TPU support\n* new early stopping strategies\n* predict and validate trainer routines\n\nInstall: [https://bit.ly/2PTVZBa](https://bit.ly/2PTVZBa)\n\nRelease notes: [https://bit.ly/3b5acTk](https://bit.ly/3b5acTk)\n\nRead more via #PyTorch's Medium blog: [https://bit.ly/3b62N6f](https://t.co/xyqAx6G94W?amp=1)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/aqt9znfohrx61.png?width=1360&amp;format=png&amp;auto=webp&amp;s=820bf7e92ba9a29258ab0adff8f2484a178f8d3e\n\nBig thanks to everyone who contributed to this release!\n\n[**@akihironitta**](https://github.com/akihironitta), [**@alessiobonfiglio**](https://github.com/alessiobonfiglio), [**@amisev**](https://github.com/amisev), [**@amogkam**](https://github.com/amogkam), [**@ananthsub**](https://github.com/ananthsub), [**@ArvinZhuang**](https://github.com/ArvinZhuang), [**@ashleve**](https://github.com/ashleve), [**@asnorkin**](https://github.com/asnorkin), [**@awaelchli**](https://github.com/awaelchli), [**@BloodAxe**](https://github.com/BloodAxe), [**@bmahlbrand**](https://github.com/bmahlbrand), [**@Borda**](https://github.com/Borda), [**@borisdayma**](https://github.com/borisdayma), [**@camruta**](https://github.com/camruta), [**@carmocca**](https://github.com/carmocca), [**@ceshine**](https://github.com/ceshine), [**@dbonner**](https://github.com/dbonner), [**@dhkim0225**](https://github.com/dhkim0225), [**@EdwardJB**](https://github.com/EdwardJB), [**@EliaCereda**](https://github.com/EliaCereda), [**@EricCousineau-TRI**](https://github.com/EricCousineau-TRI), [**@ethanwharris**](https://github.com/ethanwharris), [**@FlorianMF**](https://github.com/FlorianMF), [**@hemildesai**](https://github.com/hemildesai), [**@ifsheldon**](https://github.com/ifsheldon), [**@kaushikb11**](https://github.com/kaushikb11), [**@mauvilsa**](https://github.com/mauvilsa), [**@maxfrei750**](https://github.com/maxfrei750), [**@mesejo**](https://github.com/mesejo), [**@ramonemiliani93**](https://github.com/ramonemiliani93), [**@rohitgr7**](https://github.com/rohitgr7), [**@s-rog**](https://github.com/s-rog), [**@sadiqj**](https://github.com/sadiqj), [**@scart97**](https://github.com/scart97), [**@SeanNaren**](https://github.com/SeanNaren), [**@shuyingsunshine21**](https://github.com/shuyingsunshine21), [**@SkafteNicki**](https://github.com/SkafteNicki), [**@SpontaneousDuck**](https://github.com/SpontaneousDuck), [**@stllfe**](https://github.com/stllfe), [**@tchaton**](https://github.com/tchaton), [**@THasthika**](https://github.com/THasthika), [**@vballoli**](https://github.com/vballoli)\n\n\\**If we forgot someone due to not matching commit email with GitHub account, let us know :\\]*", "upvote_ratio": 1.0, "id": "t3_n78qtk", "created_utc": 1620420918.0}
{"sub": "pytorch", "title": "ResNet-18 Pruning PyTorch", "selftext": "I have coded \"Global, unstructured &amp; iterative\" pruning using ResNet-18 trained from scratch on CIFAR-10 dataset in PyTorch. You can refer to the code [here](https://github.com/arjun-majumdar/Neural_Network_Pruning/blob/main/ResNet18_Global_Pruning.ipynb). Let me know your comments/thoughts.\n\nCheers!", "upvote_ratio": 1.0, "id": "t3_n6v0m3", "created_utc": 1620381793.0}
{"sub": "pytorch", "title": "How is EMA different from the use of momentum", "selftext": "as far as I have seen EMA Does it on weights and momentum is done on gradients I want to know more about them or is my understanding wrong", "upvote_ratio": 1.0, "id": "t3_n6stqm", "created_utc": 1620372053.0}
{"sub": "pytorch", "title": "How is embedding layer output is calculated?", "selftext": "I have this model with its default parameters\n\n\n    class Model(nn.Module):\n\n        def __init__(self, vocab_size=59, embedding_dim=10, context_size=2,hidden_dim=256):\n            super(Model,self).__init__()\n            self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n            self.linear1 = nn.Linear(context_size * embedding_dim, hidden_dim)\n            self.linear2 = nn.Linear(hidden_dim, vocab_size)\n\n        def forward(self, inputs):\n            # inputs.shape = torch.Size([2])\n            embeds = self.embeddings(inputs)\n            print(embeds.shape) # torch.Size([2, 10])\n            embeds = embeds.view(1,-1)\n            out = F.relu(self.linear1(embeds))\n            out = self.linear2(out)\n            log_probs = F.log_softmax(out, dim=1)\n            return log_probs\n\n    model = Model()\n    model(torch.rand(2)) # forward prediction\n\nI know that my `self.embeddings.weight.shape` has a shape of `torch.Size([59, 10])` and after the forward prediction `print(embeds.shape)` shows the shape as `torch.Size([2, 10])`\n\nHow the output shape of embeds is (2,10)?", "upvote_ratio": 1.0, "id": "t3_n6b4ta", "created_utc": 1620318135.0}
{"sub": "pytorch", "title": "Mindblown \ud83e\udd2f\ud83e\udd2f: Bring your Minecraft creation into the real world - generate photorealistic images of large 3D block worlds such as those created in Minecraft! (GANcraft)", "selftext": "nan", "upvote_ratio": 0.64, "id": "t3_n6125m", "created_utc": 1620283172.0}
{"sub": "pytorch", "title": "How to \u201clearn\u201d pytorch", "selftext": "Hello all, I\u2019m brand new to pytorch. I have been learning deep learning for close to a year now, and only managed to learn CNNs for vision and implement a very trash one in Tensorflow. Anyways, I decided I wanted to switch to pytorch since it feels more like python. Issue is, i don\u2019t know how to \u201clearn\u201d pytorch. I don\u2019t want to make the same mistake as I did during the beginning of my deep learning journey where I was in tutorial hell taking tensorflow courses and doing the same vision problems as practice. \n\nIn fact, I want to learn deep learning for natural language processing, but I feel like learning pytorch and a bit of RNN theory isnt enough for me to jump right into a project. How would you all recommend I start getting good with it? I was planning on watching the getting started videos on the pytorch website and then maybe implementing basic stuff, like linear regression, logistic regression, and then work my way up to RNNs, but skip vision because that\u2019s not really what im interested in.\n\nI have extensive background in working with pandas and Numpy and python for data science, so I know how to program effectively, just new to pytorch. Also I know a bit of DL theory already.", "upvote_ratio": 0.88, "id": "t3_n566qd", "created_utc": 1620185281.0}
{"sub": "pytorch", "title": "What are the advantages and disadvantages of PyTorch Geometric vs Deep Graph Library (DGL)?", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_n4xdy0", "created_utc": 1620160183.0}
{"sub": "pytorch", "title": "Ultimate Guide to Machine Learning with Python (e-books bundle)", "selftext": "This bundle of e-books is specially crafted for beginners. Everything from Python basics to the deployment of Machine Learning algorithms to production in one place.\n\nWhat's included?\n\n\u2022 Ultimate Guide to Machine Learning with Python e-book (PDF)\n\n\u2022 Full Source Code with all examples from the book (Jupyter Notebooks)\n\nSix additional bonus materials:\n\n\u2022 Bonus #1: Python for Data Science (PDF + Full Source Code)\n\n\u2022 Bonus #2: Mathematics for Machine Learning (PDF)\n\n\u2022 Bonus #3: Guide to Data Analysis (PDF + Full Source Code)\n\n\u2022 Bonus #4: Neural Networks Zoo (PDF)\n\n\u2022 Bonus #5: Access to a private Discord Community\n\nVisit the page to learn more: [https://rubikscode.net/ultimate-guide-to-machine-learning-with-python/](https://rubikscode.net/ultimate-guide-to-machine-learning-with-python/)\n\n&amp;#x200B;\n\nhttps://reddit.com/link/n4ll8a/video/e50kztpr43x61/player", "upvote_ratio": 0.25, "id": "t3_n4ll8a", "created_utc": 1620126253.0}
{"sub": "pytorch", "title": "Latest from Baidu researchers: Automatic video generation from audio or text", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_n4fmag", "created_utc": 1620101311.0}
{"sub": "pytorch", "title": "Tesla Road Detection", "selftext": "Does anyone have an clue on how Tesla does their road detection. I can't think of much but I've started getting through some projects and get something intresting up. More importantly, I've been losing sleep on this as it intrigues me. The internet doesn't give me anything intresting to read about and I've started to build my own robot using ROS. \n\nCombatively, The real question is, how do I draw on pictures using Torch, how does Tesla do its edge detection, how would I go about using Python to stich that top view, and how to do distance estimation. Any information will be helpful, this is my second post.  \n\nSorry if this confused anyone, like earlier mentioned, this is my second post. \n\n[PyTorch - Andrej Karpathy](https://www.youtube.com/watch?v=oBklltKXtDE&amp;t=271s)", "upvote_ratio": 0.78, "id": "t3_n3t4a3", "created_utc": 1620039054.0}
{"sub": "pytorch", "title": "Run Python Code In Parallel Using Multiprocessing", "selftext": "nan", "upvote_ratio": 0.36, "id": "t3_n3nck0", "created_utc": 1620014259.0}
{"sub": "pytorch", "title": "Can nn.RNN handle variable length inputs?", "selftext": "I\u2019m building a simple RNN to classify text. Do i have to pad every sentence i send in to the RNN to make every tensor the same size? i\u2019m using GLOVE embedding\u2019s if that makes any difference.\n\nat the moment i\u2019m feeding index values of variable length sentences in to the network representing each sentence.", "upvote_ratio": 1.0, "id": "t3_n3f3ve", "created_utc": 1619987093.0}
{"sub": "pytorch", "title": "Solving multidimensional PDEs in pytorch", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_n21g4m", "created_utc": 1619810943.0}
{"sub": "pytorch", "title": "Differentiable augmentation for GANs (using Kornia)", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_n1wh16", "created_utc": 1619797072.0}
{"sub": "pytorch", "title": "Understanding Lambda function", "selftext": "I'm calculating the Jacobian matrix using a Lambda function:\n\n    #network\n    import torch \n    from torch.autograd import grad \n    import torch.nn as nn \n    import torch.optim as optim\n    \n    class net_x(nn.Module): \n        def init(self): \n            super(net_x, self).init() \n            self.fc1=nn.Linear(1, 20) \n            self.fc2=nn.Linear(20, 20) \n            self.out=nn.Linear(20, 4) \n    \n    def forward(self, x): \n        x=torch.tanh(self.fc1(x)) \n        x=torch.tanh(self.fc2(x)) \n        x=self.out(x) \n        return x\n    \n    nx = net_x() \n    t = torch.tensor([1.0, 2.0, 3.2], requires_grad = True) #input vector \n    t = torch.reshape(t, (3,1)) #reshape for batch\n    \n    #Jacobian\n    dx = torch.autograd.functional.jacobian(lambda t_: nx(t_), t)\n\nMy issue is that I also want to save the output of the network -- nx(t\\_), without rerunning it through the network.\n\nI tried saving it to a parameter\n\n    out = (lambda t_: nx(t_))(t)\n\nBut then I can't pass it through the Jacobian because it needs a function.", "upvote_ratio": 0.5, "id": "t3_n1v2qd", "created_utc": 1619793060.0}
{"sub": "pytorch", "title": "programming PyTorch for Deep Learning book", "selftext": "Did anyone finish the book without issues in Google Colab?\n\nThanks", "upvote_ratio": 0.4, "id": "t3_n1oiz7", "created_utc": 1619766685.0}
{"sub": "pytorch", "title": "How do I load a local model with torch.hub.load/", "selftext": "Hi,\n\nI need to avoid downloading the model from the web (due to restrictions on the machine installed).\n\nThis works, but downloads the model from the net\n\nmodel = torch.hub.load('pytorch/vision:v0.9.0', 'deeplabv3\\_resnet101',                                                                                                              pretrained=True)\n\nI have placed the \\`.pth\\` file and the \\`hubconf.py\\` file in the /tmp/ folder and changed my code to\n\nmodel = torch.hub.load('/tmp/', 'deeplabv3\\_resnet101', pretrained=True,s                                                                                                             ource='local')\n\n&amp;#x200B;\n\nbut to my surprise it still downloads the model from the internet. What am I doing wrong? How can I load the model locally.\n\nJust to give you a bit more details, I'm doing all this in a Docker container which has a read-only volume at runtime, so that's why the download of new files fails.\n\n&amp;#x200B;\n\nthanks,\n\nJohn", "upvote_ratio": 1.0, "id": "t3_n0ew93", "created_utc": 1619616795.0}
{"sub": "pytorch", "title": "Can I change the value of a variable using arithmetic operators and detach and still let gradients flow through it?", "selftext": "Hi. So I need to implement something and I was wondering if I can implement it the following way. \nThere is an encoder and decoder block. Let the output of encoder be x and output of decoder be y. \nFor some examples, I need to replace the value of x with a value from a file. \nSo if I just implement \nx=FILE-VALUE, \nI will understandably break the graph and gradients won\u2019t flow all the way back. \nHowever, if I do something like\nx-=(x.clone().detach()-FILE-VALUE)\nI believe I should essentially be getting the same effect while also allowing appropriate gradients to flow all the way back. \nIs my understanding correct or am I doing something wrong? Is there an easier way to do this?", "upvote_ratio": 1.0, "id": "t3_n0e8j9", "created_utc": 1619614741.0}
{"sub": "pytorch", "title": "Why is Pytorch code slower than libraries like fairseq and OpenNMT-py?", "selftext": "I have a made Transformer model using torch.nn.Transformer and it has around 18 million parameters but it's training in very slow, magnitudes of times slower than libraries like fairseq and OpenNMT-py. Are there any important points that need to be taken care of to get optimal training speed or is Pytorch code slower than these libraries?", "upvote_ratio": 0.81, "id": "t3_n0dgm1", "created_utc": 1619612101.0}
{"sub": "pytorch", "title": "Avalanche-Python Library for Continual Learning - Analytics India Magazine", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_n0c9yo", "created_utc": 1619607679.0}
{"sub": "pytorch", "title": "No CUDA capable device is detected", "selftext": "Hello,\n\nApologies if this is not the correct place to be posting this.\n\nI  am trying to run a github repo for training that is setup using  miniconda3. I am using pytorch version 1.3 and the cuda toolkit in the  envrionment is set to 9.2.\n\nI keep getting this error: No CUDA capable device is detected /opt/conda.... and the error lists a path to a file.\n\nMoreover, it also states the following line:\n\n\"NVIDIA-SMI  has failed because it couldn't communicate with the NVIDIA driver. Make  sure the latest NVIDIA driver is installed and running.\"\n\nPlease  note that I am a complete and utter noob to gpus and machine learning  and am currently just tryinng to execute a github repo that already has  the commands to set everything up. [https://github.com/amazon-research/datatuner](https://github.com/amazon-research/datatuner)\n\nI do not know how I can resolve this error. Any help or guidance will be highly appreciated.", "upvote_ratio": 0.5, "id": "t3_n0866v", "created_utc": 1619589354.0}
{"sub": "pytorch", "title": "How to train a tensor of tensors?", "selftext": "I have a class model:\n\n    class Model(nn.Module)\n\nThat has 2 learnable parameters:\n\n    self.a = torch.nn.Parameter(torch.rand(1, requires_grad=True))\nself.b = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n\nIt also has a neural network class inside:\n\n    class Net_x(nn.Module)\n\nSo in order to train all the parameters I combine the learnable parameters \"a, b\" and the NN's params (under the \"init\" of class Model):\n\n    self.net_x = self.Net_X()\nself.params = list(self.net_x.parameters())\nself.params.extend(list([self.a, self.b]))\n\nThis works well and the network is training just fine. \n\nMy issue is that now I'm trying to change one of the parameters to be a tensor of tensors of parameters:\n\n~~self.a = torch.nn.Parameter(torch.rand(1, requires\\_grad=True))~~  \nself.a = torch.tensor( \\[\\[ torch.nn.Parameter(torch.rand(1, requires\\_grad=True)) \\] for i in range(5)\\] )\n\nThat is because at every time step (not epoch), I need to use a different parameter from self.a\n\nExample:\n\n    \nfor epoch in range(n_epochs):\n    if timestep&lt;5:\n            val = 50 - a[0] * b\n        loss = 10 - val\n    elif timestep &gt;=5 and timestep &lt; 10:\n            val = 50 - a[1] * b\n        loss = 10 - val\n    \n\nThe model runs without issues, but the parameters are not being updated (i.e they stay the same at every epoch).\n\nP.S I would have added my code, but it's really long. I'm hoping the answer is simple (and if not I'll try to reduce my code and attach it)", "upvote_ratio": 0.75, "id": "t3_mzsqzl", "created_utc": 1619541977.0}
{"sub": "pytorch", "title": "Pytorch 2-D and 3-D Table Interpolation", "selftext": "I am trying to get 2-D and 3-D interpolation table lookup running in pytorch, but I don't believe [torch.lerp](https://pytorch.org/docs/stable/generated/torch.lerp.html) supports it and haven't been able to find any other pytorch native solution. Has anyone done a neural network approximation of a 2-D or 3-D linear interpolation table in pytorch? I am wondering if it is a worthwhile endeavor for speed at inference time (especially if the required size of NN to approximate a table is large) or if there is a simpler/more standard way that I missed.\n\nFor some background: I am trying to encode a known physical relationships that is in the form a linear interpolation table into a LSTM time series forecast system I am designing. The interpolation table queries would be done during training, so this isn't simply a pre-processing step.", "upvote_ratio": 1.0, "id": "t3_mz3lwd", "created_utc": 1619459653.0}
{"sub": "pytorch", "title": "[N] Lightning Transformers - Train HuggingFace Transformers at scale with PyTorch Lightning", "selftext": "[Lightning Transformers](https://github.com/PyTorchLightning/lightning-transformers) is for users who want to train, evaluate and predict using HuggingFace models and datasets with PyTorch Lightning. Full customizability of the code using the LightningModule and Trainer, with Hydra config composition for quick and easy experimentation.\u00a0No boilerplate code required; easily swap out models, optimizers, schedulers, and more without touching the code. Check out the blog post: [Training Transformers at Scale with PyTorch Lightning](https://pytorch-lightning.medium.com/training-transformers-at-scale-with-pytorch-lightning-e1cb25f6db29) for more information or the [documentation](https://lightning-transformers.readthedocs.io/).\n\nhttps://i.redd.it/md2cqcukwjv61.gif", "upvote_ratio": 0.67, "id": "t3_mz2q56", "created_utc": 1619457350.0}
{"sub": "pytorch", "title": "Pytorch Debug Log file", "selftext": "is there a way to see the backend logs for pytorch, like what functions are being called when we create a neural model, train a model. Something similar to java webapp debug logs. Any link for online reference or tutorial will be helpful. Thanks in advance.", "upvote_ratio": 0.99, "id": "t3_myyv3b", "created_utc": 1619447126.0}
{"sub": "pytorch", "title": "Best Place to start", "selftext": "Dear Redditors\n\nI'm currently trying to build my first ever neural network with PyTorch. As a school project I want to create an AI to play Connect 4 but I can't find any good tutorials on how to learn DQN and how to apply it. Is it possible to learn to create the AI in two weeks?\n\nThanks for any answers.", "upvote_ratio": 0.5, "id": "t3_myhv2w", "created_utc": 1619385431.0}
{"sub": "pytorch", "title": "[P] An introduction to PyKale https://github.com/pykale/pykale\u200b, a PyTorch library that provides a unified pipeline-based API for knowledge-aware multimodal learning and transfer learning on graphs, images, texts, and videos to accelerate interdisciplinary research. Welcome feedback/contribution!", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_mxub6p", "created_utc": 1619302381.0}
{"sub": "pytorch", "title": "The PyTorch Geometric Temporal paper is out", "selftext": "This is a preliminary version:\n\n[https://arxiv.org/abs/2104.07788](https://arxiv.org/abs/2104.07788)\n\nWe also added new layers and datasets with the new release:\n\n[https://github.com/benedekrozemberczki/pytorch\\_geometric\\_temporal](https://github.com/benedekrozemberczki/pytorch_geometric_temporal)", "upvote_ratio": 1.0, "id": "t3_mxsois", "created_utc": 1619297446.0}
{"sub": "pytorch", "title": "Accessing modules - PyTorch ResNet-18", "selftext": "I am using a ResNet-18 coded as follows:\n\n    \n    class ResidualBlock(nn.Module):\n        '''\n        Residual Block within a ResNet CNN model\n        '''\n        def __init__(self, input_channels, num_channels, \n                     use_1x1_conv = False, strides = 1):\n            # super(ResidualBlock, self).__init__()\n            super().__init__()\n         \n            self.conv1 = nn.Conv2d(\n                in_channels = input_channels, out_channels = num_channels,\n                kernel_size = 3, padding = 1, stride = strides,\n                bias = False\n                )\n            self.bn1 = nn.BatchNorm2d(num_features = num_channels)\n            \n            self.conv2 = nn.Conv2d(\n                in_channels = num_channels, out_channels = num_channels,\n                kernel_size = 3, padding = 1, stride = 1,\n                bias = False\n                )\n            self.bn2 = nn.BatchNorm2d(num_features = num_channels)\n            \n            if use_1x1_conv:\n                self.conv3 = nn.Conv2d(\n                    in_channels = input_channels, out_channels = num_channels,\n                    kernel_size = 1, stride = strides\n                    )\n                self.bn3 = nn.BatchNorm2d(num_features = num_channels)\n            else:\n                self.conv3 = None\n            \n            self.relu = nn.ReLU(inplace = True)\n    \n            self.initialize_weights()\n            \n        \n        def forward(self, X):\n            Y = F.relu(self.bn1(self.conv1(X)))\n            Y = self.bn2(self.conv2(Y))\n            \n            if self.conv3:\n                X = self.bn3(self.conv3(X))\n                # print(f\"X.shape due to 1x1: {X.shape} &amp; Y.shape = {Y.shape}\")\n            else:\n                # print(f\"X.shape without 1x1: {X.shape} &amp; Y.shape = {Y.shape}\")\n                pass\n            \n            Y += X\n            return F.relu(Y)\n        \n        \n        def shape_computation(self, X):\n            Y = self.conv1(X)\n            print(f\"self.conv1(X).shape: {Y.shape}\")\n            Y = self.conv2(Y)\n            print(f\"self.conv2(X).shape: {Y.shape}\")\n            \n            if self.conv3:\n                h = self.conv3(X)\n                print(f\"self.conv3(X).shape: {h.shape}\")\n        \n    \n        def initialize_weights(self):\n            for m in self.modules():\n                # print(m)\n                if isinstance(m, nn.Conv2d):\n                    nn.init.kaiming_uniform_(m.weight)\n    \n                    '''\n                    # Do not initialize bias (due to batchnorm)-\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                    '''\n                \n                elif isinstance(m, nn.BatchNorm2d):\n                    # Standard initialization for batch normalization-\n                    nn.init.constant_(m.weight, 1)\n                    nn.init.constant_(m.bias, 0)\n    \n                elif isinstance(m, nn.Linear):\n                    nn.init.kaiming_normal_(m.weight)\n                    nn.init.constant_(m.bias, 0)\n    \n    b0 = nn.Sequential(\n        nn.Conv2d(in_channels = 3, out_channels = 64, kernel_size = 3, stride = 1, padding = 1),\n        nn.BatchNorm2d(num_features = 64),\n        nn.ReLU())\n    \n    def create_resnet_block(input_filters, output_filters, num_residuals, first_block = False):\n        # Python list to hold the created ResNet blocks-\n        resnet_blk = []\n        \n        for i in range(num_residuals):\n            if i == 0 and first_block:\n                resnet_blk.append(ResidualBlock(input_channels = input_filters, num_channels = output_filters, use_1x1_conv = True, strides = 2))\n            else:\n                resnet_blk.append(ResidualBlock(input_channels = output_filters, num_channels = output_filters, use_1x1_conv = False, strides = 1))\n        \n        return resnet_blk\n    \n    b1 = nn.Sequential(*create_resnet_block(input_filters = 64, output_filters = 64, num_residuals = 2, first_block = True))\n    \n    b2 = nn.Sequential(*create_resnet_block(input_filters = 64, output_filters = 128, num_residuals = 2, first_block = True))\n    \n    b3 = nn.Sequential(*create_resnet_block(input_filters = 128, output_filters = 256, num_residuals = 2, first_block = True))\n    \n    b4 = nn.Sequential(*create_resnet_block(input_filters = 256, output_filters = 512, num_residuals = 2, first_block = True))\n    \n    # Initialize a ResNet-18 CNN model-\n    model = nn.Sequential(\n        b0, b1, b2, b3, b4,\n        nn.AdaptiveAvgPool2d(output_size = (1, 1)),\n        nn.Flatten(),\n        nn.Linear(in_features = 512, out_features = 10))\n\nThe layer names are now as follows:\n\n    for layer_name, param in trained_model.named_parameters():\n        print(f\"layer name: {layer_name} has {param.shape}\")\n    \n\n&gt;layer name: 0.0.weight has torch.Size(\\[64, 3, 3, 3\\])  \n&gt;  \n&gt;layer name: 0.0.bias has torch.Size(\\[64\\])  \n&gt;  \n&gt;layer name: 0.1.weight has torch.Size(\\[64\\])  \n&gt;  \n&gt;layer name: 0.1.bias has torch.Size(\\[64\\])  \n&gt;  \n&gt;layer name: 1.0.conv1.weight has torch.Size(\\[64, 64, 3, 3\\])  \n&gt;  \n&gt;layer name: 1.0.bn1.weight has torch.Size(\\[64\\])  \n&gt;  \n&gt;layer name: 1.0.bn1.bias has torch.Size(\\[64\\])  \n&gt;  \n&gt;layer name: 1.0.conv2.weight has torch.Size(\\[64, 64, 3, 3\\])  \n&gt;  \n&gt;layer name: 1.0.bn2.weight has torch.Size(\\[64\\])  \n&gt;  \n&gt;layer name: 1.0.bn2.bias has torch.Size(\\[64\\])  \n&gt;  \n&gt;layer name: 1.0.conv3.weight has torch.Size(\\[64, 64, 1, 1\\])  \n&gt;  \n&gt;layer name: 1.0.conv3.bias has torch.Size(\\[64\\])  \n&gt;  \n&gt;layer name: 1.0.bn3.weight has torch.Size(\\[64\\])  \n&gt;  \n&gt;layer name: 1.0.bn3.bias has torch.Size(\\[64\\])  \n&gt;  \n&gt;layer name: 1.1.conv1.weight has torch.Size(\\[64, 64, 3, 3\\])  \n&gt;  \n&gt;layer name: 1.1.bn1.weight has torch.Size(\\[64\\])  \n&gt;  \n&gt;layer name: 1.1.bn1.bias has torch.Size(\\[64\\])  \n&gt;  \n&gt;layer name: 1.1.conv2.weight has torch.Size(\\[64, 64, 3, 3\\])  \n&gt;  \n&gt;layer name: 1.1.bn2.weight has torch.Size(\\[64\\])  \n&gt;  \n&gt;layer name: 1.1.bn2.bias has torch.Size(\\[64\\])  \n&gt;  \n&gt;layer name: 2.0.conv1.weight has torch.Size(\\[128, 64, 3, 3\\])  \n&gt;  \n&gt;layer name: 2.0.bn1.weight has torch.Size(\\[128\\])  \n&gt;  \n&gt;layer name: 2.0.bn1.bias has torch.Size(\\[128\\])  \n&gt;  \n&gt;layer name: 2.0.conv2.weight has torch.Size(\\[128, 128, 3, 3\\])  \n&gt;  \n&gt;layer name: 2.0.bn2.weight has torch.Size(\\[128\\])  \n&gt;  \n&gt;layer name: 2.0.bn2.bias has torch.Size(\\[128\\])  \n&gt;  \n&gt;layer name: 2.0.conv3.weight has torch.Size(\\[128, 64, 1, 1\\])  \n&gt;  \n&gt;layer name: 2.0.conv3.bias has torch.Size(\\[128\\])  \n&gt;  \n&gt;layer name: 2.0.bn3.weight has torch.Size(\\[128\\])  \n&gt;  \n&gt;layer name: 2.0.bn3.bias has torch.Size(\\[128\\])  \n&gt;  \n&gt;layer name: 2.1.conv1.weight has torch.Size(\\[128, 128, 3, 3\\])  \n&gt;  \n&gt;layer name: 2.1.bn1.weight has torch.Size(\\[128\\])  \n&gt;  \n&gt;layer name: 2.1.bn1.bias has torch.Size(\\[128\\])  \n&gt;  \n&gt;layer name: 2.1.conv2.weight has torch.Size(\\[128, 128, 3, 3\\])  \n&gt;  \n&gt;layer name: 2.1.bn2.weight has torch.Size(\\[128\\])  \n&gt;  \n&gt;layer name: 2.1.bn2.bias has torch.Size(\\[128\\])  \n&gt;  \n&gt;layer name: 3.0.conv1.weight has torch.Size(\\[256, 128, 3, 3\\])  \n&gt;  \n&gt;layer name: 3.0.bn1.weight has torch.Size(\\[256\\])  \n&gt;  \n&gt;layer name: 3.0.bn1.bias has torch.Size(\\[256\\])  \n&gt;  \n&gt;layer name: 3.0.conv2.weight has torch.Size(\\[256, 256, 3, 3\\])  \n&gt;  \n&gt;layer name: 3.0.bn2.weight has torch.Size(\\[256\\])  \n&gt;  \n&gt;layer name: 3.0.bn2.bias has torch.Size(\\[256\\])  \n&gt;  \n&gt;layer name: 3.0.conv3.weight has torch.Size(\\[256, 128, 1, 1\\])  \n&gt;  \n&gt;layer name: 3.0.conv3.bias has torch.Size(\\[256\\])  \n&gt;  \n&gt;layer name: 3.0.bn3.weight has torch.Size(\\[256\\])  \n&gt;  \n&gt;layer name: 3.0.bn3.bias has torch.Size(\\[256\\])  \n&gt;  \n&gt;layer name: 3.1.conv1.weight has torch.Size(\\[256, 256, 3, 3\\])  \n&gt;  \n&gt;layer name: 3.1.bn1.weight has torch.Size(\\[256\\])  \n&gt;  \n&gt;layer name: 3.1.bn1.bias has torch.Size(\\[256\\])  \n&gt;  \n&gt;layer name: 3.1.conv2.weight has torch.Size(\\[256, 256, 3, 3\\])  \n&gt;  \n&gt;layer name: 3.1.bn2.weight has torch.Size(\\[256\\])  \n&gt;  \n&gt;layer name: 3.1.bn2.bias has torch.Size(\\[256\\])  \n&gt;  \n&gt;layer name: 4.0.conv1.weight has torch.Size(\\[512, 256, 3, 3\\])  \n&gt;  \n&gt;layer name: 4.0.bn1.weight has torch.Size(\\[512\\])  \n&gt;  \n&gt;layer name: 4.0.bn1.bias has torch.Size(\\[512\\])  \n&gt;  \n&gt;layer name: 4.0.conv2.weight has torch.Size(\\[512, 512, 3, 3\\])  \n&gt;  \n&gt;layer name: 4.0.bn2.weight has torch.Size(\\[512\\])  \n&gt;  \n&gt;layer name: 4.0.bn2.bias has torch.Size(\\[512\\])  \n&gt;  \n&gt;layer name: 4.0.conv3.weight has torch.Size(\\[512, 256, 1, 1\\])  \n&gt;  \n&gt;layer name: 4.0.conv3.bias has torch.Size(\\[512\\])  \n&gt;  \n&gt;layer name: 4.0.bn3.weight has torch.Size(\\[512\\])  \n&gt;  \n&gt;layer name: 4.0.bn3.bias has torch.Size(\\[512\\])  \n&gt;  \n&gt;layer name: 4.1.conv1.weight has torch.Size(\\[512, 512, 3, 3\\])  \n&gt;  \n&gt;layer name: 4.1.bn1.weight has torch.Size(\\[512\\])  \n&gt;  \n&gt;layer name: 4.1.bn1.bias has torch.Size(\\[512\\])  \n&gt;  \n&gt;layer name: 4.1.conv2.weight has torch.Size(\\[512, 512, 3, 3\\])  \n&gt;  \n&gt;layer name: 4.1.bn2.weight has torch.Size(\\[512\\])  \n&gt;  \n&gt;layer name: 4.1.bn2.bias has torch.Size(\\[512\\])  \n&gt;  \n&gt;layer name: 7.weight has torch.Size(\\[10, 512\\])  \n&gt;  \n&gt;layer name: 7.bias has torch.Size(\\[10\\])\n\n&amp;#x200B;\n\nIn order to prune this model, I am referring to [PyTorch pruning tutorial](https://pytorch.org/tutorials/intermediate/pruning_tutorial.html#inspect-a-module). It's mentioned here that to prune a module/layer, use the following code:\n\n&amp;#x200B;\n\n    parameters_to_prune = (\n        (model.conv1, 'weight'),\n        (model.conv2, 'weight'),\n        (model.fc1, 'weight'),\n        (model.fc2, 'weight'),\n        (model.fc3, 'weight'),\n    )\n\nBut for the code above, the modules/layers no longer have this naming convention. For example, to prune the first conv layer of this model:\n\n&gt;layer name: 0.0.weight has torch.Size(\\[64, 3, 3, 3\\])\n\n&amp;#x200B;\n\non trying the following code:\n\n    prune.random_unstructured(model.0.0, name = 'weight', amount = 0.3)\n\nIt gives me the error:\n\n&gt;prune.random\\_unstructured(trained\\_model.0.0, name = 'weight', amount = 0.3)  \n&gt;  \n&gt;\\^  \n&gt;  \n&gt;SyntaxError: invalid syntax\n\nHow do I handle this?", "upvote_ratio": 0.67, "id": "t3_mxlrir", "created_utc": 1619276776.0}
{"sub": "pytorch", "title": "Pytorch Template", "selftext": "Hi everyone,\n\nI write a pytorch template codes.\n\nI keep revising this project about 6 months. Hope more people can see it.\n\nThe aim of this template is:\n\nPytorch template can do all kinds of ML projects. Make it easy and fast to revise pytorch codes.\n\nThanks!\n\n[https://github.com/deeperlearner/pytorch-template](https://github.com/deeperlearner/Pytorch-Template)", "upvote_ratio": 0.93, "id": "t3_mxlpyq", "created_utc": 1619276646.0}
{"sub": "pytorch", "title": "How can I use a lr scheduler just for its value?", "selftext": "I have a lr scheduler:\n\n    import torch\n    import torch.nn as nn \n    import matplotlib.pyplot as plt\n    model = torch.nn.Linear(2, 1) \n    optimizer = torch.optim.SGD(model.parameters(), lr=0.05) \n    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e3, max_lr=1e-1, step_size_up=1000, mode=\"triangular2\", cycle_momentum=False) \n    \n    lrs = []\n    for i in range(30000):\n        optimizer.step()\n        #print(optimizer.param_groups[0][\"lr\"])\n        lrs.append(optimizer.param_groups[0][\"lr\"])\n        scheduler.step()\n    \n    plt.plot(lrs)\n\nThat looks like\n\n&amp;#x200B;\n\nhttps://preview.redd.it/jcaekfaahru61.png?width=782&amp;format=png&amp;auto=webp&amp;s=408a002c216fc06642fc088d1498d28004682cf2\n\nIs there a way to use this scheduler simply for its value for each epoch? That is, I want to plug its value at each epoch in a certain function:\n\n    for epoch in num_epochs:     \n    loss = 5 - optimizer.param_groups[0][\"lr\"]\n\nMy problem is that I have another lr scheduler, and when I use this one, my model just takes one of them as the scheduler. Again, I just need this one for its value, so it should be disconnected from the parameters", "upvote_ratio": 1.0, "id": "t3_mwa9iq", "created_utc": 1619113306.0}
{"sub": "pytorch", "title": "Libtorch Build Error (C++)", "selftext": "Hello,\n\nI am trying to build a project using libtorch, as I want to try out using pytorch with C++.\n\nHowever, when i try to build the project (With C++ Version 17, in Visual Studio 2019), I get the following error:\n\n&gt;function call is not allowed in a constant expression\n\nFollowing the error, it points to the following line in Macros.h, line 189:\n\n`#if __has_attribute(always_inline) || defined(__GNUC__)`\n\nHas anyone had this problem before?", "upvote_ratio": 0.72, "id": "t3_mw8gdp", "created_utc": 1619108443.0}
{"sub": "pytorch", "title": "Calculate autodif of NN outputs wrt inputs.", "selftext": "Does anyone have an example of how to syntax this goal? Any example or response I get does autodif of loss wrt to parameters.\n\nI want to make my NN a function say f: t -&gt; (x, Vx) where x is the calculated NN output from a typical training regiment, and Vx is the autodif of x wrt t (velocity of NN position output).", "upvote_ratio": 1.0, "id": "t3_mw3ub7", "created_utc": 1619095238.0}
{"sub": "pytorch", "title": "How to change PyTorch sigmoid function to be more steep", "selftext": "My model works when I use \"torch.sigmoid\". I tried to make the sigmoid steeper by creating a new sigmoid function:\n\n    def sigmoid(x):     \n        return 1 / (1 + torch.exp(-1e5*x)) \n\nBut for some reason the gradient doesn't flow through it (I get NaN). Is there a problem in my function, or is there a way to simply change the PyTorch implementation to be steeper (as my function)?\n\nCode example:\n\n    def sigmoid(x):\n    return 1 / (1 + torch.exp(-1e5*x))\n    a = torch.tensor(0.0, requires_grad=True)\n    b = torch.tensor(0.58, requires_grad=True)\n    c = sigmoid(a-b) c.backward() a.grad\n    &gt; tensor(nan)\n    \n\n&amp;#x200B;", "upvote_ratio": 1.0, "id": "t3_mvpj9k", "created_utc": 1619039406.0}
{"sub": "pytorch", "title": "LSTM, how many timesteps do you guys like to take as input?", "selftext": "Creating a predictive model, and curious how many timesteps you guys think I should take as input?  \n\n\nI've been simply passing it 1, but am beginning to think maybe more might be beneficial. I understand the whole idea is that it should somewhat remember the previous data, but I'm worried it's not remembering well enough. (it's minute by minute data, trying to predict the next few minutes)", "upvote_ratio": 1.0, "id": "t3_mvlmxn", "created_utc": 1619028548.0}
{"sub": "pytorch", "title": "The fastest polytope sampler, paper and conference. (In Pytorch)", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_mv9aq4", "created_utc": 1618984242.0}
{"sub": "pytorch", "title": ".to(device) just hangs", "selftext": "Any attempt at casting anything in nn to cuda just waits. Nothing prints, no errors. It just sits there.\n\nI'm attempting to cast an LSTM, but it applies to anything, even if I just to try to cast the final linear layer it still results in the same problem.\n\nAny ideas? Completely out of them  \n\n\nUPDATE: Looks like it was probably due to the outdated installation of PyTorch (on an RTX 3070 now, from a 2070, incompatibility with CUDA version probably)", "upvote_ratio": 1.0, "id": "t3_mv7tb9", "created_utc": 1618977689.0}
{"sub": "pytorch", "title": "PyTorch optimizer.step() doesn't update weights when I use \"if statement\"", "selftext": "My model needs to learn certain parameters to solve this function:\n\n    self.a * (r &gt; self.b) * (self.c)\n\nWhere\n\n    self.a, self.b, and self.c \n\nare learnable parameters.My issue is that \"b\" does not change. I assume it's because the function is discontinuous. Though, it is a step function so I'm not sure how to modify it.\n\nAny ideas/tips will be appreciated\n\n&amp;#x200B;\n\nMy code is\n\n    import torch \n    import torch.nn as nn \n    import torch.optim as optim  \n    \n    class model(nn.Module): \n        def __init__(self): \n            super(model, self).__init__()             \n            self.a = torch.nn.Parameter(torch.rand(1, requires_grad=True))            \n            self.b = torch.nn.Parameter(torch.rand(1, requires_grad=True))                        \n            self.c = torch.nn.Parameter(torch.rand(1, requires_grad=True))  \n    \n    model_net = model() \n    #function to learn = 5 * (r &gt; 2) * (3)  \n    optimizer = optim.Adam(model_net.parameters(), lr = 0.1)  \n    \n    for epoch in range(10):   \n        for r in range(10):     \n            optimizer.zero_grad()     \n            loss = 5 * (r &gt; 2) * (3) - model_net.a * (r &gt; model_net.b)*(model_net.c)     \n            loss.backward()     \n            optimizer.step()   \n            print(model_net.a)   \n            print(model_net.b)   \n            print(model_net.c)   \n            print()", "upvote_ratio": 0.9, "id": "t3_muhjnj", "created_utc": 1618888854.0}
{"sub": "pytorch", "title": "I know this is over asked but hear me out...", "selftext": "Hi everyone. \n\nHow do I learn PyTorch. I'm 14. I have substantial experience in Python and have previously dabbled in OpenCV. \n\nI have tried learning it in the past to no avail. Please help. I would appreciate any attention!", "upvote_ratio": 0.71, "id": "t3_muedpt", "created_utc": 1618877912.0}
{"sub": "pytorch", "title": "An overview of the ML models introduced in TorchVision v0.9", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_mude3t", "created_utc": 1618874677.0}
{"sub": "pytorch", "title": "How to optimize integer parameters as a part of the real-valued loss function?", "selftext": "I'm doing a research, where we are experimenting with different number systems for CNN. We are quantizing intermediate values after each layer with custom number of bits. The goal is to reduce the total size of the model, while retaining as much accuracy as possible.\n\nMy idea is to do an aware-training. Of course we can set a scheme (layer 1 - 8 bits, layer 2 - 7 bits...etc) and train, so that weights can readjust to retain accuracy. But setting the scheme is not obvious.\n\nI can simulate the bit effects with torch's bit ops. I tried manually setting number of bits for different layers of a pretrained model, and it's counterintuitive. Some layers are too sensitive and some are not.\n\nSo, i thought of making number of bits an integer variable and adding a weighted sum of them to the pre-existing loss function. Weight = size of that layer. So the optimizer can minimize the number of bits (hence total size) while adjusting the weights as well, to retain most of the accuracy.\n\nProblem is, number of bits for each layer is an integer. As SGD moves little by little, integers may not change at all, and might change suddenly. I read a bit on integer programming. But I'm not sure how to combine it with the rest of the real-valued loss function. Because i want to minimize number of bits WHILE changing weights accordingly to keep accuracy up.", "upvote_ratio": 1.0, "id": "t3_mu3t1h", "created_utc": 1618848050.0}
{"sub": "pytorch", "title": "AI agent plays Chrome Dino", "selftext": "nan", "upvote_ratio": 0.94, "id": "t3_mti6hf", "created_utc": 1618769775.0}
{"sub": "pytorch", "title": "Mask RCNN training time", "selftext": "Yo i am training my Mask RCNN on about 2000 images. It took me 1 hour for one epoch. Is it normal? I am so bored now cause I have 19 epochs left\n\nI am using CPU in Google Colab.", "upvote_ratio": 1.0, "id": "t3_mtcvsi", "created_utc": 1618752738.0}
{"sub": "pytorch", "title": "Guide to PyTerrier: A Python Framework for Information Retrieval", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_mtbdjk", "created_utc": 1618746652.0}
{"sub": "pytorch", "title": "Learning rate scheduler", "selftext": "Hey guys, I am trying to recreate results of \u201cThe Lottery Ticket  Hypothesis\u201d by Frankle et al. They are using CIFAR-10 which has 50K  training images. Using a batch size = 64 gives 781 iterations/steps in  one epoch. I am trying to implement this in PyTorch.\n\nFor VGG-18 &amp; ResNet-18, the authors propose the following learning rate schedule\n\n1. Linear learning rate warmup for first k = 7813 steps from 0.0 to 0.1\n\n&amp;#x200B;\n\nAfter 10 epochs or 7813 training steps, the learning rate schedule is as follows-\n\n1. For the next 21094 training steps (or, 27 epochs), use a learning rate of 0.1\n2. For the next 13282 training steps (or, 17 epochs), use a learning rate of 0.01\n3. For any remaining training steps, use a learning rate of 0.001\n\nI have implemented this in TensorFlow2 as follows:\n\n    from typing import Callable, List, Optional, Union\n    class WarmUp(tf.keras.optimizers.schedules.LearningRateSchedule):\n        \"\"\"\n        Applies a warmup schedule on a given learning rate decay schedule.\n        Args:\n            initial_learning_rate (:obj:`float`):\n                The initial learning rate for the schedule after the warmup (so this will be the learning rate at the end\n                of the warmup).            \n            decay_schedule_fn (:obj:`Callable`):\n                The schedule function to apply after the warmup for the rest of training.        \n            warmup_steps (:obj:`int`):\n                The number of steps for the warmup part of training.        \n            power (:obj:`float`, `optional`, defaults to 1):\n                The power to use for the polynomial warmup (defaults is a linear warmup).        \n            name (:obj:`str`, `optional`):\n                Optional name prefix for the returned tensors during the schedule.\n        \"\"\"\n        def __init__(\n            self,\n            initial_learning_rate: float,\n            decay_schedule_fn: Callable,\n            warmup_steps: int,\n            power: float = 1.0,\n            name: str = None,\n        ):\n            super().__init__()\n            self.initial_learning_rate = initial_learning_rate\n            self.warmup_steps = warmup_steps\n            self.power = power\n            self.decay_schedule_fn = decay_schedule_fn\n            self.name = name\n    \n        def __call__(self, step):\n            with tf.name_scope(self.name or \"WarmUp\") as name:\n                # Implements polynomial warmup. i.e., if global_step &lt; warmup_steps, the\n                # learning rate will be `global_step/num_warmup_steps * init_lr`.\n                global_step_float = tf.cast(step, tf.float32)\n                warmup_steps_float = tf.cast(self.warmup_steps, tf.float32)\n                warmup_percent_done = global_step_float / warmup_steps_float\n                warmup_learning_rate = self.initial_learning_rate * tf.math.pow(warmup_percent_done, self.power)\n                return tf.cond(\n                    global_step_float &lt; warmup_steps_float,\n                    lambda: warmup_learning_rate,\n                    lambda: self.decay_schedule_fn(step - self.warmup_steps),\n                    name=name,\n                )\n    \n        def get_config(self):\n            return {\n                \"initial_learning_rate\": self.initial_learning_rate,\n                \"decay_schedule_fn\": self.decay_schedule_fn,\n                \"warmup_steps\": self.warmup_steps,\n                \"power\": self.power,\n                \"name\": self.name,\n            }\n    \n    boundaries = [21093, 34376]\n    values = [0.1, 0.01, 0.001]\n    learning_rate_fn = tf.keras.optimizers.schedules.PiecewiseConstantDecay(boundaries, values)\n    warmup_shcedule = WarmUp(initial_learning_rate = 0.1, decay_schedule_fn = learning_rate_fn, warmup_steps = 7813)\n    optimizer = tf.keras.optimizers.SGD(learning_rate = warmup_shcedule, momentum = 0.9, decay = 0.0, nesterov = False)\n\nI then train model using \u201ctf.GradientTape\u201d and view the learning rate as follows:\n\n    optimizer._decayed_lr('float32').numpy()\n\nThe resulting learning during the 60 epochs of training can be viewed:\n\n[learning rate scheduler](https://preview.redd.it/wbx65esp3pt61.png?width=559&amp;format=png&amp;auto=webp&amp;s=5a3bdc23bf796da6e6d367e1c9cbd49fd160a4f6)\n\nYou can access the complete code [here](https://github.com/arjun-majumdar/CNN_Classifications/blob/master/VGG18_Train_from_scratch-LR_Warmup_%26_Step_Decay.ipynb). Since I am new to PyTorch, can you show me how I can achieve the same learning rate scheduler in torch?\n\nThanks", "upvote_ratio": 1.0, "id": "t3_msn34u", "created_utc": 1618648586.0}
{"sub": "pytorch", "title": "Dynamic Hyper-parameters: Change hyper-parameter values while a model is training", "selftext": "*Hyper-parameters control the learning process of models. They are set before training starts, either by intuition or a hyper-parameter search. They either stay static or change based on a pre-determined schedule. We are introducing dynamic hyper-parameters which can be manually adjusted during the training based on model training stats.*\n\nGithub: [https://github.com/lab-ml/labml/blob/master/guides/dynamic\\_hyperparameters.md](https://github.com/lab-ml/labml/blob/master/guides/dynamic_hyperparameters.md)\n\n## What are hyper-parameters?\n\nHyper-parameters are parameters that control the learning process of models, such as the learning rate, batch size, and weight decay. The model might not learn if the hyper-parameters are not set correctly. Setting hyper-parameters is a key part of deep learning research. Researchers find them based on intuition or by running a hyper-parameter search. In a hyper-parameter search, the model is trained repeatedly with different hyper-parameters to find the best set of hyper-parameters. Hyper-parameter search becomes costly as the number of hyper-parameters increases and the model training time increases.\n\nSome hyper-parameters change during the training based on a pre-determined schedule. For example, you could slowly decrease the learning rate, or you could decrease the coefficient of an auxiliary loss as the model learns. Finding these schedules is nearly impossible with a hyper-parameter search, and are usually determined based on the intuition of the researchers.\n\n## Why is it hard to determine hyper-parameters?\n\nSetting hyper-parameters require quite a bit of experience with the kind of models and sizes you are training as well as the dataset. For instance, consider fine-tuning a pre-trained language model to classify tweets. You get a pre-trained language model as the backbone and attach a layer (or two) as the classification head. First, you freeze the parameters of the backbone and train the head for a certain number of updates, and then you unfreeze all parameters and train all the parameters. The number of steps to keep the backbone frozen is generally set to 1 epoch. This is a hyper-parameter. And the common practice of freezing for 1 epoch might be too small or too large depending on the size of the model as well as the dataset size. Someone who has worked with similar models and datasets will have a good intuition on this hyper-parameter. If you are new, you will have to try training the model to get a feel about it.\n\n&lt;!-- Now consider setting the reward discount factor in a reinforcement learning agent. This determines how much the future rewards are discounted when considering the current step. A lower discount factor will only give rewards from the next few steps, whilst a discount factor close to one will get rewards from all future steps. That is, a smaller discount factor will make the agent short-sighted. It's generally faster to train agents initially with a small discount factor and increase it to be close to one towards the end of the training. Knowing how fast to change this is difficult. You will know by intuition if you have trained agents in the same environment before. Otherwise, you will have to run a few training sessions and study them to get a better understanding. --&gt;  \n\n\n## \u2699\ufe0f Introducing Dynamic Hyper-parameters\n\nDynamic hyper-parameters are hyper-parameters that researchers can adjust while the model is being trained. This allows researchers to actively control how the model trains, instead of letting the model train with a pre-determined set of hyper-parameters. Dynamic hyper-parameters help train the model faster and better on a single training session. Also, they let researchers play around with the hyper-parameters during a single training run to gather insights.\n\nSometimes researchers save the model checkpoints and restart the training with changed hyper-parameter values. This has a similar effect to dynamic hyper-parameters but it's quite cumbersome to do.\n\n## How does it work?\n\nYou need to create a dynamic hyper-parameter and register them along with other configurations.\n\n    from labml import experiment\n    from labml.configs import FloatDynamicHyperParam\n    \n    lr = FloatDynamicHyperParam(2.5e-4, range_=(0, 0.1))\n    \n    experiment.configs({\n      'learning_rate': lr,\n      ...,\n    })\n\nThen can call the dynamic hyper-parameter to get the current value. For example:\n\n    def train(batch):\n      optimizer.set_lr(lr())\n      optimizer.step()\n\nThe call `lr()` will return the current learning rate set in [labml.ai](https://labml.ai) [app](https://github.com/lab-ml/app).\n\n[This is a screenshot of the mobile web interface](https://github.com/lab-ml/labml/blob/master/guides/dynamic_hp.png) *for changing dynamic hyper-parameters. In this* [*Demo*](https://app.labml.ai/run/6eff28a0910e11eb9b008db315936e2f/hyper_params)  *experiment we adjusted the learning rate, clipping range, and the number of training epochs (per sample) to speed up the training of a* [*PPO agent (code)*](https://nn.labml.ai/rl/ppo/experiment.html) *for Atari Breakout. A standard learning rate decay and other static hyper-parameter values would have taken a lot of training updates to get over the score of 1.*\n\n## Example use-cases\n\n**Freezing pre-trained layers**: When fine-tuning a language model, you can train with the backbone frozen until the rate of improvement of loss drops, and change the hyper-parameter affecting which layers are frozen. This is better and faster than going with the common practice of keeping the backbone frozen for 1 epoch for all models and datasets.\n\n**Learning-rate warm-up and decay**: The learning rate can be manually increased during the initial training updates. You could decide how long to warm up for based on the loss curves. Similarly, you can decay the learning rate when the loss values stabilize. This allows you to use higher learning rates initially to speed up the training.\n\n**Increase sequence length**: Recurrent models train faster when the BPTT length is shorter. But you need higher BPTT lengths to improve accuracy. Therefore, it is a common practice to start with a shorter BPTT length and increase it later. Again deciding when to do this beforehand is hard. Changing this dynamically is a lot easier.\n\n**Adjusting regularization parameters**: You can start with lower weight decay and lower dropout probabilities initially. Especially if you are not sure about the representation capacity of the model. You can then increase these regularization parameters later when the validation loss stops improving (higher variance).\n\n**Adjusting reinforcement learning hyper-parameters**: Reinforcement learning tends to have more hyper-parameters. Most of which need to change during training, such as discount factor, entropy-bonus coefficients, learning-rate, etc.  Pre-determining them is almost impossible without observing a few training runs, and those training runs go many hours or days even for simple gaming environments. Changing these during training based on the agent's performance and other stats is a lot easier.\n\n## What's next\n\n**Updating hyper-parameter schedules**: Our current implementation only allows users to update hyper-parameter values. This can take too much user time. For instance, let's say based on current loss curves the user figures out that he wants to drop the learning rate from `1e-3` to `2.5e-4` during the next 100,000 updates. With our current implementation, she would have to make several manual  changes. We want to let users set and update hyper-parameter schedules so that user has to manually intervene only when necessary.\n\n**Rewind**: Often when training with dynamic hyper-parameters you feel like experimenting with them. Sort of like a small hyper-parameter search while the model is training. But when things go wrong you want to reset. To enable this we are working on a simple rewind (or undo) option, where the user could restart at  any checkpoint, with a couple of taps on the screen.", "upvote_ratio": 1.0, "id": "t3_mqq0ca", "created_utc": 1618405078.0}
{"sub": "pytorch", "title": "At which linguistic patterns and features attention heads of BERT look to ?", "selftext": "\nHello,\n\nI am developing an extractive text summarizer for french language, I would like to explore the self-attention mechanism to see at which linguistic patterns it looks to by generating a heat map or? Any help?", "upvote_ratio": 1.0, "id": "t3_mq3q2m", "created_utc": 1618325167.0}
{"sub": "pytorch", "title": "Docker container getting crashed while serving Pytorch models", "selftext": "I have to deploy some PyTorch models (around 4) in production, for demonstration purpose I have created a minimal code showing how I am deploying the models using `Flask` and `Docker Swarm`\n\nProblem is that when I load test the `API` endpoints I found that memory utilization of container/service increases and it crashes after some time, I tried increasing the container memory limit in a `docker-compose.yml` file but it only extends crashing time \n\nHere is the code\n\nhttps://preview.redd.it/7i1igpagrvs61.png?width=228&amp;format=png&amp;auto=webp&amp;s=0ac25b3d481417078350629f7f061e75f7574278\n\n**app.py**\n\n    from flask import Flask, request, jsonify\n    import numpy as np\n    import cv2 as cv\n    import traceback\n    import json\n    \n    from predict_l import get_prediction_l\n    from predict_x import get_prediction_x\n    \n    app = Flask(__name__)\n    \n    @app.route('/')\n    def index():\n        return jsonify('Serving Yolov5 Pytorch models')\n    \n    @app.route('/prediction_l', methods = ['POST'])\n    def prediction_l():\n    \n        try:\n            f1 = request.files['image']\n            f2 = np.fromstring(f1.read(), np.uint8)\n            f3 = cv.imdecode(f2, cv.IMREAD_COLOR)\n    \n            res = get_prediction_l(f3)\n    \n        except Exception as e:\n            print(str(traceback.format_exc()))\n    \n        return json.dumps(res)\n    \n    @app.route('/prediction_x', methods = ['POST'])\n    def prediction_x():\n    \n        try:\n            f1 = request.files['image']\n            f2 = np.fromstring(f1.read(), np.uint8)\n            f3 = cv.imdecode(f2, cv.IMREAD_COLOR)\n    \n            res = get_prediction_x(f3)\n    \n        except Exception as e:\n            print(str(traceback.format_exc()))\n    \n        return json.dumps(res)\n    \n    if __name__ == '__main__':\n        app.run(host='0.0.0.0', debug=True)\n\n**predict\\_l.py**\n\n    import torch\n    import os\n    \n    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n    \n    model = torch.hub.load('ultralytics/yolov5', 'custom', path_or_model='model/yolov5l.pt') \n    \n    def get_prediction_l(image):\n    \n        # Inference\n        results = model(image)\n    \n        # Results\n        results.print()\n    \n        # Data\n        pred_res = results.xyxy[0].tolist()\n    \n        return pred_res\n\n[Complete code](https://gist.github.com/atinesh-s/e2d1370b4898c2a62ae7af3c1b22e328)", "upvote_ratio": 1.0, "id": "t3_mpwbn3", "created_utc": 1618293727.0}
{"sub": "pytorch", "title": "ValueError: Expected target size (384, 384), got torch.Size([384, 384, 384])", "selftext": "Hello! \n\nI am working on a quite complicated CNN which is already given and my task is, too retrain a model. \n\nThe goal from the CNN is, that you can give it a Picture of a face and it will give you a voxelised Face-Modell\n\nThe network is can be found here: \n\n[https://github.com/amadeuzou/vrn-pytorch/blob/master/vrn\\_unguided.py](https://github.com/amadeuzou/vrn-pytorch/blob/master/vrn_unguided.py)\n\ni modified it a bit. The output should be in a Voxel-Like Shape. \n\nI now need too train this model with a dataset i got.\n\nFor this i wrote a training script (i am very new too pytorch) and in my mind this script should work but \n\nit throws the error :\n\n`ValueError: Expected target size (384, 384), got torch.Size([384, 384, 384])`\n\nwhich does not make sense for me. \n\nI am loading a picture using CV and transform it in a numpy 384 \\* 384 \\* 3 array. \n\nThis Input is then processed too a 384 \\* 384 \\* 384 Voxel Volume. \n\nHowever my problem is now: \n\nBy running this script : (the data is load manually since i have only a small amount of data rn)\n\n`for epoch in range(20):`  \n`print(epoch)`  \n`VRN.train()`  \n`predicted_model = VRN(inp)[-1].data.cpu()`  \n`vol = predicted_model[0].numpy()`  \n`optimizer.zero_grad()`  \n`loss = lossfct(torch.from_numpy(vol), torch.from_numpy(voxel_array))`  \n`loss.backward()`  \n`print(loss)`  \n`optimizer.step()` \n\nit gives me the error which is said above. \n\nThis does not make sense for me since the input and the target both are in the same dimension (384, 384, 384). The Lossfunction is CrossEntropyLoss\n\nWhat should i do too solve this error? \n\nIf needed i can show the full script.", "upvote_ratio": 0.5, "id": "t3_mopkqf", "created_utc": 1618141377.0}
{"sub": "pytorch", "title": "Oop concepts for pytorch", "selftext": "Hello, I was going through an oop in pytorch tutorial the other day to prepare myself for when I learn pytorch. I had learned about the basics of consutructors, inheritance, and how to write functions. I had also learned about attributes and the \u201cself\u201d keyword. The rest of the sections go over exceptions, setters and getters, properties, global variables, etc, but I don\u2019t often see these used much in pytorch code I have seen. Should I still learn that stuff? How much of oop class design do you need to know or what oop concepts should you be comfortable with before learning pytorch?", "upvote_ratio": 0.89, "id": "t3_mopkck", "created_utc": 1618141334.0}
{"sub": "pytorch", "title": "torch.tensor. 'module' object not callable", "selftext": "I'm getting some strange error which seems to have been resolved in previous versions in pytorch. I'm running on\n\nOS: ubuntu 20.04\n\npytorch: from source (1.9.0a0+gite359842)\n\nCUDA: 11.2\n\nEvery torch code with cuda I've run so far works, but a simple torch.tensor() call gives me an error: TypeError: 'module' object is not callable.\n\nIt's strange because if I go to the terminal and run a simple python code such as:\n\n&gt;&gt; import torch\n\n\n&gt;&gt; a = torch.tensor([[1., -1.], [1., -1.]], dtype=torch.uint8, device='cuda:0').unsqueeze(0)\n\nIt'll run fine but if I used the same code to instantiate a tensor in my .py file, it throws an error. I even tried the exact same code in a new file and it works. There's no other folder/file in my directory called 'torch' or 'tensor' so it can't be an issue with that.", "upvote_ratio": 0.5, "id": "t3_mokf1o", "created_utc": 1618115873.0}
{"sub": "pytorch", "title": "How can I set a minimum learning rate in lr_scheduler LambdaLR?", "selftext": "I'm using LambdaLR as a learning rate function:\n\n    import torch \n    import torch.nn as nn \n    import matplotlib.pyplot as plt  \n    \n    model = torch.nn.Linear(2, 1) \n    optimizer = torch.optim.SGD(model.parameters(), lr=0.01) \n    lambda1 = lambda epoch: 0.99 ** epoch \n    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1, last_epoch = -1)  \n    \n    lrs = []   \n    for i in range(2001):     \n        optimizer.step()     \n        lrs.append(optimizer.param_groups[0][\"lr\"])     \n        scheduler.step()  \n        \n    plt.plot(lrs) \n    \n\nhttps://preview.redd.it/929711v9vfs61.png?width=786&amp;format=png&amp;auto=webp&amp;s=72c52ef9788a7ff3b407569270bdaa839a71569b\n\nI'm trying to set a min learning rate so it won't go to 0. How can I do that?", "upvote_ratio": 1.0, "id": "t3_mogkjb", "created_utc": 1618100941.0}
{"sub": "pytorch", "title": "Growing neural cellular automata - Implementation from scratch", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_mo8n25", "created_utc": 1618074414.0}
{"sub": "pytorch", "title": "Need help with anaconda and cuda", "selftext": "**~\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py in _lazy_init() \n\n168 # This function throws if there's a driver initialization error, no GPUs\n\n169 # are found or any other error occurs \n\n170 torch._C._cuda_init()\n\n171 # Some of the queued calls may reentrantly call _lazy_init(); \n\n172 # we need to just return without initializing in that c**ase. RuntimeError: CUDA driver initialization failed, you might not have a CUDA gpu. \n\nI have already cheacked my gpu drivers\n\nmy cuda version and gpu drivers\n\npytorch 1.8.1 cuda 11.1.1 cudnn 8.0\n\nhttps://i.stack.imgur.com/nuL9L.png", "upvote_ratio": 1.0, "id": "t3_mnmikf", "created_utc": 1617989996.0}
{"sub": "pytorch", "title": "Pruning tutorial", "selftext": "Hey guys, I am looking for neural network pruning tutorials/implementations.\n\nI looked into [torch.nn.utils.prune](https://pytorch.org/docs/master/generated/torch.nn.utils.prune.global_unstructured.html) module but it doesn't present an end-to-end example and the [code](https://github.com/arjun-majumdar/Neural_Network_Pruning/blob/main/Iterative_Pruning_LeNet300_PyTorch.ipynb) that I came up with doesn't seem to work.\n\nHelp?", "upvote_ratio": 1.0, "id": "t3_mndfdg", "created_utc": 1617960140.0}
{"sub": "pytorch", "title": "Need help with a doubt", "selftext": "In my model , how to define output in training section of the code ? Sorry for such a basic question\n\nfor epoch in range(num_epochs)\n      for data in trainset:\n         X, y = data\n         model.zero_grad()\n         output = ?", "upvote_ratio": 1.0, "id": "t3_mnc8r0", "created_utc": 1617954388.0}
{"sub": "pytorch", "title": "Tensor Sizes Not Matching (ERROR)", "selftext": "Hi,\n\nI am working on this image classification model and I have 120 different types of images. I have read them all as grayscale and resized them to be 50x50 px. Here is the code for the model:\n\n&amp;#x200B;\n\n    data = np.load(\"training_data.npy\", allow_pickle=True)[0]\n    labels = np.load(\"training_data.npy\", allow_pickle=True)[1]\n    \n    \n    training_data = []\n    \n    for i in range(len(data)):\n        training_data.append([np.array(data[i]), np.eye(120)])\n    \n    np.random.shuffle(training_data)\n    \n    \n    \n    for item in training_data:\n        item[0] = item[0][:147]\n    \n    device = torch.device(\"cuda:0\")\n    class Net(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.conv1 = nn.Conv2d(1, 32, 5)\n            self.conv2 = nn.Conv2d(32, 64, 5)\n            self.conv3 = nn.Conv2d(64, 128, 5) \n            self.pool1 = nn.MaxPool2d((2, 2))\n            self.pool2 = nn.MaxPool2d((2, 2))\n            self.pool3 = nn.MaxPool2d((2, 2))\n    # commenting out fc layers, replace value with our output\n            self.fc1 = nn.Linear(512, 512)\n            self.fc2 = nn.Linear(512, 120)\n        \n        def forward(self, x):\n            x = F.relu(self.conv1(x))\n            x = self.pool1(x)\n            x = F.relu(self.conv2(x))\n            x = self.pool2(x)\n            x = F.relu(self.conv3(x))\n            x = self.pool3(x)\n            x = x.flatten(start_dim=1) # flattening out\n    #        print(x.shape) # printing the shape of the flattened output\n            x = F.relu(self.fc1(x))\n            x = self.fc2(x)\n            return F.softmax(x, dim=1)\n        \n    net = Net().to(device)\n    #net.forward(torch.randn(1, 1, 50, 50).to(device)) # passing a sample input (random)\n    \n    print(net)\n    print(device)\n    print(len(training_data))\n    \n    optimizer = optim.Adam(net.parameters(), lr=0.001)\n    loss_function = nn.MSELoss()\n    \n    X = torch.Tensor([i[0] for i in training_data]).view(-1,50,50)\n    X = X/255.0\n    y = torch.Tensor([i[1] for i in training_data])\n    \n    print(y[0])\n    \n    VAL_PCT = 0.1  # lets reserve 10% of our data for validation\n    val_size = int(len(X)*VAL_PCT)\n    print(val_size)\n    \n    train_X = X[:-val_size]\n    train_y = y[0][:-val_size]\n    \n    test_X = X[-val_size:]\n    test_y = y[0][-val_size:]\n    \n    print(len(train_X), len(test_X))\n    for epoch in range(EPOCHS):\n        for i in tqdm(range(0, len(train_X), BATCH_SIZE)): # from 0, to the len of x, stepping BATCH_SIZE at a time. [:50] ..for now just to dev\n            #print(f\"{i}:{i+BATCH_SIZE}\")\n            batch_X = train_X[i:i+BATCH_SIZE].view(-1, 1, 50, 50).to(device)\n            batch_y = train_y[i:i+BATCH_SIZE].to(device)\n    \n            net.zero_grad()\n    \n            outputs = net(batch_X)\n            loss = loss_function(outputs, batch_y)\n            loss.backward()\n            optimizer.step()    # Does the update\n    \n        print(f\"Epoch: {epoch}. Loss: {loss}\")\n    \n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for i in tqdm(range(len(test_X))):\n            real_class = torch.argmax(test_y[i]).to(device)\n            net_out = net(test_X[i].view(-1, 1, 50, 50).to(device))[0]  # returns a list, \n            predicted_class = torch.argmax(net_out)\n    \n            if predicted_class == real_class:\n                correct += 1\n            total += 1\n    print(\"Accuracy: \", round(correct/total, 3))\n\nWhen I run the program I run into this error:\n\n    /usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([0])) that is different to the input size (torch.Size([100, 120])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n      return F.mse_loss(input, target, reduction=self.reduction)\n      0%|                                                   | 0/159 [00:00&lt;?, ?it/s]\n    Traceback (most recent call last):\n      File \"MLFINAL.py\", line 126, in &lt;module&gt;\n        loss = loss_function(outputs, batch_y)\n      File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n        result = self.forward(*input, **kwargs)\n      File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py\", line 528, in forward\n        return F.mse_loss(input, target, reduction=self.reduction)\n      File \"/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\", line 2928, in mse_loss\n        expanded_input, expanded_target = torch.broadcast_tensors(input, target)\n      File \"/usr/local/lib/python3.6/dist-packages/torch/functional.py\", line 74, in broadcast_tensors\n        return _VF.broadcast_tensors(tensors)  # type: ignore\n    RuntimeError: The size of tensor a (120) must match the size of tensor b (0) at non-singleton dimension 1\n\nAt this point I have honestly no clue what to do. Any help would be amazing!", "upvote_ratio": 1.0, "id": "t3_mn6v3e", "created_utc": 1617932835.0}
{"sub": "pytorch", "title": "Pretrained image classification model for nuts and bolts (or similar)", "selftext": "Hello! I'm looking for some pre trained image classification models to use with PyTorch on a Jetson Nano. I already know about the model zoo and the pre trained models included in the [https://github.com/dusty-nv/jetson-inference](https://github.com/dusty-nv/jetson-inference) repo. For demonstration purposes, however, I need a model trained on small objects from the context of production, ideally nuts, bolts, and similar small objects. Does anyone happen to know a source for this? Thanks a lot!", "upvote_ratio": 0.81, "id": "t3_mmn9th", "created_utc": 1617870495.0}
{"sub": "pytorch", "title": "Combine 2 tensors", "selftext": "I have 2 tensors of size 100 each:\n\na = torch.ones(100)  \nb = torch.zeros(100)  \nI'm trying to combine them to a tensor that looks like this:\n\nc = \\[\\[1,0\\],\\[1,0\\],\\[1,0\\]...\\[1,0\\] \\]\n\nHow can I do that in an efficient way?", "upvote_ratio": 1.0, "id": "t3_mm39jb", "created_utc": 1617805538.0}
{"sub": "pytorch", "title": "Latest Innovations with Grid.ai and PyTorch Lightning", "selftext": "Join us on April 13th at 11 am ET to hear about Grid.ai and PyTorch Lightning's latest innovations with our CEO and Founder William Falcon and Thomas Chaton, Research Engineering Manager. This discussion is excellent for AI researchers, machine learning engineers, and data scientists looking for new ways to accelerate and improve their current AI model training process. Leave with tangible strategies, new tools, and great ideas.\n\nRegister now and submit any questions you have for William and Thomas!\n\n[https://zoom.us/webinar/register/1016176774118/WN\\_yW66h71HSz-MXWNGAu6OOg](https://zoom.us/webinar/register/1016176774118/WN_yW66h71HSz-MXWNGAu6OOg)", "upvote_ratio": 0.5, "id": "t3_mloj4u", "created_utc": 1617750732.0}
{"sub": "pytorch", "title": "Latest Innovations with Grid.ai and PyTorch Lightning", "selftext": "Join us on April 13th at 11 am ET to hear about Grid.ai and PyTorch Lightning's latest innovations with our CEO and Founder William Falcon and Thomas Chaton, Research Engineering Manager. This discussion is excellent for AI researchers, machine learning engineers, and data scientists looking for new ways to accelerate and improve their current AI model training process. Leave with tangible strategies, new tools, and great ideas.\n\nRegister now and submit any questions you have for William and Thomas!\n\n[https://zoom.us/webinar/register/1016176774118/WN\\_yW66h71HSz-MXWNGAu6OOg](https://zoom.us/webinar/register/1016176774118/WN_yW66h71HSz-MXWNGAu6OOg)", "upvote_ratio": 0.54, "id": "t3_mlogf5", "created_utc": 1617750513.0}
{"sub": "pytorch", "title": "How to differentiate a gradient in Pytorch", "selftext": "I'm trying to differentiate a gradient in PyTorch. I found [this](https://stackoverflow.com/questions/49149699/differentiate-gradients) link but can't get it to work.\n\nMy code looks as follows:\n\n    import torch \n    from torch.autograd import grad \n    import torch.nn as nn \n    import torch.optim as optim  \n    \n    class net_x(nn.Module): \n        def __init__(self): \n            super(net_x, self).__init__()             \n            self.fc1=nn.Linear(2, 20)              \n            self.fc2=nn.Linear(20, 20)             \n            self.out=nn.Linear(20, 4)           \n    \n        def forward(self, x):             \n            x=self.fc1(x)             \n            x=self.fc2(x)             \n            x=self.out(x)             \n            return x  \n    \n    nx = net_x() \n    r = torch.tensor([1.0,2.0]) \n    nx(r) \n    &gt;&gt;&gt;tensor([-0.2356, -0.7315, -0.2100, -0.6741], grad_fn=&lt;AddBackward0&gt;) \n\nBut when I try to differentiate the function with respect to the first parameter\n\n    grad(nx, r[0]) \n\nI get the error\n\n    TypeError: 'net_x' object is not iterable", "upvote_ratio": 1.0, "id": "t3_mllx32", "created_utc": 1617743682.0}
{"sub": "pytorch", "title": "Predicting Four Variables Using Three Variables", "selftext": "Hi, I have a dataset of 8M rows with 7 numerical features (hearing test audiogram data) and was tasked with predicting 4 variables based on 3. This is how the provided dataset looks like (used fake numbers). Does anyone know what model/approach is the best? I'm thinking Neural Network given the size.. but also have never used multiple independent AND dependent variables before... Any and all pointers are appreciated!!\n\nhttps://preview.redd.it/ltlalc3hplr61.png?width=781&amp;format=png&amp;auto=webp&amp;s=7cbef4f1c9b5e8da7fa67594e85fefcb51b82a55", "upvote_ratio": 0.84, "id": "t3_mlj2g8", "created_utc": 1617735776.0}
{"sub": "pytorch", "title": "Example help", "selftext": "I'm looking for an example pytorch example for an inverted pendulum on a cart where the gains are defined and you create a NN to update gains based on initial offset angle and force. I haven't found any good examples with code and was wondering if there were resources  for this (sorry new to ML)?", "upvote_ratio": 1.0, "id": "t3_mkz983", "created_utc": 1617669467.0}
{"sub": "pytorch", "title": "Weird assertion error", "selftext": "I asked this on SO but didn't get any useful answers. I'm really confused to why I'm having this error. The error I'm getting is `RuntimeError: all only supports torch.uint8 and torch.bool dtypes` . This happens when I run assertion to check for data leakage. \n\n[Link to SO question](https://stackoverflow.com/questions/66924836/runtimeerror-all-only-supports-torch-uint8-and-torch-bool-dtypes)", "upvote_ratio": 1.0, "id": "t3_mkqhcf", "created_utc": 1617644946.0}
{"sub": "pytorch", "title": "Gradients of model output layer and intermediate layer wrt inputs", "selftext": "I\u2019m trying to visualize model layer outputs using the [saliency core package](https://github.com/PAIR-code/saliency)  package on a simple conv net. This requires me to compute the gradients  of the model output layer and intermediate convolutional layer output  w.r.t the input. I\u2019ve attempted to do this in the last code block, but I  run into the error\n\n    RuntimeError: grad can be implicitly created only for scalar outputs\n\n. How do I retrieve these gradients and what is this error telling me?\n\n&amp;#x200B;\n\n\\`\\`\\`\n\n`import torch`\n\n`import torch.nn as nn`\n\n`import torchvision`\n\n`from torchvision import datasets, transforms`\n\n`from` [`torch.utils.data`](https://torch.utils.data) `import DataLoader`\n\n`import numpy as np`\n\n`from tqdm.notebook import tqdm`\n\n`device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")`\n\n`from google.colab import drive`\n\n`drive.mount('/content/drive')`\n\n`root = '/content/drive/MyDrive/Work/ExplainMNIST'`\n\n`import os`\n\n`# !pip install saliency`\n\n`import saliency.core as saliency`\n\n&amp;#x200B;\n\n`# Hyperparameters`\n\n`batch_size = 32`\n\n`num_classes = 10`\n\n&amp;#x200B;\n\n`data_dir = os.path.join(root, \"data\")`\n\n`mnist_train = torchvision.datasets.MNIST(data_dir, train=True, transform=transforms.Compose([`\n\n`transforms.ToTensor(),`\n\n`transforms.Normalize((0.1307,), (0.3081))`\n\n`]), download=True)`\n\n`mnist_test = torchvision.datasets.MNIST(data_dir, train=False, transform=transforms.Compose([`\n\n`transforms.ToTensor(),`\n\n`transforms.Normalize((0.1307,), (0.3081))`\n\n`]), download=True)`\n\n`train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True)`\n\n`test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True)`\n\n`checkpoint_dir = os.path.join(root, 'checkpoints')`\n\n`if not os.path.exists(checkpoint_dir):`\n\n`os.makedirs(checkpoint_dir)`\n\n&amp;#x200B;\n\n`images_, labels_ = next(iter(train_loader))`\n\n`images_ = images_.to(device)`\n\n`labels_ = labels_.to(device)`\n\n&amp;#x200B;\n\n`class ConvNet(nn.Module):`\n\n`def __init__(self, in_channels, num_classes):`\n\n`super(ConvNet, self).__init__()`\n\n`self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=10, kernel_size=3)`\n\n`self.pool1 = nn.MaxPool2d(3)`\n\n`self.conv2 = nn.Conv2d(in_channels=10, out_channels=10, kernel_size=3)`\n\n`self.activation = nn.ReLU()`\n\n`self.fc = nn.Linear(360, num_classes)`\n\n`def forward(self, x):`\n\n`x = self.conv1(x)`\n\n`x = self.pool1(x)`\n\n`x = self.activation(x)`\n\n`x = self.conv2(x)`\n\n`x = self.activation(x)`\n\n`conv = x`\n\n`x = self.fc(x.reshape(x.shape[0], -1))`\n\n`return {'output': x, 'conv': conv}`\n\n&amp;#x200B;\n\n`model = ConvNet(1, num_classes)`\n\n`model =` [`model.to`](https://model.to)`(device)`\n\n`model.eval()`\n\n&amp;#x200B;\n\n`#HELP! here is my attempt`\n\n`with torch.set_grad_enabled(True):`\n\n`output, conv = model(images_).values()`\n\n`output_class = output[:, 0]`\n\n`# Here is how i tried to do it`\n\n`grads = torch.autograd.grad(output, images_)`\n\n`grads2 = torch.autograd.grad(conv, images_)`", "upvote_ratio": 1.0, "id": "t3_mk987e", "created_utc": 1617584725.0}
{"sub": "pytorch", "title": "Iterative Pruning: LeNet-300-100 - PyTorch", "selftext": "I am trying to implement iterative pruning  algorithm (as described in the research papers in [\\[1\\]](https://arxiv.org/abs/1506.02626), [\\[2\\]](https://arxiv.org/abs/1803.03635))  which is: train a model, prune p% of smallest weights per layer,  re-train the pruned model and repeat. For experiment purposes, I  am  using LeNet-300-100 neural network on MNIST.\n\nThe code can be accessed [here](https://github.com/arjun-majumdar/Lottery_Ticket_Hypothesis-TensorFlow_2/blob/master/LeNet_300_100-Iterative_Pruning.ipynb)\n\nWithin  the function \u201ctrain\\_with\\_grad\\_freezing(model, epoch)\u201d, I am  using the  following lines of code for freezing the pruned weights by  making their  computed gradients equal to 0:\n\n    for layer_name, param in model.named_parameters():\n        if 'weight' in layer_name:\n            tensor = param.data.cpu().numpy()\n            grad_tensor = param.grad.data.cpu().numpy()\n            grad_tensor = np.where(tensor == 0, 0, grad_tensor)\n            param.grad.data = torch.from_numpy(grad_tensor).to(device)  \n\nThe first time I train the model, the code works fine after which I prune the layers by using the code:\n\n    # Prune 15% of smallest magnitude weights in FC layers and 10% in output layer- pruned_d = prune_lenet(model = best_model, pruning_params_fc = 15, pruning_params_op = 10) \n    # Initialize and load pruned Python3 dict into a new model- pruned_model = LeNet300() pruned_model.load_state_dict(pruned_d)  \n\nHowever, on re-training this pruned model, the training metric is stuck for these values:\n\n&gt;training loss = 0.0285, training accuracy = 99.04%, val\\_loss = 0.0910 &amp; val\\_accuracy = 97.68%\n\nWhat\u2019s going wrong?", "upvote_ratio": 1.0, "id": "t3_miz3p6", "created_utc": 1617415553.0}
{"sub": "pytorch", "title": "Exception: process 0 terminated with exit code 1 when use torch.multiprocessing.spawn on GPUs", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_miulhe", "created_utc": 1617400332.0}
{"sub": "pytorch", "title": "How to fix and find the source of the exception: Exception: process 1 terminated with exit code 1 for multiple GPU training with DDP and Pytorch", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_miul9q", "created_utc": 1617400315.0}
{"sub": "pytorch", "title": "tgt and src have to have equal features for a Transformer Network in Pytorch", "selftext": "I am attempting to train EEG data through a transformer network. The input dimensions are 50x16684x60 (seq x batch x features) and the output is 16684x2. Right now I am simply trying to run a basic transformer, and I keep getting an error telling me\n\n&amp;#x200B;\n\n\\`RuntimeError: the feature number of src and tgt must be equal to d\\_model\\`\n\n&amp;#x200B;\n\nWhy would the source and target feature number ever be equal? Is it possible to run such a dataset through a transformer?\n\n&amp;#x200B;\n\nHere is my basic model:\n\n&amp;#x200B;\n\n`input_size = 60 # seq x batch x features`\n\n`hidden_size = 32`\n\n`num_classes = 2`\n\n`learning_rate = 0.001`\n\n`batch_size = 64`\n\n`num_epochs = 2`\n\n`sequence_length = 50`\n\n`num_layers = 2`\n\n`dropout = 0.5`\n\n&amp;#x200B;\n\n`class Transformer(nn.Module):`\n\n`def __init__(self, input_size, hidden_size, num_layers, num_classes):`\n\n`super(Transformer, self).__init__()`\n\n`self.hidden_size = hidden_size`\n\n`self.num_layers = num_layers`\n\n`self.transformer = nn.Transformer(60, 2)`\n\n`self.fc = nn.Linear(hidden_size * sequence_length, num_classes)`\n\n`def forward(self, x, y):`\n\n`# Forward Propogation`\n\n`out, _ = self.transformer(x,y)`\n\n`out = out.reshape(out.shape[0], -1)`\n\n`out = self.fc(out)`\n\n`return out`\n\n&amp;#x200B;\n\n`model = Transformer(input_size, hidden_size, num_layers, num_classes)`\n\n&amp;#x200B;\n\n`criterion = nn.MSELoss()`\n\n`optimizer = optim.Adam(model.parameters(), lr=learning_rate)`\n\n&amp;#x200B;\n\n`for epoch in range(num_epochs):`\n\n`for index in tqdm(range(16684)):`\n\n`X, y = (X_train[index], Y_train[index])`\n\n`print(X.shape, y.shape)`\n\n`output = model(X, y)`\n\n&amp;#x200B;\n\n`loss = criterion(output, y)`\n\n`model.zero_grad()`\n\n`loss.backward()`\n\n`optimizer.step()`\n\n`if index % 500 == 0:`\n\n`print(f\"Epoch {epoch}, Batch: {index}, Loss: {loss}\")`", "upvote_ratio": 1.0, "id": "t3_mis588", "created_utc": 1617393208.0}
{"sub": "pytorch", "title": "Introducing PyTorch Profiler \u2013 The New And Improved Performance Debugging Profiler For PyTorch", "selftext": "The analysis and refinement of the large-scale deep learning model\u2019s performance is a constant challenge that increases in importance with the model\u2019s size. Owing to a lack of available resources, PyTorch users had a hard time overcoming this problem. There were common GPU hardware-level debugging tools, but PyTorch-specific background of operations was not available. Users had to merge multi-tools or apply minimal correlation information manually to make sense of the data to retrieve the missing information.\n\nThe PyTorch Profiler came to the rescue, an open-source tool for precise, efficient, and troubleshooting performance investigations of large-scale deep learning models.\u00a0\n\nSummary: [https://www.marktechpost.com/2021/04/02/introducing-pytorch-profiler-the-new-and-improved-performance-debugging-profiler-for-pytorch/](https://www.marktechpost.com/2021/04/02/introducing-pytorch-profiler-the-new-and-improved-performance-debugging-profiler-for-pytorch/)\n\nSource: https://pytorch.org/blog/introducing-pytorch-profiler-the-new-and-improved-performance-tool/", "upvote_ratio": 1.0, "id": "t3_minp5n", "created_utc": 1617380486.0}
{"sub": "pytorch", "title": "DQN loss from only one output element", "selftext": "I'm trying to implement a simple DQN. And I wonder if I have understood it correctly that it's fine to apply the loss function to only the difference between the (scalar) target and just one element of the output, something like this:\n\n&amp;#x200B;\n\n        def fn_reinforce(self,batch): # (state, action, reward, next_state)\n            for i in range(self.batch_size):\n                if batch[i].next_state is None:\n                    Q_target = batch[i].reward\n                    Q_predict = self.policy_net(batch[i].state)[0,batch[i].action]\n                    loss = self.loss_fn(Q_predict, Q_target)\n                else:\n                    with torch.no_grad():\n                        next_Q = torch.max(self.target_net(batch[i].next_state))\n                    Q_target = batch[i].reward + next_Q\n                    Q_predict = self.policy_net(batch[i].state)[0, batch[i].action]\n                    loss = self.loss_fn(Q_predict, Q_target)\n                self.optimizer.zero_grad()\n                loss.backward()\n                for param in self.policy_net.parameters():\n                    param.grad.data.clamp_(-1, 1)\n                self.optimizer.step()", "upvote_ratio": 1.0, "id": "t3_mie3kq", "created_utc": 1617342880.0}
{"sub": "pytorch", "title": "how can i create multiple 3d meshes and textures of different items/objects from an image using pytorch3d ?", "selftext": "what would be the methods / techniques used for this task ?", "upvote_ratio": 0.5, "id": "t3_mi08rx", "created_utc": 1617297808.0}
{"sub": "pytorch", "title": "Why are some tutorials in Github repo missing from pytorch.org/tutorials?", "selftext": " \n\nI tried to skim over [pytorch.org](https://pytorch.org/)'s tutorials to find some tutorials I needed.\n\nI discovered that some tutorials like [https://pytorch.org/tutorials/beginner/basics/data\\_tutorial.html](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) is not included in [https://pytorch.org/tutorials/](https://pytorch.org/tutorials/).\n\nHowever, it is reachable in tutorial's github, [https://github.com/pytorch/tutorials/tree/master/beginner\\_source/basics](https://github.com/pytorch/tutorials/tree/master/beginner_source/basics)\n\nWhy is the website's tutorial not synced properly with Github's tutorial page? Thought it was automated internally to reflect the repo's tutorial to the website.\n\nShould I stick to Github repo to access all tutorials without missing out some tutorials?", "upvote_ratio": 1.0, "id": "t3_mhzy8l", "created_utc": 1617297009.0}
{"sub": "pytorch", "title": "Why are some PyTorch missing from pytorch.org/tutorials ?", "selftext": "I tried to skim over [pytorch.org](https://pytorch.org)'s tutorials to find some tutorials I needed. \n\nI discovered that some tutorials like [https://pytorch.org/tutorials/beginner/basics/data\\_tutorial.html](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) is not included in [https://pytorch.org/tutorials/](https://pytorch.org/tutorials/). \n\n&amp;#x200B;\n\nHowever, it is reachable in tutorial's github, [https://github.com/pytorch/tutorials/tree/master/beginner\\_source/basics](https://github.com/pytorch/tutorials/tree/master/beginner_source/basics)\n\n&amp;#x200B;\n\nWhy is the website's tutorial not synced properly with Github's tutorial page? Thought it was automated internally to reflect the repo's tutorial to the website. \n\nShould I stick to Github repo to access all tutorials without missing out some tutorials?", "upvote_ratio": 0.92, "id": "t3_mhqolu", "created_utc": 1617264898.0}
{"sub": "pytorch", "title": "What is the difference between these two Net class declarations?", "selftext": "    class Net(nn.Module):\n        def __init__(self):\n            super(Net, self).__init__()\n\nand \n\n    class Net(nn.Module):  \n        def __init__(self):\n            super().__init__()\n\nfunctionally what is the difference between these two?", "upvote_ratio": 1.0, "id": "t3_mhcgu4", "created_utc": 1617215461.0}
{"sub": "pytorch", "title": "Help understanding how to implement multiple loss functions", "selftext": "Hey guys, had a quick question that I was hoping someone here could help out with.\n\n&amp;#x200B;\n\nI was going to run some experiments comparing MC Dropout to Bayes by Backprop. My implementation is like this: Run some images through efficientNetb0 and then extract the output and push that output through a single dense 512 layer. Then at test time run 100 forwards passes for each image and average the results. For MC Dropout I think this is rather easy, just train the network normally (but don't turn off dropout at test time). For my Bayesian layers however, I'm not sure how to handle the loss (the variational free energy) for the Bayesian layer while also using Cross entropy to train the efficientNet model? How is it that you're supposed to do this? Do I just work out the variational free energy using the outputs and then get the cross entropy loss of the same outputs and add those two together?\n\n&amp;#x200B;\n\nAny help at all would be appreciated.", "upvote_ratio": 1.0, "id": "t3_mgn660", "created_utc": 1617130509.0}
{"sub": "pytorch", "title": "Hotword Detection", "selftext": "Hello, I'm currently learning Machine Learning and want to implement Hotword Detection with Pytorch. I watched countless of videos, spent hours searching the internet for somewhat useable information, but yeah. I don't understand much of it. Can someone provide me a brief guide (with resources, would really appreciate that, if possible) or explain it to me like to an idiot?\n\nI know that for time-series it's preferably to use a RNN, but how do I process the audio-data with Pytorch and train with that?", "upvote_ratio": 1.0, "id": "t3_mgjcri", "created_utc": 1617120185.0}
{"sub": "pytorch", "title": "[torchtext] Checking whether batches assign the right row number to the text (rows) in the dataset", "selftext": "Hi,\n\nWe are using the standard Fields, TabularDataset, and BucketIterator classes of torchtext for a proper LSTM implementation with pytorch. Our dataset is a dataframe where every row has a column named \"index\\_names\" with row numbers, \"text\" column with texts, and \"label\" column with the true class labels.\n\n&amp;#x200B;\n\nWe need to verify that, the output of BucketIterator, a set of batches of the dataset, has the correct row numbers associated with the text in the dataset, so we can interpret the final results properly. The code is below. How can we make sure every batch in, for example, `train_iter` has the correct row numbers per text (per row)?\n\n&amp;#x200B;\n\nWe appreciate any help and we'd be grateful!\n\n&amp;#x200B;\n\n`label_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)`\n\n`text_field = Field(tokenize='spacy', lower=True, include_lengths=True, batch_first=True)`\n\n`fields = [('index_names', label_field), ('text', text_field), ('label', label_field)]`\n\n&amp;#x200B;\n\n`# TabularDataset`\n\n`train, valid, test = TabularDataset.splits(path='./', train='train.csv', validation='valid.csv',` \n\n`test='test.csv', format='CSV', fields=fields, skip_header=True)`\n\n&amp;#x200B;\n\n`# Iterators`\n\n`train_iter = BucketIterator(train, batch_size=32, sort_key=lambda x: len(x.text),`\n\n`device=device, sort=True, sort_within_batch=True)`\n\n`valid_iter = BucketIterator(valid, batch_size=32, sort_key=lambda x: len(x.text),`\n\n`device=device, sort=True, sort_within_batch=True)`\n\n`test_iter = BucketIterator(test, batch_size=32, sort_key=lambda x: len(x.text),`\n\n`device=device, sort=True, sort_within_batch=True)`", "upvote_ratio": 1.0, "id": "t3_mfpyxb", "created_utc": 1617025774.0}
{"sub": "pytorch", "title": "DistributedDataParallel (DDP) Examples", "selftext": "I'm looking for good tutorials on DDP. I usually pride myself on being able to figure things out on my own pretty well, but I've been banging my head against the wall on this one. I've used DataParallel before (which is really easy to use), but I wanted to train on multiple nodes, so I'm trying to learn DDP. The documentation leaves a lot to be desired and every online tutorial I find conflicts with other ones and many seem outdated. Does anyone have suggestions? I have it working on a single node multi-gpu setup, but I run into issues when I try multi-node. It's frustrating that I can't seem to find a single resource that shows an example of torch code that trains on a single gpu, modified to use DDP to train on a multi-node multi-gpu setup, to see exactly what needs to be changed.\n\n(Cross posted from [r/deeplearning](https://www.reddit.com/r/deeplearning/) as no one had any suggestions over there - maybe I'm not alone in my confusion?)", "upvote_ratio": 0.86, "id": "t3_mfpc6q", "created_utc": 1617023723.0}
{"sub": "pytorch", "title": "[HELP] Confusion regarding last layers of Fully Connected Network", "selftext": "Hello All. I am a beginner in Deep Learning world and have recently studied the concepts. I was trying to build a simple NN on MNIST data. Trying to apply the knowledge. Here goes the flow:\n\n* Data Type - 60000 rows; Each row consists: 28 * 28 i.e., 784 columns\n\tSo, input shape: (60000, 784)\n* Label - (60000, )\n* NN - Simple 3 layered Fully Connected Network.\n* Loss function: CrossEntropyLoss\n* Optimizer: SGD.\n* Output Layer Neurons: 10 (for each digit) via Softmax.\n* Custom Dataset initialized, fed to DataLoader.\n\nNow my understanding is that, I pass on the epoch value, the model trains and returns the output tensor with probabilities and the cross entropy calculates the loss by comparing prediction and ground truth, with optimizer updating gradiants after backpropogation.\n\n**HERE IS WHERE MY CONFUSION BEGINS.**\n\nI see in **PyTorch** people using:\n`_ , prediction  = torch.max(NNModel, 1)`\n\nto get the prediction value. Now what this essentially does is return the index at which highest probability is. E.g., `prediction = 4`.\nThis makes me wonder, whether feeding the whole data to NN, will the output tensors be trained in such a way that:\n\n`1st Neuron is for label 1.`.    \n`2nd Neuron is for label 2.`.  \n`And so on up to 9.???? `.    \n\nHow can we compare index received with the ground truth if its not the case and the numbers (in this example) are organised in random order across output layer Neurons???\n\n\nIt might sound stupid but I am really planning to get some practical knowledge and few of these basics are haunting me due to less experience in this field.\n\nAny help would be LARGELY appreciated.\n\nThanks.. :)", "upvote_ratio": 0.84, "id": "t3_meyewz", "created_utc": 1616924843.0}
{"sub": "pytorch", "title": "GPU comparison &amp; impact", "selftext": "I am planning on buying a laptop and I have two GPU options, viz. RTX 3070 Vs 3080.\n\nHow much of a difference is there from the point of view of Deep Learning training between these two?", "upvote_ratio": 0.5, "id": "t3_meszfc", "created_utc": 1616899964.0}
{"sub": "pytorch", "title": "App that lets you search docs of PyTorch, NumPy, Python, and Stack Overflow at one place", "selftext": "nan", "upvote_ratio": 0.97, "id": "t3_meknmu", "created_utc": 1616873021.0}
{"sub": "pytorch", "title": "PyTorch Geometric Temporal 0.24.", "selftext": "[https://github.com/benedekrozemberczki/pytorch\\_geometric\\_temporal](https://github.com/benedekrozemberczki/pytorch_geometric_temporal)\n\nThe new release has 2 new attention based models:\n\nMTGNN from Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks.\n\nGMAN from A Graph Multi-Attention Network for Traffic Prediction\n\nWe also added a large windmill output forecasting dataset.", "upvote_ratio": 0.95, "id": "t3_me1fln", "created_utc": 1616801637.0}
{"sub": "pytorch", "title": "Why does my pytorch distributed training (DDP) code send a SIGKILL signal on it's own?", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_mdsljr", "created_utc": 1616776493.0}
{"sub": "pytorch", "title": "Design Pattern of Pytorch based machine learning code.", "selftext": "Hello,\n\nI am looking for articles or papers on the design patterns of Pytorch based ML programmes and also patterns for the design of Pytorch framework itself. Any pointer or suggested reading will be appreciated.\n\n&amp;#x200B;\n\nThanks!", "upvote_ratio": 0.9, "id": "t3_mdjinz", "created_utc": 1616745123.0}
{"sub": "pytorch", "title": "Converting PyTorch and TensorFlow Models into Apple Core ML using CoreMLTools", "selftext": "nan", "upvote_ratio": 0.67, "id": "t3_mdjczc", "created_utc": 1616744443.0}
{"sub": "pytorch", "title": "Two Layer Network", "selftext": " I am trying to compute the forward pass of a two layer Network but I keep getting this error message. RuntimeError: 1D tensors expected, but got 2D and 2D tensors How do I resolve this issue ?\n\n`def nn_loss_part1(params, X, y=None, reg=0.0):`\n\n\n\n`W1, b1 = params['W1'], params['b1']`\n\n`W2, b2 = params['W2'], params['b2']`\n\n`N, D = X.shape`\n\n\n\n`hidden = None`\n\n`scores = None`\n\n\n\n`forwardPass =` [`torch.dot`](https://torch.dot)`(X,W1) + b1`\n\n`h = torch.maximum(forwardPass , 0)`\n\n`scores =` [`torch.dot`](https://torch.dot)`(h, W2) + b2`\n\n\n\n\n\n`return scores, hidden`\n\n\n\n`toy_X, toy_y, params = get_toy_data()`\n\n\n\n`scores, _ = nn_loss_part1(params, toy_X)`\n\n`print('Your scores:')`\n\n`print(scores)`\n\n`print()`\n\n`print('correct scores:')`\n\n`correct_scores = torch.tensor([`\n\n`[-3.8160e-07,  1.9975e-07,  1.0911e-07],`\n\n`[-5.0228e-08,  1.2784e-07, -5.2746e-08],`\n\n`[-5.9560e-07,  9.1178e-07,  1.1879e-06],`\n\n`[-3.2737e-08,  1.8820e-07, -2.8079e-07],`\n\n`[-1.9523e-07,  2.0502e-07, -6.0692e-08]], dtype=torch.float32, device=scores.device)`\n\n`print(correct_scores)`\n\n`print()`\n\n\n\n`We get &lt; 1e-10`\n\n`scores_diff = (scores - correct_scores).abs().sum().item()`\n\n`print('Difference between your scores and correct scores: %.2e' % scores_diff)`", "upvote_ratio": 1.0, "id": "t3_mdfxlw", "created_utc": 1616730115.0}
{"sub": "pytorch", "title": "how to debug pytorch c++ source", "selftext": "some methods in pytorch needs to call the function which written by c++? how could I debug with it, just like the python debugging process?   thanks", "upvote_ratio": 1.0, "id": "t3_mdeo8j", "created_utc": 1616725743.0}
{"sub": "pytorch", "title": "RNN: Prediction of words using pen points coordinates", "selftext": "Hello i'm trying to build a model that takes pen points coordinates (x,y) as an input and predicts the english word (5 letters or less) as an output.\n\nWhat is the best way to do this using LSTM or GRU ?\n\n&amp;#x200B;\n\nhttps://preview.redd.it/xx7lvs00o7p61.png?width=1222&amp;format=png&amp;auto=webp&amp;s=368768d764f6e829eca14f978f080098babed2e2", "upvote_ratio": 0.76, "id": "t3_md3ooq", "created_utc": 1616693517.0}
{"sub": "pytorch", "title": "the first time using torchvision ... got the following error, help please", "selftext": " **\\~\\\\anaconda3\\\\envs\\\\pytorch\\\\lib\\\\site-packages\\\\torchvision\\\\datasets\\\\utils.py** in \\_get\\_redirect\\_url**(url, max\\_hops)**      69      70 **def** \\_get\\_redirect\\_url**(**url**:** str**,** max\\_hops**:** int **=** **10)** **-&gt;** str**:** **---&gt; 71** **import** requests      72      73 **for** hop **in** range**(**max\\_hops **+** **1):** **ModuleNotFoundError**: No module named 'requests'", "upvote_ratio": 0.38, "id": "t3_mcm68e", "created_utc": 1616634880.0}
{"sub": "pytorch", "title": "Watch Episode 4 - PyTorch Lightning Community Talks", "selftext": "Watch Episode 4 of our Lightning #Community Talks Series with Aishwarya Srinivasan and Sachin Abeywardana, Sr. ML Engineer Canva. They discuss how Sachin uses #PyTorchLightning for training OpenAI's multilingual CLIP. [https://bit.ly/3vZFWBU](https://bit.ly/3vZFWBU) \\#deeplearning #NLP\n\nhttps://preview.redd.it/a62jxd4sv1p61.png?width=4444&amp;format=png&amp;auto=webp&amp;s=de9cf2b4619c2dede2dc9065f46d11b7e2d36ebc", "upvote_ratio": 1.0, "id": "t3_mci94d", "created_utc": 1616624081.0}
{"sub": "pytorch", "title": "From MIT CSAIL researchers! Create novel images using GANs! (checkout where they create a new face using faces of 4 different people)", "selftext": "nan", "upvote_ratio": 0.75, "id": "t3_mcf10p", "created_utc": 1616615839.0}
{"sub": "pytorch", "title": "Guide To Catalyst - A PyTorch Framework For Accelerated Deep Learning - Analytics India Magazine", "selftext": "nan", "upvote_ratio": 0.75, "id": "t3_mbdul9", "created_utc": 1616503292.0}
{"sub": "pytorch", "title": "training on partially annotated images", "selftext": "Before I write a dataloader from the ground up..\n\nmy scenario is:\n\n* a bunch of images,\n* each having a (likely) incomplete label list (drawn from about 2000 labels - some map to simplifications of eachother e.g. an image might say 'animal' or 'pet dog', there's a tree which can infer that the label 'pet dog' should also activate the outputs for 'dog' and 'animal')\n* and some of those labels have bounding box and polygonal annotations\n\nI'm thinking it should be possible to have one core net with an output for the whole label list (trained on all images including ones with no annotations), and then train something pixel level for actual annotations\n\nI note there's \"fully convolutional nets\" for using the exact same features for pixel and image level annotations , but they're not as good as dedicated conv/deconv ones (but if thats the only way to utilise such incomplete data, i'll go with it)\n\n&amp;#x200B;\n\nWhat about suppressing errors for un-annotaed pixels, in the \"partially annotated\" case (the vast majority) (absence of an anotation really means \"we dont know what this pixel is\", rather than the absence of the annotated categories)\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nIs there a common well maintaned existing dataloader that handles this scenario (i'd imagine it's common) ? \"ImageFolder\" is insufficient for this because there's multiple labels for most images. tbh i'd prefer to filter out the single label images.. they're broad scenes rarely focused on a single object\n\nI'm not adverse to writing it myself otherwise.", "upvote_ratio": 1.0, "id": "t3_mb4tvd", "created_utc": 1616467836.0}
{"sub": "pytorch", "title": "Complete Guide to PyKeen: Python KnowlEdge EmbeddiNgs for Knowledge Graphs", "selftext": "Pykeen is a python package that generates [knowledge graph](https://analyticsindiamag.com/knowledge-graphs-are-the-reason-why-you-see-mona-lisa-when-you-google-da-vinci/) embeddings while abstracting away the training loop and evaluation. The knowledge graph embeddings obtained using pykeen are reproducible, and they convey precise semantics in the knowledge graph.\n\nRead more: [https://analyticsindiamag.com/complete-guide-to-pykeen-python-knowledge-embeddings-for-knowledge-graphs/](https://analyticsindiamag.com/complete-guide-to-pykeen-python-knowledge-embeddings-for-knowledge-graphs/)", "upvote_ratio": 0.92, "id": "t3_mag9sc", "created_utc": 1616392425.0}
{"sub": "pytorch", "title": "[Questions] Implement custom optimizer", "selftext": "Hello community, I want to change the weight update iteration in the SGD optimizer, and I found a code like :\n\n    lr = 0.001\n    for param in model.parameters(): \n        weight_update = &lt;&lt; something &gt;&gt; \n        param.data.sub_(lr*weight_update)\n        \n    optimizer = torch.optim.SGD(model.parameters(),lr=lr)\n\nSo instead of updating the weight by the derivative of the loss respect to the weights, I want to customize this term as it is shown  like this.\n\n`W&lt;-- W - lr*weight_update`\n\nThe code runs, but the weights are not updating during the training.\n\n&amp;#x200B;\n\nAny suggestion ?", "upvote_ratio": 1.0, "id": "t3_ma62hw", "created_utc": 1616360510.0}
{"sub": "pytorch", "title": "CNN not updating between epochs", "selftext": "Hi,\n\nI am currently implementing a CNN for text sentiment analysis, but for some reason, the model is not updating between epochs. Any advice would be great! Please see code below:\n\n&amp;#x200B;\n\nArchitecture:\n\n&amp;#x200B;\n\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    \n    class CNN(nn.Module):\n        def __init__(self):\n            super(CNN, self).__init__()\n            self.conv1 = nn.Conv1d(in_channels = 1, out_channels = 1, kernel_size = 20, stride = 2) # 300 in, 141 out\n    \n            self.conv2 = nn.Conv1d(in_channels = 1, out_channels = 1, kernel_size = 10, stride = 2, padding = 1) # 141 in, 67 out\n    \n            self.conv3 = nn.Conv1d(in_channels = 1, out_channels = 1, kernel_size = 7, stride = 2) # 67 in, 31 out\n    \n            self.fc1 = nn.Linear(31, 10)\n            self.fc2 = nn.Linear(10, 3)\n        \n        def forward(self, x):\n            \n            number_instances = x.size()[0] # Store size of x\n    \n            x = F.relu(self.conv1(x))\n            x = F.relu(self.conv2(x))\n            x = F.relu(self.conv3(x))\n    \n            x = x.view(-1, number_instances, 31) # Flatten Layer\n            x = torch.squeeze(x) # Convert to correct dimensions\n    \n            x = F.relu(self.fc1(x))\n            x = F.softmax(self.fc2(x), dim = 1)\n            return x\n        \n    convolutional_model = CNN()\n    \n    print(convolutional_model)\n\nData and set up:\n\n    device = torch.device(\"cpu\") # use cpu\n    x_train = torch.from_numpy(np.asarray(document_embeddings_train)).float()\n    x_train = x_train.unsqueeze(1).to(device) # Add dimension \n    \n    y_train = torch.from_numpy(np.asarray(labels_encoded_train)).long().to(device)\n    \n    x_test = torch.from_numpy(np.asarray(document_embeddings_test)).float()\n    x_test = x_test.unsqueeze(1).to(device)\n    \n    y_test = torch.from_numpy(np.asarray(labels_encoded_test)).long().to(device)\n    \n    optimizer = torch.optim.Adam(convolutional_model.parameters(), lr=0.001) \n    loss_fn = nn.CrossEntropyLoss() \n    convolutional_model = convolutional_model.to(device)\n    loss_fn = loss_fn.to(device)\n\nTraining:\n\n    EPOCHS = 5 # Train model for 1000 epochs\n    \n    loss_list = np.zeros((EPOCHS,)) # Initialise variable to store loss for each epoch\n    accuracy_list = np.zeros((EPOCHS,)) # Initialise variable to store the test accuracy \n    \n    for epoch in tqdm.trange(EPOCHS):\n        \n        y_pred = convolutional_model(x_train) # Create model using training data\n        \n        loss = loss_fn(y_pred, y_train) # Compute loss on training data\n        \n        loss_list[epoch] = loss.item() # Save loss to list\n    \n        optimizer.zero_grad() # Zero gradients\n        loss.backward() # Use backpropagation to update weights\n        optimizer.step()\n    \n        with torch.no_grad():\n            total_correct = 0\n            \n            predicted_test_labels = convolutional_model(x_test) # Test model \n            predicted_test_labels_list = predicted_test_labels.tolist()\n            actual_test_labels_list = y_test.tolist()\n        \n            for i in range(len(predicted_test_labels_list)):\n                \n                max_prob = max(predicted_test_labels_list[i])\n                predicted_label = predicted_test_labels_list[i].index(max_prob)\n                actual_label = actual_test_labels_list[i]\n                \n                if predicted_label == actual_label:\n                    total_correct += 1\n                            \n            percentage_correct = total_correct/len(actual_test_labels_list)\n            accuracy_list[epoch] = percentage_correct # Save accuracy to list\n\nThis is returning the same accuracy for each epoch and I can't work out why, thanks in advance for any advice!", "upvote_ratio": 1.0, "id": "t3_m9u4wo", "created_utc": 1616323532.0}
{"sub": "pytorch", "title": "Manually assign weights using PyTorch", "selftext": "I am using Python 3.8 and PyTorch 1.7 to manually assign and change the weights and biases for a neural network. As an example, I have defined a LeNet-300-100 fully-connected neural network to train on MNIST dataset. The code for class definition is:\n\n        class LeNet300(nn.Module):\n            def __init__(self):\n                super(LeNet300, self).__init__()\n                \n                # Define layers-\n                self.fc1 = nn.Linear(in_features = input_size, out_features = 300)\n                self.fc2 = nn.Linear(in_features = 300, out_features = 100)\n                self.output = nn.Linear(in_features = 100, out_features = 10)\n                self.weights_initialization() \n    \n            def forward(self, x):\n                out = F.relu(self.fc1(x))\n                out = F.relu(self.fc2(out))\n                return self.output(out)      \n      \n            def weights_initialization(self):\n                '''\n                When we define all the modules such as the layers in '__init__()'\n                method above, these are all stored in 'self.modules()'.\n                We go through each module one by one. This is the entire network,\n                basically.\n                '''\n                for m in self.modules():\n                    if isinstance(m, nn.Linear):\n                        nn.init.xavier_normal_(m.weight)\n                        nn.init.constant_(m.bias, 0)\n\nTo experiment with trying to change the weights for this model-\n\n&amp;#x200B;\n\n        # Instantiate model-\n        mask_model = LeNet300()\n\nTo assign all of the weights in each of the layers to one (1), I use the code-\n\n        with torch.no_grad():\n            for layer in mask_model.state_dict():\n                mask_model.state_dict()[layer] = nn.parameter.Parameter(torch.ones_like(mask_model.state_dict()[layer]))\n    \n        # Sanity check-\n        mask_model.state_dict()['fc1.weight']\n\nThis output shows that the weights are not equal to 1.\n\nI also tried the code-\n\n        for param in mask_model.parameters():\n            # print(param.shape)\n            param = nn.parameter.Parameter(torch.ones_like(param))\n\nBut this does not work as well.\n\n&amp;#x200B;\n\nHelp?", "upvote_ratio": 1.0, "id": "t3_m9azt9", "created_utc": 1616259269.0}
{"sub": "pytorch", "title": "Guide To Kornia: An OpenCV-inspired PyTorch Framework - Analytics India Magazine", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_m97mpb", "created_utc": 1616248910.0}
{"sub": "pytorch", "title": "SIREN implemented from scratch", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_m94ds5", "created_utc": 1616236260.0}
{"sub": "pytorch", "title": "Model parameters weights and bias", "selftext": "Hi all, \n\nI want to print my weights and bias for my model.   \nAnd I plan to make some statistics on it and change it a bit and put it back.   \nIs it possible to change weights and bias in PyTorch?", "upvote_ratio": 0.33, "id": "t3_m8jjf1", "created_utc": 1616167175.0}
{"sub": "pytorch", "title": "Feed Forward NN Loss is calculating NaN", "selftext": "Hi,\n\nI'm trying to create a simple feed forward NN, but when computing the loss it is returning NaN. Any advice?\n\nHere is my architecture:\n\n&amp;#x200B;\n\n    class Net(nn.Module):\n        def __init__(self,k):\n            super(Net, self).__init__()\n            self.fc1 = nn.Linear(k, 5) # 1st hidden layer takes an input of size k\n            self.fc2 = nn.Linear(5, 3) # Output layer has a size of 3 neurons\n        \n        def forward(self, x):\n            x = F.relu(self.fc1(x)) # ReLu activation for 1st hidden layer\n            x = F.softmax(self.fc2(x), dim=1) # Softmax activation for output layer\n            return x\n    \n    model = Net(300) # Create model for an embedding of size k=300\n\nHere is my optimizer and loss fn:\n\n&amp;#x200B;\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001) \n    loss_fn = nn.CrossEntropyLoss() \n\nI was running a check over a single epoch to see what was happening and this is what happened:\n\n    y_pred = model(x_train) # Create model using training data\n    loss = loss_fn(y_pred, y_train) # Compute loss on training data\n    \n    print(y_pred)\n    print(y_train)\n    print(loss_fn(y_pred, y_train))\n\nThe printing returns the following output:\n\n    tensor([[0.3597, 0.2954, 0.3449],         [0.3615, 0.2955, 0.3430],         [0.3600, 0.2954, 0.3446],         ...,         [0.3590, 0.2953, 0.3457],         [0.3603, 0.2955, 0.3442],         [0.3605, 0.2955, 0.3441]], grad_fn=&lt;SoftmaxBackward&gt;) \n    tensor([0, 0, 0,  ..., 0, 2, 0]) \n    tensor(nan, grad_fn=&lt;NllLossBackward&gt;)\n\nMy Tensors are of length 300 as I am passing in word embeddings with 300 dimensions.\n\nThanks in advance\n\n&amp;#x200B;\n\nEdit: Here is the code for one specific predicition vs actual:\n\n&amp;#x200B;\n\n    in:\n    \n    print(y_pred[0])\n    print(y_train[0])\n    print(loss_fn(y_pred, y_train))\n    print(loss.item())\n    \n    out:\n    \n    tensor([0.3079, 0.1110, 0.2661], grad_fn=&lt;SelectBackward&gt;) \n    tensor(0) \n    tensor(nan, grad_fn=&lt;NllLossBackward&gt;) \n    nan\n\nEdit 2 - SOLVED:\n\n&amp;#x200B;\n\nI have sorted out the nan issue by realising some of my embeddings were non-existent due to the preprocessing of the text. Thanks for the help!", "upvote_ratio": 1.0, "id": "t3_m8efj2", "created_utc": 1616150285.0}
{"sub": "pytorch", "title": "Pytorch rans out of gpu memory when model iteratively called.", "selftext": "Hey Guys,\n\nI'm using sentence Bert to encode sentences from thousands of files. The model easily fits in gpu, and in each iteration, I load a text sentences, tokenize (return\\_type=\"pt\"), and feed that into the model. I repeat this process for each file, so theoretically, if the model runs for one input it must be able to run without any additional gpu memory requirement for all samples. However, after 5% of the samples are processed, I get out of memory error \"RuntimeError: CUDA out of memory. Tried to allocate 2.61 GiB (GPU 0; 15.78 GiB total capacity; 5.23 GiB already allocated; 1004.75 MiB free; 13.37 GiB reserved in total by PyTorch)\" . \n\nIs this a pytorch bug? anyone has faced it before? any workarounds?\n\n&amp;#x200B;\n\nTHanks", "upvote_ratio": 0.5, "id": "t3_m7v3zo", "created_utc": 1616086101.0}
{"sub": "pytorch", "title": "Hands-On Guide to Torch-Points3D: A Modular Deep Learning Framework for 3D Data", "selftext": "There has been a surge of advancements in automated analysis of [3D data ](https://analyticsindiamag.com/application-of-data-science-on-3d-imagery-data/)caused by affordable LiDAR sensors, more efficient photogrammetry algorithms, and new neural network architectures. So much that the number of papers related to 3D data being presented at vision conferences is now on par with images, although this rapid methodological development is beneficial to the young field of deep learning for 3D, with its fast pace come several shortcomings:\u00a0\n\nRead more: [https://analyticsindiamag.com/hands-on-guide-to-torch-points3d-a-modular-deep-learning-framework-for-3d-data/](https://analyticsindiamag.com/hands-on-guide-to-torch-points3d-a-modular-deep-learning-framework-for-3d-data/)", "upvote_ratio": 1.0, "id": "t3_m7p7oy", "created_utc": 1616068736.0}
{"sub": "pytorch", "title": "Installing Pytorch with ROCm but checking if CUDA enabled ? How can I know if I am running on the AMD GPU?", "selftext": "Hi there,\n\nThis is my first time using Pytorch. I am installing it while trying to use an AMD GPU. My understanding is that I can use the new ROCm platform (I am aware that is in beta) to use Pytorch.\n\n&amp;#x200B;\n\nHow can I check that what I am running is running in the GPU?. I know for CUDA enabled GPUS I can just print torch\\*\\*.**cuda**.\\*\\*is\\_available(), but how about while using ROCm?.\n\n&amp;#x200B;\n\nMaybe I am missing something but they do not provide instructions on how to check on their official website for ROCm. \n\n[https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/)", "upvote_ratio": 0.79, "id": "t3_m7jjbu", "created_utc": 1616043585.0}
{"sub": "pytorch", "title": "Pytorch C++ and generating CMakeLists.txt for project - can't find &lt;torch/torch.h&gt;", "selftext": "I have a file structure like this:\n\nproj/\n    includes/\n        foo.h\n    src/\n        foo.cpp\n    main.cpp\n\nHere is my CMakeLists.txt for main.cpp: \n\n        cmake_minimum_required(VERSION 3.18)\n        project(proj)\n\n        set(Torch_DIR /path/to/cmake/Torch)\n        find_package(Torch REQUIRED)\n        \n        include_directories(${PROJECT_SOURCE_DIR}/includes ${TORCH_INCLUDE_DIR})\n        add_subdirectory(includes)\n        add_subdirectory(src)\n\n        set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} ${TORCH_CXX_FLAGS}\")\n\n        include_directories(includes) \n        include_directories(${OpenCV_INCLUDE_DIRS})\n        include_directories(${TORCH_INCLUDE_DIR})\n\n        add_executable(proj main.cpp)\n        target_link_libraries(seg foo_lib ${OpenCV_LIBS} \"${TORCH_LIBRARIES}\")\n        set_property(TARGET proj PROPERTY CXX_STANDARD 14)\n\nHere is my CMakeLists.txt for includes:\n\n        set(Torch_DIR /path/to/cmake/Torch)\n        find_package(Torch REQUIRED)\n        include_directories(${PROJECT_SOURCE_DIR}/includes ${TORCH_INCLUDE_DIR})\n\nHere is my CMakeLists.txt for src: \n\n        include_directories(${PROJECT_SOURCE_DIR}/includes)\n        add_library(foo_lib foo.cpp)\n\nUpon executing make I get: \n\n        fatal error: torch/torch.h: No such file or directory\n        1 | #include &lt;torch/torch.h&gt;\n\nBut it can link in main.cpp. So obviously my CMakeLists are not created correctly, so any advice on how to create these properly would be appreciated.", "upvote_ratio": 0.33, "id": "t3_m7fn37", "created_utc": 1616029940.0}
{"sub": "pytorch", "title": "gradient of embedding for padded tokens", "selftext": "Hey Guys,\n\n[Here](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) when explaining nn.Embedding, it says \"The gradient for this vector from Embedding is always zero.\", I assume once you train the weights, the vector at padding index is no longer going to be all zeros? Even after that the gradient still returns 0?", "upvote_ratio": 0.75, "id": "t3_m79gl0", "created_utc": 1616011952.0}
{"sub": "pytorch", "title": "HyperBand and BOHB: understanding hyperparameter optimization algorithms", "selftext": "If you want to learn about state-of-the-art hyperparameter optimization algorithms (HPO), in this article I\u2019ll tell you what they are and how they work.\n\nWe cover:\n- A bit about HPO Approaches \n- What is Bayesian Optimization, and why is this method effective? \n- How do state-of-the-art Hyperparameter Optimization algorithms work? \n- **Hyperband vs BOHB comparison**\n\n[HyperBand vs. BOHB](https://neptune.ai/blog/hyperband-and-bohb-understanding-state-of-the-art-hyperparameter-optimization-algorithms?utm_source=reddit&amp;utm_medium=post&amp;utm_campaign=blog-hyperband-and-bohb-understanding-state-of-the-art-hyperparameter-optimization-algorithms&amp;utm_content=pytorch)", "upvote_ratio": 1.0, "id": "t3_m658nh", "created_utc": 1615887477.0}
{"sub": "pytorch", "title": "What is Trax and How is it a Better Framework for Advanced Deep Learning?", "selftext": "nan", "upvote_ratio": 0.33, "id": "t3_m5iee1", "created_utc": 1615806818.0}
{"sub": "pytorch", "title": "Guide To PyTorch Metric Learning: A Library For Implementing Metric Learning Algorithms", "selftext": "nan", "upvote_ratio": 0.88, "id": "t3_m5hfcw", "created_utc": 1615802801.0}
{"sub": "pytorch", "title": "Object localization from scratch", "selftext": "I am reading and watching tutorials for performing object localization. Lately, I watched this [video](https://www.youtube.com/watch?v=GSwYGkTfOKk)  from Andrew Ng who proposes different loss functions for the target  label 'y'. Now, theoretically, I understand the underlying concepts,  however, can you point me to code examples implementing this from  scratch with an associated dataset? \n\nThanks!", "upvote_ratio": 0.81, "id": "t3_m5bsup", "created_utc": 1615779305.0}
{"sub": "pytorch", "title": "Fast weights transformer implementation/tutorial", "selftext": "We added an implementation of \u201cLinear Transformers Are Secretly Fast Weight Memory Systems\u201d to our collection of paper implementations with notes in PyTorch.\n\nAnnotated code: [https://nn.labml.ai/transformers/fast_weights/index.html](https://nn.labml.ai/transformers/fast_weights/index.html)\n\nThis paper by Imanol Schlag, Kazuki Irie and J\u00fcrgen Schmidhuber compares self attention to fast weight systems and introduces a new linear self attention update rule and a projection function.\n\nWe have also implemented a simple experiment to train the model on the Tiny Shakespeare dataset.\n\n* [Github repo](https://github.com/lab-ml/nn)\n* [Paper](https://arxiv.org/abs/2102.11174)\n* [Colab notebook](https://colab.research.google.com/github/lab-ml/nn/blob/master/labml_nn/transformers/fast_weights/experiment.ipynb)\n* [Training loss charts](https://app.labml.ai/run/928aadc0846c11eb85710242ac1c0002)", "upvote_ratio": 1.0, "id": "t3_m4tdni", "created_utc": 1615720388.0}
{"sub": "pytorch", "title": "Self-Attention Computer Vision - PyTorch Code - Analytics India Magazine", "selftext": "As discussed in [one of our articles](https://analyticsindiamag.com/going-beyond-cnn-stand-alone-self-attention/), Self-Attention is gradually gaining prominent place from sequence modeling in natural language processing to Medical Image Segmentation.  [https://analyticsindiamag.com/pytorch-code-for-self-attention-computer-vision/](https://analyticsindiamag.com/pytorch-code-for-self-attention-computer-vision/)", "upvote_ratio": 0.75, "id": "t3_m4sodo", "created_utc": 1615717120.0}
{"sub": "pytorch", "title": "Basic sequence prediction with attention/transformer in pytorch", "selftext": "So I've been working on this problem for a few days and just not making progress.\n\nMy goal is really simple: Use a transformer to predict future values of a Sine wave.\n\nI know that sounds trivial, but I can't get the code right. All the examples in the tutorials are using Transformers for NLP and have complicated embedding code.\n\nI haven't been able to reverse engineer a simple model that uses the Attention/Transformer network to predict a simple floating point time series.\n\nThis paper is what really got me investigating this technique for floating point time series: [https://arxiv.org/pdf/2001.08317.pdf](https://arxiv.org/pdf/2001.08317.pdf)\n\nHas anyone done this or do you know of any tutorials?", "upvote_ratio": 1.0, "id": "t3_m4p4r1", "created_utc": 1615700827.0}
{"sub": "pytorch", "title": "Looking for someone who can review my code", "selftext": "Hi,\n\nI am a self learner.\n\nI built a Seq2Seq model following some online tutorials.\n\nBut my model is giving some error during training.\n\nI am thus looking for a mentor who can review my code and help me in resolving the issue.\n\n\\-----------------------\n\nError:\n\nAttributeError                            Traceback (most recent call last)\n\n&amp;#x200B;\n\n&lt;ipython-input-63-472071541d41&gt; in &lt;module&gt;()\n\n8     start\\_time = time.time()\n\n9 \n\n\\---&gt; 10     train\\_loss = train(model, train\\_iterator, optimizer, criterion, CLIP)\n\n11     valid\\_loss = evaluate(model, valid\\_iterator, criterion)\n\n12 \n\n&amp;#x200B;\n\n6 frames\n\n&amp;#x200B;\n\n/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling\\_bert.py in forward(self, input\\_ids, attention\\_mask, token\\_type\\_ids, position\\_ids, head\\_mask, inputs\\_embeds, encoder\\_hidden\\_states, encoder\\_attention\\_mask, past\\_key\\_values, use\\_cache, output\\_attentions, output\\_hidden\\_states, return\\_dict)\n\n917             raise ValueError(\"You cannot specify both input\\_ids and inputs\\_embeds at the same time\")\n\n918         elif input\\_ids is not None:\n\n\\--&gt; 919             input\\_shape = input\\_ids.size()\n\n920             batch\\_size, seq\\_length = input\\_shape\n\n921         elif inputs\\_embeds is not None:\n\n&amp;#x200B;\n\nAttributeError: 'Field' object has no attribute 'size'\n\n\\---------------------\n\nMy code is available in this github repo for your review:\n\n[https://github.com/Ninja16180/BERT/blob/main/Training\\_Seq2Seq\\_Model\\_using\\_Pre-Trained\\_BERT\\_Model.ipynb](https://github.com/Ninja16180/BERT/blob/main/Training_Seq2Seq_Model_using_Pre-Trained_BERT_Model.ipynb)\n\n&amp;#x200B;\n\nAppreciate your help.\n\n&amp;#x200B;\n\nThanks in advance!", "upvote_ratio": 0.81, "id": "t3_m48njj", "created_utc": 1615649875.0}
{"sub": "pytorch", "title": "ResNet from scratch - ImageNet", "selftext": "Hey Guys, I have been experimenting with ResNet architectures. As of now I have coded 18 and 34 using Pytorch with CIFAR-10, however I would like to experiment training with ImageNet dataset. I read that the original dataset is around 400 GB (approx) which might need an AWS EC2 instance to compute.\n\nIs there any smaller version of it which I can read/explore? Haven't really found a good online resource(s) which talks about ImageNet specific data preparation.\n\nHelp?", "upvote_ratio": 1.0, "id": "t3_m3yo8b", "created_utc": 1615608238.0}
{"sub": "pytorch", "title": "How to access a class object when I use torch.nn.DataParallel()?", "selftext": "Hello,\n\nI want to train my model using PyTorch with multiple GPUs. I included the following line:\n\n`model = torch.nn.DataParallel(model, device_ids=opt.gpu_ids)`\n\nThen, I tried to access the optimizer that was defined in my model definition:\n\n`G_opt = model.module.optimizer_G`\n\nHowever, I got an error:\n\n&gt;AttributeError: 'DataParallel' object has no attribute optimizer\\_G\n\nI think it is related with the definition of optimizer in my model definition. It works when I use single GPU without \\`torch.nn.DataParallel\\`. But it does not work with multi GPUs even though I call with `module` and I could not find the solution.\n\nHere is the model definition:\n\n`class MyModel(torch.nn.Module):`\n\n`...`\n\n`self.optimizer_G = torch.optim.Adam(params,` [`lr=opt.lr`](https://lr=opt.lr)`, betas=(opt.beta1, 0.999))`   \n\nI used Pix2PixHD implementation in [GitHub](https://github.com/NVIDIA/pix2pixHD) if you want to see the full code.\n\nThank you,\n\nBest.", "upvote_ratio": 0.75, "id": "t3_m3sm6t", "created_utc": 1615587387.0}
{"sub": "pytorch", "title": "When does one divide by the meta_batch_size for MAML during meta-learning?", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_m3q01j", "created_utc": 1615579877.0}
{"sub": "pytorch", "title": "Pytorch Geometric on Google Colab", "selftext": "Hello all,\n\nI am trying to install Pytorch Geometric on google colab, but I keep running into errors. Does anybody have a notebook with a working installation that I could use? \n\nThank you", "upvote_ratio": 1.0, "id": "t3_m3auq7", "created_utc": 1615527345.0}
{"sub": "pytorch", "title": "Repeated inference causes slowdown?", "selftext": "I call a model on an input once, it takes 0.02s.\n\nI call the same model on the same input twice, back to back, it takes 0.04s, sensible so far.\n\nI call the same model in the same input five times, back to back to back to back to back, it now takes 0.9s, when it should take 0.1s.\n\nTen times, 5.7s, when it should take 0.2s.\n\nWhen I clear the cache between every run, the sum of the times for the runs goes down to something that could be expected, but clearing the cache takes a second each time or so.\n\nWhat the Hell is going on?", "upvote_ratio": 1.0, "id": "t3_m2vpmr", "created_utc": 1615483890.0}
{"sub": "pytorch", "title": "ResNet-18 vs ResNet-34", "selftext": "I have trained [ResNet-18](https://github.com/arjun-majumdar/CNN_Classifications/blob/master/ResNet-18_CIFAR10-PyTorch.ipynb) and [ResNet-34](https://github.com/arjun-majumdar/CNN_Classifications/blob/master/ResNet_34_CIFAR10_PyTorch.ipynb)   from scratch using PyTorch on CIFAR-10 dataset. The validation  accuracy  I get for ResNet-18 is 84.01%, whereas for ResNet-34 is  82.43%. Is this  a sign of ResNet-34 overfitting as compared to  ResNet-18? Ideally,  ResNet-34 should achieve a higher validation  accuracy as compared to  ResNet-18.\n\nThoughts?", "upvote_ratio": 1.0, "id": "t3_m1cftx", "created_utc": 1615313467.0}
{"sub": "pytorch", "title": "ResNet-18 from scratch", "selftext": "I have implemented ResNet-18 CNN from scatch in Python and PyTorch using CIFAR-10 dataset. You can see it [here](https://github.com/arjun-majumdar/CNN_Classifications/blob/master/ResNet-18_CIFAR10-PyTorch.ipynb).\n\nLet me know your comments/feedbacks.", "upvote_ratio": 0.79, "id": "t3_m12byo", "created_utc": 1615278994.0}
{"sub": "pytorch", "title": "Converting a model from Pytorch to Tensorflow: Guide to ONNX", "selftext": "Open Neural Network Exchange (ONNX) is a powerful and open format built to represent machine learning models. The final outcome of training any machine learning or deep learning algorithm is a model file that represents the mapping of input data to output predictions in an efficient manner.\n\nRead  more: [https://analyticsindiamag.com/converting-a-model-from-pytorch-to-tensorflow-guide-to-onnx/](https://analyticsindiamag.com/converting-a-model-from-pytorch-to-tensorflow-guide-to-onnx/)", "upvote_ratio": 0.82, "id": "t3_m0eby3", "created_utc": 1615205196.0}
{"sub": "pytorch", "title": "Is there a flexible Dataloader similar to tf.data.Datasets?", "selftext": "Hi,\n\nI'm considering a swap from TF 2.0 to PyTorch (because whole academia did so), but before doing such drastic actions I need to ensure that PyTorch can provide the same set of tools in some way.\n\nI have found most features in tf2.0 to have a equivalent in pytorch, but I have not found anything that comes close to tf.data.Datasets. Specifically, I am looking for something that can eagerly prefetch and automatically batch my dataset. Is there such library for pytorch?", "upvote_ratio": 1.0, "id": "t3_m0anh5", "created_utc": 1615189206.0}
{"sub": "pytorch", "title": "AI Show: What's new in Cognitive Search and PyTorch", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_lykjjs", "created_utc": 1614974522.0}
{"sub": "pytorch", "title": "Vision Transformer implemented from scratch", "selftext": "nan", "upvote_ratio": 0.85, "id": "t3_lykf5n", "created_utc": 1614974184.0}
{"sub": "pytorch", "title": "PyTorch 1.8 released", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_lya2n2", "created_utc": 1614944627.0}
{"sub": "pytorch", "title": "Local and Global loss", "selftext": "I have a requirement of training pipeline similar to Mixture of Experts (https://github.com/davidmrau/mixture-of-experts/blob/master/moe.py) but I want to train the Experts on a local loss for 1 epoch before predicting outputs from them (which would then be concatenated for the global loss of MoE). Can anyone suggest what\u2019s the best way to set up this training pipeline?", "upvote_ratio": 1.0, "id": "t3_ly1qee", "created_utc": 1614910974.0}
{"sub": "pytorch", "title": "[python package] Tensorguard helps to handle shapes of multidimensional tensors", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_lxwunu", "created_utc": 1614895907.0}
{"sub": "pytorch", "title": "Most efficient way to swap rows between 2D tensors?", "selftext": "This is probably an incredibly basic question, but I'm pretty new to torch and haven't come across a simple solution in the docs.\n\nI have two 2D tensors `tokenized_text` and `translated_words`, and I'd like to swap certain rows in one with rows from the other. The tensors aren't guaranteed to be the same dimensions. The end result is then flattened to 1D and the padding values (x for x &lt; 5) are removed.\n\nMy first quick-and-dirty attempt (in order to test the rest of my code) involved converting the two tensors to lists, swapping the rows, flattening the result and building a new tensor from that. But that strikes me as super inefficient.\n\nMy updated version keeps everything as tensors by padding them so they have the same row length. See here:\n\n        max_length = max(tokenized_text.size()[1], translated_words.size()[1])\n            \n        tokenized_text = torch.nn.ConstantPad1d((0, max_length - tokenized_text.size()[1]), tokenizer.pad_token_id)(tokenized_text)\n        translated_words = torch.nn.ConstantPad1d((0, max_length - translated_words.size()[1]), tokenizer.pad_token_id)(translated_words)\n            \n        for n, word_index in enumerate(word_indexes):\n            tokenized_text[word_index] = translated_words[n]\n            \n        # strip special chars\n        tokenized_text = tokenized_text[tokenized_text &gt; 5]\n\nJust wondering if there's a more efficient / more idiomatic way to accomplish this same task. Is there anything built in to the library?", "upvote_ratio": 1.0, "id": "t3_lxrlds", "created_utc": 1614882596.0}
{"sub": "pytorch", "title": "Pytorch Geometric or Pytorch DGL? Which one do you prefer?", "selftext": "nan", "upvote_ratio": 0.75, "id": "t3_lxndny", "created_utc": 1614872569.0}
{"sub": "pytorch", "title": "PyTorch Geometric Temporal: What Is it &amp; Your InDepth Guide -", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_lxk0wl", "created_utc": 1614863072.0}
{"sub": "pytorch", "title": "Low train accuracy using pretrained torchvision model", "selftext": "Hello,\n\nI am trying to evaluate a pre-trained mobilenetv2 model from torchvision on the ImageNet training dataset using the official example ([https://github.com/pytorch/examples/blob/master/imagenet/main.py](https://github.com/pytorch/examples/blob/master/imagenet/main.py)).\n\nTo do so, I modify lines 235-237 to perform validation on the train loader instead of the val loader:\n\n        if args.evaluate:\n            validate(train_loader, model, criterion, args)\n            return\n\nEverything else is left untouched. The command I use to run is:\n\n    python imagenet_train_example.py -a mobilenet_v2 -j 16 -b 1024 -e --pretrained /data/ImageNet\n\nHowever, the results are much lower than expected:\n\n&gt;Acc@1 2.926 Acc@5 15.079 Loss  11.795791\n\nI was wondering if anyone knows why that might be? Am I doing something wrong?\n\nCheers!", "upvote_ratio": 1.0, "id": "t3_lxjc07", "created_utc": 1614860717.0}
{"sub": "pytorch", "title": "How to save model in pytorch?", "selftext": "I am a web developer, I don't know much about ML. I have been given a model that I'm supposed to integrate into my website but first I need to save it. It already has a save\\_checkpoint method in the trainer file but I don't know how to use it. Please help.\n\n&amp;#x200B;\n\nI posted this on stackoverflow but nobody responded.\n\n[https://stackoverflow.com/questions/66449973/how-to-use-the-save-checkpoint-method](https://stackoverflow.com/questions/66449973/how-to-use-the-save-checkpoint-method)", "upvote_ratio": 0.67, "id": "t3_lxcouw", "created_utc": 1614832476.0}
{"sub": "pytorch", "title": "Why people alot use alot tensorflow instead of pytorch?", "selftext": "Even Knowing that Pytorch is so much flexible than tensorflow.", "upvote_ratio": 0.58, "id": "t3_lwy52r", "created_utc": 1614789464.0}
{"sub": "pytorch", "title": "Getting Started with Distributed Machine Learning with PyTorch and Ray", "selftext": "nan", "upvote_ratio": 0.94, "id": "t3_lwcpwp", "created_utc": 1614719700.0}
{"sub": "pytorch", "title": "How to fix a SIGSEGV in pytorch when using distributed training (e.g. DDP)?", "selftext": "nan", "upvote_ratio": 0.6, "id": "t3_lwbb72", "created_utc": 1614715855.0}
{"sub": "pytorch", "title": "How does one set the pytorch distributed hostname, port and GLOO_SOCKET_IFNAME so that DDP works?", "selftext": "nan", "upvote_ratio": 0.67, "id": "t3_lw1tkv", "created_utc": 1614691316.0}
{"sub": "pytorch", "title": "My partner made a shitty code in a jupyter and then exported it as .py, but it runs faster than my pytorch code.", "selftext": "Same data, same model\nTheir code: num_workers and pin memory has not been set. Gradients are not zeroed at before train (its grid search), gradients are calculated during validation. Just that all inputs are passed to the gpu. Has loads of redundant variables and print functions. Nothing related to threads specified.\n\nMy code: all of the above is done, but still the code runs slowly as compared to the their code. First I had a file system with all relevant code and functions segregated. Then I thought that might be taking up time, so I put it in one code, but it didn't change the time much.\n\nCan't exactly understand why this is happening. Any tips?", "upvote_ratio": 1.0, "id": "t3_lw08z0", "created_utc": 1614685890.0}
{"sub": "pytorch", "title": "Implementing FC layer as conv layer", "selftext": "Hey Guys, I wrote a sample code which implements [Fully Connected (FC) layer as Conv layer](https://github.com/arjun-majumdar/CNN_Classifications/blob/master/Implementing_FC_as_conv_layer.py) in PyTorch. Let me know your thoughts. This is going to be used for optimized \"Sliding Windows object detection\" algorithm.", "upvote_ratio": 0.5, "id": "t3_lvurr5", "created_utc": 1614662859.0}
{"sub": "pytorch", "title": "How does one set the pytorch distributed hostname, port and GLOO_SOCKET_IFNAME so that DDP works?", "selftext": "nan", "upvote_ratio": 0.67, "id": "t3_lvefcn", "created_utc": 1614618401.0}
{"sub": "pytorch", "title": "How does one setup the set_sharing_strategy strategy for multiprocessing in Pytorch?", "selftext": "nan", "upvote_ratio": 0.5, "id": "t3_lvdze1", "created_utc": 1614617340.0}
{"sub": "pytorch", "title": "C++ trainable semantic segmentation models", "selftext": "&amp;#x200B;\n\nhttps://preview.redd.it/xvdaj4vm98k61.png?width=1461&amp;format=png&amp;auto=webp&amp;s=67051689cf9252c8a6e1b5386e4c531685aa00be\n\nI wrote a [C++ trainable semantic segmentation open source project](https://github.com/AllentDan/SegmentationCpp) supporting UNet, FPN, PAN, LinkNet, DeepLabV3 and DeepLabV3+ architectures.\n\nThe main features of this library are:\n\n* High level API (just a line to create a neural network)\n* 6 models architectures for binary and multi class segmentation (including legendary Unet)\n* 7 available encoders\n* All encoders have pre-trained weights for faster and better convergence\n* 2x or more faster than pytorch cuda inferece, same speed for cpu. (Unet tested in gtx 2070s).\n\n## 1. Create your first Segmentation model with Libtorch Segment\n\nSegmentation model is just a LibTorch torch::nn::Module, which can be created as easy as:\n\n    #include \"Segmentor.h\"\n    auto model = UNet(1, /*num of classes*/\n                      \"resnet34\", /*encoder name, could be resnet50 or others*/\n                      \"path to resnet34.pt\"/*weight path pretrained on ImageNet, it is produced by torchscript*/\n                      );\n\n* see [table](#architectures) with available model architectures\n* see [table](#encoders) with available encoders and their corresponding weights\n\n## 2. Generate your own pretrained weights\n\nAll encoders have pretrained weights. Preparing your data the same way as during weights pre-training may give your better results (higher metric score and faster convergence). And you can also train only the decoder and segmentation head while freeze the backbone.\n\n    import torch\n    from torchvision import models\n    \n    # resnet50 for example\n    model = models.resnet50(pretrained=True)\n    model.eval()\n    var=torch.ones((1,3,224,224))\n    traced_script_module = torch.jit.trace(model, var)\n    traced_script_module.save(\"resnet50.pt\")\n\nCongratulations! You are done! Now you can train your model with your favorite backbone and segmentation framework.\n\n## 3. \ud83d\udca1 Examples\n\n* Training model for person segmentation using images from PASCAL VOC Dataset. \"voc\\_person\\_seg\" dir contains 32 json labels and their corresponding jpeg images for training and 8 json labels with corresponding images for validation.\n\n&amp;#x200B;\n\n    Segmentor&lt;FPN&gt; segmentor;\n    segmentor.Initialize(0/*gpu id, -1 for cpu*/,\n                        512/*resize width*/,\n                        512/*resize height*/,\n                        {\"background\",\"person\"}/*class name dict, background included*/,\n                        \"resnet34\"/*backbone name*/,\n                        \"your path to resnet34.pt\");\n    segmentor.Train(0.0003/*initial leaning rate*/,\n                    300/*training epochs*/,\n                    4/*batch size*/,\n                    \"your path to voc_person_seg\",\n                    \".jpg\"/*image type*/,\n                    \"your path to save segmentor.pt\");\n\n* Predicting test. A segmentor.pt file is provided in the project. It is trained through a FPN with ResNet34 backbone for a few epochs. You can directly test the segmentation result through:\n\n&amp;#x200B;\n\n    cv::Mat image = cv::imread(\"your path to voc_person_seg\\\\val\\\\2007_004000.jpg\");\n    Segmentor&lt;FPN&gt; segmentor;\n    segmentor.Initialize(0,512,512,{\"background\",\"person\"},\n                          \"resnet34\",\"your path to resnet34.pt\");\n    segmentor.LoadWeight(\"segmentor.pt\"/*the saved .pt path*/);\n    segmentor.Predict(image,\"person\"/*class name for showing*/);\n\nthe predicted result shows as follow:\n\n&amp;#x200B;\n\n## 4. \ud83e\uddd1\u200d\ud83d\ude80 Train your own data\n\n* Create your own dataset. Using [labelme](https://github.com/wkentaro/labelme) through \"pip install\" and label your images. Split the output json files and images into folders just like below:\n\n&amp;#x200B;\n\n    Dataset\n    \u251c\u2500\u2500 train\n    \u2502   \u251c\u2500\u2500 xxx.json\n    \u2502   \u251c\u2500\u2500 xxx.jpg\n    \u2502   \u2514......\n    \u251c\u2500\u2500 val\n    \u2502   \u251c\u2500\u2500 xxxx.json\n    \u2502   \u251c\u2500\u2500 xxxx.jpg\n    \u2502   \u2514......\n\n* Training or testing. Just like the example of \"voc\\_person\\_seg\", replace \"voc\\_person\\_seg\" with your own dataset path.\n\n## \ud83d\udce6 Models\n\n## Architectures\n\n* \\[x\\] Unet \\[[paper](https://arxiv.org/abs/1505.04597)\\]\n* \\[x\\] FPN \\[[paper](http://presentations.cocodataset.org/COCO17-Stuff-FAIR.pdf)\\]\n* \\[x\\] PAN \\[[paper](https://arxiv.org/abs/1805.10180)\\]\n* \\[x\\] LinkNet \\[[paper](https://arxiv.org/abs/1707.03718)\\]\n* \\[x\\] DeepLabV3 \\[[paper](https://arxiv.org/abs/1706.05587)\\]\n* \\[x\\] DeepLabV3+ \\[[paper](https://arxiv.org/abs/1802.02611)\\]\n* \\[ \\] PSPNet \\[[paper](https://arxiv.org/abs/1612.01105)\\]\n\n## Encoders\n\n* \\[x\\] ResNet\n* \\[x\\] ResNext\n* \\[ \\] ResNest\n\nThe following is a list of supported encoders in the Libtorch Segment. All the encoders weights can be generated through torchvision except resnest. Select the appropriate family of encoders and click to expand the table and select a specific encoder and its pre-trained weights.\n\n|Encoder|Encoder|Encoder|\n|:-|:-|:-|\n|Weights|Weights|Weights|\n|Params, M|Params, M|Params, M|\n|resnet18|resnext50\\_32x4d|timm-resnest14d|\n|imagenet|imagenet|imagenet|\n|11M|22M|8M|\n|resnet34|resnext101\\_32x8d|timm-resnest26d|\n|imagenet|imagenet|imagenet|\n|21M|86M|15M|\n|resnet50|timm-resnest50d|imagenet|\n|imagenet|23M|25M|\n|resnet101|timm-resnest101e|imagenet|\n|imagenet|42M|46M|\n|resnet152|timm-resnest200e|imagenet|\n|imagenet|58M|68M|\n|timm-resnest269e|imagenet|108M|\n|timm-resnest50d\\_4s2x40d|imagenet|28M|\n|timm-resnest50d\\_1s4x24d|imagenet|23M|\n\n## \ud83d\udee0 Installation\n\nWindows:\n\nConfigure the environment for libtorch development. [Visual studio](https://allentdan.github.io/2020/12/16/pytorch%E9%83%A8%E7%BD%B2torchscript%E7%AF%87) and [Qt Creator](https://allentdan.github.io/2021/01/21/QT%20Creator%20+%20Opencv4.x%20+%20Libtorch1.7%E9%85%8D%E7%BD%AE/#more) are verified for libtorch1.7x release. Only chinese configuration blogs provided by now, english version ASAP.\n\nLinux &amp;&amp; MacOS:\n\nFollow the official pytorch c++ tutorials [here](https://pytorch.org/tutorials/advanced/cpp_export.html). It can be no more difficult than windows.\n\n## \ud83e\udd1d Thanks\n\nThis project is under developing. By now, these projects helps a lot.\n\n* [official pytorch](https://github.com/pytorch/pytorch)\n* [qubvel SMP](https://github.com/qubvel/segmentation_models.pytorch)\n* [wkentaro labelme](https://github.com/wkentaro/labelme)\n* [nlohmann json](https://github.com/nlohmann/json)\n\n## \ud83d\udcdd Citing\n\n    @misc{Chunyu:2021,\n      Author = {Chunyu Dong},\n      Title = {Libtorch Segment},\n      Year = {2021},\n      Publisher = {GitHub},\n      Journal = {GitHub repository},\n      Howpublished = {\\url{https://github.com/AllentDan/SegmentationCpp}}\n    }\n\n## \ud83d\udee1\ufe0f License\n\nProject is distributed under [MIT License](https://github.com/qubvel/segmentation_models.pytorch/blob/master/LICENSE)", "upvote_ratio": 1.0, "id": "t3_luh7ge", "created_utc": 1614522141.0}
{"sub": "pytorch", "title": "Image Classification with Unbalanced Dataset", "selftext": "I have a 5 classes unbalanced dataset for classification. I'm using RandomWeightSampler to feed the dataloader to avoid the consequences due to unbalancing stuff and using CrossEntropy as a loss function on training.\n\nAs you know CrossEntropy can used with a weight, so should I pass a class weight to the loss function or the sampler which is create balanced batches for training is enough for handle the dataset balance tweak ?", "upvote_ratio": 0.81, "id": "t3_ltzw6s", "created_utc": 1614468013.0}
{"sub": "pytorch", "title": "Reshaping Operations in Pytorch", "selftext": "What reshape function can I use to convert this tensor Reshape = rank1.view(2, 2, 6)\n\ntensor([[[ 0,  1,  2,  3,  4,  5],\n             [ 6,  7,  8,  9, 10, 11]],\n             [[12, 13, 14, 15, 16, 17],\n             [18, 19, 20, 21, 22, 23]]])\n\nTo this ? \nexpected = [\n    [0, 1,  2,  3, 12, 13, 14, 15],\n    [4, 5,  6,  7, 16, 17, 18, 19],\n    [8, 9, 10, 11, 20, 21, 22, 23]]", "upvote_ratio": 1.0, "id": "t3_ltt8hm", "created_utc": 1614447858.0}
{"sub": "pytorch", "title": "Questions about reproducing DARTS code implementation", "selftext": "I am not sure about [how to implement](https://gist.github.com/promach/ae0e48974ccf4bdee07c9d69148cf21b) `Ltrain(w+)` and  `Ltrain(w-)`  for [DARTS: Differentiable Architecture Search](https://arxiv.org/abs/1806.09055)\n\nCould anyone advise ?\n\nhttps://preview.redd.it/swqi3tl230k61.png?width=1920&amp;format=png&amp;auto=webp&amp;s=cafe8dacb197f4db8e56ec59e189103f741fe59f", "upvote_ratio": 1.0, "id": "t3_ltlo8f", "created_utc": 1614423083.0}
{"sub": "pytorch", "title": "Tom Cruise deepfake videos are all over the internet and passing the best deepfake detectors!", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_lt9lg4", "created_utc": 1614378523.0}
{"sub": "pytorch", "title": "Element from a column", "selftext": "How can I use integer array indexing to get   elements from different columns? Lets say I wanted to grab 12,64,34; how can I get the elements?\n\n  numbers = torch.tensor(\\[\\[2, 4, 8\\], \\[12, 16, 32\\], \\[64, 23, 3\\], \\[34, 56, 23\\]\\])", "upvote_ratio": 0.67, "id": "t3_lspv50", "created_utc": 1614313743.0}
{"sub": "pytorch", "title": "MLP Win Prediction Model does not converge", "selftext": "Hi,\n\nI built a model to predict a winning team in a game of dota. I wanted to evaluate each player (10 in total, each has 599 features encoded with 1 or 0) using the same criteria (player\\_model). Then I wanted to predict the outcome of the match (match\\_model) bases on the player evaluations of each team. (the commented variant includes an additional team evaluation)\n\nThe problem I have is that the model just favors (depending on the weight init) one outcome and does not converge at all.\n\nI am not sure if the gradients are calculated correctly, when throwing the whole batch x player tensor in the player\\_model like this: p = self.player\\_model(x)\n\nOr maybe something else screws it up...\n\nDo you have any hints?\n\nThanks\n\nLogs:\n\n    torch.Size([64000, 10, 599]) torch.Size([64000, 2]) ---- Train set\n    torch.Size([16000, 10, 599]) (16000, 2) ----- Test set\n    accuracy_score: 0.5034375\n    max_test_accuracy  0.5034375\n    train 0.0006832404183223843\n    eval  4.3321520090103147e-05\n    [[   0 7945]\n     [   0 8055]]\n    accuracy_score: 0.5034375\n    max_test_accuracy  0.5034375\n    train 0.0006823675045743584\n    eval  4.3321534991264343e-05\n    [[   0 7945]\n     [   0 8055]]\n    accuracy_score: 0.5034375\n    max_test_accuracy  0.5034375\n    train 0.0006823821607977151\n    eval  4.332022368907928e-05\n    [[   0 7945]\n     [   0 8055]]\n\nModel:\n\n        def __init__(self):\n            super(Model, self).__init__()\n    \n            self.player_model = nn.Sequential(\n                nn.Linear(feature_count, 128),\n                nn.ReLU(),\n                nn.Linear(128, 64),\n                nn.ReLU(),\n                nn.Linear(64, 32)\n            )\n    \n            self.team_model = nn.Sequential(\n                nn.Linear(32 * 5, 128),\n                nn.ReLU(),\n                nn.Linear(128, 64),\n                nn.ReLU(),\n                nn.Linear(64, 32)\n            )\n    \n            self.match_model = nn.Sequential(\n                nn.Linear(32 * 10, 8),\n                nn.ReLU(),\n                nn.Linear(8, 4),\n                nn.ReLU(),\n                nn.Linear(4, 2),\n                nn.Softmax(dim=1)\n            )\n    \n        def forward(self, x):\n            p = self.player_model(x)\n            # t1_in = p[:, :5, :].reshape(p.size(0), 5 * p.size(2))\n            # t1 = self.team_model(t1_in)\n    \n            # t2_in = p[:, 5:, :].reshape(p.size(0), 5 * p.size(2))\n            # t2 = self.team_model(t2_in)\n    \n            # m = torch.cat((t1, t2), 1)\n            m = p.reshape(p.size(0), 10 * p.size(2))\n    \n            x = self.match_model(m)\n    \n            return x\n\nOther\n\n    criterion = nn.BCELoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0004)", "upvote_ratio": 1.0, "id": "t3_lsigs0", "created_utc": 1614291370.0}
{"sub": "pytorch", "title": "How to skip the images in a custom dataset and deal with None values?", "selftext": "Hi,\n\nI have an object detection dataset with RGB images and annotations in Json. I use a custom DataLoader class to read the images and the labels. One issue that I\u2019m facing is that I would like to skip images when training my model if/when labels don\u2019t contain certain objects.\n\nFor example, If one image doesn\u2019t contain any target labels belonging to the class \u2018Cars\u2019, I would like to skip them. When parsing my Json annotation, I tried checking for labels that don\u2019t contain the class \u2018Cars\u2019 and returned None. Subsequently, I used a collate function to filter the None but unfortunately, It is not working\n\n    \n    import torch\n    from torch.utils.data.dataset import Dataset\n    import json\n    import os\n    from PIL import Image\n    from torchvision import transforms\n    #import cv2\n    import numpy as np\n    general_classes = {\n        # Cars\n        \"Toyota Corolla\" : 0,\n        \"VW Golf\" : 0,\n        \"VW Beetle\" : 0,\n    \n        # Motor-cycles\n        \"Harley Davidson\" : 1,\n        \"Yamaha YZF-R6\" : 1,\n    }\n    \n    car_classes={\n    \"Toyota Corolla\" : 0,\n    \"VW Golf\" : 0,\n    \"VW Beetle\" : 0\n    }\n    \n    def get_transform(train):\n        transforms = []\n        # converts the image, a PIL image, into a PyTorch Tensor\n        transforms.append(T.ToTensor())\n        if train:\n            # during training, randomly flip the training images\n            # and ground-truth for data augmentation\n            transforms.append(T.RandomHorizontalFlip(0.5))\n        return T.Compose(transforms)\n    \n    \n    def my_collate(batch):\n        batch = list(filter(lambda x: x is not None, batch))\n        return torch.utils.data.dataloader.default_collate(batch)\n    \n    \n    class FilteredDataset(Dataset):\n        # The dataloader will skip the image and corresponding labels based on the dictionary 'car_classes'\n        def __init__(self, data_dir, transforms):\n            self.data_dir = data_dir\n            img_folder_list = os.listdir(self.data_dir)\n            self.transforms = transforms\n    \n            imgs_list = []\n            json_list = []\n            self.filter_count=0\n            self.filtered_label_list=[]\n    \n            for img_path in img_folder_list:\n                #img_full_path = self.data_dir + img_path\n                img_full_path=os.path.join(self.data_dir,img_path)\n                json_file = os.path.join(img_full_path, 'annotations-of-my-images.json')\n                img_file = os.path.join(img_full_path, 'Image-Name.png')\n    \n                json_list.append(json_file)\n                imgs_list.append(img_file)\n            self.imgs = imgs_list\n            self.annotations = json_list\n            total_count=0\n    \n            for one_annotation in self.annotations:\n                filtered_obj_id=[]\n                with open(one_annotation) as f:\n                    img_annotations = json.load(f)\n    \n                parts_list = img_annotations['regions']\n                for part in parts_list:\n                    current_obj_id = part['tags'][0] # bbox label \n                    check_obj_id = general_classes[current_obj_id]\n                    if(check_obj_id==0):\n                        subclass_id=car_classes[current_obj_id]\n                        filtered_obj_id.append(subclass_id)\n                        total_count=total_count+1\n    \n                if(len(filtered_obj_id)&gt;0):\n                    self.filter_count=self.filter_count+1\n                    self.filtered_label_list.append(one_annotation)\n    \n            print(\"The total number of the objects in all images: \",total_count)\n    \n    \n        # get one image and the bboxes,img_id, labels of parts, etc in the image as target.\n        def __getitem__(self, idx):\n    \n            img_path = self.imgs[idx]\n            image_id = torch.tensor([idx])\n            \n            with open(self.annotations[idx]) as f:\n                img_annotations = json.load(f)\n            parts_list = img_annotations['regions']\n            obj_ids = []\n            boxes = []\n            for part in parts_list:\n                obj_id = part['tags'][0]\n                check_obj_id = general_classes[obj_id]\n                if(check_obj_id==0):\n                   obj_id=car_classes[obj_id]\n                   obj_ids.append(obj_id)\n                    #print(\"---------------------------------------------------\")\n                    \n            if(len(obj_ids)&gt;0):\n                img = Image.open(img_path).convert(\"RGB\")\n                labels = torch.as_tensor(obj_ids, dtype = torch.int64)\n                target = {}\n                target['labels'] = labels\n                \n                if self.transforms is not None:\n                    img, target = self.transforms(img, target)\n                    return img, target\n            else:\n                return None\n    \n    \n        def __len__(self):\n            return len(self.filtered_label_list)\n    \n    \n    \n    \n    train_data_path = \"path-to-my-annotation\"\n    # Generators\n    train_dataset = FilteredDataset(train_data_path,get_transform(train=True))\n    print(\"Total files in the train_dataset: \",len(train_dataset))\n    #print(\"The first instance in the train dataset : \",train_dataset[0])\n    #training_generator = torch.utils.data.DataLoader(train_dataset)\n    training_generator = torch.utils.data.DataLoader(train_dataset,collate_fn=my_collate)\n    print(\"\\n\\n Iterator in action! \")\n    print(\"---------------------------------------------------------\")\n    count=0\n    for img,target in training_generator:\n        #print(\"The img name : \",img[0])\n        count=count+1\n        print(\"target name : \",target)\n        print(\"count : \",count)\n        print(\"**************************************************\")\n\nHowever, I get the following error,\n\n&amp;#x200B;\n\n[Traceback that I get](https://preview.redd.it/hxsytz62wnj61.png?width=691&amp;format=png&amp;auto=webp&amp;s=633aa87dfa1d405cf853b93a1bab7b03e859d8b3)\n\n&amp;#x200B;\n\nCould anyone please suggest a way to skip the images that do not contain a particular categorical label?", "upvote_ratio": 1.0, "id": "t3_lsce7e", "created_utc": 1614275458.0}
{"sub": "pytorch", "title": "Slicing a tensor", "selftext": "How do I get the rows 0 and 2  and a columns 1 and 4 from a tensor  below ?\n\nb= torch.tensor(\\[\\[2, 6, 12, 18, 20\\], \\[3, 9, 12, 24, 15\\], \\[14, 15, 16, 19, 25\\]\\])", "upvote_ratio": 1.0, "id": "t3_lrx0oq", "created_utc": 1614223671.0}
{"sub": "pytorch", "title": "Faster builds using libtorch c++ (question)", "selftext": "Including torch/torch.h makes builds times unbearable, at least combined with Eigen, faiss, etc. \n\nThe problem is, I really need torch::Tensor, as it is an enormously useful container type I'd like to use in many files.\n\nAnybody know any good fix for this? I tried precompiled headers, but could not get it to work.", "upvote_ratio": 1.0, "id": "t3_lrlazm", "created_utc": 1614195193.0}
{"sub": "pytorch", "title": "Beginner | Simple NN to find occurrences in array", "selftext": "Hey everyone,\n\nI'm just starting with pytorch and though a simple NN to take in an array of 0's and 1's and output the number of ones would be good just to see how everything works, I have this data (input, target):\n\ndata = ((\\[0, 0, 1, 0, 1\\], \\[0, 0, 1, 0, 0, 0\\]),(\\[0, 1, 1, 0, 1\\], \\[0, 0, 0, 1, 0, 0\\]),(\\[1, 0, 1, 0, 0\\], \\[0, 0, 1, 0, 0, 0\\]),(\\[1, 1, 1, 1, 1\\], \\[0, 0, 0, 0, 0, 1\\]),(\\[1, 1, 1, 0, 1\\], \\[0, 0, 0, 0, 1, 0\\]),(\\[0, 0, 0, 0, 0\\], \\[1, 0, 0, 0, 0, 0\\]),(\\[1, 0, 1, 1, 0\\], \\[0, 0, 0, 1, 0, 0\\]),(\\[1, 1, 1, 1, 1\\], \\[0, 0, 0, 0, 0, 1\\]),(\\[1, 0, 0, 0, 1\\], \\[0, 0, 1, 0, 0, 0\\]),)\n\ncould anyone please spin up a really quick network to get this working? I've tried but I'm struggling implementing it myself, would just like to see how it could be done, im struggling with the loss functions specifically!\n\n&amp;#x200B;\n\nany help is appreciated!\n\nso I think I managed to get it working, it's very hacky (yes I know the data creation is terrible)\n\nrepo: [https://github.com/Torbet/Pytorch-Linear-Regression](https://github.com/Torbet/Pytorch-Linear-Regression)", "upvote_ratio": 0.5, "id": "t3_lrkn23", "created_utc": 1614193487.0}
{"sub": "pytorch", "title": "Is the higher level library for meta-learning compatible with pytorch's distributed libraries?", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_lqpeie", "created_utc": 1614105142.0}
{"sub": "pytorch", "title": "Train on main thread, validation on background thread?", "selftext": "I am training a RL model that has a heavy simulation component within the training loop. While training, I want to be able to \"step aside\" and run a couple of validation simulations on a separate data set.\n\nSince the validation set should never update the weights, I was wondering if it's possible to freeze the PyTorch model, pass it to a background thread, and run the validation data set in the background.\n\nI've come across the `torch.multiprocessing` and Python `multiprocessing`, but I am unsure which would be best suited to this use. Looking at the [PyTorch Multiprocessing Best Practices](https://pytorch.org/docs/stable/notes/multiprocessing.html#), it seems a direct handle to the current model is passed to the background thread. While this is good for asynchronous learning across threads, I don't want the model to update as the validation data is being run.\n\nMy first thought is to build a standard Python function to accept a deep copied PyTorch model and run that background dataset completely independently (using Python's native `multiprocessing` module). Does anyone have any recommendations on how to go about this?", "upvote_ratio": 0.81, "id": "t3_lqlheq", "created_utc": 1614095387.0}
{"sub": "pytorch", "title": "How does Quantize per tensor work in relation with gradient??", "selftext": "Specifically, how does it not create problems with the derivative of the function since a quantizing function is a step-like linear with grad=0. How is it possible to round decimals without messing with backpropagation?", "upvote_ratio": 1.0, "id": "t3_lqi315", "created_utc": 1614085863.0}
{"sub": "pytorch", "title": "Why does my pytorch rpc workers deadlock, is it because I am using main as my master?", "selftext": "nan", "upvote_ratio": 0.75, "id": "t3_lq0gu7", "created_utc": 1614029298.0}
{"sub": "pytorch", "title": "Nan LOSS while training Mask RCNN on custom data", "selftext": "I'm trying to train the mask RCNN on custom data but I get Nans as loss values in the first step itself. \n\n{'loss\\_classifier': tensor(nan, device='cuda:0', grad\\_fn=&lt;NllLossBackward&gt;), 'loss\\_box\\_reg': tensor(nan, device='cuda:0', grad\\_fn=&lt;DivBackward0&gt;), 'loss\\_mask': tensor(-1.1146e+30, device='cuda:0',        grad\\_fn=&lt;BinaryCrossEntropyWithLogitsBackward&gt;), 'loss\\_objectness': tensor(574.7335, device='cuda:0',        grad\\_fn=&lt;BinaryCrossEntropyWithLogitsBackward&gt;), 'loss\\_rpn\\_box\\_reg': tensor(169.8945, device='cuda:0', grad\\_fn=&lt;DivBackward0&gt;)}\n\nThe images have 3 channels and the mask input is of the dimension \\[N,H,W\\]. What can cause the loss to explode?", "upvote_ratio": 1.0, "id": "t3_lplyvb", "created_utc": 1613993804.0}
{"sub": "pytorch", "title": "How do I visualize the output from the encoder in an autoencoder model?", "selftext": "I have defined a simple autoencoder using DGL (PyTorch backend). The code looks like this:\n\n`from dgl.nn import GraphConv`  \n`class AEGCN(nn.Module):`  \n `def __init__(self,\u00a0in_feats,\u00a0hidden_size,\u00a0num_classes):`  \n `super(AEGCN,\u00a0self).__init__()`  \n `self.conv1\u00a0=\u00a0GraphConv(in_feats,\u00a0hidden_size)`  \n `self.conv2\u00a0=\u00a0GraphConv(hidden_size,\u00a0in_feats)`  \n `def forward(self,\u00a0g,\u00a0inputs):`  \n `h\u00a0=\u00a0self.conv1(g,\u00a0inputs)`  \n `h\u00a0=\u00a0torch.relu(h)`  \n `h\u00a0=\u00a0self.conv2(g,\u00a0h)`  \n `return h`  \n`net\u00a0=\u00a0AEGCN(192,\u00a020,\u00a0192)`\n\n&amp;#x200B;\n\nI have trained it. How, I intend to see the output of conv1. How do I do that? Also, if I add multiple layers in the encoder part, how do I get the output of those layers of just those layers?", "upvote_ratio": 1.0, "id": "t3_lpi7rk", "created_utc": 1613979470.0}
{"sub": "pytorch", "title": "Deconvolution operation in PyTorch", "selftext": "Hi, Im trying to implement visualizations from ZFNet paper, but i dont know how to do deconvolution.\nAny advice? :)", "upvote_ratio": 0.5, "id": "t3_lpheu9", "created_utc": 1613976558.0}
{"sub": "pytorch", "title": "Is nn.Parameter learnable?", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_lpfwz2", "created_utc": 1613971151.0}
{"sub": "pytorch", "title": "torch.nn.Embedding explained (+ Character-level language model)", "selftext": "nan", "upvote_ratio": 0.84, "id": "t3_lp6467", "created_utc": 1613940083.0}
{"sub": "pytorch", "title": "Newcomer to PyTorch in need of help", "selftext": "Hello all. New to PyTorch and ML in general.\n\nMy end goal right now is to use PyTorch and RL specifically to calculate the movement of a robotic arm to a given target location. To take some small steps towards my end goal, I\u2019m starting off with a single link arm in a 2D environment which will try and point towards a goal position.\n\nTo reiterate, I am brand new to machine learning but do have over a decade of programming experience.\n\nI have followed this basic tutorial [here](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) to get the infamous CartPole model running from OpenAI. After getting that all running on my computer I began to try and modify the CartPole env source code from [here](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py) and the PyTorch example to get to my \u201c2D Single link arm\u201d environment.\n\nI have gotten this modified environment to run as you can see below:\n\nhttps://preview.redd.it/y2vu5vkqvvi61.png?width=2682&amp;format=png&amp;auto=webp&amp;s=b62e643654f8b61c4bc4eaa1a6465a30af234f00\n\n&amp;#x200B;\n\nhttps://preview.redd.it/4fbcktpn91j61.png?width=2696&amp;format=png&amp;auto=webp&amp;s=fed27980876ade082ceecfaa7d886401d750268a\n\nI'm not sure that I am using the env state and reward calculations correctly as the model doesn't seen to converge on a good reward in training. I'm also not sure if I'm making the training loop correct, although the only thing I've change so far in the PyTorch tutorial is the `get_screen` function.\n\nI would love some input on the code I've written as I'm not sure where to go from here.\n\nI am attaching the code to the modified PyTorch example and Env.\n\narmenv.py:\n\n    import gym\n    from gym import spaces\n    from gym.utils import seeding\n    import numpy as np\n    \n    MAX_STEPS=200\n    STEPS_ON_GOAL_TO_FINISH=10\n    \n    class ArmEnv(gym.Env):\n      \"\"\"Arm Environment that follows gym interface\"\"\"\n      metadata = {\n          'render.modes': ['human', 'rgb_array'],\n          'video.frames_per_second': 50\n      }\n    \n      def __init__(self):\n        self.length = 1.0  # length of arm\n        self.goal = [2.,2.]\n        self.tau = 0.02  # seconds between state updates\n        self.theta_adj = 2.0\n    \n        self.angle_difference_threshold = 0.5\n    \n        # The max and min values that can be observed\n        high = np.array([np.pi/2, np.pi, 1.8, 2.1], dtype=np.float32)\n        low = np.array([-np.pi/2, 0, -1.8, 1.1], dtype=np.float32)\n    \n        self.action_space = spaces.Discrete(3)\n        self.observation_space = spaces.Box(low, high, dtype=np.float32)\n    \n        self.on_goal = 0\n        self.current_step = 0\n    \n        self.seed()\n        self.viewer = None\n        self.state = None\n    \n      def seed(self, seed=None):\n        self.np_random, seed = seeding.np_random(seed)\n        return [seed]\n    \n      def calc_angle_difference(self, theta):\n        # Get state of arm\n        arm_mag = self.length\n        arm_vector = np.array([arm_mag * np.sin(theta), arm_mag * np.cos(theta)])\n    \n        # Get distance vector between the goal and end of arm\n        distance_vector = np.array([self.goal[0] - arm_vector[0], self.goal[1] - arm_vector[1]])\n        distance_mag = np.sqrt(distance_vector[0]**2 + distance_vector[1]**2)\n    \n        # Get the angle between this distance vector and the arm vector\n        return np.arccos(np.dot(arm_vector, distance_vector)/(arm_mag*distance_mag), dtype=np.float32)\n    \n      def step(self, action):\n        err_msg = \"%r (%s) invalid\" % (action, type(action))\n        assert self.action_space.contains(action), err_msg\n    \n        self.current_step += 1\n        theta, _, goalx, goaly = self.state\n    \n        # Adjust theta based on the chosen action\n        # if action == 1:\n        #   theta_adj = self.theta_adj\n        # else:\n        #   theta_adj = -self.theta_adj\n        if action == 1:\n          theta_adj = self.theta_adj\n        elif action == 2:\n          theta_adj = -self.theta_adj\n        else:\n          theta_adj = 0\n    \n        theta += theta_adj * self.tau\n        theta = max(min(theta, np.pi/2), -np.pi/2)\n      \n        # Get the angle between this distance vector and the arm vector\n        angle_difference = self.calc_angle_difference(theta)\n    \n        self.state = (theta, angle_difference, goalx, goaly)\n    \n        # delay_modifier = float(self.current_step / MAX_STEPS)\n        # r = float(1 - angle_difference*2)\n        # r = np.exp(-angle_difference, dtype=np.float32)\n        r = np.exp(-angle_difference*3, dtype=np.float32)\n        # r = np.exp(-angle_difference, dtype=np.float32) * delay_modifier\n        # r = np.exp(-angle_difference, dtype=np.float32) * (1.0 - delay_modifier)\n    \n        if theta &gt;= np.pi/2 or theta &lt;= -np.pi/2:\n          r = 0.0 # Baaaad boi\n    \n        if angle_difference &lt;= self.angle_difference_threshold and \\\n          angle_difference &gt;= -self.angle_difference_threshold:\n          self.on_goal += 1\n          r = 1.0 # Goooood boi\n        else:\n          self.on_goal = self.on_goal - 2 if self.on_goal &gt; 0 else 0\n    \n        done = bool(self.on_goal &gt;= STEPS_ON_GOAL_TO_FINISH or\n          self.current_step &gt;= MAX_STEPS or\n          theta &gt;= np.pi/2 or\n          theta &lt;= -np.pi/2\n        )\n    \n        print(self.current_step, np.array(self.state), r, self.on_goal, action)\n        return np.array(self.state), float(r), done, {}\n    \n      def reset(self):\n        self.goal = np.array([self.np_random.rand() * 3.6 - 1.8, self.np_random.rand() + 1.1])\n        self.on_goal = 0\n        self.current_step = 0\n    \n        new_theta = self.np_random.rand()*np.pi - np.pi/2\n        new_angle_difference = self.calc_angle_difference(new_theta)\n    \n        self.state = np.array([new_theta, new_angle_difference, *self.goal], dtype=np.float32)\n        return np.array(self.state)\n    \n      def render(self, mode='human'):\n          screen_width = 600\n          screen_height = 400\n    \n          world_width = self.length * 4\n          scale = screen_width/world_width\n          polewidth = 10.0\n          polelen = scale * (self.length)\n          goalwidth = 15.0\n          goalheight = 15.0\n    \n          if self.viewer is None:\n              from gym.envs.classic_control import rendering\n              self.viewer = rendering.Viewer(screen_width, screen_height)\n              l, r, t, b = -goalwidth / 2, goalwidth / 2, goalheight / 2, -goalheight / 2\n              goal = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n              self.goaltrans = rendering.Transform(translation=(self.goal[0] * scale + screen_width / 2.0, self.goal[0] * scale))\n              goal.add_attr(self.goaltrans)\n              self.viewer.add_geom(goal)\n              l, r, t, b = -polewidth / 2, polewidth / 2, polelen - polewidth / 2, -polewidth / 2\n              pole = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n              pole.set_color(.8, .6, .4)\n              self.poletrans = rendering.Transform(translation=(screen_width / 2.0, 0))\n              pole.add_attr(self.poletrans)\n              self.viewer.add_geom(pole)\n              self._pole_geom = pole\n    \n          if self.state is None:\n              return None\n    \n          # Edit the pole polygon vertex\n          pole = self._pole_geom\n          l, r, t, b = -polewidth / 2, polewidth / 2, polelen - polewidth / 2, -polewidth / 2\n          pole.v = [(l, b), (l, t), (r, t), (r, b)]\n    \n          x = self.state\n          self.goaltrans.set_translation(self.goal[0] * scale + screen_width / 2.0, self.goal[1] * scale)\n          self.poletrans.set_rotation(-x[0])\n    \n          return self.viewer.render(return_rgb_array=mode == 'rgb_array')\n    \n      def close(self):\n          if self.viewer:\n              self.viewer.close()\n              self.viewer = None\n\nmain.py:\n\n    from armenv import ArmEnv\n    import math\n    import random\n    import numpy as np\n    import matplotlib\n    import matplotlib.pyplot as plt\n    from collections import namedtuple\n    from itertools import count\n    from PIL import Image\n    \n    import torch\n    import torch.nn as nn\n    import torch.optim as optim\n    import torch.nn.functional as F\n    import torchvision.transforms as T\n    \n    env = ArmEnv()\n    \n    # set up matplotlib\n    is_ipython = 'inline' in matplotlib.get_backend()\n    if is_ipython:\n        from IPython import display\n    \n    plt.ion()\n    \n    # if gpu is to be used\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n    \n    \n    class ReplayMemory(object):\n      def __init__(self, capacity):\n          self.capacity = capacity\n          self.memory = []\n          self.position = 0\n    \n      def push(self, *args):\n          \"\"\"Saves a transition.\"\"\"\n          if len(self.memory) &lt; self.capacity:\n              self.memory.append(None)\n          self.memory[self.position] = Transition(*args)\n          self.position = (self.position + 1) % self.capacity\n    \n      def sample(self, batch_size):\n          return random.sample(self.memory, batch_size)\n    \n      def __len__(self):\n          return len(self.memory)\n    \n    class DQN(nn.Module):\n    \n      def __init__(self, h, w, outputs):\n        super(DQN, self).__init__()\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n        self.bn1 = nn.BatchNorm2d(16)\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n        self.bn2 = nn.BatchNorm2d(32)\n        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n        self.bn3 = nn.BatchNorm2d(32)\n    \n        # Number of Linear input connections depends on output of conv2d layers\n        # and therefore the input image size, so compute it.\n        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n            return (size - (kernel_size - 1) - 1) // stride  + 1\n        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n        linear_input_size = convw * convh * 32\n        self.head = nn.Linear(linear_input_size, outputs)\n    \n      # Called with either one element to determine next action, or a batch\n      # during optimization. Returns tensor([[left0exp,right0exp]...]).\n      def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.relu(self.bn3(self.conv3(x)))\n        return self.head(x.view(x.size(0), -1))\n    \n    resize = T.Compose([T.ToPILImage(),\n      T.Resize(40, interpolation=Image.CUBIC),\n      T.ToTensor()])\n    \n    def get_screen():\n      # Returned screen requested by gym is 400x600x3, but is sometimes larger\n      # such as 800x1200x3. Transpose it into torch order (CHW).\n      screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n      screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n      screen = torch.from_numpy(screen)\n      # Resize, and add a batch dimension (BCHW)\n      return resize(screen).unsqueeze(0).to(device)\n    \n    env.reset()\n    plt.figure()\n    plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n               interpolation='none')\n    plt.title('Example extracted screen')\n    plt.show()\n    \n    BATCH_SIZE = 128\n    GAMMA = 0.999\n    EPS_START = 0.9\n    EPS_END = 0.05\n    EPS_DECAY = 200\n    TARGET_UPDATE = 10\n    \n    # Get screen size so that we can initialize layers correctly based on shape\n    # returned from AI gym. Typical dimensions at this point are close to 3x40x90\n    # which is the result of a clamped and down-scaled render buffer in get_screen()\n    init_screen = get_screen()\n    _, _, screen_height, screen_width = init_screen.shape\n    \n    # Get number of actions from gym action space\n    n_actions = env.action_space.n\n    \n    policy_net = DQN(screen_height, screen_width, n_actions).to(device)\n    target_net = DQN(screen_height, screen_width, n_actions).to(device)\n    target_net.load_state_dict(policy_net.state_dict())\n    target_net.eval()\n    \n    optimizer = optim.RMSprop(policy_net.parameters())\n    memory = ReplayMemory(10000)\n    \n    steps_done = 0\n    \n    def select_action(state):\n      global steps_done\n      sample = random.random()\n      eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n        math.exp(-1. * steps_done / EPS_DECAY)\n      steps_done += 1\n      if sample &gt; eps_threshold:\n        with torch.no_grad():\n          # t.max(1) will return largest column value of each row.\n          # second column on max result is index of where max element was\n          # found, so we pick action with the larger expected reward.\n          return policy_net(state).max(1)[1].view(1, 1)\n      else:\n        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n    \n    episode_rewards = []\n    \n    def plot_rewards():\n      plt.figure(2)\n      plt.clf()\n      rewards_t = torch.tensor(episode_rewards, dtype=torch.float)\n      plt.title('Training...')\n      plt.xlabel('Episode')\n      plt.ylabel('Reward')\n      plt.plot(rewards_t.numpy())\n      # Take 100 episode averages and plot them too\n      if len(rewards_t) &gt;= 10:\n        means = rewards_t.unfold(0, 10, 1).mean(1).view(-1)\n        means = torch.cat((torch.zeros(9), means))\n        plt.plot(means.numpy())\n    \n      plt.pause(0.001)  # pause a bit so that plots are updated\n      if is_ipython:\n        display.clear_output(wait=True)\n        display.display(plt.gcf())\n    \n    def optimize_model():\n      if len(memory) &lt; BATCH_SIZE:\n        return\n      transitions = memory.sample(BATCH_SIZE)\n      # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n      # detailed explanation). This converts batch-array of Transitions\n      # to Transition of batch-arrays.\n      batch = Transition(*zip(*transitions))\n    \n      # Compute a mask of non-final states and concatenate the batch elements\n      # (a final state would've been the one after which simulation ended)\n      non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n                                              batch.next_state)), device=device, dtype=torch.bool)\n      non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n      state_batch = torch.cat(batch.state)\n      action_batch = torch.cat(batch.action)\n      reward_batch = torch.cat(batch.reward)\n    \n      # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n      # columns of actions taken. These are the actions which would've been taken\n      # for each batch state according to policy_net\n      state_action_values = policy_net(state_batch).gather(1, action_batch)\n    \n      # Compute V(s_{t+1}) for all next states.\n      # Expected values of actions for non_final_next_states are computed based\n      # on the \"older\" target_net; selecting their best reward with max(1)[0].\n      # This is merged based on the mask, such that we'll have either the expected\n      # state value or 0 in case the state was final.\n      next_state_values = torch.zeros(BATCH_SIZE, device=device)\n      next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n      # Compute the expected Q values\n      expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n    \n      # Compute Huber loss\n      loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n    \n      # Optimize the model\n      optimizer.zero_grad()\n      loss.backward()\n      for param in policy_net.parameters():\n        param.grad.data.clamp_(-1, 1)\n      optimizer.step()\n    \n    num_episodes = 400\n    for i_episode in range(num_episodes):\n      # Initialize the environment and state\n      env.reset()\n      last_screen = get_screen()\n      current_screen = get_screen()\n      state = current_screen - last_screen\n      for t in count():\n        # Select and perform an action\n        action = select_action(state)\n        _, reward, done, _ = env.step(action.item())\n        reward = torch.tensor([reward], device=device)\n    \n        # Observe new state\n        last_screen = current_screen\n        current_screen = get_screen()\n        if not done:\n          next_state = current_screen - last_screen\n        else:\n          next_state = None\n    \n        # Store the transition in memory\n        memory.push(state, action, next_state, reward)\n    \n        # Move to the next state\n        state = next_state\n    \n        # Perform one step of the optimization (on the target network)\n        optimize_model()\n        if done:\n          episode_rewards.append(reward)\n          plot_rewards()\n          break\n      # Update the target network, copying all weights and biases in DQN\n      if i_episode % TARGET_UPDATE == 0:\n        target_net.load_state_dict(policy_net.state_dict())\n    \n    print('Complete')\n    env.render()\n    env.close()\n    plt.ioff()\n    plt.show()", "upvote_ratio": 0.6, "id": "t3_lp4sv0", "created_utc": 1613936501.0}
{"sub": "pytorch", "title": "How does one implement parallel SGD with the pytorch autograd RPC library so that gradients can be received from different processes without errors?", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_lp4cok", "created_utc": 1613935211.0}
{"sub": "pytorch", "title": "How to feed output of LSTM into itself?", "selftext": "In almost every text generation context, when a character or word is generated by the LSTM, it is fed back into the LSTM as input for the next character or word generation round. With pytorch LSTM, however, you input the whole sequence at once. How can you make text generation with pytorch then?", "upvote_ratio": 1.0, "id": "t3_loedsu", "created_utc": 1613848599.0}
{"sub": "pytorch", "title": "Writing large amounts of generated data to HDF5 to later be used for training.", "selftext": "I originally asked a question about writing large amounts of data in r/learnpython and was directed towards HDF5 - my new question around that format I think might be better suited here - apologies if not.\n\nI'm currently working on a project involving multichannel audio. The dataset I'm using needs to be processed into the desired target signals. This takes a database of 64 recordings at around 80GB and prodcues target data of around 5TB. I should also add that I'm taking the 64 recordings and splitting them into 30 second chunks, resulting in 1,280 recordings in total. The size of each chunk after processing is 1,440,000  x 93.\n\nWhat's the best way of writing this to a HDF5 file, which means I can then utilise Dataloader to load data in for training in batches and allows me to access the data in such a way that I can index into the appropriate part of the data to be used for each training example. Is it best just to have one HDF5 file and have different subfiles within that, or should I split it down into smaller individual HDF5 files with each containing all the chunks from the parent recording, for example.\n\nWhen it comes to training/test sets, I'm also intending for all the chunks taken from a parent recording to be in the same set. So parent recording \"A\" produces 20 smaller chunks, I wouldn't have those spread across both training and test sets. So being able to store and organise by parent recording is important.\n\nAlong with the target data I'm going to have input data where each example is 1,440,000 x 2 which also needs to be stored.\n\nThe example I've found so far for using dataloader with HDF5 is [here](https://towardsdatascience.com/hdf5-datasets-for-pytorch-631ff1d750f5)\\- but with this method it loads the entire HDF5 file into memory, and I can't do that unless I just have a bunch of seperate HDF5 files for each 30 second clips worth of data. Which seems a pretty inefficient way of doing it and would make sorting them into test/training sets a bit more difficult.\n\nI hope this all makes sense :-)", "upvote_ratio": 1.0, "id": "t3_lo5gm6", "created_utc": 1613821296.0}
{"sub": "pytorch", "title": "NOW AVAILABLE! - 1.2.0 Release of PyTorch Lightning", "selftext": "Lightning 1.2.0 is now available!\n\nSome highlights:\n\n* [#DeepSpeed](https://twitter.com/hashtag/DeepSpeed?src=hashtag_click) integration\n* [@PyTorch](https://twitter.com/PyTorch) autograd Profiler integration\n* PyTorch model pruning, [#Quantization](https://twitter.com/hashtag/Quantization?src=hashtag_click) and SWA\n\nBlogpost: [http://bit.ly/3k4JiOx](https://t.co/qyMd7mfEKr?amp=1)\n\nRelease notes: [http://bit.ly/3udVqBu](https://t.co/bZPJ9wDeg4?amp=1)\n\nhttps://preview.redd.it/8j14a3bhehi61.png?width=1176&amp;format=png&amp;auto=webp&amp;s=975cecae19c282791f615eca4e7f6f19cbd2db1a", "upvote_ratio": 1.0, "id": "t3_lnnikc", "created_utc": 1613761041.0}
{"sub": "pytorch", "title": "[p] No-code PyTorch model builder package just like YOLOv5", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_lnaiaj", "created_utc": 1613721855.0}
{"sub": "pytorch", "title": "Why is mp.spawn spawning 4 processes when I only want 2?", "selftext": "nan", "upvote_ratio": 0.5, "id": "t3_lmx71y", "created_utc": 1613682250.0}
{"sub": "pytorch", "title": "How to parallelize a training loop ever samples of a batch when CPU is only available in pytorch?", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_lmvp5k", "created_utc": 1613678361.0}
{"sub": "pytorch", "title": "HELP: Skorch GridSearchCV best accuracy is nan", "selftext": "I'm trying to use skorch GridSearchCV to fit a custom model that uses dgl.GATConv. I used the SliceDataset to split the dataset to x and y to run GridSearchCV. I added a custom collate() in iterator_train_collate as I did while training the model normally.\n\nHowever on running gs.fit(x,y) every trial takes 0 seconds and the final best accuracy is nan.\n\nWhat do I do to solve this issue?\n\nnet = NeuralNetClassifier(  \n    module=GATconvClassifier,  \n    module__in_dim=24,  \n    module__num_classes=2,  \n    module__residual=True,  \n    module__activation=F.relu,     \n    module__topkf=15,      \n    max_epochs=100,  \n    lr=0.001,  \n    criterion=torch.nn.NLLLoss,  \n    train_split=False,  \n    iterator_train__collate_fn=collate,  \n    iterator_valid__collate_fn=collate,  \n   \n)", "upvote_ratio": 0.5, "id": "t3_lmqaj9", "created_utc": 1613664830.0}
{"sub": "pytorch", "title": "How to add collate_fn when using Skorch?", "selftext": "I need a collate_fn in my data loader for running a normal pytorch code.\n\nNow I plan to use skorch's gridsearchcv() on the same model. But here we pass our dataset as input and label, and it uses the default Data Loader from pytorch. How do I make sure GridSearchCV uses my collate().", "upvote_ratio": 0.67, "id": "t3_lmpv04", "created_utc": 1613663755.0}
{"sub": "pytorch", "title": "Need help passing in 2D Matrix to Conv1D layer and outputting a softmax probability", "selftext": "Hi how's it going? I'm trying to build a model that takes in a 2D Matrix as a single sample and outputs the row index that's the best action by using softmax.\n\nThis is what I have so far:\n\n&amp;#x200B;\n\n'''\n\nnames = \\['Bob','Henry','Mike','Phil'\\]\n\nmax\\_squat = \\[300,400,200,100\\]\n\nmax\\_bench = \\[200,100,225,100\\]\n\nmax\\_deadlift = \\[600,400,300,225\\]\n\nstrongest\\_worker\\_df = pd.DataFrame({'Name':names,'Max\\_Squat':max\\_squat,'Max\\_Bench':max\\_bench,'Max\\_Deadlift':max\\_deadlift})\n\n'''\n\n&amp;#x200B;\n\nhttps://preview.redd.it/0plouuysp3i61.png?width=458&amp;format=png&amp;auto=webp&amp;s=4ee60d6539b4319419aa84dd24e47749f2ac2228\n\n\\`\\`\\`\n\nclass Policy(nn.Module):\n\ndef \\_\\_init\\_\\_(self):\n\nsuper(Policy, self).\\_\\_init\\_\\_()\n\nself.layer1 = torch.nn.Conv1d(in\\_channels=4, out\\_channels=4, kernel\\_size=3, stride=1)\n\n&amp;#x200B;\n\ndef forward(self, x):\n\nx = self.layer1(x)\n\nx = F.softmax(x,dim=1)\n\nreturn x\n\n&amp;#x200B;\n\ndef act(self, state):\n\nstate = state.float()\n\nvalue = self.forward(state)\n\nreturn value\n\n&amp;#x200B;\n\npolicy = Policy()\n\nresult = policy.act(input\\_torch)\n\n\\`\\`\\`\n\nThe result shape is torch.Size(\\[1,4,1\\]). How do I get this to output a column vector instead?\n\n&amp;#x200B;\n\nAlso, is Conv1D with kernel size equal to number of features per row the most logical approach to this input state representation?", "upvote_ratio": 1.0, "id": "t3_lm4fx8", "created_utc": 1613595325.0}
{"sub": "pytorch", "title": "Help saving prediction values as csv !", "selftext": "&amp;#x200B;\n\n[I cant think how to save the prediction values from torch.max  into a csv file has anyone tried this before, Cheers!](https://preview.redd.it/jkd0rr76a3i61.png?width=274&amp;format=png&amp;auto=webp&amp;s=eb71daa508d02c11d86bbc2144272b126b9b8c64)", "upvote_ratio": 1.0, "id": "t3_lm2iop", "created_utc": 1613590178.0}
{"sub": "pytorch", "title": "Video processing for live video using resnet, processing takes longer than each frame lasts", "selftext": " Hi, I have a segmentation Unet based on a resnet that takes a around half a second to execute. I want to be able to use it on a live video feed but of course the execution takes much longer than each frame lasts. I imagine there's a way with multi threading to allow a smooth output albeit with a lag but I need some pointers to how/if this is possible?", "upvote_ratio": 1.0, "id": "t3_lm05gk", "created_utc": 1613583979.0}
{"sub": "pytorch", "title": "What is the most suitable loss function for text summarization?", "selftext": "nan", "upvote_ratio": 0.83, "id": "t3_llupzn", "created_utc": 1613569238.0}
{"sub": "pytorch", "title": "How to use multiprocessing in PyTorch?", "selftext": "nan", "upvote_ratio": 0.25, "id": "t3_ll4rov", "created_utc": 1613485266.0}
{"sub": "pytorch", "title": "Text summarization code giving only padding as output", "selftext": "I am following this code [https://github.com/bentrevett/pytorch-seq2seq](https://github.com/bentrevett/pytorch-seq2seq) for text summarization but the output is always blank padding, how should I proceed to resolve this? I tried to debug it multiple times but the output is always either blank padding or EOS character.", "upvote_ratio": 0.75, "id": "t3_ll2hn8", "created_utc": 1613477285.0}
{"sub": "pytorch", "title": "HELP: How do I get the gradients for a GAT model in dgl??", "selftext": "I found a way to find gradients from a model in PyTorch as shown here: [https://medium.com/datadriveninvestor/visualizing-neural-networks-using-saliency-maps-in-pytorch-289d8e244ab4](https://medium.com/datadriveninvestor/visualizing-neural-networks-using-saliency-maps-in-pytorch-289d8e244ab4)\n\nI tried to recreate this at inference stage as follows:\n\npred, \\_ = model(graph, node\\_features)  \nargmax\\_Y = pred.max(dim=1)\\[1\\]  \nbest\\_pred = pred\\[0, argmax\\_Y\\]  \nbest\\_pred.backward()  \nsaliency= node\\_features.grad  \nprint(saliency)\n\nHowever, the saliency is None\n\nI am completely unaware about how to calculate gradients for graphs. How do I do it?Here, \\`model\\` is a few GATConv layers followed by a Linear layer for Graph classification.\n\nI need these gradients to generate saliency maps.", "upvote_ratio": 1.0, "id": "t3_lkk6gm", "created_utc": 1613415046.0}
{"sub": "pytorch", "title": "Pytorch CNN help?", "selftext": "Hi I am new to using pytorch and cannot for the life of me think how to create a convolutional network for a 2d dataset(time-series) with 14 input channels and 3 possible outputs can anyone point me in the right direction or know of any similar projects? \n\nhttps://preview.redd.it/fcu2q3y0boh61.png?width=1029&amp;format=png&amp;auto=webp&amp;s=e3a9a6a8b74cf7a01f7790f69460bb217b42009c", "upvote_ratio": 0.5, "id": "t3_lkhwl8", "created_utc": 1613408754.0}
{"sub": "pytorch", "title": "[Overview] MLOps: What It Is, Why it Matters, and How To Implement it", "selftext": "Both legacy companies and many tech companies doing commercial ML have pain points regarding:\n\n- Moving to the cloud,\n- Creating and managing ML pipelines,\n- Scaling,\n- Dealing with sensitive data at scale,\n- And about a million other problems.\n\nAt the same time, if we want to be serious and actually have models touch real-life business problems and real people, we have to deal with the essentials like:\n\n- acquiring &amp; cleaning large amounts of data;\n- setting up tracking and versioning for experiments and model training runs;\n- setting up the deployment and monitoring pipelines for the models that do get to production. \n- and we need to find a way to scale our ML operations to the needs of the business and/or users of our ML models.\n\nThis article gives you broad overview on the topic:\n\n[What is MLOps](https://neptune.ai/blog/mlops-what-it-is-why-it-matters-and-how-to-implement-it-from-a-data-scientist-perspective?utm_source=reddit&amp;utm_medium=post&amp;utm_campaign=blog-mlops-what-it-is-why-it-matters-and-how-to-implement-it-from-a-data-scientist-perspective&amp;utm_content=pytorch)", "upvote_ratio": 1.0, "id": "t3_lkh7z1", "created_utc": 1613406858.0}
{"sub": "pytorch", "title": "Gradient with respect to input (Integrated gradients + FGSM attack)", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_lkdfmh", "created_utc": 1613394836.0}
{"sub": "pytorch", "title": "b44e8280 I am worlds him and the one - Pytorch Crypto Art", "selftext": "nan", "upvote_ratio": 0.5, "id": "t3_ljj6vs", "created_utc": 1613284374.0}
{"sub": "pytorch", "title": "Learn numpy before pytorch?", "selftext": "Hello, I\u2019m about to start learning pytorch soon but I\u2019ve read it\u2019s a lot like numpy. Do you need a deep understanding of numpy before using it? I already know how to do general purpose python and have used it for data science so I\u2019m comfortable with the language, but just haven\u2019t used numpy a lot. Is knowing how to build a neural network in numpy a prerequisite for learning pytorch? Or in general a deep understanding of numpy?", "upvote_ratio": 0.93, "id": "t3_ljfhjn", "created_utc": 1613270418.0}
{"sub": "pytorch", "title": "What's good practice for debugging distributed training?", "selftext": "Hi all, first time posting here.\n\nI'd like to get your opinion on some of your best practices when using distributed training (e.g., nn.DistributedDataParallel). I find it a little hard to debug on this distributed setting since it spawns multiple processes. This makes it quite hard to run python debugger (on an IDE or pdb), and even printing outputs is really messy. Although I guess the latter can be resolved by adding a \\`\\`\\`if is\\_main\\_process\\`\\`\\` block every time you print. \n\nIs it best to just keep it to single process + single GPU until debugging is done, then switch to DDP? Or are there other alternatives that are better?\n\nThanks in advance", "upvote_ratio": 1.0, "id": "t3_lj3cck", "created_utc": 1613232641.0}
{"sub": "pytorch", "title": "TheSequence interviews ML practitioners: Jan Beitner, creator of PyTorch Forecasting", "selftext": "#  Hi everyone, TheSequence interviews ML practitioners to merge you into the real world of machine learning and artificial intelligenceHere we spoke with Jan Beitner, creator of PyTorch Forecasting\n\nA few highlights.\n\nWhy haven\u2019t we seen the same level of advancements of time-series forecasting comparing to other domains such as computer vision or language?\n\n**JB**: I believe there are a number of reasons. **First**, we deal with very heterogeneous datasets. Pixels or language have each a common underlying process generating them. Pixels are recordings of light particles and language consists of words. A time-series, on the other hand, could be a stock price, a sensor reading from an IoT device or the sales of a product. For each, the process of generating the data is vastly different. This makes it really difficult to build a model to rule them all.\n\n**Second**, stacking convolutions to understand pixels has revolutionized computer vision, because it exploits the nature of images so well. In time-series forecasting, statistical models are already doing a pretty good job at understanding the nature of the problem. The bar to beat is higher.\n\n**Last but not least**, there is a lack of common benchmarks. Everyone seems to evaluate their model on a different dataset. This is partially because there are so many different applications to time-series forecasting but it also makes it very difficult to spot progress when it happens. I hope that PyTorch Forecasting can contribute to the latter. It aims to provide an interface that makes it easy to apply your algorithm to multiple datasets.\n\nCheck the full interview here:\n\n[https://thesequence.substack.com/p/-jan-beitner-creator-of-pytorch-forecasting](https://thesequence.substack.com/p/-jan-beitner-creator-of-pytorch-forecasting)", "upvote_ratio": 1.0, "id": "t3_lil5om", "created_utc": 1613165091.0}
{"sub": "pytorch", "title": "PyTorch 1.8.0 coming out soon", "selftext": "https://github.com/pytorch/pytorch/issues/51886\n\nHopefully they include cuDNN 8.1.0 with it. Let's get those RTX 3000 speedups.", "upvote_ratio": 1.0, "id": "t3_li583n", "created_utc": 1613110439.0}
{"sub": "pytorch", "title": "CGCNN pytorchGeo", "selftext": "Anybody used CGCNN for time series data? I have few questions.", "upvote_ratio": 1.0, "id": "t3_lhrkg3", "created_utc": 1613069382.0}
{"sub": "pytorch", "title": "My DC-GAN on grayscale face images is not training well.", "selftext": " So I trained by pytorch DC-GAN (deep convolutional GAN) for 30 epochs on grayscale faces, and my GAN pretty much failed. I added batch normalization and leaky relu's to my generator and discriminator (I heard those are ways to make the GAN converge), and the Adam optimizer. My GAN still only putting out random grayscale pixels (nothing even remotely related to faces.) I have no problem with the discriminator, my discriminator works very well. I then implemented weight decay of 0.01 on my discriminator to make my GAN train better (since my discriminator was doing better than my generator) but to no avail. My GAN still generates just random pixels, sometimes outputting completely black.\n\nPlease view my code here: [https://www.kaggle.com/rohjoshi828/emotiongan](https://www.kaggle.com/rohjoshi828/emotiongan)\n\nso that you can give me feedback on how to improve my GAN, because nothing I am trying is working (I once even tried training for 60 epochs but that failed too). Anyway, more more info, the GAN training method I used worked for the MNIST dataset (but I used a way simpler GAN architecture for that.)", "upvote_ratio": 1.0, "id": "t3_lhar51", "created_utc": 1613010687.0}
{"sub": "pytorch", "title": "Minimal implementation of SSD: Single Shot MultiBox Detector", "selftext": "nan", "upvote_ratio": 0.99, "id": "t3_lh6wsz", "created_utc": 1612999241.0}
{"sub": "pytorch", "title": "Retrieval Augmented Generation with Huggingface Transformers, PyTorch, and Ray", "selftext": "nan", "upvote_ratio": 0.81, "id": "t3_lh1rg3", "created_utc": 1612985861.0}
{"sub": "pytorch", "title": "Optuna best trial not reproducible", "selftext": "I'm using optuna for the first time, and after the study was completed, I picked the best parameters as per minimum loss and tried to train the same model again on the same data independently (without optuna).\n\nHowever the lowest val loss on this 2nd training is much higher than that given in the trial.\n\nAny idea on what could be the issue?", "upvote_ratio": 1.0, "id": "t3_lg41ac", "created_utc": 1612879653.0}
{"sub": "pytorch", "title": "I can't find a way to use pytorch for machine learning", "selftext": "I need to train a model using the imagenet dataset. I have a version of imagenet stored in a s3 bucket. I use kaggle or google colab notebooks. I have to access the dataset, which is stored in s3, from the notebooks. I can't find a way of doing it using pytorch.", "upvote_ratio": 0.5, "id": "t3_lfoyn6", "created_utc": 1612826919.0}
{"sub": "pytorch", "title": "State of the art in image manipulation (stylegan)!", "selftext": "nan", "upvote_ratio": 0.8, "id": "t3_lfkowo", "created_utc": 1612815345.0}
{"sub": "pytorch", "title": "GANs implemented in PyTorch", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_lfhr01", "created_utc": 1612807641.0}
{"sub": "pytorch", "title": "UNet encoder/decoder concatenate memory issues", "selftext": "I have a UNet architecture for a GAN which requires to save the downsample tensor results then concatenate them with those of the same size on the upsample.  The only problem is that this requires me to store 8 tensors in memory, which totally kills my batch size to 16 even on a v100.  I don't really want to do double GPUs because it'd get pretty expensive, so is there a better way to organize this or do I just have to deal with it?", "upvote_ratio": 1.0, "id": "t3_lesbia", "created_utc": 1612722829.0}
{"sub": "pytorch", "title": "Implementing a custom optimizer (Video Tutorial)", "selftext": "nan", "upvote_ratio": 0.94, "id": "t3_lenk6o", "created_utc": 1612708459.0}
{"sub": "pytorch", "title": "DDP with model parallelism with multi host multi GPU system", "selftext": "https://pytorch.org/tutorials/intermediate/ddp_tutorial.html#combine-ddp-with-model-parallelism\n\nAbove link explains how to combine distributed data parallel with model parallelism on single machine with multiple GPUs.\n\nIs it possible to do such with multi machine multi GPU (multiple GPUs per machine) system? If so how?", "upvote_ratio": 1.0, "id": "t3_lelxnm", "created_utc": 1612702507.0}
{"sub": "pytorch", "title": "Visualizing activations with forward hooks (Video Tutorial)", "selftext": "nan", "upvote_ratio": 0.88, "id": "t3_le5kap", "created_utc": 1612641902.0}
{"sub": "pytorch", "title": "Is there a wrapper package of pytorch that help to visualize the intermediate layer for the purpose of explainable AI", "selftext": "nan", "upvote_ratio": 0.75, "id": "t3_le2k0z", "created_utc": 1612633491.0}
{"sub": "pytorch", "title": "PyTorch: weight sharing", "selftext": "Hi guys,\n\nhere is part of a code from hugging faces that is support to share the weights of two embedding layers, can someone explain why simply setting .weight from one module to the other shares the parameter? \n\nI'm confused by the way tying the weights work in PyTorch, and there are so many posts that are really confusing.\n\n&amp;#x200B;\n\nthanks", "upvote_ratio": 1.0, "id": "t3_ldiopx", "created_utc": 1612563029.0}
{"sub": "pytorch", "title": "Libtorch - worth it?", "selftext": "Sorry if this is the wrong place to post this, but I could not find a forum for libtorch.\n\nIs learning libtorch worth it? \nDoes anyone use libtorch for training models or for anything for that matter?\n\nAre there any cloud services that provide the environment for this, free providers would be nice!!\n\nThanks in advance for the answers!!\n\nP.S - formatting maybe bad because I am posting from mobile.", "upvote_ratio": 1.0, "id": "t3_ld4ln7", "created_utc": 1612521638.0}
{"sub": "pytorch", "title": "What is the standard way to batch and do a forward pass through tree structured data (ASTs) in pytorch so to leverage the power of GPUs?", "selftext": "nan", "upvote_ratio": 0.81, "id": "t3_lcn8xz", "created_utc": 1612465578.0}
{"sub": "pytorch", "title": "Latest from KDnuggets: Find code implementation for any AI/ML paper using this new chrome extension", "selftext": "nan", "upvote_ratio": 0.72, "id": "t3_lcagui", "created_utc": 1612424103.0}
{"sub": "pytorch", "title": "Latest from google researchers: state of the art in video stabilization!", "selftext": "nan", "upvote_ratio": 0.84, "id": "t3_lc6dh8", "created_utc": 1612409263.0}
{"sub": "pytorch", "title": "Image dataset normalization is one of the most common practises to avoid neural network overfitting but do you know how to calculate the mean and standard deviation of your own custom image dataset?", "selftext": "nan", "upvote_ratio": 0.67, "id": "t3_lc1y56", "created_utc": 1612396118.0}
{"sub": "pytorch", "title": "Pure pytorch before lightning?", "selftext": "Hello, I\u2019ve been using tensorflow for a while but I wanted to try out pytorch. However I can across another framework for pytorch known as pytorch lightning. I was going to start out by learning pure pytorch, but then I realized pytorch lightning is faster to start working with. Do you recommend I start with learning plain vanilla pytorch or (pure pytorch) before jumping into the various frameworks?", "upvote_ratio": 1.0, "id": "t3_lbqpdd", "created_utc": 1612367972.0}
{"sub": "pytorch", "title": "How does one execute an individual patched module using the higher pytorch library?", "selftext": "nan", "upvote_ratio": 0.5, "id": "t3_lb7py6", "created_utc": 1612302934.0}
{"sub": "pytorch", "title": "Model is able to overfit random data, and a small subset of the dataset, but when I use the full dataset the model struggles to train.", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_lb63de", "created_utc": 1612299124.0}
{"sub": "pytorch", "title": "TackleBox - A simple hook management framework for PyTorch", "selftext": "Hi all,\n\nI've been doing research with PyTorch for a while now, and I just packaged up some code that I wrote to handle module hook registration and published it to PyPI. If there are any of you who use module hooks in your work and haven't yet developed an infrastructure for handling them of your own, I'm hoping you'll find it useful.\n\nPlease check out the [github](https://github.com/IsaacRe/tacklebox) for usage documentation. I've also made some wakthrough videos that you can access through the [website](https://isaacrehg.com/tacklebox/). I haven't made a readthedocs for it yet, but was hoping to get some feedback before sinking more time into it.\n\nIf you run into any issues in using it, open up an issue on the github and I'll respond back as quickly as possible.\n\nThanks!", "upvote_ratio": 0.96, "id": "t3_layx25", "created_utc": 1612281529.0}
{"sub": "pytorch", "title": "5950x than RTX 3070 for Deep Learning with PyTorch (CPU vs GPU)", "selftext": "I'm somewhat new to deep learning and I was really surprised to see that training a simple CNN was actually slightly faster on my CPU (97 seconds) vs my GPU (99 seconds).  \nIs this normal? \n\nI play games very rarely and I bought this GPU for some \"light\" deep learning projects and I feel kinda stupid right now.", "upvote_ratio": 0.91, "id": "t3_la70no", "created_utc": 1612196547.0}
{"sub": "pytorch", "title": "Is there a smarter way to preprocess my big dataset in the cloud?", "selftext": "I have this roughly 200 gb dataset of medical images that I get from a zip and unzip them into my vm.  The images are 16 bit so I have to convert them to 8 bit to \"colorize\" the image from its raw state, which I do by just looping over the directories with a separate script and converting the PIL images to 8 bit numpy arrays then resaving them in that form.\n\nOnly problem is that this takes a stupid long amount of time.  I was running this script for maybe 4 hours yesterday on a 50 GB 170k image subset for testing, and it still didn't get close to finishing.  It's really slow because of all the I/O.  I thought of parallelizing it a bit but I don't know if there's some smarter way to do things.\n\nAs far as I know I can't exactly do this process in the transforms since there's some non trivial transformations that I have to do on the images, and I wasn't getting it to work with a custom data transform either.  Do I just have to bit the bullet here and let the script run for a long time until it's done?", "upvote_ratio": 0.81, "id": "t3_l9uhpq", "created_utc": 1612152843.0}
{"sub": "pytorch", "title": "Different results on RTX 2060s vs GTX 1060 with gpt model", "selftext": "Anybody on this subreddit having the same issue?\n\n[https://github.com/pytorch/pytorch/issues/51426](https://github.com/pytorch/pytorch/issues/51426)\n\n## \ud83d\udc1b Bug\nWhen running karpathy minigpt:\n[play_math.ipynb](https://github.com/karpathy/minGPT/blob/master/play_math.ipynb)\nOn my local computer with two GPUs I get different results on my RTX 2060s vs my GTX 1060 with the exact same code, all I do is choose which card is available for CUDA.\nI ran these tests multiple times, always same outcome for both. Also same issue whether I use Ubuntu 18.04 or Windows 10 (dual boot, Ubuntu is not on a VM running inside windows)\n\nGTX 1060 result:\n```\nepoch 50 iter 17: train loss 0.05737. lr 6.000000e-05: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 18/18 [00:00&lt;00:00, 24.47it/s] \n01/31/2021 13:17:58 - INFO - mingpt.trainer -   test loss: 0.004358\nfinal score: 9000/9000 = 100.00% correct\n```\n\nRTX 2060 super result:\n```\nepoch 50 iter 17: train loss 0.04646. lr 6.000000e-05: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 18/18 [00:00&lt;00:00, 23.80it/s]\n01/31/2021 13:24:21 - INFO - mingpt.trainer -   test loss: 0.004733\nfinal score: 9000/9000 = 100.00% correct\nGPT claims that 055 + 045 = 090 (gt is 100; NOPE)\nfinal score: 999/1000 = 99.90% correct\n```\n\n## Expected behavior\nI would expect to be the same\n\n## Environment\n\nPyTorch version: 1.7.1+cu110        \nIs debug build: False\nCUDA used to build PyTorch: 11.0    \nROCM used to build PyTorch: N/A\n\nOS: Microsoft Windows 10 Pro\nGCC version: Could not collect\nClang version: Could not collect\nCMake version: Could not collect\n\nPython version: 3.7 (64-bit runtime)\nIs CUDA available: True\nCUDA runtime version: Could not collect\nGPU models and configuration:\nGPU 0: GeForce GTX 1060 5GB\nGPU 1: GeForce RTX 2060 SUPER\n\nNvidia driver version: 461.40\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\n\nVersions of relevant libraries:\n[pip3] numpy==1.19.5\n[pip3] torch==1.7.1+cu110\n[pip3] torchaudio==0.7.2\n[pip3] torchvision==0.8.2+cu110\n[conda] Could not collect", "upvote_ratio": 1.0, "id": "t3_l9sp4s", "created_utc": 1612147045.0}
{"sub": "pytorch", "title": "Transformers : add src mask to the forward function", "selftext": "Hello community, I'm working on Transformer and did some texting, and was wondering how to add a mask to the transformers to only look into the past, by adding a src mask?\n\n&amp;#x200B;\n\nThank you !", "upvote_ratio": 1.0, "id": "t3_l9igdg", "created_utc": 1612117755.0}
{"sub": "pytorch", "title": "How do I install pytorch on a 32 bit system?", "selftext": "So there is only a 64 bit pytorch wheel, and everytime I try to install pytorch on my 32 bit system it gives an error. When I look up if it is possible, some people on the internet say it is not possible to install pytorch on a 32 bit system. Does anybody have any suggestions for installing pytorch on a 32 bit system: I really need it for local hosting.", "upvote_ratio": 0.5, "id": "t3_l94t6b", "created_utc": 1612068397.0}
{"sub": "pytorch", "title": "GPU", "selftext": "Hello\nGraphics device installed in my laptop is Intel(R) HD\nCan i use GPU to train a image model ? (By using cuda)\nOr how can i check if my graphics devices is compatible for train or not.\n\nI will appreaciate any command \nSisterly", "upvote_ratio": 0.67, "id": "t3_l8ywjs", "created_utc": 1612049757.0}
{"sub": "pytorch", "title": "PyTorch Dataloaders and Transorms", "selftext": "nan", "upvote_ratio": 0.4, "id": "t3_l88xm5", "created_utc": 1611966056.0}
{"sub": "pytorch", "title": "Switch Transformer Single GPU PyTorch implementation/tutorial", "selftext": "Added Switch Transformer implementation to our collection of  deep learning algorithms.\n\nSwitch Transformer routes (switches) tokens among a set  of position-wise feed forward networks based on the token embedding. This allows it to have a many more parameters but use the same amount of compute.\n\nCode with side-by-side notes: [https://nn.labml.ai/transformers/switch/index.html](https://nn.labml.ai/transformers/switch/index.html)\n\nGithub: [https://github.com/lab-ml/nn/blob/master/labml\\_nn/transformers/switch/\\_\\_init\\_\\_.py](https://github.com/lab-ml/nn/blob/master/labml_nn/transformers/switch/__init__.py)\n\nPaper: [https://arxiv.org/abs/2101.03961](https://arxiv.org/abs/2101.03961)", "upvote_ratio": 0.93, "id": "t3_l7wxf1", "created_utc": 1611937866.0}
{"sub": "pytorch", "title": "Serving PyTorch models with TorchServe \ud83d\udd25", "selftext": "Medium Post: [https://alvarobartt.medium.com/serving-pytorch-models-with-torchserve-6b8e8cbdb632](https://alvarobartt.medium.com/serving-pytorch-models-with-torchserve-6b8e8cbdb632)\n\nSource Code: [https://github.com/alvarobartt/serving-pytorch-models](https://github.com/alvarobartt/serving-pytorch-models)", "upvote_ratio": 0.91, "id": "t3_l77c7d", "created_utc": 1611864691.0}
{"sub": "pytorch", "title": "Generating music with PyTorch and HuggingFace", "selftext": "nan", "upvote_ratio": 0.82, "id": "t3_l6xfxv", "created_utc": 1611842806.0}
{"sub": "pytorch", "title": "Really need help here.", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_l6om6s", "created_utc": 1611810324.0}
{"sub": "pytorch", "title": "Constrain outputs in a regression problem", "selftext": " Hi, everyone.\n\nI am attempting to constrain some outputs of my regression network, say x, y, z = model(data), where x, y, z are scalars. The constrain that I want to impose is that when predicting all three dependent variables, the condition \u201cx + y &lt;=1.0\u201d must be honored. Given this description, can I implement this in a forward function?\n\nThank you!", "upvote_ratio": 1.0, "id": "t3_l6borr", "created_utc": 1611773576.0}
{"sub": "pytorch", "title": "Tool for Complex Data Labelling Tasks", "selftext": "Hi /r/pytorch readers!\n\nWe have created a [labelling tool](https://humanlambdas.com/solutions/data-labelling) that can be customized to display all sorts of data models and tasks. Here are a couple of examples for [NLP](https://humanlambdas.com/templates/nlp-news-article-annotation) and [CV](https://humanlambdas.com/templates/computer-vision-annotation). \n\nI hope some of you will find this useful, and if you have any thoughts I would love to hear your feedback!", "upvote_ratio": 0.67, "id": "t3_l67u11", "created_utc": 1611763986.0}
{"sub": "pytorch", "title": "How can I decrease my test loss?", "selftext": "My model is training and while the training loss is decreasing, my test loss is increasing.\n\nI understand that this is because my model is overfitting. There is already a dropout layer in my architecture, so how can I decrease my test loss more?", "upvote_ratio": 0.75, "id": "t3_l4qpws", "created_utc": 1611590417.0}
{"sub": "pytorch", "title": "Transformer and attention mechanism", "selftext": "Hello community, I\u2019m reading the famous \" attention is all you need \" paper, and was wondering:\nDoes   A multi head attention  with only one head , is equivalent to an attention layer ( the classical/basic one ) ?", "upvote_ratio": 0.88, "id": "t3_l455fn", "created_utc": 1611513599.0}
{"sub": "pytorch", "title": "How to easily deploy any PyTorch model to the web?!", "selftext": "[https://medium.com/towards-artificial-intelligence/deploy-deep-learning-models-using-streamlit-and-heroku-22f6efae9141](https://medium.com/towards-artificial-intelligence/deploy-deep-learning-models-using-streamlit-and-heroku-22f6efae9141)\n\nDeploying Deep Learning models with an interactive UI isn't easy. In this hands-on tutorial blog, a NLP model with a very minimal frontend is deployed using Streamlit. The last part of the blog includes steps to deploy the frontend and the backend to the internet using Heroku. A live version is available @ [https://classifyquestions.herokuapp.com.](https://classifyquestions.herokuapp.com/?fbclid=IwAR3ly66YyJ0mZNW_omXLcODQSGcr8P_ARhWfydKaYZl0u4Xk1M3aT0IthwU)\n\nHave a nice read. If you have any feedback or question comment down below.\n\n&amp;#x200B;\n\nhttps://reddit.com/link/l3t9f4/video/f9fq25llt7d61/player", "upvote_ratio": 0.45, "id": "t3_l3t9f4", "created_utc": 1611465794.0}
{"sub": "pytorch", "title": "Want to learn how to train the neural network to classify the images?", "selftext": "nan", "upvote_ratio": 0.38, "id": "t3_l29aqn", "created_utc": 1611267600.0}
{"sub": "pytorch", "title": "Trying to create a batch generator, sorry for the mess. More info in the comments.", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_l25mvl", "created_utc": 1611257166.0}
{"sub": "pytorch", "title": "Wanting to make age detector, but valid loss is high with low accuracy, not more than 46%", "selftext": "I am planning to make an age detector with 10 classes, each of them has range from 2-6 years old, 7-12 years old and so on.I use pretrained model from resnet18. During training, I did not freeze the layers, instead, I just let it update all the parameters. The loss function I am using is cross-entropy, with Adam optimizer and lr-scheduler. The datasets contain 10000 images for training and about 3000 images for validation.\n\nThe problem that keeping me stuck is that although the training loss seems able to continuous decreased, but the validation loss is not. No matter how I change the hyper-parameters, It will drop until minimum 33 and bounce back to 60+, which is the initial value when I started training. The accuracy for validation is only at most 46%\n\nThis is my code. Please have a look at what is causing this problem. Or is there any problem with datasets? Such as not enough datasets for training etc?\n\n`if train_mode:`  \n`train_loader, test_loader = data_load()`  \n `for iteration in range(iteration_start, epoch):`  \n`time_start = time.time()`  \n `for current_mode in ['train', 'valid']:`  \n`total_loss = 0`  \n `total_accuracy = 0`  \n `if current_mode == 'train':`  \n`loader = train_loader`  \n`model.train()`  \n `else:`  \n`loader = test_loader`  \n`model.eval()`  \n\n\n`for batch in loader:`  \n`images, labels = batch`  \n`images = images.to(device)`  \n`labels = labels.to(device)`  \n\n\n`with torch.set_grad_enabled(current_mode == 'train'):`  \n`output = model(images)`  \n`loss = loss_function(output, labels)`  \n`total_loss += loss.item()`  \n `if current_mode == 'train':`  \n`optimizer.zero_grad()`  \n`loss.backward()`  \n`optimizer.step()`  \n `else:`  \n`accuracy = calculate_accuracy(output, labels)`  \n`total_accuracy += accuracy`  \n\n\n`record_loss(total_loss, current_mode, iteration)`  \n `if current_mode == 'valid':`  \n`scheduler.step(total_loss)  # total_loss or total_accuracy, based on which you want to enhance`  \n `record_accuracy(total_accuracy / len(loader), iteration)`\n\nthis is my model\n\n`def custom_model():`  \n`my_model = models.resnet18(pretrained=True)`  \n `# for paras in my_model.parameters():`  \n`#     paras.requires_grad = False`  \n `num_fc_layer = my_model.fc.in_features`  \n`custom_fc_layers = nn.Sequential(`  \n`nn.BatchNorm1d(num_fc_layer),`  \n `nn.Dropout(0.5),`  \n `nn.Linear(num_fc_layer, num_class)`  \n`)`  \n`my_model.fc = custom_fc_layers`  \n\n\n`return my_model`\n\nAnd this is my optimizer and loss function, and scheduler\n\n`if __name__ == '__main__':`  \n`model = custom_model()`  \n`model.to(device)`  \n`optimizer = optim.Adam(model.parameters(), lr=learning_rate)`  \n`scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=1, verbose=True)`  \n`loss_function = nn.CrossEntropyLoss()`  \n`main()`  \n", "upvote_ratio": 1.0, "id": "t3_l20uv2", "created_utc": 1611243737.0}
{"sub": "pytorch", "title": "How to keep GPU from getting full?", "selftext": "(Apologies if this is repeatitive) So I have an RL pipeline, and the GPU RAM is exceeded after a certain epochs. I have already tried using Python's del() function as well as torch.cuda.empty\\_cache()", "upvote_ratio": 0.83, "id": "t3_l20955", "created_utc": 1611241833.0}
{"sub": "pytorch", "title": "Why does Neural Style Transfer work on images with range [0,255] if pytorch models are trained on images with range [0,1]?", "selftext": "[Original paper](https://arxiv.org/abs/1508.06576). The [PyTorch docs](https://pytorch.org/docs/stable/torchvision/models.html) state that all models were trained using images that were in the range of `[0, 1]`. However, there seem to be better results when using images in the range `[0, 255]`:\n\n&amp;#x200B;\n\nConsider this output, which uses the `style loss` described in the original paper. Both set of results use an identical process, but the results on the bottom transform the tensor into the range of `[0, 255]` before applying backpropagation.\n\nhttps://preview.redd.it/5dhpfkpo4ic61.png?width=1776&amp;format=png&amp;auto=webp&amp;s=690d509eb074e27d7a2dbfc0e86cd89ba62823c8\n\nThe results are more visually appealing for `[0, 255]`, and the behavior of the loss is better as well - images in the range of `[0, 1]` reach a nonzero convergence limit, whereas images in the range of `[0, 255]` do not reach this limit for 1000+ epochs.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nWhy does the range of `[0, 255]` work at all? If these models were trained in the range of `[0, 1]`, wouldn't it interpret any pixel above `1` as being purely white?", "upvote_ratio": 0.78, "id": "t3_l1av1b", "created_utc": 1611154955.0}
{"sub": "pytorch", "title": "Any additional books to level up my skill in pytorch?", "selftext": "I'm currently doing the pytorch's tutorial itself and I want something that I can use as a supplement but there so much many books, and I want to narrow it down.", "upvote_ratio": 1.0, "id": "t3_l12yg1", "created_utc": 1611121765.0}
{"sub": "pytorch", "title": "Ways to save your neural network", "selftext": "nan", "upvote_ratio": 0.5, "id": "t3_l0x61e", "created_utc": 1611101883.0}
{"sub": "pytorch", "title": "RecBole: A unified, comprehensive and efficient recommendation library.", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_l0m1fj", "created_utc": 1611070094.0}
{"sub": "pytorch", "title": "RecBole: A unified, comprehensive and efficient recommendation library.", "selftext": "&amp;#x200B;\n\nhttps://preview.redd.it/m9kajptoz8c61.png?width=432&amp;format=png&amp;auto=webp&amp;s=9625b414178899503c944f66b40f391ab72b2a24", "upvote_ratio": 1.0, "id": "t3_l0fiw6", "created_utc": 1611044125.0}
{"sub": "pytorch", "title": "i want remove mosaic of picture by python.what should need for first step of beginner?", "selftext": "i find some libraly. it called gan or tecogan.\n\ni find some tutorial about gan but tecogan tutorial not many.\n\n What should i do at first?", "upvote_ratio": 0.25, "id": "t3_l046bs", "created_utc": 1611004993.0}
{"sub": "pytorch", "title": "A bit of code to implement binary masking for arbitrary models a la Lottery Ticket Hypothesis", "selftext": "I felt that the packages available online to do this were getting ahead of themselves, so I whipped this up. It's a simple wrapper that goes around whatever module you want, keeps track of masks, lets you apply whatever function you want to get to change masks over time, can load the parameters from another model with the same architecture, can be saved and loaded, trained, etc. No fancy work, just use it to wrap an existing model, and away you go.\n\n    def mask_fn(param, old_mask, percentile):\n        if len(param.shape) &gt; 1:\n            absparam = abs(param)\n            new_mask = (absparam &gt; torch.quantile(absparam, percentile)).to(param.dtype)\n            return(new_mask * old_mask)\n        else:\n            return(old_mask)\n    \n    class binary_mask_model(torch.nn.Module):\n        # Becomes a copy of a 'parent model,' with the extra attribute that it holds onto\n        # binary element-wise masks of each parameter, and can apply or modify them at will.\n        # This allows you to easily prune models according to methodologies like the one\n        # presented in \"The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks\"\n        # (Frankle, J. and Carbin, M., 2019)\n        \n        def __init__(self, parent_model):\n            super(binary_mask_model, self).__init__()\n            \n            # Copies the existing model, and adds it to itself\n            self.model = copy.deepcopy(parent_model)\n            self.mask_set = []\n            \n            # Adds, to every parameter in the model, another 1-valued mask tensor of equal shape\n            for param in self.model.parameters():\n                self.mask_set.append(torch.ones(param.shape, dtype = param.dtype, device = param.device))\n    \n        def apply_mask(self):\n            # Multiplies all parameters by their mask\n            for mask, param in zip(self.mask_set, self.model.parameters()):\n                param.data = mask * param.data\n    \n        def forward(self, *args, **kwargs):\n            # Re-masks the tensors in case training changed some 0-values entries, then\n            # executes the exact same forward pass as the parent model\n            if self.training:\n                self.apply_mask()\n            return(self.model(*args, **kwargs))\n            \n        def mod_mask(self, mask_fn = mask_fn, *args, **kwargs):\n            # Applies a function of one's own design to update the masks, and then\n            # masks the parameters again\n            for i, param in zip(range(len(self.mask_set)), self.model.parameters()):\n                self.mask_set[i] = mask_fn(param, self.mask_set[i], *args, **kwargs)\n            self.apply_mask()\n            \n        def load_parameters(self, source_model):\n            # Pulls in the parameters from another model with the same architecture, in case you\n            # want to reload the parent model, or pull from a pretrained model\n            for param, source_param in zip(self.model.parameters(), source_model.parameters()):\n                param.data = source_param.data\n            self.apply_mask()\n            \n    # Example\n    import torchvision.models as models\n    \n    parent_model = models.resnet18().to('cuda')\n    masked_model = binary_mask_model(parent_model)\n    masked_model.mod_mask(percentile = 0.9)\n    torch.save(masked_model, 'mask.torch')\n    loaded_model = torch.load('mask.torch')\n    opt = torch.optim.Adam(loaded_model.parameters(), lr = 0.0005)\n    loss = loaded_model(torch.randn(5, 3, 128, 128, device = 'cuda')).mean()\n    loss.backward()\n    opt.step()", "upvote_ratio": 1.0, "id": "t3_l03udy", "created_utc": 1611004015.0}
{"sub": "pytorch", "title": "Detectron2 Installation | 2021 Tutorial", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_l01sx4", "created_utc": 1610998247.0}
{"sub": "pytorch", "title": "The right way to make custom losses", "selftext": "I noticed that a few codes make their custom losses just as a method while some make an inherited class of nn.Module and define its forward and backward methods. \n\nWhat's the right way to do it?", "upvote_ratio": 1.0, "id": "t3_kxw9y0", "created_utc": 1610723088.0}
{"sub": "pytorch", "title": "Is the amount of GPU on each machine expected to be identical when multi-machine multi-gpu training using distributed.launch ?", "selftext": "I have 8 GPUs on machine 1 and 4 GPUs on machine 2, and I'd like to perform multi-gpu training on all 12 GPUs.  \n\nIs it ok to train on 8 GPUs and 4 GPUs in two machines, despite using the different amount of GPUs on each one?", "upvote_ratio": 1.0, "id": "t3_kxr2cr", "created_utc": 1610702448.0}
{"sub": "pytorch", "title": "ErrorQuestion about NSPopoverBarltemButton", "selftext": "When I try to run OpenAI\u2019 gym CartPole I got this\n\nWarning: Expected min height of view: (&lt;NSPopoverTouchBarItemButton: 0x7fcd151c7b10&gt;) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n\nAnyone got idea what is this error?", "upvote_ratio": 1.0, "id": "t3_kvfudr", "created_utc": 1610410349.0}
{"sub": "pytorch", "title": "Plateau-ing training loss and LOW train_accuracy/test_accuracy", "selftext": "With regards to the thread topic, could anyone help to advise what is wrong with the [loss\\_function() computation logic](https://gist.github.com/promach/52c5199e98647c694163abd0b3af3dae#file-net-py-L149-L173) (especially `policy_output_discrete` and `value_output_discrete`) in my NN ?\n\n&amp;#x200B;\n\n    # Forward Pass\n    policy_output, value_output = net(_board_features_and_turn)\n    \n    # Since both policy_output and value_output are of continuous probability nature,\n    # we need to change them to discrete number for loss_function() computation\n    policy_output_discrete = torch.zeros(len(_score), NUM_OF_POSSIBLE_MOVES, requires_grad=True)\n    if USE_CUDA:\n        policy_output_discrete = policy_output_discrete.cuda()\n    \n    for topk_index in range(len(_score)):  # functionally equivalent to softmax()\n        policy_output_discrete[topk_index][policy_output.topk(1).indices[topk_index]] = 1\n    \n    # substract 1 because score is one of these [-1, 0, 1] values\n    value_output_discrete = torch.topk(value_output, 1).indices - 1\n    \n    # Loss at each iteration by comparing to target(moves)\n    loss1 = loss_function(policy_output_discrete, move)\n    # Loss at each iteration by comparing to target(score)\n    loss2 = loss_function(value_output_discrete, _score)\n    \n    loss = loss1 + loss2\n    \n    # Backpropagating gradient of loss\n    optimizer.zero_grad()\n    loss.backward()\n\n&amp;#x200B;", "upvote_ratio": 1.0, "id": "t3_kusuln", "created_utc": 1610331111.0}
{"sub": "pytorch", "title": "Feedback Transformer PyTorch implementation", "selftext": "Added Feedback Transformer implementation/guide to our collection of neural network architectures/algorithms. Feedback Transformer uses recurrent attention to previous steps, and there for can give fast predictions.\n\nGithub Repo: [https://github.com/lab-ml/nn](https://github.com/lab-ml/nn)\n\nSource code with side-by-side notes: [https://lab-ml.com/labml\\_nn/transformers/feedback/](https://lab-ml.com/labml_nn/transformers/feedback/)", "upvote_ratio": 1.0, "id": "t3_kufh9e", "created_utc": 1610289317.0}
{"sub": "pytorch", "title": "How implicit registration of modules work", "selftext": "Hey Guys,\n\nI have a question which is more about Python than Pytorch. When we assign a module to a member field in the construction (e.g. self.linear = nn.Linear(5,10)) it gets registered implicitly, how does that work?", "upvote_ratio": 1.0, "id": "t3_ku4ngk", "created_utc": 1610243168.0}
{"sub": "pytorch", "title": "Reproduced YOLOv3 based on Pytorch (darknet)", "selftext": "I reproduced YOLOv3 by a single short script.\n\nIt loads the pre-training parameters provided by the darknet official website directly without conversion. This means that a model trained with Darknet can be converted to a Pytorch model using this script.\n\nThe required weight file and test picture are automatically downloaded from the official website, and no other files are dependent.\n\nExcept for the basic library of Python, it only depends on OpenCV and Pytorch 1.7 (including TorchVision).\n\nThe Forward does not use the advanced features of Pytorch, and can be directly Scripted or Traced for further deployment.\n\n[https://gist.github.com/devymex/1f76224b2428d0ddbf92b93def6c587c](https://gist.github.com/devymex/1f76224b2428d0ddbf92b93def6c587c)", "upvote_ratio": 1.0, "id": "t3_kts9zp", "created_utc": 1610204148.0}
{"sub": "pytorch", "title": "PyTorch grid_sample to TensorRT with or without ONNX", "selftext": "Is there a way to convert this layer to TensorRT? ONNX opset doesn\u2019t seem to support it", "upvote_ratio": 1.0, "id": "t3_ktmj24", "created_utc": 1610177676.0}
{"sub": "pytorch", "title": "2-hour tutorial on PyTorch Basics &amp; Gradient Descent | Deep Learning with PyTorch: Part 1 of 6", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_kt17aw", "created_utc": 1610107552.0}
{"sub": "pytorch", "title": "PyTorch Conv-6 CIFAR-10", "selftext": "Hey guys, I have implemented a [Conv-6 CNN CIFAR-10](https://github.com/arjun-majumdar/CNN_Classifications/blob/master/PyTorch_Conv6_CIFAR10.ipynb) classification in PyTorch. I will  be happy to hear your feedback.", "upvote_ratio": 0.17, "id": "t3_kt0nsa", "created_utc": 1610105250.0}
{"sub": "pytorch", "title": "PyTorch convolutional block - CIFAR10 - RuntimeError", "selftext": "I am using PyTorch 1.7 and Python 3.8 with CIFAR-10 dataset. I am  trying to create a block with: conv -&gt; conv -&gt; pool -&gt; fc.  Fully connected layer (fc) has 256 neurons. The code for this is as  follows:\n\n    # Testing- conv1 = nn.Conv2d(in_channels = 3, out_channels = 64, kernel_size = 3, stride = 1, padding = 1, bias = True) \n    conv2 = nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = 3, stride = 1,     padding = 1, bias = True)\n    pool = nn.MaxPool2d(kernel_size = 2, stride = 2)\n    fc1 = nn.Linear(in_features = 64 * 16 * 16, out_features = 256, bias = True)  \n    \n    images.shape\n    # torch.Size([32, 3, 32, 32])\n    x = conv1(images)\n    x.shape\n    # torch.Size([32, 64, 32, 32])\n    x = conv2(x)\n    x.shape\n    # torch.Size([32, 64, 32, 32])\n    x = pool(x)\n    x.shape\n    # torch.Size([32, 64, 16, 16])\n    \n    # This line of code gives error-\n    x = fc1(x)\n\n&gt;RuntimeError: mat1 and mat2 shapes cannot be multiplied (32768x16 and 16384x256)  \n \n\n&amp;#x200B;\n\nWhat's going wrong?", "upvote_ratio": 0.67, "id": "t3_ksxp6o", "created_utc": 1610091009.0}
{"sub": "pytorch", "title": "Custom Regularization in PyTorch", "selftext": "Hello Pytorch enthusiasts, has anyone tried doing custom regularization using pytorch and do you have any recommendations, links to share on how to implement this?", "upvote_ratio": 0.81, "id": "t3_kss820", "created_utc": 1610070141.0}
{"sub": "pytorch", "title": "What is a good cca, cka library for pytorch that works (ideally with GPU)?", "selftext": "nan", "upvote_ratio": 0.6, "id": "t3_ksocf8", "created_utc": 1610058015.0}
{"sub": "pytorch", "title": "Pytorch for beginners", "selftext": "Pytorch is a deep learning framework and a scientific computing package. This is how the PyTorch team defines it. Originally torch was built on Lua programming language and for the ease of use, it is converted in Python by the Facebook AI research teams and many others.\n\n[learn PyTorch chapter_1](https://www.dataspoof.info/post/pytorch-for-beginners-basics\n)", "upvote_ratio": 0.29, "id": "t3_ksfsjx", "created_utc": 1610034219.0}
{"sub": "pytorch", "title": "Tutorial: How to accelerate training using PyTorch with CUDA (notebook, code)", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_kseohr", "created_utc": 1610030879.0}
{"sub": "pytorch", "title": "what is the difference between BertModelLMHeadModel and BertForMaskedLM", "selftext": "In huggingface [https://huggingface.co/transformers/model\\_doc/bert.html](https://huggingface.co/transformers/model_doc/bert.html), there are two models: BertModelLMHeadModel and BertForMaskedLM. What is the difference between these two models? Thanks.", "upvote_ratio": 1.0, "id": "t3_ks7tcs", "created_utc": 1610002509.0}
{"sub": "pytorch", "title": "Issue with train_test_split()", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_krn4pm", "created_utc": 1609936843.0}
{"sub": "pytorch", "title": "Hands-On Guide To Imaginaire: Nvidia Recently Launched GAN Library", "selftext": "nan", "upvote_ratio": 0.79, "id": "t3_krl351", "created_utc": 1609927788.0}
{"sub": "pytorch", "title": "How do I set up the fully connected layers for a Seq2Seq LSTM?", "selftext": "I have a simple LSTM layer and a fully connected later (n_hidden, n_outputs), however I was t to build a Seq2Seq model, where the model takes in a sequence and outputs a sequence. \n\n\nThe model architecture is like:\n\nSelf.lstm = nn.LSTM(n_inp, n_hidden)\nSelf.fc = nn.Linear(n_hidden, n_output) \n\nWith a relu in between. \n\nBut I understand this gives me a 1xn_output vector, but I want a 1 x sequence_length x n_output.\n\nHow would I set up the linear layers", "upvote_ratio": 1.0, "id": "t3_krii24", "created_utc": 1609915909.0}
{"sub": "pytorch", "title": "How to perform spline interpolation of zeroth order?", "selftext": "I have a 3D model, where the authors have used scipy to rescale the image as follows,\n\n    from scipy import ndimage\n    \n    #input_image shape = [D x H x W] \n    out = ndimage.interpolation.zoom(input_image, scale, order=0)\n\nI don't have much knowledge about spline interpolation. I've searched online to understand the topic, and some places it has been hinted that zeroth order is equivalent to 'Nearest Neighbour' interpolation, but nothing concrete has been written. \n\nI tried using nn.functional.interpolate with mode 'nearest' and 'trilinear' but the output image is not the same as when obtained by using scipy.\n\nIs there any way I can perform the above operation in Pytorch?", "upvote_ratio": 1.0, "id": "t3_krgjxy", "created_utc": 1609908296.0}
{"sub": "pytorch", "title": "A Simple Emotion Analysis Application with Pytorch", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_kqxves", "created_utc": 1609850928.0}
{"sub": "pytorch", "title": "Coding Attention is All You Need in PyTorch for Question Classification", "selftext": "Hi Guys,\n\nRecently, I have posted a series of blogs on medium regarding Self Attention networks and how can one code those using PyTorch and build and train a Classification model. In the series, I have shown various approaches to train a classification model for the dataset available [here](https://cogcomp.seas.upenn.edu/Data/QA/QC/).\n\nPart - 1: [https://thevatsalsaglani.medium.com/question-classification-using-self-attention-transformer-part-1-33e990636e76](https://thevatsalsaglani.medium.com/question-classification-using-self-attention-transformer-part-1-33e990636e76)\n\nPart - 1.1: [https://thevatsalsaglani.medium.com/question-classification-using-self-attention-transformer-part-1-1-3b4224cd4757](https://thevatsalsaglani.medium.com/question-classification-using-self-attention-transformer-part-1-1-3b4224cd4757)\n\nPart - 2: [https://thevatsalsaglani.medium.com/question-classification-using-self-attention-transformer-part-2-910b89c7116a](https://thevatsalsaglani.medium.com/question-classification-using-self-attention-transformer-part-2-910b89c7116a)\n\nPart - 3: [https://thevatsalsaglani.medium.com/question-classification-using-self-attention-transformer-part-3-74efbda22451](https://thevatsalsaglani.medium.com/question-classification-using-self-attention-transformer-part-3-74efbda22451)\n\nHave a nice read. Share if you like the content. Comment for any discussions.\n\nThanks", "upvote_ratio": 0.92, "id": "t3_kqxten", "created_utc": 1609850729.0}
{"sub": "pytorch", "title": "Theory to pytorch", "selftext": "Hi everyone,\n\nI want to learn how to program in pytorch while learning the theory of artificial intelligence.\n\n\nDoes anyone know any courses or tutorials which explains the fundamentals of artificial intelligence and also explains the applications of pytorch.\n\n\nFor example, the definition of neural network and everything that comes with it. Afterwards, how to code in pytorch a neural network.", "upvote_ratio": 0.6, "id": "t3_kqq208", "created_utc": 1609819303.0}
{"sub": "pytorch", "title": "3-D Reconstruction of a moving person from a video!", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_kqpbez", "created_utc": 1609816810.0}
{"sub": "pytorch", "title": "Autocomplete Python code with transformers", "selftext": "This is a small project we created to train a character level autoregressive transformer (or LSTM) model to predict Python source code. We trained it on GitHub repositories found on awesome pytorch list.\n\nGithub repo: [https://github.com/lab-ml/python\\_autocomplete](https://github.com/lab-ml/python_autocomplete)\n\nYou can try training on Google Colab: [https://colab.research.google.com/github/lab-ml/python\\_autocomplete/blob/master/notebooks/train.ipynb](https://colab.research.google.com/github/lab-ml/python_autocomplete/blob/master/notebooks/train.ipynb)\n\nHere are some sample evaluations/visualizations of the trained model: [https://colab.research.google.com/github/lab-ml/python\\_autocomplete/blob/master/notebooks/evaluate.ipynb](https://colab.research.google.com/github/lab-ml/python_autocomplete/blob/master/notebooks/evaluate.ipynb)\n\nWorking on a simple VSCode extension to test this out. Will open source it soon on the same repository.", "upvote_ratio": 1.0, "id": "t3_kq84hs", "created_utc": 1609764547.0}
{"sub": "pytorch", "title": "MeanSquaredError() troubles", "selftext": "Hello All. New here! Ive been messing around with this error and I cant seem to get ignite to give me the MSError after each batch.\n\nHere's a few blocks of my code that I'm trying to get to work. Most of it is copied from this link\n\n[https://colab.research.google.com/github/pytorch/ignite/blob/master/examples/notebooks/FashionMNIST.ipynb](https://colab.research.google.com/github/pytorch/ignite/blob/master/examples/notebooks/FashionMNIST.ipynb)\n\n    trainer = create_supervised_trainer(model, optimizer, criterion, device=device)\n    \n    metrics = {\n        'accuracy':Accuracy(),\n        'mse':MeanSquaredError(),\n        'cm':ConfusionMatrix(num_classes=10)\n    }\n    train_evaluator = create_supervised_evaluator(model, metrics=metrics, device=device)\n    val_evaluator = create_supervised_evaluator(model, metrics=metrics, device=device)\n    training_history = {'accuracy':[],'loss':[]}\n    validation_history = {'accuracy':[],'loss':[]}\n    last_epoch = []\n\n&amp;#x200B;\n\n    # creating model, and defining optimizer and loss\n    model = CNN()\n    # moving model to gpu if available\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate) \n    criterion = nn.MSELoss()\n    #criterion = nn.NLLLoss()\n\n&amp;#x200B;\n\n    @trainer.on(Events.EPOCH_COMPLETED)\n    def log_training_results(trainer):\n        train_evaluator.run(train_loader)\n        metrics = train_evaluator.state.metrics\n        accuracy = metrics['accuracy']*100\n        loss = metrics['mse']\n        last_epoch.append(0)\n        training_history['accuracy'].append(accuracy)\n        training_history['loss'].append(loss)\n        print(\"Training Results - Epoch: {}  Avg accuracy: {:.2f} Avg MSE loss: {:.2f} \"\n              .format(trainer.state.epoch, accuracy, loss))\n    \n    def log_validation_results(trainer):\n        val_evaluator.run(val_loader)\n        metrics = val_evaluator.state.metrics\n        accuracy = metrics['accuracy']*100\n        loss = metrics['mse']\n        validation_history['accuracy'].append(accuracy)\n        validation_history['loss'].append(loss)\n        print(\"Validation Results - Epoch: {}  Avg accuracy: {:.2f} Avg MSE loss: {:.2f} \"\n              .format(trainer.state.epoch, accuracy, loss))\n        \n    trainer.add_event_handler(Events.EPOCH_COMPLETED, log_validation_results) \n\nAnd my model looks like this\n\n# ----------------------------------------------------------------Layer (type)               Output Shape         Param\n\nConv2d-1           \\[64, 64, 28, 28\\]             576\n\nReLU-2           \\[64, 64, 28, 28\\]               0\n\nMaxPool2d-3           \\[64, 64, 14, 14\\]               0\n\nLinear-4                 \\[64, 6000\\]      75,270,000\n\nLinear-5                 \\[64, 1200\\]       7,201,200\n\nLinear-6                  \\[64, 120\\]         144,120\n\nLinear-7                   \\[64, 10\\]           1,210\n\n&amp;#x200B;\n\nBut i get this error\n\n&amp;#x200B;\n\n    RuntimeError: The size of tensor a (10) must match the size of tensor b (64) at non-singleton dimension 1\n\nAnd I have no idea how to fix it. My main goal is to find the MSError after each batch, and this works if my criterion is NLLLoss but not with MSE. I'm not sure if its a pytorch or an ignite issue. Any help would be greatly appreciated!", "upvote_ratio": 0.5, "id": "t3_kq34be", "created_utc": 1609742076.0}
{"sub": "pytorch", "title": "How do I perform transfer learning on a model with different number of outputs?", "selftext": "I have a model which is train on 14 labels.\nI have to fine tune the model on a new set of data with 19 labels. The weights of the initial training are there, but I'm unable to load these weights, as the output size is different.\n\nDo I just save the weights of the previous model without the output layer, load that into the new model and then explicitly attach a new output layer for training?\n\nI'm still new to this, a little confused.", "upvote_ratio": 1.0, "id": "t3_kpot4i", "created_utc": 1609695487.0}
{"sub": "pytorch", "title": "HyperLSTM PyTorch implementation", "selftext": "Added HyperLSTM (introduced in paper HyperNetworks by Ha et al.) implementation with explanations to our collection of implementations of neural network architectures/algorithms. HyperLSTM uses a smaller LSTM network (hyper network) to alter (row-wise scale) parameters of the actual LSTM. That is, the parameters of the LSTM change at each step.\n\nSource code with side-by-side notes: [https://lab-ml.com/labml\\_nn/hypernetworks/hyper\\_lstm.html](https://lab-ml.com/labml_nn/hypernetworks/hyper_lstm.html)\n\nGithub Repo: [https://github.com/lab-ml/nn](https://github.com/lab-ml/nn)", "upvote_ratio": 1.0, "id": "t3_kpjvzc", "created_utc": 1609677242.0}
{"sub": "pytorch", "title": "Gradient backpropagation over transformation operations", "selftext": "I feed an image as an input to a pre-trained cnn model after applying the following transformation operations to the image.\n\n    self.transform = transforms.Compose([\n        transforms.Resize(352, 352),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406],\n                             [0.229, 0.224, 0.225])])\n\nI want to calculate the gradients over these operations. One method that has been suggested is to use the [kornia](https://kornia.readthedocs.io/en/latest/introduction.html) library. But I is there a way I can do this by adding another layer at the beginning of the pre-trained unet model that perform the same transformations.\n\nTill now I have tried the following methods to create a custom layer and then add it using nn.Sequential() method\n\n1) By modifying the source code of [kornia.enhance.normalize](https://kornia.readthedocs.io/en/latest/_modules/kornia/enhance/normalize.html#normalize) method.\n\n    class MyModel(nn.Module):\n    \n        def __init__(self):\n            super().__init__()\n    \n        def forward(self, input):\n            image = input.view(3,-1)\n            mean = image.mean(1)\n            std = image.std(1)\n    \n            if mean.shape:\n                mean = mean[..., :, None, None].to(input.device)\n    \n            if std.shape:\n                std = std[..., :, None, None].to(input.device)\n    \n            out = (input - mean) / std\n    \n            return out.unsqueeze(0)\n    \n    myModel = MyModel()\n    new_model = nn.Sequential(myModel, unet)\n\n2) By simply adding a BatchNorm Layer and modifying the mean and standard deviation parameters of that layer\n\n    new_model = nn.Sequential(nn.BatchNorm2d(3), unet)\n\n3) \n\n    class MyModel(nn.Module):\n      def __init__(self, mean, std):\n        super().__init__()\n        self.mean = mean\n        self.std = std\n    \n      def forward(self, image):\n        img = transforms.Normalize(self.mean, self.std)(image)\n        img = img.unsqueeze(0)\n        return img\n\nIn all the methods, the image wasn't even transformed in the first place (the unnormalized input was passed onto the second layer).\n\nHow can I  \n1) Transform the image inside the model  \n2) Back-propogate the gradients during inference.", "upvote_ratio": 1.0, "id": "t3_kpjinv", "created_utc": 1609675395.0}
{"sub": "pytorch", "title": "Neural Network for tic tac toe", "selftext": "The [inference coding](https://github.com/promach/mcts/blob/main/play.py) gave out repeated wrong results, it seems like the model output trained by the [training code](https://github.com/promach/mcts/blob/main/Net.py) is wrong.  Any idea why ?\n\n&amp;#x200B;\n\nline 64 of the inference coding : \n\n    next_move = np.binary_repr(next_move_probabilities.argmax())\n\nalways give the same result this means the trained model is definitely wrong  \n\n\nCould anyone advise ?", "upvote_ratio": 0.67, "id": "t3_ko9u7p", "created_utc": 1609500503.0}
{"sub": "pytorch", "title": "Issue of recent updates with RL algorithms.", "selftext": " It seems the newer versions of Pytorch are giving errors with certain Deep RL implementations, especially those involving a common network stem bracnhed to give two different outputs( like in Actor Critic methods). Appreciate any help, and please ask for further details if required.\n\nThis is the error -\n\nRuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation:\n\nDowngrading to Pytorch &lt;1.4.0 doesn't give the same error, hence the issue since all the implementation seem to be based on earlier versions.", "upvote_ratio": 1.0, "id": "t3_knoaf3", "created_utc": 1609413450.0}
{"sub": "pytorch", "title": "why using log_sum_exp in calculating forward features in BLSTM-CRF", "selftext": " \n\nIn the pytorch implementation of BLSTM-CRF tutorial ([https://pytorch.org/tutorials/beginner/nlp/advanced\\_tutorial.html#advanced-making-dynamic-decisions-and-the-bi-lstm-crf](https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html#advanced-making-dynamic-decisions-and-the-bi-lstm-crf)), log-sum-exp operation is used to calcualted forward features, which is lised below\n\n        def _forward_alg(self, feats):\n            # Do the forward algorithm to compute the partition function\n            init_alphas = torch.full((1, self.tagset_size), -10000.)\n            # START_TAG has all of the score.\n            init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n    \n            # Wrap in a variable so that we will get automatic backprop\n            forward_var = init_alphas\n    \n            # Iterate through the sentence\n            for feat in feats:\n                alphas_t = []  # The forward tensors at this timestep\n                for next_tag in range(self.tagset_size):\n                    # broadcast the emission score: it is the same regardless of\n                    # the previous tag\n                    emit_score = feat[next_tag].view(\n                        1, -1).expand(1, self.tagset_size)\n                    # the ith entry of trans_score is the score of transitioning to\n                    # next_tag from i\n                    trans_score = self.transitions[next_tag].view(1, -1)\n                    # The ith entry of next_tag_var is the value for the\n                    # edge (i -&gt; next_tag) before we do log-sum-exp\n                    next_tag_var = forward_var + trans_score + emit_score\n                    # The forward variable for this tag is log-sum-exp of all the\n                    # scores.\n                    alphas_t.append(log_sum_exp(next_tag_var).view(1))\n                forward_var = torch.cat(alphas_t).view(1, -1)\n            terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n            alpha = log_sum_exp(terminal_var)\n            return alpha\n    \n\nThe log\\_sum\\_exp operation on the calculated features is in the line\n\n                   alphas_t.append(log_sum_exp(next_tag_var).view(1)) \n\nMy question is that why log\\_sum\\_exp operation is needed? Thanks.", "upvote_ratio": 1.0, "id": "t3_knnpj0", "created_utc": 1609410463.0}
{"sub": "pytorch", "title": "Guide To GluonTS and PytorchTS For Time-Series Forecasting (With Python Implementation)", "selftext": "nan", "upvote_ratio": 0.91, "id": "t3_kn74yw", "created_utc": 1609351217.0}
{"sub": "pytorch", "title": "Building pytorch on rocm", "selftext": "Hi everyone, I am trying to build pytorch from the rocm github. Using the script to transpile CUDA to ROCm is working, but when compiling it fails linkink libtorch_hip.so and c++ tells me that -E or -x is required when the input is feom the standard input. Is there anyone that can help me ?", "upvote_ratio": 0.75, "id": "t3_kn6xqa", "created_utc": 1609350598.0}
{"sub": "pytorch", "title": "How, specifically, does the pre-fetching in DataLoaders work?", "selftext": "I'm putting something together where, in one process, I am training a model, and in another process, I am loading samples from disk, performing pre-processing, and assembling my input and label tensors. I currently have code that will do this, so that the moment my model finishes with one batch, bam, the next batch is ready to go.\n\nThe thing is, I'm wondering whether or not this pre-fetching is something that is already implemented in the dataloaders? I saw some things about \"pre-fetch factors\" in the source code, but I'm not super certain how that works when it comes to actually enumerating the dataloader, if it does all the pre-fetching right when you enumerate it, if each individual batch is being pre-fetched while the model runs, and is delivered when needed, etc.\n\nI went through some of the source code, but I couldn't really make heads or tails of it. So if somebody could explain to me what's actually going on under the hood with dataloaders, I would appreciate it greatly.", "upvote_ratio": 0.43, "id": "t3_kmo5k4", "created_utc": 1609279635.0}
{"sub": "pytorch", "title": "Very basic pytorch installation question", "selftext": "Hey, \n\nI am dabbling with AI to create visuals for bands and found this repo:  \n[https://github.com/JCBrouwer/maua-stylegan2](https://github.com/JCBrouwer/maua-stylegan2)  \nI would love to get it to run, but finally my lack of pytorch knowledge stands in my way.\n\nWhen I try to run generate\\_audiovisual.py (on Windows, with a 2080TI GPU), ninja starts building. And fails. I have cl.exe and gcc on my machine, but first of all, I think it should not even compile anything at all as I instlled pytorch\n\n    pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 torchaudio===0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n\nI have spent a lot of time with this already, maybe I am missing something that is obvious to you.\n\nThanks in advance!", "upvote_ratio": 1.0, "id": "t3_kmggsf", "created_utc": 1609255771.0}
{"sub": "pytorch", "title": "List concatenation in Pytorch", "selftext": "Hi, I have two lists containing 3d tensors, any idea how to merge them in PyTorch. [torch.cat](https://torch.cat) is causing problem.", "upvote_ratio": 1.0, "id": "t3_kman4y", "created_utc": 1609231472.0}
{"sub": "pytorch", "title": "Add normalization layer in the beginning of a pretrained model", "selftext": "I have a pretrained UNet model with the following architecture\n\n    UNet(\n      (encoder1): Sequential(\n        (enc1conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (enc1norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (enc1relu1): ReLU(inplace=True)\n        (enc1conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (enc1norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (enc1relu2): ReLU(inplace=True)\n      )\n      (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (encoder2): Sequential(\n        (enc2conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (enc2norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (enc2relu1): ReLU(inplace=True)\n        (enc2conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (enc2norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (enc2relu2): ReLU(inplace=True)\n      )\n      (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (encoder3): Sequential(\n        (enc3conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (enc3norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (enc3relu1): ReLU(inplace=True)\n        (enc3conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (enc3norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (enc3relu2): ReLU(inplace=True)\n      )\n      (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (encoder4): Sequential(\n        (enc4conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (enc4norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (enc4relu1): ReLU(inplace=True)\n        (enc4conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (enc4norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (enc4relu2): ReLU(inplace=True)\n      )\n      (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (bottleneck): Sequential(\n        (bottleneckconv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bottlenecknorm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (bottleneckrelu1): ReLU(inplace=True)\n        (bottleneckconv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bottlenecknorm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (bottleneckrelu2): ReLU(inplace=True)\n      )\n      (upconv4): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n      (decoder4): Sequential(\n        (dec4conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (dec4norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (dec4relu1): ReLU(inplace=True)\n        (dec4conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (dec4norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (dec4relu2): ReLU(inplace=True)\n      )\n      (upconv3): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n      (decoder3): Sequential(\n        (dec3conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (dec3norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (dec3relu1): ReLU(inplace=True)\n        (dec3conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (dec3norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (dec3relu2): ReLU(inplace=True)\n      )\n      (upconv2): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n      (decoder2): Sequential(\n        (dec2conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (dec2norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (dec2relu1): ReLU(inplace=True)\n        (dec2conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (dec2norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (dec2relu2): ReLU(inplace=True)\n      )\n      (upconv1): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n      (decoder1): Sequential(\n        (dec1conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (dec1norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (dec1relu1): ReLU(inplace=True)\n        (dec1conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (dec1norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (dec1relu2): ReLU(inplace=True)\n      )\n      (conv): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n    )\n\nThe model takes an input image which has been normalized using min-max normalization. I want add a batch/layer norm layer before the first layer which does the same work for me (i.e. normalize the input image) so that I can feed the original image as it is.\n\nEdit: Made changes clarifying the aim of this question.", "upvote_ratio": 1.0, "id": "t3_kmalfc", "created_utc": 1609231253.0}
{"sub": "pytorch", "title": "How to know what is the required parameters to specify?", "selftext": "Say I am building a neural network using torch.nn. \n\nHow do I know which parameters I need to specify in the argument call for each object layer?", "upvote_ratio": 1.0, "id": "t3_kld488", "created_utc": 1609108514.0}
{"sub": "pytorch", "title": "PyTorch NN for Numerical Inputs: Questions", "selftext": "Hi, I'm *very* new to PyTorch and neural networks as a whole so excuse this post.\n\nMy goal is to implement a machine learning algorithm that can predict a specific single numerical value. The inputs from which it learns from are NumPy arrays; each data entry is as follows:\n\narray 1 | array 2 | numerical value\n\nCan I easily make a neural network in PyTorch that will learn from arrays (filled with numerical values) and have it predict what the numerical value will be if I made a test set of other arrays?\n\nAnything helps,\n\nThanks.", "upvote_ratio": 0.75, "id": "t3_kkw674", "created_utc": 1609039392.0}
{"sub": "pytorch", "title": "Why do Conv2d layers have 2 parameters?", "selftext": "I've created a network with a single Conv2d layer. Printing out the length of the parameters shows that it has 2 parameters. Printing out the parameter object shows that one of them is the matrix I would expect, a large tensor with size corresponding to the layer's number of input channels, output channels, and convolution kernel; then I also see an unexpected 2nd tensor of size 1x(num\\_output\\_channels). \n\nWhat is the purpose of this 2nd tensor? Isn't a convolution layer completely defined by the first tensor?", "upvote_ratio": 0.84, "id": "t3_kkuwyk", "created_utc": 1609034477.0}
{"sub": "pytorch", "title": "Keras4Torch: A Ready-to-Use Wrapper for Training PyTorch Models\u2728", "selftext": "`keras4torch` is a high-level API like pytorch-lightning. It is designed for beginners who are new to pytorch but familar with Keras, then reduce the cost of migration.\n\n[https://github.com/blueloveTH/keras4torch](https://github.com/blueloveTH/keras4torch)\n\nHere is [an example of MNIST](https://nbviewer.jupyter.org/github/blueloveTH/keras4torch/blob/main/tutorials/MNIST_example.ipynb).\n\nThis project was developed when I'm migrating models from tf.keras to pytorch. `keras4torch` provides NumPy workflow conforming to Keras interfaces as much as possible. There is also a DataLoader workflow for more flexible usage.\n\nThe stable version of `keras4torch` can be installed via pip now. You can fork it for further development or modify it as a template with no limitations.\n\nWelcome to take a look at our [repo page](https://github.com/blueloveTH/keras4torch) and documentations.\n\nThanks for reading!", "upvote_ratio": 0.78, "id": "t3_kkh51u", "created_utc": 1608981862.0}
{"sub": "pytorch", "title": "Trying to add a new trainable variable to a layer I defined for ResNet", "selftext": "Thank you!", "upvote_ratio": 1.0, "id": "t3_kj63v6", "created_utc": 1608774004.0}
{"sub": "pytorch", "title": "Are there ways to construct a network to improve accuracy of derivatives?", "selftext": "I'm working on a problem that requires me to have a neural network produce accurate results were the accuracy of the derivatives of network output with respect to the network input are also equally as important in terms of accuracy.\n\nIt there a particular network architecture, acitavation function, or normalization scheme that can aid in this problem?", "upvote_ratio": 0.5, "id": "t3_kj380c", "created_utc": 1608763876.0}
{"sub": "pytorch", "title": "Breadcrumbs on PyTorch Hanging", "selftext": "I am running a pretty straightforward DRN training loop using openai gym, and my training loop is hanging and there really aren't any clues coming from PyTorch as to why it hangs. I am sure it is GPU related because when I set the device to cuda:0 it hangs and I have to kill the process. Does anyone have any hits for figuring out why this may be happening?  \n\n\n    PyTorch version: 1.7.1\n    Is debug build: False\n    CUDA used to build PyTorch: 10.2\n    ROCM used to build PyTorch: N/A\n    \n    OS: Ubuntu 20.04.1 LTS (x86_64)\n    GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0\n    Clang version: Could not collect\n    CMake version: version 3.16.3\n    \n    Python version: 3.7 (64-bit runtime)\n    Is CUDA available: True\n    CUDA runtime version: 10.1.243\n    GPU models and configuration: GPU 0: GeForce RTX 2080 Ti\n    Nvidia driver version: 460.27.04\n    cuDNN version: Probably one of the following:\n    /usr/lib/x86_64-linux-gnu/libcudnn.so.8.0.5\n    /usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.0.5\n    /usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.0.5\n    /usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.0.5\n    /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.0.5\n    /usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.0.5\n    /usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.0.5", "upvote_ratio": 1.0, "id": "t3_kj0aza", "created_utc": 1608754268.0}
{"sub": "pytorch", "title": "Gradient backpropagation through transformation operations.", "selftext": "Say, I have an input image `img` and I apply transformations to this image \n\n    transform = transforms.Compose([transforms.ToTensor(),\n                                    tansforms.Normalize(mean, std)])\n\nIs there a way I can backpropagate the gradients over these transformations?", "upvote_ratio": 1.0, "id": "t3_kiux96", "created_utc": 1608736844.0}
{"sub": "pytorch", "title": "LabML: Monitor PyTorch Lightening model training from a smartphone", "selftext": "For those who are not familiar, LabML ([https://github.com/lab-ml/labml](https://github.com/lab-ml/labml)) is a little library and an app that let you monitor model training on a mobile web app. We just implemented a callback for PyTorch lightening, and you can integrate it with a single line of code.\n\nHere are lightening MNIST samples modified with labml callback: [https://github.com/lab-ml/samples/tree/master/labml\\_samples/lightening](https://github.com/lab-ml/samples/tree/master/labml_samples/lightening)", "upvote_ratio": 0.96, "id": "t3_kit02y", "created_utc": 1608729792.0}
{"sub": "pytorch", "title": "Pytorch does not load the model properly", "selftext": "I want to train a character-level language model and use it to do a rescoring. For that purpose, I took a github code that contains all the necessary files, so that I can just train it on my data.\n\nAfter I train the model and try to do text generation, the output seems alright, but if I load the trained model and do text generation, the output seems random.\n\nHere is an example output: \"jgJty&amp;JWJ[C1HW1KJWJ&amp;?&amp;lKRx\".\n\nI am saving the model the following way: `torch.save(model.state_dict(), 'lm.pt')`\n\nand then loading it as:\n`model = CharRNN(chars, n_hidden, n_layers).cuda()`\n\n `model.load_state_dict(torch.load('lm.pt'))`\n\n `model.eval()`\n\nI have successfully saved and loaded models this way many times, but for some reason it doesn't work with the language model.\n\nI have also tried printing the `model.state_dict()` after training and after loading the model and they look the same.\n\nSaving the model with `torch.save(model, 'lm.pt')` and then doing:\n`model = torch.load('lm.pt')` seems to work but I don't want to use that.\n\nMy suspicion is that the `state_dict` does not have all the weights, so when I load the model some of them are randomly initialized, but I am not sure.\n\nDoes anyone know what might be the issue and how to solve it?", "upvote_ratio": 1.0, "id": "t3_kismui", "created_utc": 1608728368.0}
{"sub": "pytorch", "title": "lstm example?", "selftext": "Hiya\n\n&amp;#x200B;\n\nI'm trying to find a full lstm example where it demonstrates how to predict tomorrow's (or even a week's) future result  of whatever based on the past data used in training.\n\nI seem to find many examples of people getting training data and splitting it, training and then using the last N% to \"predict\" - which seems incorrect as you already have the data that you normally wouldn't have. I can build  an lstm but just need that little bit to show me how to use it to forecast the future\n\n&amp;#x200B;\n\nany suggestions would be most welcome", "upvote_ratio": 1.0, "id": "t3_kinba8", "created_utc": 1608702459.0}
{"sub": "pytorch", "title": "JIT the collate function in Pytorch", "selftext": "I need to create a DataLoader where the collator function would require to have non trivial computation, actually a double layer loop which is significantly slowing down the training process. For example, consider this toy code where I try to use numba to JIT the collate function:\n\n    import torch\n    import torch.utils.data\n    \n    import numba as nb\n    \n    \n    class Dataset(torch.utils.data.Dataset):\n        def __init__(self):\n            self.A = np.zeros((100000, 300))\n            self.B = np.ones((100000, 300))\n        \n        def __getitem__(self, index):\n            return self.A[index], self.B[index]\n        \n        def __len__(self):\n            return self.A.shape[0]\n    \n    @nb.njit(cache=True)\n    def _collate_fn(batch):\n        batch_data = np.zeros((len(batch), 300))\n        for i in range(len(batch)):\n            batch_data[i] = batch[i][0] + batch[i][1]\n    \n        return batch_data\n\nand then I create the DataLoader as follows:\n\n    train_dataset = Dataset()\n    \n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=256,\n        num_workers=6,\n        collate_fn=_collate_fn,\n        shuffle=True)\n\nHowever, this just gets stuck but works fine if I remove the JITing of the \\_collate\\_fn. I am not able to understand what is happening here. I don't have to stick to numba and can use anything which will help me overcome the loop inefficiencies in Python. TIA and Happy 12,021", "upvote_ratio": 1.0, "id": "t3_ki8bpk", "created_utc": 1608653753.0}
{"sub": "pytorch", "title": "Guide to Pytorch Time-Series Forecasting", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_ki3c74", "created_utc": 1608635413.0}
{"sub": "pytorch", "title": "ValueError: Expected target size (16, 87), got torch.Size([16, 64, 87]) in CrossEntropyLoss", "selftext": "Trying a many to many LSTM for learning purposes. Code snippet:\n\n\\`\\`class model(nn.Module):  \ndef \\_\\_init\\_\\_(self, BATCH\\_SIZE, SEQ\\_LEN, vocab\\_size):  \nsuper(model, self).\\_\\_init\\_\\_()  \nself.batch\\_size = BATCH\\_SIZE  \nself.seq\\_len = SEQ\\_LEN  \nself.vocab\\_size = vocab\\_size  \nself.emb = nn.Embedding(vocab\\_size, 512)  \nself.lstm = nn.LSTM(512, 256, 3, dropout = 0.2)  \nself.lin = nn.Linear(256, 87)  \nself.criterion = nn.CrossEntropyLoss()\n\ndef forward(self, X, Y):  \nout = self.emb(X)  \nh, c = self.lstm(out)  \nout = self.lin(h)  \nloss = self.criterion(out, Y)\n\nreturn loss\\`\\`\n\n&amp;#x200B;\n\nBATCH\\_SIZE = 16  \nSEQ\\_LEN = 64  \nvocab\\_size = 87\n\nSize of X: \\[16, 87\\]  \nSize of Y = \\[16, 64, 87\\]  \nSize of out = \\[16, 64, 87\\]\n\nStill I get the above error in the \\`\\`loss = self.criterion(out, Y)\\`\\` line. I can't understand why. Please help.\n\n&amp;#x200B;\n\nRef: [https://github.com/ranasingh-gkp/Music\\_generation\\_char-RNN](https://github.com/ranasingh-gkp/Music_generation_char-RNN)", "upvote_ratio": 1.0, "id": "t3_khg3tj", "created_utc": 1608552208.0}
{"sub": "pytorch", "title": "Trouble connecting to TPUs using XLA library", "selftext": "Hey, again, \n\nI'm trying to connect to the TPU's accessible on google colab, and having some trouble.  My end goal is to install the TPU's so I can use Pytorch Lightning to run the training on the TPU. \n\nCurrently, my code to import the XLA library (needed to connect to TPU from Pytorch)  is returning a really cryptic error. \n\n&amp;#x200B;\n\nCode to install XLA: \n\n`#\u00a0install\u00a0XLA\u00a0to\u00a0allow\u00a0connection\u00a0between\u00a0Pytorch\u00a0and\u00a0TPU`  \n`VERSION\u00a0=\u00a0\"20200325\" #@param\u00a0[\"1.5\"\u00a0,\u00a0\"20200325\",\u00a0\"nightly\"]`  \n`!curl\u00a0https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py\u00a0-o\u00a0pytorch-xla-env-setup.py`  \n`!python`\u00a0[`pytorch-xla-env-setup.py`](https://pytorch-xla-env-setup.py)\u00a0`--version\u00a0$VERSION`\n\n&amp;#x200B;\n\nCode to import XLA: \n\n`#\u00a0imports\u00a0pytorch`  \n`import\u00a0torch`  \n`#\u00a0imports\u00a0the\u00a0torch_xla\u00a0package`  \n`import\u00a0torch_xla`  \n`import\u00a0torch_xla.core.xla_model\u00a0as\u00a0xm`\n\n&amp;#x200B;\n\nError being returned: \n\n `--------------------------------------------------------------------------- ImportError                               Traceback (most recent call last)` [`&lt;ipython-input-66-ebe519c076f6&gt;`](https://localhost:8080/#) `in &lt;module&gt;()3        4 # imports the torch_xla package ----&gt; 5 import torch_xla       6 import torch_xla.core.xla_model as xm`  [`/usr/local/lib/python3.6/dist-packages/torch_xla/__init__.py`](https://localhost:8080/#) `in &lt;module&gt;()      39 import torch      40 from .version import __version__ ---&gt; 41 import _XLAC      42       43 _XLAC._initialize_aten_bindings()  ImportError: /usr/local/lib/python3.6/dist-packages/_XLAC.cpython-36m-x86_64-linux-gnu.so: undefined symbol: _ZN6caffe28TypeMeta21_typeMetaDataInstanceISt7complexIfEEEPKNS_6detail12TypeMetaDataEv` \n\n&amp;#x200B;\n\nI copied this import code straight from the [official guide](https://colab.research.google.com/github/pytorch/xla/blob/master/contrib/colab/getting-started.ipynb#scrollTo=42avAvSg17by) which works for me, but just doesn't work on my own document. \n\nHere's my [Colab document](https://colab.research.google.com/drive/1EaCJpyYK8xLuzO79ESAsIlGZAcWjOh8u?usp=sharing), if anyone is wondering. The XLA stuff is at the very top.\n\nThank you! \n\nA", "upvote_ratio": 1.0, "id": "t3_kgkg08", "created_utc": 1608426425.0}
{"sub": "pytorch", "title": "torchMTL: A simple multi-task learning module for PyTorch", "selftext": "Hey everyone!\n\nI wrote a small helper library to make multi-task learning with PyTorch easier: [torchMTL](https://github.com/chrisby/torchMTL). You just need to define a dictionary of layers and torchMTL builds a model that returns the losses of the different tasks that you can then combine in the standard training loop. \n\nI'd be happy to get some feedback on it!", "upvote_ratio": 1.0, "id": "t3_kgeb73", "created_utc": 1608405713.0}
{"sub": "pytorch", "title": "carefree-learn: Tabular Datasets \u2764\ufe0f PyTorch", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_kfprkx", "created_utc": 1608313352.0}
{"sub": "pytorch", "title": "More audio feature transformations in pytorch?", "selftext": "I've seen that in pytorch you can do the short time fourier transform through the torch module. Can you/are there plans for more direct transformations like audio to mel frequency cepstrum, chroma, etc.?", "upvote_ratio": 1.0, "id": "t3_kfldc1", "created_utc": 1608299121.0}
{"sub": "pytorch", "title": "Applying transforms to both image and mask", "selftext": "I have an image segmentation task but a very small dataset. I want to use data augmentation, but I can\u2019t seem to find a way to apply the same transformations to images and masks. \n\nAny help would be appreciated!", "upvote_ratio": 1.0, "id": "t3_keuyhs", "created_utc": 1608200518.0}
{"sub": "pytorch", "title": "15 minute training time per epoch limiting ability to improve algorithm... any way to speed it up?", "selftext": "Hi All, \n\nFor one my first solo machine learning project I'm trying to create an algorithm that can distinguish between six remarkably similar species of bird (for those wanting to search it up, the Willow Flycatcher, Pacific-Slope Flycatcher, Least Flycatcher, Hammond's Flycatcher, Western-Wood Pewee and Olive-sided Flycatcher).  I'm using data from Flickr and making a CNN from \"scratch\" (in scratch I mean using pytorch tools but not transferring from a premade model)  \n\nI have exactly 2000 images per my six classes. Since I did not have the ability to access a larger database (at least, yet), I was only able to get about 600-1000 unique images per class. I created a function that would automatically fill the gaps in to 2000 with augmented data. \n\nAlone, it takes about 55 minutes of runtime for all the data to be loaded AND for the augmented data to be added. \n\nOnce that is done, and all the other stuff is done, training can begin. My images are quite large (256 by 256), which might slow me down.  I'm going to add dropout in a later phase of the experiment (looking over the training and validation losses shows that I'm not overfitting anyways). \n\nMy convolutional model is as follows.\n\n`model\u00a0=\u00a0nn.Sequential(`  \n   \n `#\u00a0before,\u00a0(bs,\u00a03,\u00a0256,\u00a0256)`  \n\u00a0\u00a0\u00a0\u00a0`nn.Conv2d(3,\u00a016,\u00a0kernel_size=3,\u00a0stride=1,\u00a0padding=1),`  \n\u00a0\u00a0\u00a0\u00a0`nn.ReLU(),`\u00a0  \n\u00a0\u00a0\u00a0\u00a0`nn.MaxPool2d(2,\u00a02),\u00a0#\u00a0output\u00a0will\u00a0be\u00a0(bs,\u00a016,\u00a0128,\u00a0128)`  \n\u00a0\u00a0\u00a0\u00a0`nn.Conv2d(16,\u00a016,\u00a0kernel_size=3,\u00a0stride=1,\u00a0padding=1),`  \n\u00a0\u00a0\u00a0\u00a0`nn.ReLU(),`  \n\u00a0\u00a0\u00a0\u00a0`nn.MaxPool2d(2,\u00a02),\u00a0#ouput\u00a0will\u00a0be\u00a0(bs,\u00a016,\u00a064,\u00a064)`  \n\u00a0\u00a0\u00a0\u00a0`nn.Conv2d(16,\u00a016,\u00a0kernel_size=3,\u00a0stride=1,\u00a0padding=1),`  \n\u00a0\u00a0\u00a0\u00a0`nn.ReLU(),`\u00a0  \n\u00a0\u00a0\u00a0\u00a0`nn.MaxPool2d(2,\u00a02),\u00a0#\u00a0output\u00a0is\u00a0(bs,\u00a016,\u00a032,\u00a032)`  \n\u00a0\u00a0\u00a0\u00a0`nn.Conv2d(16,\u00a016,\u00a0kernel_size=3,\u00a0stride=1,\u00a0padding=1),`  \n\u00a0\u00a0\u00a0\u00a0`nn.ReLU(),`\u00a0  \n\u00a0\u00a0\u00a0\u00a0`nn.MaxPool2d(2,\u00a02),\u00a0#\u00a0output\u00a0is\u00a0(bs,\u00a016,\u00a016,\u00a016)`  \n\u00a0\u00a0\u00a0\u00a0`nn.Conv2d(16,\u00a016,\u00a0kernel_size=3,\u00a0stride=1,\u00a0padding=1),`  \n\u00a0\u00a0\u00a0\u00a0`nn.ReLU(),`\u00a0  \n\u00a0\u00a0\u00a0\u00a0`nn.MaxPool2d(2,\u00a02),\u00a0#\u00a0output\u00a0is\u00a0(bs,\u00a016,\u00a08,\u00a08)`\u00a0  \n\u00a0\u00a0\u00a0\u00a0`nn.Conv2d(16,\u00a016,\u00a0kernel_size=3,\u00a0stride=1,\u00a0padding=1),`  \n\u00a0\u00a0\u00a0\u00a0`nn.ReLU(),`\u00a0  \n\u00a0\u00a0\u00a0\u00a0`nn.MaxPool2d(2,\u00a02),\u00a0#\u00a0output\u00a0is\u00a0(bs,\u00a016,\u00a04,\u00a04)`\u00a0  \n\u00a0\u00a0\u00a0\u00a0`nn.Conv2d(16,\u00a016,\u00a0kernel_size=3,\u00a0stride=1,\u00a0padding=1),`  \n\u00a0\u00a0\u00a0\u00a0`nn.ReLU(),`\u00a0  \n\u00a0\u00a0\u00a0\u00a0`nn.MaxPool2d(2,\u00a02),\u00a0#\u00a0output\u00a0is\u00a0(bs,\u00a016,\u00a02,\u00a02)`\u00a0  \n\u00a0\u00a0\u00a0\u00a0`nn.Conv2d(16,\u00a016,\u00a0kernel_size=3,\u00a0stride=1,\u00a0padding=1),`  \n\u00a0\u00a0\u00a0\u00a0`nn.ReLU(),`\u00a0  \n\u00a0\u00a0\u00a0\u00a0`nn.MaxPool2d(2,\u00a02),\u00a0#\u00a0output\u00a0is\u00a0(bs,\u00a016,\u00a01,\u00a01)`\u00a0  \n `#\u00a0connected\u00a0layer`  \n\u00a0\u00a0\u00a0\u00a0`nn.Flatten(),\u00a0#\u00a0output\u00a0a\u00a0bs,\u00a016\u00a0size\u00a0vector`  \n\u00a0\u00a0\u00a0\u00a0`nn.Linear(16,\u00a06)\u00a0#\u00a0output\u00a0a\u00a0bs\u00a0x\u00a06`  \n`)`\n\nAt  batch size = 81, I've been taking exactly 15 minutes for each epoch to train.  Since I'm on colab, which has usage limits for GPUs, this means getting about 13 or 14 epochs max before I reach my daily limit of GPU usage. \n\nLooking at the accuracy, it seems as though if I am able to run a few more epochs, I will be able to get a better score accuracy on the validation set. \n\n&amp;#x200B;\n\n[Seems to be a steady increase in accuracy.](https://preview.redd.it/2u1o8bmnlp561.png?width=517&amp;format=png&amp;auto=webp&amp;s=51e5f368ab2fe522e9a6b5b0d276d0c935b41cf6)\n\nIf randomly picked, the accuracy of identifying the right bird would be around 16%. The highest accuracy I've gotten so far is 41%, on the 13th epoch. Colab kicked me after that one, so I couldn't find out if I could have gone higher.\n\nAlso, my losses if you'd like:\n\n&amp;#x200B;\n\nhttps://preview.redd.it/1cq3z890mp561.png?width=527&amp;format=png&amp;auto=webp&amp;s=4825f567cc03c8c558228cfd60b292aba5f05ab3\n\nAs well as that, the long training time makes it hard to make changes to the algorithm and immediately see their effect - I'm worried that this'll make improving upon the 41% impossible! \n\nI have a few ideas (mainly, reducing image size) but I'm worried that will reduce the accuracy, since the differentiating features between the birds are so minute.\n\nThanks for reading this the whole way through, and if you'd have any suggestions on cutting the time, that'd be much appreciated! \n\n**edit:** just found out I was adding augmented data before splitting which most likely raised the accuracies on the val set. The question still stands, though. \n\nThanks, \n\nA", "upvote_ratio": 1.0, "id": "t3_kett98", "created_utc": 1608194727.0}
{"sub": "pytorch", "title": "How do I concat 4 images to be the last layer for a ResNet for transfer learning?", "selftext": "I have about 400 samples. I need to do a binary classification task. Each sample comprises 4 images (all images have a single channel). \n\nI am planning to use transfer learning with ResNet18 and just retrain the last layer. \n\nI want to concatenate these 4 images as my last layer. Can someone tell me how to do it?\n\nSay, each of my images is - (1, 120, 90). So how do I concatenate 4 such images so that they can be used as the last layer of a Resnet?\n\nPardon me but I am not well versed with computer vision.", "upvote_ratio": 1.0, "id": "t3_ketdgw", "created_utc": 1608192492.0}
{"sub": "pytorch", "title": "Paid ML gigs: Get compensated while further sharpening your skills on your own schedule.", "selftext": "nan", "upvote_ratio": 0.5, "id": "t3_kenk99", "created_utc": 1608170055.0}
{"sub": "pytorch", "title": "deep regression", "selftext": "Have people been using deep learning to do regression? I noticed that fitting polynomials using least squares leads to much better accuracy! Is there any rule of thumb to get arbitrary accuracy with deep regression?", "upvote_ratio": 0.5, "id": "t3_keco2b", "created_utc": 1608137057.0}
{"sub": "pytorch", "title": "How to check test accuracy on every n train epochs using pytorch lightning?", "selftext": "I have searched pytorch lightning docs but only found ways for finding metrics on train data for every n epochs.", "upvote_ratio": 1.0, "id": "t3_kebauw", "created_utc": 1608132777.0}
{"sub": "pytorch", "title": "PyTorch recommenders", "selftext": "Hi!\n\nI came across this [tensorflow](https://www.tensorflow.org/recommenders/) wrapper and it has some really nice guides and helpers to get going with building recommender systems - is there anything similar in the PyTorch ecosystem?\n\nThanks!", "upvote_ratio": 1.0, "id": "t3_ke9wfo", "created_utc": 1608127984.0}
{"sub": "pytorch", "title": "[P] NLP Tutorial PyTorch", "selftext": "I have put together an nlp tutorial in pytorch, check it out: [https://github.com/will-thompson-k/deeplearning-nlp-models](https://github.com/will-thompson-k/deeplearning-nlp-models) . Would love some feedback!", "upvote_ratio": 0.94, "id": "t3_ke8n72", "created_utc": 1608123163.0}
{"sub": "pytorch", "title": "[beginners tutorial] Guide to Pytorch Loss Functions + How to Build Custom Functions", "selftext": "The way you configure your loss functions can either make or break the performance of your algorithm.\n\nBy correctly configuring the loss function, you can make sure your model will work how you want it to.\n\nA few key things to learn before you can properly choose the correct loss function are:\n\n- What are loss functions and how to use them in PyTorch?\n- Which loss functions are available?\n- How to create a custom loss function?\n\nHere\u2019s our tutorial that will help you:\n\n[PyTorch loss functions](https://neptune.ai/blog/pytorch-loss-functions?utm_source=reddit&amp;utm_medium=post&amp;utm_campaign=blog-pytorch-loss-functions&amp;utm_content=pytorch)", "upvote_ratio": 0.9, "id": "t3_kdp5bd", "created_utc": 1608050545.0}
{"sub": "pytorch", "title": "Testing Framework", "selftext": "Hi everybody!\n\nI was wondering if there's a testing framework out there for PyTorch that you have used? For the past several months, I've been working with an existing codebase that has massive networks and huge datasets. I've been making changes to implement some new models. Running simple experiments just to ensure that everything *runs* without breaking due to size mismatches can take a while. The other day, I finished an epoch (took several hours) and the code bugged on some logging functionality that wasn't updated for the changes that were made to the model. \n\nI'm a Ph.D. student now but I came from industry in software engineering. We were spoiled for choice in terms of testing frameworks for anything we could ever write. I'm wondering if there are testing frameworks that you've used to make sure that your model *can* run before actually running it.", "upvote_ratio": 1.0, "id": "t3_kd2dk4", "created_utc": 1607967906.0}
{"sub": "pytorch", "title": "Why is PyTorch filling the GPU memory?", "selftext": "I am working for a uni project on an implementation of the sliding window approach for object detection. For each frame I take the image, unfold it and run all the patches through a CNN classifier.\n\nIt was working fine until I changed the classifier model and the GPU memory started filling the 4GB (laptop) in a matter of seconds. I have been trying to debug the allocation/deallocation of tensors but I can't really understand what is taking so much space.\n\nBy proceeding one line at a time with the debugger I noticed that once I call\n\n    classifier(patches) \n\nthe memory usage jumps by +200MB on the first call, +1000MB on the second and by the third 3.5GB are used. This does not depend on the scope, once I exit the function the memory usage does not decrease. How can I check what is kept in memory? Is it storing some king of history? Can I disable it somehow since I am in eval mode? Many thanks", "upvote_ratio": 1.0, "id": "t3_kctr3x", "created_utc": 1607934235.0}
{"sub": "pytorch", "title": "Does Pytorch source code contains facebook telemetry codes?", "selftext": "\\[I love pytorch\\]\n\nBut bit more free software activist, I worry if Pytorch contains some survellience features\\]", "upvote_ratio": 0.64, "id": "t3_kc6u6f", "created_utc": 1607846282.0}
{"sub": "pytorch", "title": "Free browser extension for ML community that thousands of machine learning engineers/data scientists use everyday! Drop a comment for any questions/feature requests you may have!", "selftext": "nan", "upvote_ratio": 0.5, "id": "t3_kc2z6u", "created_utc": 1607828773.0}
{"sub": "pytorch", "title": "Can I calculate gradient w.r.t. the input?", "selftext": "I have a classifier module (nn.Linear) and I want the weights to be static, but treat the input vector as parameters to be updated. So I will still minimize the crossentropy(output, target), but the result discovers the values for an input that approximately minimizes the classifier loss. Is calculating the gradient w.r.t the input the right way to describe that?", "upvote_ratio": 0.9, "id": "t3_kbl2hq", "created_utc": 1607758413.0}
{"sub": "pytorch", "title": "Inference using a single thread is faster than using 4 threads on a raspberry pi", "selftext": "I have been running a model on a raspberry pi 3 A+ and got curious performance results.\n\nFor example an inference using resnet18 on a 256x256 image:\n\n* 4 threads:  dt = 1.869842\n* 1 thread: dt  = 1.510674\n\nUsing mobilenetv2 on a 256x256 image:\n\n* 4 threads: dt = 2.509571\n* 1 thread: dt = 1.208802\n\nUsing resnet18 on a 512x512 image:\n\n* 4 threads: dt = 7.229154\n* 1 thread: dt = 6.206228\n\nUsing mobilnetv2 on a 512x512 image:\n\n* 4 threads: dt = 4.412468\n* 1 thread: dt = 3.804896\n\n&amp;#x200B;\n\nThe raspberry 3 chip has 4 cores, so I would expect a better performance when using all the cores. Has anyone experience something similar and know of any tricks to get a better performance using all cores?", "upvote_ratio": 1.0, "id": "t3_kb5ovd", "created_utc": 1607703151.0}
{"sub": "pytorch", "title": "Is it possible to convert the type of the output tensor from float to int in a custom loss function?", "selftext": "Hi! I\u2019m working on a segmentation model, and I am using a custom dice loss. I\u2019m working on medical scans and I realised that the output doesn\u2019t quite perform well and demarcates the textured area of the image rather than the smooth area. And the smooth area is the ROI.\nI was thinking maybe if I convert the output to int type, then only a few areas will be highlighted and a larger loss will be generated to penalise the model.\n\nBut as gradients are involved, I\u2019m unable to convert the dtype of the output which is a GPU tensor to int. How do I do it?", "upvote_ratio": 1.0, "id": "t3_kb2ko8", "created_utc": 1607692566.0}
{"sub": "pytorch", "title": "How to get the autograd backward graph with shape information in Pytorch\uff1f", "selftext": "nan", "upvote_ratio": 0.5, "id": "t3_kau6vd", "created_utc": 1607655442.0}
{"sub": "pytorch", "title": "[P] Pytorch NLP Models (run w/ GPUs)", "selftext": "I've put together a small, annotated library of deeplearning models used in NLP here:\n\n[https://github.com/will-thompson-k/deeplearning-nlp-models](https://github.com/will-thompson-k/deeplearning-nlp-models)\n\n&amp;#x200B;\n\n[BERT: Reading. Comprehending.](https://preview.redd.it/bly0gs3rvg461.jpg?width=320&amp;format=pjpg&amp;auto=webp&amp;s=84c9f0873c2ac9ef393f8f47943cd32f2550427b)\n\n&amp;#x200B;\n\n[Attention patterns ](https://preview.redd.it/0aeyi9vuvg461.png?width=1074&amp;format=png&amp;auto=webp&amp;s=29ddaabc11e1a8e2b0e1272a9bb45f43d70fed3f)\n\nIt's by no means comprehensive, but meant as a primer for those delving into model architectures. Let me know if you have any feedback!", "upvote_ratio": 1.0, "id": "t3_katg8g", "created_utc": 1607652926.0}
{"sub": "pytorch", "title": "PyTorch 1.7.1 - Bug fix release with updated binaries for Python 3.9 and cuDNN 8.0.5", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_kakwez", "created_utc": 1607624905.0}
{"sub": "pytorch", "title": "LSTM ENCODING", "selftext": "Hello community,\nLet\u2019s say I have a data shape of =( 1300,60).\n\nI applied windowing on it  with a sequence length of =20   to the data, and the shape become =(1297,20,60).\nMy goal is to compress this entity like this:\n(1297,20,60) -&gt; (1297,1,3) \nKnowing that I apply a LSTM layer at the beginning.\nIn summary :\n1. Reshape the data using windowing \n2. Apply an LSTM Layer\n3. Apply an appropriate layer to compress lstm output into (1297,1,3)\n\nDo you know any layers which can apply such a thing  to compress data ?", "upvote_ratio": 1.0, "id": "t3_kaitem", "created_utc": 1607618878.0}
{"sub": "pytorch", "title": "Generate new training data with StyleGAN2 ada ?", "selftext": "Hello, I want to increase my dataset for a project where I do semantic segmentation of plants. Do you think that I can generate new images with a StyleGAN2 ada model? And that it would be high quality enough to help my semantic segmentation model with additional training data?\n\nHave anyone done anything similar ?", "upvote_ratio": 1.0, "id": "t3_kacefq", "created_utc": 1607592433.0}
{"sub": "pytorch", "title": "[P] Deeplearning NLP Models Tutorial in PyTorch (w/ Colab GPU Notebooks)", "selftext": "I've put together a small, annotated library of deeplearning models used in NLP here:\n\n[https://github.com/will-thompson-k/deeplearning-nlp-models](https://github.com/will-thompson-k/deeplearning-nlp-models)\n\n&amp;#x200B;\n\n[BERT: Reading. Comprehending.](https://preview.redd.it/sfx5yhm8r9461.jpg?width=320&amp;format=pjpg&amp;auto=webp&amp;s=4856b694e87ddb9e5aea056c9802e1ac3a12a662)\n\nIt's by no means comprehensive, but meant as a primer for those delving into model architectures. Let me know if you have any feedback!", "upvote_ratio": 0.92, "id": "t3_ka6c1b", "created_utc": 1607566579.0}
{"sub": "pytorch", "title": "Hands-on Vision Transformers with PyTorch - Analytics India Magazine", "selftext": "https://zcu.io/FRBx", "upvote_ratio": 0.75, "id": "t3_k9q8xc", "created_utc": 1607513312.0}
{"sub": "pytorch", "title": "Can someone help me? Can a Generative Adversarial Network (GAN) predict a new state?", "selftext": "I have a problem where I need to get a new state of an object (e.g. the position) given the current state. However, I am not really sure if I can use GAN for this process. I already know that a basic GAN helps us to learn to generate samples from a given dataset. Nevertheless, I don't know if it is possible to predict a new state or states that change in time. If you knew some paper or information about that, it will very useful for me.", "upvote_ratio": 1.0, "id": "t3_k9mf1w", "created_utc": 1607494029.0}
{"sub": "pytorch", "title": "Generative models for time-series data", "selftext": "Hi, guys! I hope you are staying safe and well!\n\nI am currently looking for papers and blogs (with codes if possible) that describe how to use the generative models (e.g. GANs) for time-series data.\n\nCan you please share recourses about the abovementioned topic if you know any?\n\nThanks a lot in advance!", "upvote_ratio": 0.84, "id": "t3_k9igvh", "created_utc": 1607479286.0}
{"sub": "pytorch", "title": "How to check for membership of elements of one Tensor in another?", "selftext": "Say I have the following two tensors\n\na = torch.Tensor([\n\n    [1, 2, 3, 4],\n\n    [5, 6, 7, 8],\n\n    [9, 10, 11, 12],\n\n])\n\nb = torch.Tensor([3, 3, 10])\n\nIs there some kind of \"in\" function that I can use to get this output?\n\n[True, False, True] (or [1, 0, 1])\n\nBasically the first element of b is in the first element of A, and the second element of b is not in the second element of A, ...", "upvote_ratio": 1.0, "id": "t3_k8qhre", "created_utc": 1607378058.0}
{"sub": "pytorch", "title": "Archai for NAS", "selftext": "Archai is a platform for Neural Network Search (NAS) that allow you to generate efficient deep networks for your applications. \n\n[https://github.com/microsoft/archai](https://github.com/microsoft/archai)", "upvote_ratio": 0.83, "id": "t3_k8jncc", "created_utc": 1607358033.0}
{"sub": "pytorch", "title": "Manual MSE vs BinaryCrossEntropy", "selftext": "I have an errors function that looks like:\n\n```\ndef errors(x, y):\n    err = (x - y).pow(2).sum(dim=1)\n    return err\n```\n\nI pass to it an array of x values and a corresponding y labels array\n\nHowever if I implement it with BCE:\n```\ndef errors(x, y):\n    err = F.binary_cross_entropy(x, y)\n    return err\n```\n\nI get a single value. Is it possible to implement BCE on per row value?", "upvote_ratio": 0.84, "id": "t3_k7the3", "created_utc": 1607260130.0}
{"sub": "pytorch", "title": "Do I need to save the loss function if I'm checkpointing my model during training?", "selftext": "Before I begin training, I initialize my loss function like this\n\nloss_func = torch.nn.CrossEntropyLoss()\n\nThen during the training I use\n\nloss = loss_func(logits, train_y)\n\nIf I want to checkpoint my model during the training process by saving to file and resuming at the same point later, do I need to save loss_func too, or can I re-initialize with a clean slate and see the same behavior. Basically I'm curious if the loss_func object updates some attributes when it's used so it behaves differently on further uses, which I would want to save if that does happen.\n\nOn a similar note I am wondering if I can use the same loss function object with different models being trained at the same time without them interfering with each other, and I guess answering my original question will give me an answer to this too.", "upvote_ratio": 1.0, "id": "t3_k7hnvz", "created_utc": 1607208931.0}
{"sub": "pytorch", "title": "Pytorch using Simulink model inputs", "selftext": "I'm trying to perform SL or RL on a super simple model to predict the gain of a transfer function. I can use pandas and the Matlab python engine to correctly pass inputs to my model and extract workspace variables back out. However, then I try and wrap my NN around this training data, my gradients never update and my rained solution is essentially identical to my inputs, which is nonsensical. I'm wondering if anyone has experience performing SL or RL using pytorch that calls a Simulink model or if there is literature (examples) i could search for reference?", "upvote_ratio": 1.0, "id": "t3_k7h6o9", "created_utc": 1607207236.0}
{"sub": "pytorch", "title": "The amount of boilerplate and imperative magic-behind-the-scenes is infuriating in pytorch", "selftext": "I think everything could be so much simpler with a more functional-programming style approach. I haven't coded in a mostly imperative style in ages - and suddenly it hits me just how confusing an imperative approach can be.  \n\n\nThe main pytorch tutorial is written like that, and the installation/dependencies is yet another pita to resolve.  \n\n\nI'm really optimistic about pytorch/ML in general, but yeah this could be so much better and we would be able to reach a much bigger audience.", "upvote_ratio": 0.4, "id": "t3_k79ulh", "created_utc": 1607183785.0}
{"sub": "pytorch", "title": "I want to make my decoder weights equal the transpose of my encoder part", "selftext": "Hello community,  I want to reduce overfitting of my auto encoder because I don't have enough data to train on, and I found that making the decoder weight = transpose of the encoder weight will increase the efficiently of my model.\n\nHow can I do so in my model ?", "upvote_ratio": 1.0, "id": "t3_k6pkza", "created_utc": 1607103640.0}
{"sub": "pytorch", "title": "Can't use num_workers &gt; 0 with CUDA", "selftext": "I'm currently using the following code:\n\n```\ndataloader = DataLoader(dataset=data, batch_size=batch_size, shuffle=True, num_workers=0)\n```\n\nIf I enable CUDA with my model it works. But the moment I set `num_workers` to a value `&gt; 0`, it produces the following error:\n\n```\nRuntimeError: Caught RuntimeError in DataLoader worker process 0.\n```\n\nIs this a known bug?\n\nPerformance is just as good as CPU with this setup. But I can confirm that the GPU is running with `nvidia-smi`\n\nStack trace:\n\n```\nTraceback (most recent call last):\n  File \"/home/ubuntu/anaconda3/envs/research/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/home/ubuntu/anaconda3/envs/research/lib/python3.8/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/home/ubuntu/workspace/pyntrainer/pyntrainer/__main__.py\", line 107, in &lt;module&gt;\n    net.train(training_data, epochs=epochs, lr=lr, batch_size=batch_size, loss=\"aml\")\n  File \"/home/ubuntu/workspace/pyntrainer/pyntrainer/./lib/autoencoder.py\", line 153, in train\n    for i, (inputs, labels) in enumerate(dataloader):\n  File \"/home/ubuntu/anaconda3/envs/research/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 345, in __next__\n    data = self._next_data()\n  File \"/home/ubuntu/anaconda3/envs/research/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 856, in _next_data\n    return self._process_data(data)\n  File \"/home/ubuntu/anaconda3/envs/research/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 881, in _process_data\n    data.reraise()\n  File \"/home/ubuntu/anaconda3/envs/research/lib/python3.8/site-packages/torch/_utils.py\", line 394, in reraise\n    raise self.exc_type(msg)\nRuntimeError: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/ubuntu/anaconda3/envs/research/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/ubuntu/anaconda3/envs/research/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/ubuntu/anaconda3/envs/research/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in &lt;listcomp&gt;\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/ubuntu/workspace/pyntrainer/pyntrainer/./lib/./abstract_dataset.py\", line 10, in __getitem__\n    return self.x[index], self.x[index]\nRuntimeError: CUDA error: initialization error\n```", "upvote_ratio": 0.84, "id": "t3_k6o3yh", "created_utc": 1607099235.0}
{"sub": "pytorch", "title": "Augmenting Image Dataset Based Upon Distance", "selftext": "Hello all, I am doing a project which involves me to get a comprehensive image set via augmentation, although the one which is the most hard for me to do (if possible) is augmenting based on distance. \n\nBasically, I want to be able to take a set of images which is considered to be like a seed for augmentation, and augment them based on distance to make it seem that the image is farther away from the point of the picture (along with other factors to augment them on).\n\nI have been looking through torchvision.transforms for a viable transformation, but I can't seem to find one. I was wondering for those who may know more about different image processing/computer graphics algorithms to know if this is possible for me to do, or if I will have to go through the work of getting more data from different distances.", "upvote_ratio": 0.8, "id": "t3_k5m8ps", "created_utc": 1606955157.0}
{"sub": "pytorch", "title": "Neural network on CIFAR-10 and GPU showing wildly different accuracies on different sessions", "selftext": "Newbie to PyTorch.\n\nI'm following along with the third in this tutorial online [https://www.youtube.com/watch?v=GIsg-ZUy0MY&amp;ab\\_channel=freeCodeCamp.org](https://www.youtube.com/watch?v=GIsg-ZUy0MY&amp;ab_channel=freeCodeCamp.org) for one-layer neural network with the MNIST dataset. I decided to try out the same thing for practice on the CIFAR-10 dataset, which can be accessed directly through the pytorch library.\n\nI'm working in a google colab document and wrote a bunch of notes for myself. I worked my way through the tutorial pretty well.\n\nI ran it a bunch of times,  and only got accuracies of around 35%, a little worse than even logistic regression on the same dataset.\n\nThen, I tinkered around, I think I had forgot to switch to the GPU - that was the issue - and replayed, and was able to get my algorithm to a much better 67% accuracy, and after tinkering with the learning rate, I settled at 0.0005 being the best.\n\nBut now, when I run my algorithm, even on the GPU, I get stuff around the 35% (sometimes lower) mark. I looked in my diff, and even *restored the version that I was getting 67% on,* but when I ran it now, I got low accuracy.\n\n[Heres my Colab Doc](https://colab.research.google.com/drive/14lhvXjjOHeHoVrQYjGxffsiNlB9dbncz?usp=sharing)\n\nI'm pretty confused as to what this may be - any ideas? More importantly, any ideas on how to start getting high accuracies again?\n\nThanks a lot!\n\na", "upvote_ratio": 0.86, "id": "t3_k4wsd2", "created_utc": 1606864808.0}
{"sub": "pytorch", "title": "need help with cnn pytorch", "selftext": "Hello mates,\n\nIm trying to do a cnn for chord recognition based on a paper that i found. Im a noob so im stuck.For now i have this:\n\n    class CNN(nn.Module):\n        def __init__(self):\n            super(CNN, self).__init__()\n            in_channels=1\n            out_channels=1\n            kernel_size=(1,16,6,25)\n            self.conv1 = nn.Sequential(nn.Conv3d(in_channels,\n                out_channels,kernel_size,stride=1,\n                padding=2), nn.Conv3d(in_channels,\n                out_channels,kernel_size,stride=1,\n                padding=2))\n\nI know maybe its wrong so any help will be usefull. Hope someone can let me a hand to continue with it because i dont know how to continue.\n\nAnd this is the explanation on the paper to model the cnn:\n\nhttps://preview.redd.it/iis0k4ivzk261.png?width=549&amp;format=png&amp;auto=webp&amp;s=e1bfe7a472c30442ac20dc522da8878be479c3c7", "upvote_ratio": 1.0, "id": "t3_k4lc7w", "created_utc": 1606831187.0}
{"sub": "pytorch", "title": "A library to help run distributed PyTorch training on remote computers", "selftext": "We wrote this simple library [https://github.com/lab-ml/remote](https://github.com/lab-ml/remote) that can connect to remote computers and run PyTorch training jobs.\n\n[Here's a guide on running distributed PyTorch \ud83d\udd25 training with it](https://github.com/lab-ml/remote/blob/master/notes/pytorch-ddp.md)\n\nLooking forward to any feedback that could help us improve the library. Thanks", "upvote_ratio": 0.92, "id": "t3_k4i7lc", "created_utc": 1606817366.0}
{"sub": "pytorch", "title": "Semantic segmentation - background removal preprocess?", "selftext": " Hello, I want to do semantic segmentation with U-Net, with the data I have I'm able to remove the background automatically. Is it beneficial for the model feature extraction if remove the background and replace it with a white/geen/yellow ect background. Maybe use multiple colors mixed in the training set or something.", "upvote_ratio": 1.0, "id": "t3_k493c8", "created_utc": 1606781731.0}
{"sub": "pytorch", "title": "HOW to build PyTorch with OpenBLAS and CUDA on Linux??????", "selftext": "Hi, After trying everything I can (and after hours of trying to debug the problems) I have been still unable to build Pytorch using Openblas and Cuda (on a AMD cpu + Nvidia gpu using Manjaro/Arch). I've tried deleting all mkl libraries from the system, using conda, building from source - but everything runs into error when I try to build it without MKL (conda has conflicts with nomkl installation and building from source runs into errors is using BLAS=OpenBLAS) and I can only build Pytorch with MKL (via pip or aur). This is harming my system's performance significantly.", "upvote_ratio": 0.6, "id": "t3_k3zlxo", "created_utc": 1606755066.0}
{"sub": "pytorch", "title": "are there any good fixed point arithmetic libraries for pytorch?", "selftext": "Hello,\n\nI am searching for a library that enables me to work with int8 based fixed point arithmetic in pytorch on the gpu, are there any libraries out there ?", "upvote_ratio": 1.0, "id": "t3_k3vq9f", "created_utc": 1606742829.0}
{"sub": "pytorch", "title": "Deep Learning with PyTorch: Free Course", "selftext": "nan", "upvote_ratio": 0.87, "id": "t3_k3m54s", "created_utc": 1606700432.0}
{"sub": "pytorch", "title": "How to implement differentiable sign function?", "selftext": "I'm trying to implement DNF-Net ( [https://arxiv.org/abs/2006.06465](https://arxiv.org/abs/2006.06465) ) in pytorch.\n\nIn the paper, the authors used differentiable sign function, using the following trick:\n\nCalculate the step function *exactly* in forward pass, use a differentiable proxy in backward pass. So:\n\n*  sign(x) - forward pass\n* tanh(x) - backward pass\n\nHow to create such function in pytorch?\n\nI will be calling this function on a 1 dimensional tensor, if it helps.\n\nAlso, do tell your thoughts about this architecture if you happen to know it.", "upvote_ratio": 1.0, "id": "t3_k0aen0", "created_utc": 1606241777.0}
{"sub": "pytorch", "title": "[Tutorial] A Comprehensive Guide to the DataLoader Class and Abstractions in PyTorch", "selftext": "In this tutorial, we'll deal with a fundamental challenge in Machine Learning and Deep Learning that is easier said than done: loading and handling different types of data. Specifically, we'll cover:\n\n* Looking at built-in datasets in-depth\n* Using the Dataloader class\n* Using GPUs vs. CPUs\n* Transforming and rescaling images\n* Loading and visualizing built-in datasets\n* Building and loading custom datasets with PyTorch\n\nTutorial link: [https://blog.paperspace.com/dataloaders-abstractions-pytorch/](https://blog.paperspace.com/dataloaders-abstractions-pytorch/)\n\nRun the code on a free GPU with Gradient Community Notebooks: [https://ml-showcase.paperspace.com/projects/working-with-data-in-pytorch](https://ml-showcase.paperspace.com/projects/working-with-data-in-pytorch)", "upvote_ratio": 0.95, "id": "t3_k07uzq", "created_utc": 1606234255.0}
{"sub": "pytorch", "title": "[Q] How can I extract the positive labels to calculate the recall ?", "selftext": "Hi all,\n\nI trained a CNN in Pytorch to classify images as benign or malignant and calculated the accuracy for a training and a testing set. Now I wanna additionally calculate the recall. The recall is defined as TP/TP+FN. Therefore, I wanna extract the cases where my labels match 0 and then calculate the fraction of where my prediction also equals 0 at these places. Could you maybe give me a hint on how to do this in Pytorch? I have insered an image of my code here.\n\nhttps://preview.redd.it/i7fzlrf7zz061.png?width=626&amp;format=png&amp;auto=webp&amp;s=eeb91b0faf48f0fc65746021fd898015fe25557a", "upvote_ratio": 1.0, "id": "t3_jzi8m8", "created_utc": 1606140555.0}
{"sub": "pytorch", "title": "Why does a model definition have both a __init__ and forward function?", "selftext": " Could someone explain to me why pytorch has both **init** and forward function in a model definition, and what each one does, and why do we need 2 function rather than just one?", "upvote_ratio": 0.58, "id": "t3_jyneu4", "created_utc": 1606010429.0}
{"sub": "pytorch", "title": "Deep Dive in Datasets for Machine translation in NLP Using TensorFlow and PyTorch", "selftext": "[https://analyticsindiamag.com/deep-dive-in-datasets-for-machine-translation-in-nlp-using-tensorflow-and-pytorch/](https://analyticsindiamag.com/deep-dive-in-datasets-for-machine-translation-in-nlp-using-tensorflow-and-pytorch/)", "upvote_ratio": 0.75, "id": "t3_jy87s7", "created_utc": 1605951986.0}
{"sub": "pytorch", "title": "How do the loss function and model parameters connect with each other ?", "selftext": "To connect the optimizer and model together, we pass in \\`model.parameters()\\` as an arguement to the optimizer, so that it can zero out the gradients and perform the step. Whereas I wasn't able to understand how does the backprop weights from the loss function reflect onto the model, when we never connected them ?", "upvote_ratio": 0.88, "id": "t3_jxw02k", "created_utc": 1605901528.0}
{"sub": "pytorch", "title": "How can I label my data?", "selftext": "Hi Pytorch Community,\n\nI am really new to Pytorch (some hours now). I have two datasets of images (malignant vs benign pictures of breast cancer). I have uploaded them into Google Colab and now I have all my variables for malignant in one variable and for benign in another. How can I \"label\" them, such that I can mix them together afterwards into training and testing data?", "upvote_ratio": 1.0, "id": "t3_jxv8j5", "created_utc": 1605899015.0}
{"sub": "pytorch", "title": "When using nn.Transformer for inference is there any way to speed up the autoregressive generation?", "selftext": "I have a basic transformer model:\n\n```\nclass Reconstructor(nn.Module):\n    \"\"\"Container module with an encoder, a recurrent or transformer module, and a decoder.\"\"\"\n\n    def __init__(self, input_dim, output_dim, dim_embedding, num_layers=4, nhead=8, dim_feedforward=2048, dropout=0.5):\n        super(Reconstructor, self).__init__()\n\n        self.model_type = 'Transformer'\n        self.embedding = nn.Linear(input_dim, dim_embedding)\n        self.pos_encoder = PositionalEncoding(d_model=dim_embedding, dropout=dropout)\n        self.transformer = nn.Transformer(d_model=dim_embedding, nhead=nhead, dim_feedforward=dim_feedforward, num_encoder_layers=num_layers, num_decoder_layers=num_layers)\n\n        self.decoder = nn.Linear(dim_embedding, output_dim)\n\n        self.init_weights()\n\n    def _generate_square_subsequent_mask(self, sz):\n        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n        return mask\n\n    def init_weights(self):\n        initrange = 0.1\n        nn.init.zeros_(self.decoder.weight)\n        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n\n    def forward(self, src, tgt, tgt_mask=None):\n        embedding_inp = self.embedding(src).permute(1, 0, 2)\n        embedding_out = self.embedding(tgt).permute(1, 0, 2)\n\n        pe_src = embedding_inp + self.pos_encoder(embedding_inp)  # (seq, batch, features)\n        pe_tgt = embedding_out + self.pos_encoder(embedding_out)  # (seq, batch, features)\n        transformer_output = self.transformer(pe_src, pe_tgt)\n        decoder_output = self.decoder(transformer_output).permute(1, 0, 2)\n        decoder_output = self.decoder_act_fn(decoder_output)\n        return decoder_output\n\n\n    def initHidden(self):\n        return torch.zeros(1, 1, 128)\n```\n\nDuring training, I have:\n```\ntgt_mask = gen_nopeek_mask(ground_truth.size(1)).to(DEVICE)\ntgt = torch.ones(ground_truth.size()).to(DEVICE) * -1\ntgt[:, 1:, :] = ground_truth[:, 0:-1, :]\npred = model(x, tgt, tgt_mask=tgt_mask)\n```\n\nwhich I believe will do the training in parallel.\n\nDuring inference, I have:\n```\n\n        tgt = torch.ones(x.size(0), 1, 128) * -1\n\n        for i in range(x.size(1)):\n            print(i)\n            tgt = torch.randn(tgt.size())\n            pred = reconstruct_spect_model(x, tgt)\n\n            tgt = torch.cat((tgt, pred[:, -1, :].unsqueeze(1)), 1)\n```\n\ngiven that my sequence has 499 steps, it takes A WHILE to go through autoregressively. Is there any way to speed this up?", "upvote_ratio": 1.0, "id": "t3_jxt55g", "created_utc": 1605892637.0}
{"sub": "pytorch", "title": "What stands in the way of making your model useful?", "selftext": "Hey there,\n\nI've been working on a concept that allows you to upload your ML models (Tensorflow, Pytorch, etc) and turn them into sharable web apps super quickly, without writing code (for the deployment/ UI part at least).\n\nHere's the landing page I threw together for a more in-depth description of what I'm imagining it could look like: [https://www.getaiko.com/](https://www.getaiko.com/?fbclid=IwAR2eWORlgX2FlBpl9Y42R-_b1wzNuIcfWTaItESr1BP6WmAMmN994SugD78)\n\nSome questions that would help guide me on this project:\n\n* What stands in your way of making your model useful/ getting it out into the world?\n* What are your pain points and major goals when it comes to getting your model deployed?", "upvote_ratio": 1.0, "id": "t3_jx81zd", "created_utc": 1605810032.0}
{"sub": "pytorch", "title": "Intel vs AMD cpu performance", "selftext": "How is the performance of AMD CPUs vs Intel? Are there any problems faced by AMD due to the use of MKL in pytorch?", "upvote_ratio": 1.0, "id": "t3_jx6qzg", "created_utc": 1605806118.0}
{"sub": "pytorch", "title": "GuitarML/PedalNetRT - PyTorch for emulating guitar amplifiers/effects (for anyone interested in deep learning on audio)", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_jx1eip", "created_utc": 1605787007.0}
{"sub": "pytorch", "title": "PyTorch Releases Prototype Features To Execute Machine Learning Models On-Device Hardware Engines", "selftext": "PyTorch has recently released four new PyTorch prototype features. The first three enable mobile machine-learning developers to execute models on the full set of hardware (HW) engines making up a system-on-chip (SOC) system. This allows developers to optimize their model execution for a unique performance, power, and system-level concurrency.\n\nSummary: [https://www.marktechpost.com/2020/11/18/pytorch-releases-prototype-features-to-execute-machine-learning-models-on-device-hardware-engines/](https://www.marktechpost.com/2020/11/18/pytorch-releases-prototype-features-to-execute-machine-learning-models-on-device-hardware-engines/) \n\nGitHub: [https://github.com/pytorch/tutorials/tree/master/prototype\\_source](https://github.com/pytorch/tutorials/tree/master/prototype_source) \n\nSource: [https://pytorch.org/blog/prototype-features-now-available-apis-for-hardware-accelerated-mobile-and-arm64-builds/](https://pytorch.org/blog/prototype-features-now-available-apis-for-hardware-accelerated-mobile-and-arm64-builds/)", "upvote_ratio": 0.88, "id": "t3_jwyg5z", "created_utc": 1605771188.0}
{"sub": "pytorch", "title": "What is the performance of Pytorch running on Apple M1?", "selftext": "I haven't received my M1, but I see that TensorFlow has optimized  for training on M1, so I am looking forward to the performance of Pytorch on M1, although it may be weaker than on x86.", "upvote_ratio": 0.96, "id": "t3_jwye04", "created_utc": 1605770885.0}
{"sub": "pytorch", "title": "Setting up a C++ project in Visual Studio 2019 with LibTorch 1.6", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_jwq25r", "created_utc": 1605738696.0}
{"sub": "pytorch", "title": "Sequence of for loops to access different dimensions of a tensor", "selftext": "My problem is a bit hard to explain.\n\n* I want to update probability distributions recursively in \\[0, 1\\]\\^n.\n* I discretized it with 10 bins per dimension, so there are 10\\^n cells indexed by an n-tuple.\n* Initially, I should have a uniform distribution; but then I take a threshold parameter that dictates how I propagate the distribution: every point that is \"below\" all the n thresholds must receive zero probability.\n\nI would need to access and update `tensor[j, :, ..., :], tensor[:, j, ..., :], ..., tensor[:, :, ..., j]` recursively, and the problem is that I wanted the number of dimensions to be dynamic. How could I implement it?\n\nI have added a question on StackOverflow with a snippet that may help to understand: [https://stackoverflow.com/questions/64885859/sequence-of-for-loops-to-access-different-dimensions-of-a-tensor](https://stackoverflow.com/questions/64885859/sequence-of-for-loops-to-access-different-dimensions-of-a-tensor)", "upvote_ratio": 1.0, "id": "t3_jwfj2f", "created_utc": 1605705176.0}
{"sub": "pytorch", "title": "Pytorch image recognition logistic regression - CIFAR-10 has flat loss curve (no improvement in accuracy after training)", "selftext": "&amp;#x200B;\n\nComplete newbie to pytorch, Just started pytorch last week, but I've been dabbling in the theory and mathematics of machine learning for a few months, so I understand that part, mostly.\n\nI'm following along with the second course in this tutorial online [https://www.youtube.com/watch?v=GIsg-ZUy0MY&amp;ab\\_channel=freeCodeCamp.org](https://www.youtube.com/watch?v=GIsg-ZUy0MY&amp;ab_channel=freeCodeCamp.org) for logistic regression with the MNIST dataset. I decided to try out the same thing for practice on the CIFAR-10 dataset, which can be accessed directly through the pytorch library.\n\nI'm working in a google colab document and wrote a bunch of notes for myself. I worked my way through the tutorial pretty well.\n\nNear the end, where I step through each epoch, I tried (a probably very code inefficent way) to plot the training and validation accuracy w.r.t the amount of epochs and found it to be a flat line, my accuracy starting at around 30% and wavering around there indefinitely.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/enu9urvrnyz51.png?width=536&amp;format=png&amp;auto=webp&amp;s=ab475d11fffe8220203e3b8ebca424534fee1818\n\nI'm really confused at what this is suggesting!! It isn't at 10%, so that means that it worked a bit (since it's more than just guessing) but why would it stay? I tried a bunch of arbitrary learning rates and found it hover between 10-35% accuracy, but it would never improve much more than 5 percentage points after the initial step.\n\nHere's my google colab page for this document: [https://colab.research.google.com/drive/1lWmBlI2BTLw3B-jW5uum0GfYiFNA5kQv?usp=sharing](https://colab.research.google.com/drive/1lWmBlI2BTLw3B-jW5uum0GfYiFNA5kQv?usp=sharing)\n\nI'm pretty confused - also, some tips on how to better plot learning curves and loss curves would be much appreciated! I feel like my implementation is not efficient.\n\nThanks, A", "upvote_ratio": 0.67, "id": "t3_jwcaci", "created_utc": 1605688870.0}
{"sub": "pytorch", "title": "How to load checkpoints across different versions of pytorch (1.3.1 and 1.6.x) using ppc64le and x86?", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_jvza7v", "created_utc": 1605640546.0}
{"sub": "pytorch", "title": "[Q] Rtx 3000", "selftext": "Hey,\n\nSo I got my rtx 3070 on my ubuntu pc.\n\nIs there already support for pytorch with rtx 3070? \n\nI read that you need cuda 11.1 for it, but pytorch didn't release a version with cuda 11.1 yet", "upvote_ratio": 1.0, "id": "t3_jvycs7", "created_utc": 1605637721.0}
{"sub": "pytorch", "title": "Pytorch vs Pytorch Lightning speed", "selftext": "Hello,  \n\n\nI've started to port some of my Pytorch trainers to Pytorch Lightning. I like the modularity of it but it seems to train a lot slower than regular Pytorch. Have any of you noticed any significant differences in speed between Pytorch and Pytorch Lightning? I'm using the same data loading and network codes in both versions.", "upvote_ratio": 0.81, "id": "t3_jvqtbo", "created_utc": 1605609157.0}
{"sub": "pytorch", "title": "PyTorch 3D: Digging Deeper in Deep Learning", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_jvp39i", "created_utc": 1605599504.0}
{"sub": "pytorch", "title": "Checking test accuracy using vectorized code?", "selftext": "I have checked pytorch docs to find test accuracy after each epoch and found basic for loop and updating a counter.\n\nThis works but is fairly slow on cifar10 dataset. Is there a way to use numpy like vectorizations to make it faster.\n\nI'm new to pytorch. Any help would be appreciated.", "upvote_ratio": 0.5, "id": "t3_jv905w", "created_utc": 1605541745.0}
{"sub": "pytorch", "title": "RuntimeError: arguments are located on different GPUs at /pytorch/aten/src/THC/generic/THCTensorIndex.cu:403", "selftext": "Hi there,\n\n&amp;#x200B;\n\nI have a Pytorch Bert model was originally trained with 1 GPU. Now i moved it to 4GPU due to memory issue. However, when I moved it to 4GPU. I got an error message as below during the validation part:\n\n\\`\\`\\`\n\nRuntimeError: Caught RuntimeError in replica 1 on device 1.\n\nOriginal Traceback (most recent call last):\n\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/parallel/parallel\\_apply.py\", line 60, in \\_worker\n\noutput = module(\\*input, \\*\\*kwargs)\n\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 550, in \\_\\_call\\_\\_\n\nresult = self.forward(\\*input, \\*\\*kwargs)\n\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/parallel/data\\_parallel.py\", line 155, in forward\n\noutputs = self.parallel\\_apply(replicas, inputs, kwargs)\n\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/parallel/data\\_parallel.py\", line 165, in parallel\\_apply\n\nreturn parallel\\_apply(replicas, inputs, kwargs, self.device\\_ids\\[:len(replicas)\\])\n\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/parallel/parallel\\_apply.py\", line 85, in parallel\\_apply\n\noutput.reraise()\n\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/\\_utils.py\", line 395, in reraise\n\nraise self.exc\\_type(msg)\n\nRuntimeError: Caught RuntimeError in replica 0 on device 0.\n\nOriginal Traceback (most recent call last):\n\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/parallel/parallel\\_apply.py\", line 60, in \\_worker\n\noutput = module(\\*input, \\*\\*kwargs)\n\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 550, in \\_\\_call\\_\\_\n\nresult = self.forward(\\*input, \\*\\*kwargs)\n\n  File \"/home/ec2-user/SageMaker/Anecdotes/model.py\", line 117, in forward\n\ninputs\\_embeds=None,\n\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 550, in \\_\\_call\\_\\_\n\nresult = self.forward(\\*input, \\*\\*kwargs)\n\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling\\_bert.py\", line 727, in forward\n\ninput\\_ids=input\\_ids, position\\_ids=position\\_ids, token\\_type\\_ids=token\\_type\\_ids, inputs\\_embeds=inputs\\_embeds\n\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 550, in \\_\\_call\\_\\_\n\nresult = self.forward(\\*input, \\*\\*kwargs)\n\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/modeling\\_bert.py\", line 174, in forward\n\ninputs\\_embeds = self.word\\_embeddings(input\\_ids)\n\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 550, in \\_\\_call\\_\\_\n\nresult = self.forward(\\*input, \\*\\*kwargs)\n\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/modules/sparse.py\", line 114, in forward\n\nself.norm\\_type, self.scale\\_grad\\_by\\_freq, self.sparse)\n\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py\", line 1724, in embedding\n\nreturn torch.embedding(weight, input, padding\\_idx, scale\\_grad\\_by\\_freq, sparse)\n\nRuntimeError: arguments are located on different GPUs at /pytorch/aten/src/THC/generic/THCTensorIndex.cu:403\n\n\\`\\`\\`\n\n&amp;#x200B;\n\nMy [model.py](https://model.py) is as below:\n\n\\`\\`\\`\n\nimport torch\n\nimport torch.nn as nn\n\nimport torch.nn.functional as F\n\nfrom torch.nn import MultiheadAttention, EmbeddingBag, CrossEntropyLoss, MultiLabelSoftMarginLoss, BCEWithLogitsLoss\n\nfrom transformers import BertPreTrainedModel, BertModel, BertTokenizer, AdamW\n\n&amp;#x200B;\n\nfrom utils import LABEL\\_NAME\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nclass ReviewClassification(BertPreTrainedModel):\n\ndef \\_\\_init\\_\\_(self, config,\n\nadd\\_agent\\_text, agent\\_text\\_heads):\n\n\"\"\"\n\n:param config: Bert configuration, can set up some parameters, like  output\\_attention, output\\_hidden\\_states\n\n:param add\\_agent\\_text: whether to use the non text feature, and how.\n\nIt can have three options: None, \"concat\" and \"attention\"\n\n:param agent\\_text\\_heads: number of the heads in agent attention mechanism. Only useful if add\\_agent\\_text are set to\n\n\"attention\"\n\n\"\"\"\n\nsuper().\\_\\_init\\_\\_(config)\n\n\\# self.num\\_labels = 2\n\nself.add\\_agent\\_text = add\\_agent\\_text\n\n&amp;#x200B;\n\nself.bert = BertModel(config)\n\nself.dropout = nn.Dropout(config.hidden\\_dropout\\_prob)\n\n&amp;#x200B;\n\nembedding\\_size = config.hidden\\_size\n\n&amp;#x200B;\n\nif self.add\\_agent\\_text == \"concat\":\n\nembedding\\_size = 2 \\* embedding\\_size\n\nelif self.add\\_agent\\_text == \"attention\":\n\nself.agent\\_attention = nn.MultiheadAttention(embedding\\_size, num\\_heads=agent\\_text\\_heads)\n\nelse:\n\n\\# don't use the information in Agent text\n\npass\n\n&amp;#x200B;\n\nself.classifier = nn.Linear(embedding\\_size, 1) # self.classifier = nn.Linear(embedding\\_size, len(LABEL\\_NAME)) # bias: If set to False, the layer will not learn an additive bias\n\nself.init\\_weights()\n\n&amp;#x200B;\n\nprint(\n\n\"\"\"            \n\nadd agent text         :{}\n\nagent text multi-head  :{}\n\n\"\"\".format(self.add\\_agent\\_text, agent\\_text\\_heads)\n\n)\n\n&amp;#x200B;\n\ndef forward(\n\nself,\n\nreview\\_input\\_ids=None,\n\nreview\\_attention\\_mask=None,\n\nreview\\_token\\_type\\_ids=None,\n\nagent\\_input\\_ids=None,\n\nagent\\_attention\\_mask=None,\n\nagent\\_token\\_type\\_ids=None,\n\nlabels=None,\n\n):\n\n\"\"\"\n\nlabels (:obj:\\`torch.LongTensor\\` of shape :obj:\\`(batch\\_size,)\\`, \\`optional\\`, defaults to :obj:\\`None\\`):\n\nLabels for computing the sequence classification/regression loss.\n\nIndices should be in :obj:\\`\\[0, ..., config.num\\_labels - 1\\]\\`.\n\nIf :obj:\\`config.num\\_labels == 1\\` a regression loss is computed (Mean-Square loss),\n\nIf :obj:\\`config.num\\_labels &gt; 1\\` a classification loss is computed (Cross-Entropy).\n\n&amp;#x200B;\n\nReturns:\n\n:obj:\\`tuple(torch.FloatTensor)\\` comprising various elements depending on the configuration (:class:\\`\\~transformers.BertConfig\\`) and inputs:\n\nloss (:obj:\\`torch.FloatTensor\\` of shape :obj:\\`(1,)\\`, \\`optional\\`, returned when :obj:\\`label\\` is provided):\n\nClassification (or regression if config.num\\_labels==1) loss.\n\nlogits (:obj:\\`torch.FloatTensor\\` of shape :obj:\\`(batch\\_size, config.num\\_labels)\\`):\n\nClassification (or regression if config.num\\_labels==1) scores (before SoftMax).\n\nhidden\\_states (:obj:\\`tuple(torch.FloatTensor)\\`, \\`optional\\`, returned when \\`\\`config.output\\_hidden\\_states=True\\`\\`):\n\nTuple of :obj:\\`torch.FloatTensor\\` (one for the output of the embeddings + one for the output of each layer)\n\nof shape :obj:\\`(batch\\_size, sequence\\_length, hidden\\_size)\\`.\n\n&amp;#x200B;\n\nHidden-states of the model at the output of each layer plus the initial embedding outputs.\n\nattentions (:obj:\\`tuple(torch.FloatTensor)\\`, \\`optional\\`, returned when \\`\\`config.output\\_attentions=True\\`\\`):\n\nTuple of :obj:\\`torch.FloatTensor\\` (one for each layer) of shape\n\n:obj:\\`(batch\\_size, num\\_heads, sequence\\_length, sequence\\_length)\\`.\n\n&amp;#x200B;\n\nAttentions weights after the attention softmax, used to compute the weighted average in the self-attention\n\nheads.\n\n&amp;#x200B;\n\nExamples::\n\n&amp;#x200B;\n\nfrom transformers import BertTokenizer, BertForSequenceClassification\n\nimport torch\n\n&amp;#x200B;\n\ntokenizer = BertTokenizer.from\\_pretrained('bert-base-uncased')\n\nmodel = BertForSequenceClassification.from\\_pretrained('bert-base-uncased')\n\n&amp;#x200B;\n\ninput\\_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add\\_special\\_tokens=True)).unsqueeze(0)  # Batch size 1\n\nlabels = torch.tensor(\\[1\\]).unsqueeze(0)  # Batch size 1\n\noutputs = model(input\\_ids, labels=labels)\n\n&amp;#x200B;\n\nloss, logits = outputs\\[:2\\]\n\n&amp;#x200B;\n\n\"\"\"\n\n&amp;#x200B;\n\nreview\\_outputs = self.bert(\n\nreview\\_input\\_ids,\n\nattention\\_mask=review\\_attention\\_mask,\n\ntoken\\_type\\_ids=review\\_token\\_type\\_ids,\n\nposition\\_ids=None,\n\nhead\\_mask=None,\n\ninputs\\_embeds=None,\n\n)\n\nif self.add\\_agent\\_text is not None:\n\n\\# means that self.add\\_agent\\_text is \"concat\" or \"attention\"\n\n\\# TODO: we can try that agent\\_outputs do not share the same parameter\n\nagent\\_outputs = self.bert(\n\nagent\\_input\\_ids,\n\nattention\\_mask=agent\\_attention\\_mask,\n\ntoken\\_type\\_ids=agent\\_token\\_type\\_ids,\n\nposition\\_ids=None,\n\nhead\\_mask=None,\n\ninputs\\_embeds=None,\n\n)\n\n&amp;#x200B;\n\nif self.add\\_agent\\_text == \"attention\":\n\nreview\\_hidden\\_states = review\\_outputs\\[0\\].transpose(0, 1)  # before trans: (bs, seq\\_len, hidden\\_size)\n\nagent\\_hidden\\_states = agent\\_outputs\\[0\\].mean(axis=1).unsqueeze(dim=0)  # (1, batch\\_size, hidden\\_size)\n\n&amp;#x200B;\n\nattn\\_output, \\_ = self.agent\\_attention(agent\\_hidden\\_states, review\\_hidden\\_states, review\\_hidden\\_states)\n\nfeature = attn\\_output.squeeze()  # (batch\\_size, seq\\_len)\n\nelse:\n\nfeature = review\\_outputs\\[1\\]  # (batch\\_size, seq\\_len) -? Should it be (batch\\_size, hidden\\_size)\n\n&amp;#x200B;\n\nif self.add\\_agent\\_text == \"concat\":\n\nfeature = [torch.cat](https://torch.cat)(\\[feature, agent\\_outputs\\[1\\]\\], axis=1)\n\n\n\n&amp;#x200B;\n\n\\# nn.CrossEntropyLoss applies F.log\\_softmax and nn.NLLLoss internally on your input,\n\n\\# so you should pass the raw logits to it.\n\n&amp;#x200B;\n\n\\# torch.nn.functional.binary\\_cross\\_entropy takes logistic sigmoid values as inputs\n\n\\# torch.nn.functional.binary\\_cross\\_entropy\\_with\\_logits takes logits as inputs\n\n\\# torch.nn.functional.cross\\_entropy takes logits as inputs (performs log\\_softmax internally)\n\n\\# torch.nn.functional.nll\\_loss is like cross\\_entropy but takes log-probabilities (log-softmax) values as inputs\n\n&amp;#x200B;\n\n\\# CrossEntropyLoss takes prediction logits (size: (N,D)) and target labels (size: (N,)) \n\n\\# CrossEntropyLoss expects logits i.e whereas BCELoss expects probability value\n\nlogits = self.classifier(feature).squeeze()\n\n&amp;#x200B;\n\noutputs = (logits,)  # + outputs\\[2:\\]  # add hidden states and attention if they are here\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nif labels is not None:\n\n\\##### original\n\n\\# loss\\_fct = MultiLabelSoftMarginLoss()\n\n\\# loss = loss\\_fct(logits, labels)\n\n\\# outputs = (loss,) + outputs\n\n\\#### Version 1 try\n\n\\# pos\\_weight = dataset.label\\_proportion.iloc\\[0\\]/dataset.label\\_proportion.iloc\\[1\\]\n\n&amp;#x200B;\n\n\\# Version 1.1 for weight\n\n\\# weight = torch.tensor(\\[0.101521, 0.898479\\]) # hard code from entire training dataset\n\n\\# pos\\_weight = weight\\[labels.data.view(-1).long()\\].view\\_as(labels)\n\n\\# Version 1.2 for weight\n\npos\\_weight=torch.tensor(1)\n\n\\# Version 1.3 for weight\n\n\\#weight = torch.tensor(\\[1.0, 8.85\\]) # hard code from entire training dataset\n\n\\#pos\\_weight = weight\\[labels.data.view(-1).long()\\].view\\_as(labels)\n\n\n\nloss\\_fct = nn.BCEWithLogitsLoss(pos\\_weight=pos\\_weight).cuda() \n\nloss = loss\\_fct(logits, labels)\n\n\\# loss = loss\\_fct(logits.view(-1, self.num\\_labels), labels.view(-1, self.num\\_labels))\n\noutputs = (loss,) + outputs\n\n\\### Version 2 try\n\n\\# loss\\_fct = nn.CrossEntropyLoss()\n\n\\# loss = loss\\_fct(logits.view(-1, self.num\\_labels), labels.view(-1))\n\n\\# outputs = (loss,) + outputs\n\n&amp;#x200B;\n\nreturn outputs  # (loss, logits, hidden\\_states, attentions)\n\n\\`\\`\\`\n\n&amp;#x200B;\n\nAnd my train\\_valid\\_test.py for the training, validation, test process is as below:\n\n\\`\\`\\`\n\nimport time\n\nimport pickle\n\nfrom path import Path\n\nimport numpy as np\n\nimport pandas as pd\n\n&amp;#x200B;\n\nfrom sklearn.metrics import precision\\_recall\\_fscore\\_support, classification\\_report, confusion\\_matrix\n\nimport torch\n\nimport torch.nn as nn\n\n&amp;#x200B;\n\nfrom utils import LABEL\\_NAME, isnotebook, set\\_seed, format\\_time\n\n&amp;#x200B;\n\nif isnotebook():\n\nfrom tqdm.notebook import tqdm\n\nelse:\n\nfrom tqdm import tqdm\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nset\\_seed(seed=228)\n\n&amp;#x200B;\n\ndef model\\_train(model, train\\_data\\_loader, valid\\_data\\_loader, test\\_data\\_loader,\n\nlogger, optimizer, scheduler, num\\_epochs, seed, out\\_dir):\n\n\\# move model to gpu\n\ndevice = torch.device('cuda' if torch.cuda.is\\_available() else 'cpu')\n\n[model.to](https://model.to)(device)\n\nif torch.cuda.device\\_count() &gt; 1:\n\nmodel = nn.DataParallel(model)\n\n&amp;#x200B;\n\nnum\\_gpus = torch.cuda.device\\_count()\n\n[logger.info](https://logger.info)(\"Let's use {} GPUs!\".format(num\\_gpus))\n\n&amp;#x200B;\n\n\\# Set the seed value all over the place to make this reproducible.\n\n\\#     set\\_seed(seed=seed)\n\n&amp;#x200B;\n\n\\# We'll store a number of quantities such as training and validation loss,\n\n\\# validation accuracy, and timings.\n\ntraining\\_stats = \\[\\]\n\nprint\\_interval = 100\n\n&amp;#x200B;\n\n\\# Measure the total training time for the whole run.\n\ntotal\\_t0 = time.time()\n\nbatch\\_size = train\\_data\\_loader.batch\\_size\n\nnum\\_batch = len(train\\_data\\_loader)\n\nbest\\_f1\\_score = {\n\n\"weighted\": 0,\n\n\"averaged\": 0\n\n}\n\nbest\\_test\\_f1\\_score = 0\n\n&amp;#x200B;\n\n\\# For each epoch...\n\nfor epoch\\_i in range(0, num\\_epochs):\n\n&amp;#x200B;\n\n\\# ========================================\n\n\\#               Training\n\n\\# ========================================\n\n&amp;#x200B;\n\n\\# Perform one full pass over the training set.\n\n[logger.info](https://logger.info)(\"\")\n\n[logger.info](https://logger.info)('======== Epoch {:} / {:} ========'.format(epoch\\_i + 1, num\\_epochs))\n\n[logger.info](https://logger.info)('Training...')\n\n&amp;#x200B;\n\n\\# Reset the total loss for this epoch.\n\ntotal\\_train\\_loss = 0\n\n&amp;#x200B;\n\n\\# Measure how long the training epoch takes.\n\nt\\_train = time.time()\n\n&amp;#x200B;\n\nmodel.train()\n\n&amp;#x200B;\n\n\\# For each batch of training data...\n\nfor step, batch in tqdm(enumerate(train\\_data\\_loader), desc=\"Training Iteration\", total=num\\_batch):\n\n\\# Progress update every 100 batches.\n\nif step % print\\_interval == 0 and not step == 0:\n\n\\# Calculate elapsed time in minutes.\n\nelapsed = format\\_time(time.time() - t\\_train)\n\navg\\_train\\_loss = total\\_train\\_loss / print\\_interval\n\n&amp;#x200B;\n\n\\# Report progress.\n\n[logger.info](https://logger.info)('| epoch {:3d} | {:5d}/{:5d} batches | lr {:.3e} | loss {:5.3f} | Elapsed {:s}'.format(\n\nepoch\\_i+1, step, num\\_batch, scheduler.get\\_last\\_lr()\\[0\\], avg\\_train\\_loss, elapsed)\n\n)\n\ntotal\\_train\\_loss = 0\n\ntraining\\_stats.append(\n\n{\n\n'epoch': epoch\\_i + 1,\n\n'step': step,\n\n'train loss': avg\\_train\\_loss,\n\n}\n\n)\n\n&amp;#x200B;\n\n\\# Unpack this training batch from our dataloader.\n\n\\#\n\n\\# As we unpack the batch, we'll also copy each tensor to the GPU using the\n\n\\# \\`to\\` method.\n\n\\#\n\n\\# \\`batch\\` contains four pytorch tensors:\n\n\\#   \"input\\_ids\"\n\n\\#   \"attention\\_mask\"\n\n\\#   \"token\\_type\\_ids\"\n\n\\#   \"binarized\\_labels\"\n\n&amp;#x200B;\n\nb\\_review\\_input\\_ids = batch\\[\"review\\_input\\_ids\"\\].to(device)\n\nb\\_review\\_attention\\_mask = batch\\[\"review\\_attention\\_mask\"\\].to(device)\n\nb\\_review\\_token\\_type\\_ids = batch\\[\"review\\_token\\_type\\_ids\"\\].to(device)\n\nb\\_agent\\_input\\_ids = batch\\[\"agent\\_input\\_ids\"\\].to(device)\n\nb\\_agent\\_attention\\_mask = batch\\[\"agent\\_attention\\_mask\"\\].to(device)\n\nb\\_agent\\_token\\_type\\_ids = batch\\[\"agent\\_token\\_type\\_ids\"\\].to(device)\n\n&amp;#x200B;\n\nb\\_binarized\\_label = batch\\[\"binarized\\_label\"\\].to(device)\n\n&amp;#x200B;\n\nmodel.zero\\_grad()\n\n(loss, \\_) = model(review\\_input\\_ids=b\\_review\\_input\\_ids,\n\nreview\\_attention\\_mask=b\\_review\\_attention\\_mask,\n\nreview\\_token\\_type\\_ids=b\\_review\\_token\\_type\\_ids,\n\nagent\\_input\\_ids=b\\_agent\\_input\\_ids,\n\nagent\\_attention\\_mask=b\\_agent\\_attention\\_mask,\n\nagent\\_token\\_type\\_ids=b\\_agent\\_token\\_type\\_ids,\n\n&amp;#x200B;\n\nlabels=b\\_binarized\\_label\n\n)\n\n&amp;#x200B;\n\n\\# Accumulate the training loss over all of the batches so that we can\n\n\\# calculate the average loss at the end. \\`loss\\` is a Tensor containing a\n\n\\# single value; the \\`.item()\\` function just returns the Python value\n\n\\# from the tensor.\n\n&amp;#x200B;\n\nif num\\_gpus &gt; 1:\n\ntotal\\_train\\_loss += loss.mean().item()\n\nloss.mean().backward()  # use loss.mean().backward() instead of loss.backward() for multiple gpu trainings\n\nelse:\n\ntotal\\_train\\_loss += loss.item()\n\nloss.backward()\n\n&amp;#x200B;\n\n\\# Clip the norm of the gradients to 1.0.\n\n\\# This is to help prevent the \"exploding gradients\" problem.\n\ntorch.nn.utils.clip\\_grad\\_norm\\_(model.parameters(), 1.0)\n\n&amp;#x200B;\n\n\\# Update parameters and take a step using the computed gradient.\n\n\\# The optimizer dictates the \"update rule\"--how the parameters are\n\n\\# modified based on their gradients, the learning rate, etc.\n\noptimizer.step()\n\nscheduler.step()\n\n\\# End of training epoch\n\n&amp;#x200B;\n\n\\# Measure how long this epoch took.\n\ntraining\\_time = format\\_time(time.time() - t\\_train)\n\n&amp;#x200B;\n\n[logger.info](https://logger.info)(\"\")\n\n[logger.info](https://logger.info)(\"  Training epoch took: {:s}\".format(training\\_time))\n\n&amp;#x200B;\n\n\\# evaluate the model after one epoch.\n\n&amp;#x200B;\n\n\\# ========================================\n\n\\#               Validation\n\n\\# ========================================\n\n\\# After the completion of each training epoch, measure our performance on\n\n\\# our validation set.\n\n&amp;#x200B;\n\n[logger.info](https://logger.info)(\"\")\n\n[logger.info](https://logger.info)(\"Validating...\")\n\n&amp;#x200B;\n\nt\\_valid = time.time()\n\nmodel.eval()\n\nave\\_valid\\_loss, valid\\_f1\\_table, cm\\_table, f1\\_score = model\\_validate(model=model, data\\_loader=valid\\_data\\_loader)\n\n\\# Measure how long this epoch took.\n\nvalidation\\_time = format\\_time(time.time() - t\\_valid)\n\n&amp;#x200B;\n\n[logger.info](https://logger.info)(\"\")\n\n[logger.info](https://logger.info)('| loss {:5.3f} | Elapsed {:s}'.format(ave\\_valid\\_loss, validation\\_time))\n\n[logger.info](https://logger.info)(\"  \\\\n{:s}\".format(valid\\_f1\\_table.to\\_string()))\n\n[logger.info](https://logger.info)(\"\")\n\n[logger.info](https://logger.info)(\"  \\\\n{:s}\".format(cm\\_table.to\\_string()))\n\n&amp;#x200B;\n\n\\# need to store the best model\n\nfor key in best\\_f1\\_score.keys():\n\nif best\\_f1\\_score\\[key\\] &lt; f1\\_score\\[key\\]:\n\n\\# remove the old model:\n\nfile\\_list = \\[f for f in out\\_dir.files() if f.name.endswith(\".pt\") and f.name.startswith(key)\\]\n\nfor f in file\\_list:\n\nPath.remove(f)\n\nmodel\\_file = out\\_dir.joinpath('{:s}\\_epoch\\_{:02d}-f1\\_{:.3f}.pt'.format(\n\nkey, epoch\\_i + 1, f1\\_score\\[key\\])\n\n)\n\nbest\\_f1\\_score\\[key\\] = f1\\_score\\[key\\]\n\nif num\\_gpus &gt; 1:\n\n[torch.save](https://torch.save)(model.module.state\\_dict(), model\\_file)\n\nelse:\n\n[torch.save](https://torch.save)(model.state\\_dict(), model\\_file)\n\n&amp;#x200B;\n\n\\# ========================================\n\n\\#               Test\n\n\\# ========================================\n\n[logger.info](https://logger.info)(\"\")\n\n[logger.info](https://logger.info)(\"Testing...\")\n\n&amp;#x200B;\n\nresult\\_df = model\\_test(model=model, data\\_loader=test\\_data\\_loader)\n\n\n\ny\\_true = np.array(result\\_df\\[\"review\\_label\"\\], dtype=np.bool) # This part may need double check\n\ny\\_pred = result\\_df\\[\"Probability\"\\] &gt; 0.5\n\n&amp;#x200B;\n\nreport = classification\\_report(y\\_true, y\\_pred, output\\_dict=True)\n\nmetrics\\_df = pd.DataFrame(report).transpose()\n\n&amp;#x200B;\n\nmetrics\\_df = metrics\\_df.sort\\_index()\n\n&amp;#x200B;\n\nweighted\\_f1\\_score = metrics\\_df.loc\\['weighted avg', 'f1-score'\\]\n\naveraged\\_f1\\_score = metrics\\_df.loc\\['macro avg', 'f1-score'\\]\n\n&amp;#x200B;\n\nbest\\_test\\_f1\\_score = metrics\\_df.loc\\['weighted avg', 'f1-score'\\] \\\\\n\nif best\\_test\\_f1\\_score &lt; metrics\\_df.loc\\['weighted avg', 'f1-score'\\] else best\\_test\\_f1\\_score\n\n&amp;#x200B;\n\nmetrics\\_df = metrics\\_df.astype(float).round(3)\n\n&amp;#x200B;\n\n\\# Calculate confusion matrix\n\ntn, fp, fn, tp  = confusion\\_matrix(y\\_true, y\\_pred).ravel()\n\ncm\\_df = pd.DataFrame(columns = \\['Predicted No', 'Predicted Yes'\\],  \n\nindex = \\['Actual No', 'Actual Yes'\\]) \n\n\\# adding rows to an empty  \n\n\\# dataframe at existing index \n\ncm\\_df.loc\\['Actual No'\\] = \\[tn,fp\\] \n\ncm\\_df.loc\\['Actual Yes'\\] = \\[fn,tp\\]\n\n\n\n[logger.info](https://logger.info)(\"use model: {} batch / {} step\".format(epoch\\_i + 1, step))\n\n[logger.info](https://logger.info)(\"\\\\n\" + \"=\" \\* 50)\n\n[logger.info](https://logger.info)(\"\\\\n\" + metrics\\_df.to\\_string())\n\n[logger.info](https://logger.info)(\"\\\\n\" + \"=\" \\* 50)\n\n[logger.info](https://logger.info)(\"\\\\n\" + cm\\_df.to\\_string())\n\n[logger.info](https://logger.info)(\"best test F1 score: {}\".format(best\\_test\\_f1\\_score))\n\n[logger.info](https://logger.info)(\"\\\\n\" + \"=\" \\* 50)\n\n\\# Below is to save the result files\n\n\\#         result\\_filename = \"result\\_df\\_epoch\\_\" + str(epoch\\_i + 1) + \".xlsx\"\n\n\\#         result\\_df.to\\_excel(out\\_dir.joinpath(result\\_filename), index=False)\n\n&amp;#x200B;\n\n[logger.info](https://logger.info)(\"\")\n\n[logger.info](https://logger.info)(\"Training complete!\")\n\n[logger.info](https://logger.info)(\"Total training took {:} (h:mm:ss)\".format(format\\_time(time.time() - total\\_t0)))\n\n&amp;#x200B;\n\n\\# Save training\\_stats to csv file\n\npd.DataFrame(training\\_stats).to\\_csv(out\\_dir.joinpath(\"model\\_train.log\"), index=False)\n\nreturn model, optimizer, scheduler\n\n&amp;#x200B;\n\n&amp;#x200B;\n\ndef model\\_validate(model, data\\_loader):\n\n\\# Put the model in evaluation mode--the dropout layers behave differently\n\n\\# during evaluation.\n\nmodel.eval()\n\ndevice = torch.device('cuda' if torch.cuda.is\\_available() else 'cpu')\n\n[model.to](https://model.to)(device)\n\nif torch.cuda.device\\_count() &gt; 1:\n\nmodel = nn.DataParallel(model)\n\n&amp;#x200B;\n\nlabel\\_prop = data\\_loader.dataset.dataset.label\\_prop()\n\n&amp;#x200B;\n\ntotal\\_valid\\_loss = 0\n\n&amp;#x200B;\n\nbatch\\_size = data\\_loader.batch\\_size\n\nnum\\_batch = len(data\\_loader)\n\n&amp;#x200B;\n\ny\\_pred, y\\_true = \\[\\], \\[\\]\n\n&amp;#x200B;\n\n\\# Evaluate data\n\nfor step, batch in tqdm(enumerate(data\\_loader), desc=\"Validation...\", total=num\\_batch):\n\nb\\_review\\_input\\_ids = batch\\[\"review\\_input\\_ids\"\\].to(device)\n\nb\\_review\\_attention\\_mask = batch\\[\"review\\_attention\\_mask\"\\].to(device)\n\nb\\_review\\_token\\_type\\_ids = batch\\[\"review\\_token\\_type\\_ids\"\\].to(device)\n\nb\\_agent\\_input\\_ids = batch\\[\"agent\\_input\\_ids\"\\].to(device)\n\nb\\_agent\\_attention\\_mask = batch\\[\"agent\\_attention\\_mask\"\\].to(device)\n\nb\\_agent\\_token\\_type\\_ids = batch\\[\"agent\\_token\\_type\\_ids\"\\].to(device)\n\n&amp;#x200B;\n\nb\\_binarized\\_label = batch\\[\"binarized\\_label\"\\].to(device)\n\n&amp;#x200B;\n\n\\# Tell pytorch not to bother with constructing the compute graph during\n\n\\# the forward pass, since this is only needed for backprop (training).\n\nwith torch.no\\_grad():\n\n(loss, logits,) = model(review\\_input\\_ids=b\\_review\\_input\\_ids,\n\nreview\\_attention\\_mask=b\\_review\\_attention\\_mask,\n\nreview\\_token\\_type\\_ids=b\\_review\\_token\\_type\\_ids,\n\nagent\\_input\\_ids=b\\_agent\\_input\\_ids,\n\nagent\\_attention\\_mask=b\\_agent\\_attention\\_mask,\n\nagent\\_token\\_type\\_ids=b\\_agent\\_token\\_type\\_ids,\n\n&amp;#x200B;\n\nlabels=b\\_binarized\\_label)\n\n&amp;#x200B;\n\ntotal\\_valid\\_loss += loss.item()\n\n\\### The sigmoid function is used for the two-class logistic regression, \n\n\\### whereas the softmax function is used for the multiclass logistic regression\n\n\n\n\\# Version 1\n\n\\# numpy\\_probas = logits.detach().cpu().numpy()\n\n\\# y\\_pred.extend(np.argmax(numpy\\_probas, axis=1).flatten())\n\n\\# y\\_true.extend(b\\_binarized\\_label.cpu().numpy())\n\n&amp;#x200B;\n\n\\# Version 2\n\n\\# transfored\\_logits = F.log\\_softmax(logits,dim=1)\n\n\\# numpy\\_probas = transfored\\_logits.detach().cpu().numpy()\n\n\\# y\\_pred.extend(np.argmax(numpy\\_probas, axis=1).flatten())\n\n\\# y\\_true.extend(b\\_binarized\\_label.cpu().numpy())\n\n&amp;#x200B;\n\n\\# Version 3\n\n\\# transfored\\_logits = torch.sigmoid(logits)\n\n\\# numpy\\_probas = transfored\\_logits.detach().cpu().numpy()\n\n\\# y\\_pred.extend(np.argmax(numpy\\_probas, axis=1).flatten())\n\n\\# y\\_true.extend(b\\_binarized\\_label.cpu().numpy())\n\n&amp;#x200B;\n\n\\# New version - for num\\_label = 1\n\ntransfored\\_logits = torch.sigmoid(logits)\n\nnumpy\\_probas = transfored\\_logits.detach().cpu().numpy()\n\ny\\_pred.extend(numpy\\_probas)\n\ny\\_true.extend(b\\_binarized\\_label.cpu().numpy())\n\n\n\n\\# End of an epoch of validation\n\n&amp;#x200B;\n\n\\# put model to train mode again.\n\nmodel.train()\n\n&amp;#x200B;\n\nave\\_loss = total\\_valid\\_loss / (num\\_batch \\* batch\\_size)\n\n&amp;#x200B;\n\ny\\_pred = np.array(y\\_pred)\n\ny\\_pred\\[y\\_pred &lt; 0.5\\] = 0\n\ny\\_pred\\[y\\_pred &gt;= 0.5\\] = 1\n\n\n\n\\# Below is in case the input and target are not the same data format\n\ny\\_pred = np.array(y\\_pred, dtype=np.bool)\n\ny\\_true = np.array(y\\_true, dtype=np.bool)\n\n\n\n\n\n\\# compute the various f1 score for each label\n\nreport = classification\\_report(y\\_true, y\\_pred, output\\_dict=True)\n\nmetrics\\_df = pd.DataFrame(report).transpose()\n\n\\# metrics\\_df = pd.DataFrame(0, index=LABEL\\_NAME, columns=\\[\"Precision\", \"Recall\", \"F1\",\"support\"\\])\n\n\\# metrics\\_df.Precision = precision\\_recall\\_fscore\\_support(y\\_true, y\\_pred)\\[0\\]\n\n\\# metrics\\_df.Recall = precision\\_recall\\_fscore\\_support(y\\_true, y\\_pred)\\[1\\]\n\n\\# metrics\\_df.F1 = precision\\_recall\\_fscore\\_support(y\\_true, y\\_pred)\\[2\\]\n\n\\# metrics\\_df.support = precision\\_recall\\_fscore\\_support(y\\_true, y\\_pred)\\[3\\]\n\n&amp;#x200B;\n\n\\# y\\_pred = np.array(y\\_pred)\n\n\\# y\\_pred\\[y\\_pred &lt; 0\\] = 0\n\n\\# y\\_pred\\[y\\_pred &gt; 0\\] = 1\n\n\\# y\\_pred = np.array(y\\_pred, dtype=np.bool)\n\n\\# y\\_true = np.array(y\\_true, dtype=np.bool)\n\n&amp;#x200B;\n\n\\# metrics\\_df = pd.DataFrame(0, index=LABEL\\_NAME, columns=\\[\"Precision\", \"Recall\", \"F1\"\\], dtype=np.float)\n\n\\# # or\\_y\\_pred = np.zeros(y\\_pred.shape\\[0\\], dtype=np.bool)\n\n\\# # or\\_y\\_true = np.zeros(y\\_true.shape\\[0\\], dtype=np.bool)\n\n\\# for i in range(len(LABEL\\_NAME)):\n\n\\#     metrics\\_df.iloc\\[i\\] = precision\\_recall\\_fscore\\_support(\n\n\\#         y\\_true=y\\_true\\[:, i\\], y\\_pred=y\\_pred\\[:, i\\], average='binary', zero\\_division=0)\\[0:3\\]\n\n&amp;#x200B;\n\n\\# or\\_y\\_pred = or\\_y\\_pred | y\\_pred\\[:, i\\]\n\n\\# or\\_y\\_true = or\\_y\\_true | y\\_true\\[:, i\\]\n\n&amp;#x200B;\n\nmetrics\\_df = metrics\\_df.sort\\_index()\n\n\\# metrics\\_df.loc\\['Weighted Average'\\] = metrics\\_df.transpose().dot(label\\_prop)\n\n\\# metrics\\_df.loc\\['Average'\\] = metrics\\_df.mean()\n\n&amp;#x200B;\n\n\\# metrics\\_df.loc\\['Weighted Average', 'F1'\\] = 2 / (1/metrics\\_df.loc\\['Weighted Average', \"Recall\"\\] +\n\n\\#                                                 1/metrics\\_df.loc\\['Weighted Average', \"Precision\"\\])\n\n\\# metrics\\_df.loc\\['Average', 'F1'\\] = 2 / (1/metrics\\_df.loc\\['Average', \"Recall\"\\] +\n\n\\#                                        1/metrics\\_df.loc\\['Average', \"Precision\"\\])\n\n&amp;#x200B;\n\nweighted\\_f1\\_score = metrics\\_df.loc\\['weighted avg', 'f1-score'\\]\n\naveraged\\_f1\\_score = metrics\\_df.loc\\['macro avg', 'f1-score'\\]\n\n&amp;#x200B;\n\n\\# Calculate confusion matrix\n\ntn, fp, fn, tp  = confusion\\_matrix(y\\_true, y\\_pred).ravel()\n\ncm\\_df = pd.DataFrame(columns = \\['Predicted No', 'Predicted Yes'\\],  \n\nindex = \\['Actual No', 'Actual Yes'\\]) \n\n\\# adding rows to an empty  \n\n\\# dataframe at existing index \n\ncm\\_df.loc\\['Actual No'\\] = \\[tn,fp\\] \n\ncm\\_df.loc\\['Actual Yes'\\] = \\[fn,tp\\]\n\n&amp;#x200B;\n\n\\# pooled\\_f1\\_score = f1\\_score(y\\_pred=or\\_y\\_pred, y\\_true=or\\_y\\_true)\n\n&amp;#x200B;\n\nreturn ave\\_loss, metrics\\_df, cm\\_df,{\n\n\"weighted\": weighted\\_f1\\_score,\n\n\"averaged\": averaged\\_f1\\_score,\n\n}\n\n&amp;#x200B;\n\n&amp;#x200B;\n\ndef model\\_test(model, data\\_loader):\n\n\\# Put the model in evaluation mode--the dropout layers behave differently\n\n\\# during evaluation.\n\ndevice = torch.device('cuda' if torch.cuda.is\\_available() else 'cpu')\n\nmodel.eval()\n\n[model.to](https://model.to)(device)\n\nif torch.cuda.device\\_count() &gt; 1:\n\nmodel = nn.DataParallel(model)\n\n&amp;#x200B;\n\nnum\\_batch = len(data\\_loader)\n\n\\# Below need to modify if change the input\n\nreview\\_id, review\\_label, hmd\\_text, head\\_cust\\_text = \\[\\], \\[\\], \\[\\], \\[\\]\n\nagent = \\[\\]\n\npred\\_logits = \\[\\]\n\n&amp;#x200B;\n\n\\# Evaluate data\n\nfor step, batch in tqdm(enumerate(data\\_loader), desc=\"Inference...\", total=num\\_batch):\n\nif \"anecdote\\_lead\\_final\" in batch.keys():\n\nreview\\_label.extend(batch\\[\"anecdote\\_lead\\_final\"\\])\n\nreview\\_id.extend(batch\\[\"\\_id\"\\].tolist())\n\nhmd\\_text.extend(batch\\[\"hmd\\_comments\"\\])\n\nhead\\_cust\\_text.extend(batch\\[\"head\\_cust\"\\])\n\nagent.extend(batch\\[\"new\\_transcript\\_agent\"\\])\n\n&amp;#x200B;\n\nb\\_review\\_input\\_ids = batch\\[\"review\\_input\\_ids\"\\].to(device)\n\nb\\_review\\_attention\\_mask = batch\\[\"review\\_attention\\_mask\"\\].to(device)\n\nb\\_review\\_token\\_type\\_ids = batch\\[\"review\\_token\\_type\\_ids\"\\].to(device)\n\nb\\_agent\\_input\\_ids = batch\\[\"agent\\_input\\_ids\"\\].to(device)\n\nb\\_agent\\_attention\\_mask = batch\\[\"agent\\_attention\\_mask\"\\].to(device)\n\nb\\_agent\\_token\\_type\\_ids = batch\\[\"agent\\_token\\_type\\_ids\"\\].to(device)\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\# Tell pytorch not to bother with constructing the compute graph during\n\n\\# the forward pass, since this is only needed for backprop (training).\n\nwith torch.no\\_grad():\n\n(logits,) = model(review\\_input\\_ids=b\\_review\\_input\\_ids,\n\nreview\\_token\\_type\\_ids=b\\_review\\_token\\_type\\_ids,\n\nreview\\_attention\\_mask=b\\_review\\_attention\\_mask,\n\nagent\\_input\\_ids=b\\_agent\\_input\\_ids,\n\nagent\\_token\\_type\\_ids=b\\_agent\\_token\\_type\\_ids,\n\nagent\\_attention\\_mask=b\\_agent\\_attention\\_mask\n\n)\n\n&amp;#x200B;\n\nif logits.detach().cpu().numpy().size == 1:\n\npred\\_logits.extend(logits.detach().cpu().numpy().reshape(1,))  \n\nelse:\n\npred\\_logits.extend(logits.detach().cpu().numpy())\n\n\n\n\\# End of an epoch of validation\n\n\\# put model to train mode again.\n\nmodel.train()\n\npred\\_logits = np.array(pred\\_logits)\n\npred\\_prob = np.exp(pred\\_logits)\n\npred\\_prob = pred\\_prob / (1 + pred\\_prob)\n\npred\\_label = pred\\_prob.copy()\n\npred\\_label\\[pred\\_label &lt; 0.5\\] = 0\n\npred\\_label\\[pred\\_label &gt;= 0.5\\] = 1\n\n\\# compute the f1 score for each tags\n\nd = {'Probability':pred\\_prob,'Anecdotes Prediction':pred\\_label}\n\npred\\_df = pd.DataFrame(d, columns=\\['Probability','Anecdotes Prediction'\\])\n\nresult\\_df = pd.DataFrame(\n\n{\n\n\"review\\_id\": review\\_id,\n\n\"hmd\\_text\": hmd\\_text,\n\n\"head\\_cust\\_text\": head\\_cust\\_text,\n\n\"agent\": agent\n\n}\n\n)\n\nif len(review\\_label) != 0:\n\nresult\\_df\\[\"review\\_label\"\\] =  \\[x.item() for x in review\\_label\\] \n\nreturn pd.concat(\\[result\\_df, pred\\_df\\], axis=1).set\\_index(\"review\\_id\")\n\n\\`\\`\\`\n\n&amp;#x200B;\n\nCan anyone help me how to fix this issue? Thank you so much!!!", "upvote_ratio": 0.25, "id": "t3_jv2s5o", "created_utc": 1605511955.0}
{"sub": "pytorch", "title": "LibreASR \u2013 An On-Premises, Streaming Speech Recognition System", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_juotjr", "created_utc": 1605458658.0}
{"sub": "pytorch", "title": "Introduce Pytorch C++ API Go binding", "selftext": "[https://github.com/sugarme/gotch](https://github.com/sugarme/gotch)\n\nWe are happy to share a new toolkit for developing deep learning in Go - [gotch](https://github.com/sugarme/gotch). \n\n**Some features are**:\n\n* Comprehensive Pytorch tensor APIs (\\~ 1404)\n* Fully featured Pytorch dynamic graph computation\n* JIT interface to run model trained/saved using PyTorch Python API\n* Load pretrained Pytorch models and run inference\n* Pure Go APIs to build and train neural network models with both CPU and GPU support\n* Most recent image models\n* NLP Language models - [Transformer](https://github.com/sugarme/transformer) in separate package built with GoTch and [pure Go Tokenizer](https://github.com/sugarme/tokenizer).", "upvote_ratio": 0.92, "id": "t3_juek7b", "created_utc": 1605408161.0}
{"sub": "pytorch", "title": "PyTorch Dataloading for Videos: A Small but Powerful Helper Repo", "selftext": "Need to use a video dataset for Training? Theres not much on the internet about easily and efficiently using video datasets for deep learning. So, I hope this is useful to some people. Would greatly appreciate any feedback!\n\n[https://github.com/RaivoKoot/Video-Dataset-Loading-Pytorch](https://github.com/RaivoKoot/Video-Dataset-Loading-Pytorch)", "upvote_ratio": 1.0, "id": "t3_ju3zjb", "created_utc": 1605369127.0}
{"sub": "pytorch", "title": "Convenience library for PyTorch training", "selftext": "For the past year or so I have been plugging away on a side project - [torchutils](https://gitlab.com/avilay/torchutils) - a PyTorch library for quick but systematic experimentation. It'd be great if you could take it for a spin when training your next mL model and give me some feedback. Here are the [detailed docs](https://avilay.gitlab.io/torchutils/).", "upvote_ratio": 0.94, "id": "t3_ju38cn", "created_utc": 1605366190.0}
{"sub": "pytorch", "title": "Facebook AI and OpenMined create PyTorch privacy and machine learning courses", "selftext": "nan", "upvote_ratio": 0.94, "id": "t3_jtlyjq", "created_utc": 1605294324.0}
{"sub": "pytorch", "title": "Real-world video Super resolution!", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_jte5qa", "created_utc": 1605259546.0}
{"sub": "pytorch", "title": "Plexiglass: A PyTorch toolbox for cybersecurity research and testing against adversarial attacks and deepfakes.", "selftext": " Hi everyone, my name is Enoch and I am a researcher studying deep generative models.\n\nI've started this project called Plexiglass, which is a PyTorch toolbox for cybersecurity research and testing against adversarial attacks and deepfakes.\n\nI would very much appreciate any suggestions/ feedbacks and even contributions.\n\nRepo is here: [https://github.com/enochkan/p](https://github.com/enochkan/safetynet)lexiglass", "upvote_ratio": 1.0, "id": "t3_jtchnd", "created_utc": 1605250397.0}
{"sub": "pytorch", "title": "iou computation code", "selftext": "Could anyone explain the last four lines (*lt, rb, inter,* and the return expression) of the following code ? I do not quite understand how : works inside \\[\\]  for pytorch\n\n    def box_iou(boxes1, boxes2):\n        # https://github.com/pytorch/vision/blob/master/torchvision/ops/boxes.py\n        \"\"\"\n        Return intersection-over-union (Jaccard index) of boxes.\n        Both sets of boxes are expected to be in (x1, y1, x2, y2) format.\n        Arguments:\n            boxes1 (Tensor[N, 4])\n            boxes2 (Tensor[M, 4])\n        Returns:\n            iou (Tensor[N, M]): the NxM matrix containing the pairwise\n                IoU values for every element in boxes1 and boxes2\n        \"\"\"\n    \n        def box_area(box):\n            # box = 4xn\n            return (box[2] - box[0]) * (box[3] - box[1])\n    \n        area1 = box_area(boxes1.t())\n        area2 = box_area(boxes2.t())\n    \n        lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]\n        rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # [N,M,2]\n    \n        inter = (rb - lt).clamp(min=0).prod(2)  # [N,M]\n        return inter / (area1[:, None] + area2 - inter)  # iou = inter / (area1 + area2 - inter)", "upvote_ratio": 1.0, "id": "t3_jsy47s", "created_utc": 1605199881.0}
{"sub": "pytorch", "title": "More info on the popular browser extension in AI/ML community!", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_jsm1md", "created_utc": 1605147185.0}
{"sub": "pytorch", "title": "How To Use UCF101, The Largest Dataset Of Human Actions", "selftext": "nan", "upvote_ratio": 0.9, "id": "t3_js7gwr", "created_utc": 1605098660.0}
{"sub": "pytorch", "title": "Appending tensor to itself ?", "selftext": "Hello All\n\nI have a tensor of size (1, 8, 1024). I wish to append this tensor to itself to make it (77, 8, 1024). How can I do it ?", "upvote_ratio": 1.0, "id": "t3_jrsvk6", "created_utc": 1605040153.0}
{"sub": "pytorch", "title": "Random seed with external GPU", "selftext": "Hi all,\n\nI bought a new Palit GeForce RTX 3070 GPU, to speed up my deep learning projects. My laptop is a Dell Latitude 5491 with an Nvidia GeForce MX130 and Intel UHD Graphics 630. I am using the GeForce RTX 3070 in a Razer Core X via Thunderbolt 3.0.\n\nI would like to make my pytorch training reproducible, so I am using:\ntorch.manual_seed(1)\nnp.random.seed(1)\nrandom.seed(1)\ntorch.cuda.manual_seed(1)\ntorch.cuda.manual_seed_all(1)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\nSymptom: When the device=\u201ccuda:0\u201d its addressing the MX130, and the seeds are working, I got the same result every time. When the device=\u201ccuda:1\u201d its addressing the RTX 3070 and I dont get the same results. Seems like with the external GPU the random seed is not working. When device=\u201ccuda\u201d its automatically uses the RTX 3070 and no reproducibility.\nI am working with num_workers=0 and worker_init_fn=np.random.seed(1) in the dataloder. So practically changing the executor GPU has effect on the random seed. I dont want to, and I am not using both GPU-s in parallel.\n\nHow can I make the work with external GPU reproducible? I would very appreciate any help. Thanks in advance!\n\nPytorch version: 1.7.0\nCuda toolkit: 11.0.221\nAnaconda version: 2020.07\n\nAccording to NVIDIA-SMI:\nCuda version: 11.1\nDriver version: 457.09", "upvote_ratio": 1.0, "id": "t3_jri43u", "created_utc": 1605000992.0}
{"sub": "pytorch", "title": "Constrained Optimization", "selftext": "Let's say I have a dataset X of size n x m (n rows, m columns) and Y of size n x 1. \n\nI have a model that uses X_i as an input and makes a prediction Y_hat_i.\n\nI suppose this means that I have a differentiable function:\n\ny_hat_i = F(X_i) \n\nAssuming my model is accurate and precise,\n\nI want to find the values of X_i that minimize Y_hat_i. Where X_i is a 1 x m vector and Y_hat_i is a scaler value, under a set of constraints. \n\nI have looked into many ways to this including SciPy's Optimize.minimize as well as linear programming but either I'm doing something wrong or it simply doesn't work for this use case. \n\nWhat is a good way to approach this? \n\nEssentially I have a function that models a business and I want to find optimal operational values (inventory, costs etc) that minimize (or maximize) a given performance metric. \n\nSimply predicting is not enough here, I want to actually \"optimize\" the business so I am looking for some insights to this.", "upvote_ratio": 1.0, "id": "t3_jr3gpy", "created_utc": 1604947385.0}
{"sub": "pytorch", "title": "CompressAI: A PyTorch Library For End-To-End Compression Research", "selftext": "A recent\u00a0[research paper published by InterDigital AI Lab](https://arxiv.org/abs/2011.03029)\u00a0introduces CompressAI. CompressAI is a platform that provides custom operations, layers, models, and tools to research, develop, and evaluate end-to-end image and video compression codecs. It uses pre-trained models and evaluation tools to compare learned methods with traditional codecs. Various models have been trained on learned end-to-end compression from scratch and re-implemented in PyTorch. Artificial Neural Network (ANN) based codecs have shown remarkable outcomes for compressing images. This framework currently implements models only for still-picture compression; however, it is believed to soon extend over to the video compression domain.\n\nSummary: [https://www.marktechpost.com/2020/11/09/compressai-a-pytorch-library-for-end-to-end-compression-research/](https://www.marktechpost.com/2020/11/09/compressai-a-pytorch-library-for-end-to-end-compression-research/)\n\nPaper: [https://arxiv.org/abs/2011.03029](https://arxiv.org/abs/2011.03029) \n\n&amp;#x200B;\n\nhttps://preview.redd.it/ef0i6odlr8y51.png?width=696&amp;format=png&amp;auto=webp&amp;s=cffc12b8130db42d4190081bef2fc3eb14e315cd", "upvote_ratio": 1.0, "id": "t3_jr0u11", "created_utc": 1604939542.0}
{"sub": "pytorch", "title": "How do I train two different models on two different GPUs at the same time?", "selftext": "Hello everyone!\n\nThis might sound as a very basic question, but I'm new to distributed training in Pytorch.\n\n&amp;#x200B;\n\nSo I have two models (net1 and net2) and I have two dataloaders for training and testing (train\\_dl and test\\_dl). I also have two available GPUs (\"cuda:0\" and \"cuda:1\"). There is a function called \"training\" that takes a model and two dataloders and returns the testing accuracy. How do I use this function a the same time?\n\n[net1.to](https://net1.to)(\"cuda:0\")\n\n[net2.to](https://net2.to)(\"cuda:1\")\n\naccuracy1 = training(net1, train\\_dl, test\\_dl)\n\naccuracy2 = training(net2, train\\_dl, test\\_dl)\n\n&amp;#x200B;\n\nWhat do I need to do to make the last two lines to execute at the same time? Simultaneously, not sequentially. I read something about \"spawn\" in the Pytorch distributed packages, but I have no clue of how to implement it. The simplest solution would be appreciated.\n\n&amp;#x200B;\n\nThank you!\n\n&amp;#x200B;\n\n**EDIT:** I should mention that this is for a Neuroevolution project. This means, I have, say, 10 networks saved in python list. I need to train them all, and I have two GPUs. To speed up the process, I'm going to take the networks in groups of two networks to train them. This means that I need to be able to train them at the same time inside the same program/script.", "upvote_ratio": 1.0, "id": "t3_jqzfsy", "created_utc": 1604935109.0}
{"sub": "pytorch", "title": "How does Pytorch handles BackPropagation in this case?", "selftext": "Hello! \n\n  \nI have a question. Let's say I have this network example.   \nclass NetExple(nn.Module):  \ndef \\_\\_init\\_\\_(self):  \nsuper(NetExple,self).\\_\\_init\\_\\_()  \nself.fc1 = nn.Linear(784,128)  \nself.fc2 = nn.Linear(128,64)  \nself.fc3 = nn.Linear(64,10)\n\n   def forward(self,x,mask1, mask2):  \nx = F.relu(self.fc1(x))   \nx = x \\* mask1  \nx = F.relu(self.fc2(x))  \nx = x \\* mask2  \nx = F.softmax(self.fc3(x),dim=1)\n\nreturn x  \n\n\nA custom mask is multiplied (This mask is customized). In this case, how does Pytorch handles backpropagation? Does it still apply the mask1/mask2 on the gradients during BackProp?  \n\n\nThanks a lot!", "upvote_ratio": 0.84, "id": "t3_jqw27b", "created_utc": 1604921717.0}
{"sub": "pytorch", "title": "In-reproducible loss/result for Bert model training with same settings but &gt;=2 times", "selftext": "Hi there,\n\n&amp;#x200B;\n\nI am using my customized bert script to train a model. However, everything even I keep the same setting for lr, AdamW weight decay and epoch, and run on the same platform (cuda on SageMaker) with same torch (1.5.0) and transformers (2.11.0) versions,  the results still change a lot in terms of the loss. This make my different experiments not comparable.\n\n&amp;#x200B;\n\nCan someone who has experienced this before or have any ideas please advice me on what should I do? I really want to solve this inreproducible issue so that I can continue on my experiments. Super appreciated for your help!\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nDetails as below:\n\n&amp;#x200B;\n\nFor example, if I set epoch = 4, lr = 1e-5, decay for AdamW as 0.01.\n\nFor one run I got this result for the first epoch only showing the last complete 100 batches result:\n\n`2020-10-19 03:45:29,032 - utils - INFO - | epoch   1 |  1300/ 1320 batches | lr 2.261e-05 | loss 0.267 | Elapsed 0:12:29`\n\n`2020-10-19 03:45:40,550 - utils - INFO -   Training epoch took: 0:12:41`\n\n`2020-10-19 03:45:40,550 - utils - INFO - Validating...`\n\n`2020-10-19 03:46:14,588 - utils - INFO - | loss 0.019 | Elapsed 0:00:34`\n\n`precision    recall  f1-score      support`\n\n`False          0.906472  0.979875  0.941745  2087.000000`\n\n`True           0.475000  0.152610  0.231003   249.000000`\n\n`accuracy       0.891695  0.891695  0.891695     0.891695`\n\n`macro avg      0.690736  0.566243  0.586374  2336.000000`\n\n`weighted avg   0.860480  0.891695  0.865986  2336.000000`\n\n`2020-10-19 03:46:15,403 - utils - INFO - Testing...`\n\n`2020-10-19 03:46:55,182 - utils - INFO - use model: 1 batch / 1319 step`\n\n`precision  recall  f1-score   support`\n\n`False             0.906   0.984     0.944  2344.000`\n\n`True              0.413   0.098     0.159   265.000`\n\n`accuracy          0.894   0.894     0.894     0.894`\n\n`macro avg         0.659   0.541     0.551  2609.000`\n\n`weighted avg      0.856   0.894     0.864  2609.000`\n\n`2020-10-19 03:46:55,188 - utils - INFO - best test F1 score: 0.8638224640164368`\n\n&amp;#x200B;\n\nAnd for the second attempt I got this for the first epoch:\n\n`2020-11-07 17:08:08,821 - utils - INFO - | epoch   1 |  1300/ 1320 batches | lr 2.261e-05 | loss 0.286 | Elapsed 0:12:25`\n\n`2020-11-07 17:08:20,487 - utils - INFO -   Training epoch took: 0:12:37`\n\n`2020-11-07 17:08:20,487 - utils - INFO - Validating...`\n\n`2020-11-07 17:08:54,609 - utils - INFO - | loss 0.018 | Elapsed 0:00:34`\n\n`precision    recall  f1-score      support`\n\n`False          0.893408  1.000000  0.943703  2087.000000`\n\n`True           0.000000  0.000000  0.000000   249.000000`\n\n`accuracy       0.893408  0.893408  0.893408     0.893408`\n\n`macro avg      0.446704  0.500000  0.471852  2336.000000`\n\n`weighted avg   0.798177  0.893408  0.843112  2336.000000`\n\n`2020-11-07 17:08:55,313 - utils - INFO - Testing...`\n\n`2020-11-07 17:09:34,934 - utils - INFO - use model: 1 batch / 1319 step`\n\n`precision  recall  f1-score   support`\n\n`False             0.898   1.000     0.946  2344.000`\n\n`True              0.000   0.000     0.000   265.000`\n\n`accuracy          0.898   0.898     0.898     0.898`\n\n`macro avg         0.449   0.500     0.473  2609.000`\n\n`weighted avg      0.807   0.898     0.850  2609.000`\n\n`2020-11-07 17:09:34,938 - utils - INFO - best test F1 score: 0.8503599608647853`\n\n&amp;#x200B;\n\nNote that, the last used lr rate per 100 batches are the same, while the average loss per 100 batches are slightly different. But this result in the predictions for the validation and testing data set very different.\n\n&amp;#x200B;\n\nI already set the seed during my model with this function below:\n\n&amp;#x200B;\n\n`def set_seed(seed):`\n\n`\"\"\" Set all seeds to make results reproducible (deterministic mode).`\n\n`When seed is a false-y value or not supplied, disables deterministic mode. \"\"\"`\n\n`random.seed(seed)`\n\n`np.random.seed(seed)`\n\n`torch.manual_seed(seed)`\n\n`torch.cuda.manual_seed_all(seed)`\n\n`torch.backends.cudnn.deterministic = True`\n\n`torch.backends.cudnn.benchmark = False`\n\n&amp;#x200B;\n\nThe model is just using the bert layers + nn.linear layer on the top. ", "upvote_ratio": 1.0, "id": "t3_jq3agg", "created_utc": 1604800702.0}
{"sub": "pytorch", "title": "Prepare strings for prediction using torchtext", "selftext": "Hey all :) \n\nThere are good instructions on how train a model using torchtext.  \n\n\nBut how do I preper it to production?  \n**How to create a PREDICT pipeline?**\n\nCuz the eval function is simple - I have everything ready to go.  \n**How to preper some new data in the same way?**\n\nFor example, here is some standard training process:  \ntokenize =&gt; padding =&gt; split=&gt; iterator\n\n&amp;#x200B;\n\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    \n    # Model parameter\n    MAX_SEQ_LEN = 32\n    PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n    UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)\n    \n    # Fields\n    id_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)\n    label_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)\n    text_field = Field(use_vocab=False, tokenize=tokenizer.encode, lower=False, include_lengths=False, batch_first=True,\n                       fix_length=MAX_SEQ_LEN, pad_token=PAD_INDEX, unk_token=UNK_INDEX)\n    fields = [('id',id_field), ('message', text_field),('label', label_field),]\n    \n    # TabularDataset\n    train, valid, test = TabularDataset.splits(path=data_dir, train='train.csv', validation='valid.csv',test='test.csv', format='CSV', fields=fields, skip_header=True)\n    \n    # Iterators\n    train_iter = BucketIterator(train, batch_size=16, sort_key=lambda x: len(x.message),\n                                device=device, train=True, sort=True, sort_within_batch=True)\n    valid_iter = BucketIterator(valid, batch_size=16, sort_key=lambda x: len(x.message),\n                                device=device, train=True, sort=True, sort_within_batch=True)\n    test_iter = Iterator(test, batch_size=16, device=device, train=False, shuffle=False, sort=False)\n\n  \n\n**How to prepare a simple list of strings for the model to predict?**\n\n    list_of_sentences=\n    ['not sure, still in progress',\n     'Yes',\n     'How can I increase my mbps',\n     'I have had to call every single month',\n     'Hi! Can you help me get started with my new phone?']\n    ]\n    \n    ....?\n    ....?\n    ....?\n    \n    model(please_predict_this)\n\nThanks :)", "upvote_ratio": 1.0, "id": "t3_jp27ha", "created_utc": 1604655104.0}
{"sub": "pytorch", "title": "Image Classification with OpenCV Java", "selftext": "We have used OpenCV with C++ and Python API and now we have a surprise for you. In this blog, we will show an example of how it can be used in a 3rd language - Java - using\u00a0OpenCV Java API.\n\nhttps://www.learnopencv.com/image-classification-with-opencv-java-2/\n\nHere is what we will do in the blog post:\n\n1. Convert the MobileNet classification model trained in PyTorch to ONNX\n\n2. Check the model prediction on a simple example\n\n3. Construct a Java pipeline for image classification\n\nhttps://preview.redd.it/9utokc01ujx51.png?width=600&amp;format=png&amp;auto=webp&amp;s=86e2a73805fbd10245ab289674c0cb3caeca8089", "upvote_ratio": 0.67, "id": "t3_joyr9z", "created_utc": 1604638352.0}
{"sub": "pytorch", "title": "2x 2080 Ti vs 1x 3080?", "selftext": "What would you rather get?", "upvote_ratio": 1.0, "id": "t3_jomks9", "created_utc": 1604596179.0}
{"sub": "pytorch", "title": "Is this loss function differentiable?", "selftext": "I am trying to develop a loss function by combining dice loss and cross-entropy loss for semantic segmentation (Multiclass). Got the idea from [this](https://www.kaggle.com/bigironsphere/loss-function-library-keras-pytorch#BCE-Dice-Loss) (Look at DiceBCELoss class for PyTorch),  but it's for single class. I want an exact definition for multiclass for which I have written this code in my forward method, (`inputs` are predictions from model &amp; `targets` is ground truth tensor one-hot encoded from masks. Both are of shape `(batch_size, C, H, W)`)\n\n    inputs = F.softmax(inputs, 1)\n            \n    CE = F.cross_entropy(inputs, torch.argmax(targets, 1), reduction='mean')\n    \n    inputs = inputs.view(-1)\n    targets = targets.view(-1)\n    \n    intersection = (inputs * targets).sum()                            \n    dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n    \n    Dice_CE = CE + dice_loss\n    \n    return Dice_CE\n\nThe problem is cross-entropy requires `targets` tensor to contain values b.w. 0 to C-1. And in dice loss, targets need to be one-hot encoded. So I chose to apply `torch.argmax` on targets (which is already one-hot encoded) to find cross-entropy loss. But I saw somewhere that `torch.argmax` is not differentiable, though I only use it on `targets`. So I am confused, is this loss function differentiable? If not how can I make it differentiable? Thanks for your time.", "upvote_ratio": 1.0, "id": "t3_jojoau", "created_utc": 1604586497.0}
{"sub": "pytorch", "title": "Is this a good way to use pytorch to build dense cnn?", "selftext": "I'm new to pytorch and I'm trying to build a dense cnn architecture.\nSo I made 4 classes DenseUnit, DenseBlock, BottleNeck and Output.\n\n    class DenseUnit(nn.Module):\n      def __init__(self,in_channels):\n    \n      def forward(self, x):\n  \n        return x\n    class DenseBlock(nn.Module):\n      def __init__(self,in_channels):\n    \n      def forward(self, x):\n          for _ in range(12):\n            \n        return x\n    class BottleNeck(nn.Module):\n      def __init__(self,in_channels):\n    \n      def forward(self, x):\n      \n        return x\n    class Output(nn.Module):\n      def __init__(self,in_channels):\n    \n      def forward(self, x):\n      \n        return x\nand finally I'm using these class objects in my model's `__init__()` and `forward()` to build my model.\n\nI have sent a random input and it gives me output without errors, but when used model summary it shows 0 trainable params for denseblocks.\n\nI'm wondering am I using pytorch correctly? If yes, how can I get params info for class objects too.", "upvote_ratio": 1.0, "id": "t3_jn8wsa", "created_utc": 1604404129.0}
{"sub": "pytorch", "title": "Suggested Environment for Developing C++/Cuda Extensions in Windows 10", "selftext": "I am trying to write a c++ extension using CUDA libraries in windows 10 following the tutorial \\[here\\]([https://pytorch.org/tutorials/advanced/cpp\\_extension.html](https://pytorch.org/tutorials/advanced/cpp_extension.html))\n\nI have python 3.6.11, pytorch 1.8.0.dev20201021, rtx 3080 gpu, cuda 11.1.\n\nI ended up getting pytorch c++ 1.7 (stable/debug) with cuda 11.0 working in Microsoft Visual Studio 2019 v 16.6.5 with the cl.exe compiler, and I am wondering what is the best way to write code with syntax completion, debugging abilities so that when I run python [setup.py](https://setup.py) install, that I can be sure it will work.\n\nQuestions are what is your suggested environment/debugging tools for windows 10 ?\n\nDo I need to use Nvidia Nsight Compute to debug the Cuda Code?\n\nWhat flags will I need to pass in to the nvcc compiler (c++11 and maybe --gpu-architecture=compute\\_86 --gpu-code=sm\\_86)\n\nShould I use debug/release pytorch c++ ?\n\n&amp;#x200B;\n\nMy question with code is in the discuss pytorch forum here\n\n[https://discuss.pytorch.org/t/suggested-environment-for-developing-c-cuda-extensions-in-windows-10/101426](https://discuss.pytorch.org/t/suggested-environment-for-developing-c-cuda-extensions-in-windows-10/101426)\n\n&amp;#x200B;\n\nAny tips or insights very much appreciated, would be happy to provide more information.", "upvote_ratio": 1.0, "id": "t3_jn1erz", "created_utc": 1604368230.0}
{"sub": "pytorch", "title": "Multi GPU Training", "selftext": "I want to test the performance of a power supply unit by accelerating multiple GPU usage. I have 6 parallel GTX 1050 ti. Looking for a deep learning program which will take long time to train and put pressure on the GPUs. I will use the CUDA platform, can anyone help me out with this?", "upvote_ratio": 0.67, "id": "t3_jn0vz5", "created_utc": 1604366377.0}
{"sub": "pytorch", "title": "Weights resets after each kfold?", "selftext": "Is is normal that the weights 'resets' after each kfold run ? Because the loss value seems to be poor at the beginning of each training iteration. Or do I have to load the best weights for every kfold  in some way?\n\n&amp;#x200B;\n\n \n\nfor\u00a0n\u00a0in range(EPOCHS):  \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0num\\_epochs\\_run=n  \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0train\\_loss=\u00a0eng.train(train\\_loader)  \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0valid\\_loss=\u00a0eng.validate(valid\\_loader)  \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0score\u00a0+=train\\_loss  \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0score\\_v\u00a0+=valid\\_loss  \n \\#Early\u00a0stopping\u00a0checking\u00a0if\u00a0model\u00a0validation\u00a0loss\u00a0does\u00a0imporve\u00a0other\u00a0wise\u00a0stop\u00a0after\u00a0n\u00a0steps.  \n \\#Bstops\u00a0if\u00a0no\u00a0improves\u00a0is\u00a0seen\u00a0  \n if\u00a0valid\\_loss\u00a0&lt;\u00a0best\\_loss:  \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0epochs\\_no\\_improve=0  \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0best\\_loss=\u00a0valid\\_loss  \n print(\u00a0f\"Best\u00a0loss:\u00a0{best\\_loss}\u00a0at\u00a0epoch:\u00a0{n}\")  \n if\u00a0n==EPOCHS:  \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0torch.save(model.state\\_dict(),f\"fold{fold}-epoch{num\\_epochs\\_run}.pth\")  \n else:  \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0epochs\\_no\\_improve\u00a0+=\u00a01  \n if\u00a0epochs\\_no\\_improve\u00a0==\u00a0early\\_stopping:  \n print(f\"\u00a0Early\u00a0stopping\u00a0at\u00a0epoch:\u00a0{n}\")  \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0early\\_stop=\u00a0True  \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0torch.save(model.state\\_dict(),weights\\_path+'best\\_pytorch\\_weights.pth')  \n break  \n else:  \n continue  \n if\u00a0early\\_stop:  \n print('Stopped')  \n \\#torch.save(model.state\\_dict(),weights\\_path+'best\\_pytorch\\_weights.pth')  \n break", "upvote_ratio": 1.0, "id": "t3_jmcx8l", "created_utc": 1604274858.0}
{"sub": "pytorch", "title": "Weights resets after each epoch?", "selftext": "Is is normal that the weights 'resets' after each kfold run ? Because the loss value seems to be poor at the beginning of each training iteration. Or do I have to load the best weights for every kfold  in some way?\n\nhttps://preview.redd.it/sajzy1pwrpw51.png?width=336&amp;format=png&amp;auto=webp&amp;s=582ce271f99915c8fb1b3829bfc06ae9cb7f63c0\n\n This is the model training code\n\nfor\u00a0n\u00a0in range(EPOCHS):  \nnum\\_epochs\\_run=n  \ntrain\\_loss=\u00a0eng.train(train\\_loader)  \nvalid\\_loss=\u00a0eng.validate(valid\\_loader)  \nscore\u00a0+=train\\_loss  \nscore\\_v\u00a0+=valid\\_loss  \n \\#Early\u00a0stopping\u00a0checking\u00a0if\u00a0model\u00a0validation\u00a0loss\u00a0does\u00a0imporve\u00a0other\u00a0wise\u00a0stop\u00a0after\u00a0n\u00a0steps.  \n \\#Bstops\u00a0if\u00a0no\u00a0improves\u00a0is\u00a0seen\u00a0  \n if\u00a0valid\\_loss\u00a0&lt;\u00a0best\\_loss:  \nepochs\\_no\\_improve=0  \nbest\\_loss=\u00a0valid\\_loss  \n print(\u00a0f\"Best\u00a0loss:\u00a0{best\\_loss}\u00a0at\u00a0epoch:\u00a0{n}\")  \n if\u00a0n==EPOCHS:  \ntorch.save(model.state\\_dict(),f\"fold{fold}-epoch{num\\_epochs\\_run}.pth\")  \n else:  \nepochs\\_no\\_improve\u00a0+=\u00a01  \n if\u00a0epochs\\_no\\_improve\u00a0==\u00a0early\\_stopping:  \n print(f\"\u00a0Early\u00a0stopping\u00a0at\u00a0epoch:\u00a0{n}\")  \nearly\\_stop=\u00a0True  \ntorch.save(model.state\\_dict(),weights\\_path+'best\\_pytorch\\_weights.pth')  \n break  \n else:  \n continue  \n if\u00a0early\\_stop:  \n print('Stopped')  \n \\#torch.save(model.state\\_dict(),weights\\_path+'best\\_pytorch\\_weights.pth')  \n break  \n", "upvote_ratio": 0.75, "id": "t3_jmcmin", "created_utc": 1604273738.0}
{"sub": "pytorch", "title": "How to use PyTorch\u2019s DataLoader together with skorch\u2019s GridSearchCV", "selftext": "I have a question on how to use PyTorch\u2019s DataLoader together with skorch\u2019s GridSearchCV, which I have posted here in stackoverflow: https://stackoverflow.com/questions/64628130/how-to-use-pytorch-s-dataloader-together-with-skorch-s-gridsearchcv\n\nWould really appreciate any help on this. Many thanks in advance.", "upvote_ratio": 1.0, "id": "t3_jlvrmt", "created_utc": 1604201161.0}
{"sub": "pytorch", "title": "CNN: accuracy and loss are increasing and decreasing", "selftext": "Hello,\n\ni am trying to create 3d CNN using pytorch.\n\nthe problem that the accuracy and loss are increasing and decreasing (accuracy  values are between 37% 60%)  \n\nNOTE: if I delete dropout layer the accuracy and loss values remain unchanged for all epochs\n\nDo you know what I am doing wrong here?\n\nThanks in advance!\n\n    import numpy as np\n    import torch\n    torch.autograd.set_detect_anomaly(True)\n    REBUILD_DATA = True # set to true to one once, then back to false unless you want to change something in your training data.\n    import time\n    start_time = time.time()\n    import os\n    import cv2\n    import numpy as np\n    import nibabel as nib\n    from nibabel.testing import data_path\n    from sklearn.utils import shuffle\n    import matplotlib.pyplot as plt\n    import cv2\n    \n    from sklearn.metrics import ConfusionMatrixDisplay# Build the confusion matrix of our 2-class classification problem  \n    from sklearn.metrics import confusion_matrix\n    from sklearn.metrics import accuracy_score # for evaluating the model\n    import torch # PyTorch libraries and modules\n    import torch.nn as nn\n    import torch.nn.functional as F\n    from torch.optim import *\n    from sklearn.metrics import classification_report\n    import seaborn as sns # color of graphe\n    '''from  owlready2 import *\n    import textdistance\n    '''\n    import numpy as np\n    \n    \n    \n    #Channels ordering : first channel (taille , shape of each element ) to ==&gt; last channel ( shape, size )\n    def changechannel(data, a, b):\n        data = np.asarray(data)\n        data = np.rollaxis(data, a, b)\n        return(data)\n    \n    # convert (240,240,155) to ======&gt; (120, 120, 120) #\n    def resize3Dimages(data):\n        train_x = []\n        for i in range(len(data)):\n            image = data[i] \n            width = 120\n            height = 120\n            img_zeros = np.zeros((len(image), width, height)) #### len(image) means the first dim /// in other words shape[0]\n    \n            for idx in range(len(image)):\n                img = data[i][idx, :, :]\n                img_sm = cv2.resize(img, (width, height), interpolation=cv2.INTER_CUBIC) #une interpolation bicubique sur un voisinage de 4 \u00d7 4 voxels \n                img_zeros[idx, :, :] = img_sm\n    #  convert (240,120,120) to ======&gt; (120,120,120)\n            img_zeros = img_zeros[::2, :, :] ### 240/2 =120 \n            train_x.append(img_zeros)\n    ############################# save images in list ################################\n        return(np.asarray(train_x)) ## convert list to nd array \n    # end ...\n    # 1 channel to 3 channel \n    def channel1to3 (data): \n        data = np.stack((data,) * 3, axis=-1)\n        return(data)\n    print(\" preprocessing  --- %s seconds ---\" % (time.time() - start_time))\n    \n    print('Building of CNN')\n    import os\n    # for reading and displaying images\n    import matplotlib.pyplot as plt\n    # for evaluating the model\n    from sklearn.metrics import accuracy_score\n    # PyTorch libraries and modules\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    from torch.optim import *\n    \n    num_classes = 2\n    \n    # Create CNN Model\n    class CNNModel(nn.Module):\n        def __init__(self):\n            super(CNNModel, self).__init__() \n            \n            self.conv_layer1 = self._conv_layer_set(3, 32) \n                                                           \n            self.conv_layer2 = self._conv_layer_set(32, 64) \n            self.conv_layer3 = self._conv_layer_set(64, 128)\n            self.conv_layer4 = self._conv_layer_set(128, 256)\n            self.conv_layer5 = self._conv_layer_set(256, 512)\n    \n          \n            self.fc1 = nn.Linear(512, 128)\n            self.fc2 = nn.Linear(128, num_classes)\n            self.relu = nn.ReLU()\n            self.batch=nn.BatchNorm1d(128)\n            self.drop=nn.Dropout(p=0.6, inplace = True)   \n            \n        def _conv_layer_set(self, in_c, out_c):\n            conv_layer = nn.Sequential(\n            nn.Conv3d(in_c, out_c, kernel_size=(3, 3, 3), padding=0),\n            nn.ReLU(),\n            nn.MaxPool3d((2, 2, 2)),\n            )\n            return conv_layer\n        \n    \n        def forward(self, x):\n            # Set 1\n            out = self.conv_layer1(x)\n            out = self.conv_layer2(out)\n            out = self.conv_layer3(out)\n            out = self.conv_layer4(out)\n            out = self.conv_layer5(out)\n            out = out.view(out.size(0), -1)\n            out = self.fc1(out)\n            out = self.relu(out)\n            out = self.batch(out) # batchnormalization \n            out = self.drop(out)\n            out = self.fc2(out)\n            #out = F.softmax(out, dim=1)\n            return out\n    #Definition of hyperparameters\n    n_iters = 2\n    num_epochs =25\n    # Create CNN\n    model = CNNModel()\n    model.cuda() #  GPU\n    print(model)\n    # Cross Entropy Loss \n    for param in model.parameters():\n        param.requires_grad = False # prendre false si \"you want to freeze model weights\" TRUE si le poids change \n        error = nn.CrossEntropyLoss()\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=0.000001)\n    \n    ###################################################accuracy function ##################################\n    def accuracyCalc (predicted, targets):\n        correct = 0\n        p = predicted.tolist()\n        t = targets.flatten().tolist() # flatten [[0],[1]] ===&gt; [1,0] tolist \n        for i in range(len(p)):\n            if (p[i] == t[i]):\n                correct +=1\n        accuracy = 100 * correct / targets.shape[0]\n        return(accuracy)\n    #######################################################################################################\n    print(\" build model --- %s seconds ---\" % (time.time() - start_time))\n    #######################################################{{{{{{{training}}}}}}}##################################\n    print('data preparation ')\n    training_data = np.load(\"Datasets/brats/Train/training_data.npy\", allow_pickle=True)\n    \n    targets = np.load(\"Datasets/brats/Train/targets.npy\", allow_pickle=True)\n    \n    \n    from sklearn.utils import shuffle\n    training_data, targets = shuffle(training_data, targets)\n    \n    training_data = changechannel(training_data, 1, 5) #Channels ordering : first channel to ==&gt; last channel'\n    training_data  = resize3Dimages(training_data) #resize images\n    training_data = channel1to3(training_data,)#1 channel to 3 channel ===&gt; RGB\n    training_data = changechannel(training_data, 4, 1)# last to first\n    \n    #Definition of hyperparameters\n    loss_list_train = []\n    accuracy_list_train = []\n    for epoch in range(num_epochs): \n        outputs = []\n        outputs= torch.tensor(outputs).cuda()\n        for fold in range(0, len(training_data), 4): # we will take 4 images\n            xtrain = training_data[fold : fold+4]\n            xtrain =torch.tensor(xtrain).float().cuda() #  GPU\n            xtrain = xtrain.view(4, 3, 120, 120, 120) \n            # Clear gradients\n            # Forward propagation\n            optimizer.zero_grad() \n            v = model(xtrain)\n            outputs = torch.cat((outputs,v.detach()),dim=0)\n            # Calculate softmax and ross entropy loss\n        targets = torch.Tensor(targets)\n        labels = targets.cuda()\n        outputs = torch.tensor(outputs,  requires_grad=True) \n        _, predicted = torch.max(outputs, 1) \n        accuracy = accuracyCalc(predicted, targets)\n        labels = labels.long() \n        labels=labels.view(-1) #\n        loss = nn.CrossEntropyLoss()\n        loss = loss(outputs, labels)    \n        # Calculating gradients\n        loss.backward()\n        # Update parameters\n        optimizer.step()\n        loss_list_train.append(loss.data)\n        accuracy_list_train.append(accuracy/100)\n        np.save('Datasets/brats/accuracy_list_train.npy', np.array(accuracy_list_train))\n        np.save('Datasets/brats/loss_list_train.npy', np.array(loss_list_train)) \n        print('Iteration: {}/{}  Loss: {}  Accuracy: {} %'.format(epoch+1,  num_epochs, loss.data, accuracy))\n    print('Model training  : Finished')\n\n&amp;#x200B;\n\nresult is :\n\n    Iteration: 1/25 Loss: 0.8488530516624451 Accuracy: 48.0 %\n    Iteration: 2/25 Loss: 0.7767133116722107 Accuracy: 48.0 %\n    Iteration: 3/25 Loss: 0.7962564826011658 Accuracy: 52.0 %\n    Iteration: 4/25 Loss: 0.7540275454521179 Accuracy: 49.333333333333336 %\n    Iteration: 5/25 Loss: 0.9554114937782288 Accuracy: 38.666666666666664 %\n    Iteration: 6/25 Loss: 0.8776708245277405 Accuracy: 45.333333333333336 %\n    Iteration: 7/25 Loss: 0.9581964015960693 Accuracy: 37.333333333333336 %\n    Iteration: 8/25 Loss: 0.8645199537277222 Accuracy: 52.0 %\n    Iteration: 9/25 Loss: 0.862994909286499 Accuracy: 50.666666666666664 %\n    Iteration: 10/25 Loss: 0.6595868468284607 Accuracy: 60.0 %\n    Iteration: 11/25 Loss: 0.8252826929092407 Accuracy: 45.333333333333336 %\n\n&amp;#x200B;", "upvote_ratio": 0.75, "id": "t3_jlgwrw", "created_utc": 1604144194.0}
{"sub": "pytorch", "title": "A template to write my first neural network (that classifies digits) with pytorch??", "selftext": "I have searched a lot, and I am still confused. Is there any easy guide to write a neural network in pytorch? I work on the classic example with digits - image classification).", "upvote_ratio": 1.0, "id": "t3_jlf0ku", "created_utc": 1604133786.0}
{"sub": "pytorch", "title": "current GPU recommendation for pytorch", "selftext": "Hi i am building a new computer specifically for pytorch ML and looking to make a purchase around December. First question, are AMD rx 6000 series compatible with pytorch?\n\nI hear the nvidia rtx 3080/3090 are not very well optimized for pytorch at the moment, when is it likely developers will make full use of these cards? How do these cards compare to the rtx 2000 series in terms of performance at the moment with torch?", "upvote_ratio": 1.0, "id": "t3_jl1a15", "created_utc": 1604079253.0}
{"sub": "pytorch", "title": "Struggling with moving code over to a GPU, Reposted with link to forum with better formatting.", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_jkkxz8", "created_utc": 1604012517.0}
{"sub": "pytorch", "title": "Tips for training PyTorch models on TPUs", "selftext": "I spend the last few weeks getting my PyTorch code working on TPUs. I made a short thread on points that might be helpful ([https://twitter.com/KaliTessera/status/1321454533671870466](https://twitter.com/KaliTessera/status/1321454533671870466)).", "upvote_ratio": 1.0, "id": "t3_jjok7x", "created_utc": 1603894644.0}
{"sub": "pytorch", "title": "MRI Image Classification : A step by step guide", "selftext": "Today we have an exciting post on Classifying Knee MRI images using Deep Learning. In this post, you will learn  \n\n\n1. What an MRI dataset looks like.\n2. What is Stanford MRNet Challenge\n3. How to create an AI model for MRI data classification\n4. Results\n5. Suggestions for alternative approaches.\n\n[https://www.learnopencv.com/stanford-mrnet-challenge-classifying-knee-mris/](https://click.convertkit-mail.com/v8u6ek0726urhm5krpcw/9qhzhdu277vrwkf9/aHR0cHM6Ly93d3cubGVhcm5vcGVuY3YuY29tL3N0YW5mb3JkLW1ybmV0LWNoYWxsZW5nZS1jbGFzc2lmeWluZy1rbmVlLW1yaXMv)  \n\n\nand the **PyTorch code** is at  \n\n\n[https://github.com/spmallick/learnopencv/tree/master/MRNet-Single-Model](https://click.convertkit-mail.com/v8u6ek0726urhm5krpcw/3ohphdudww5gmoar/aHR0cHM6Ly9naXRodWIuY29tL3NwbWFsbGljay9sZWFybm9wZW5jdi90cmVlL21hc3Rlci9NUk5ldC1TaW5nbGUtTW9kZWw=) \n\nhttps://preview.redd.it/xmovkhbp4pv51.jpg?width=600&amp;format=pjpg&amp;auto=webp&amp;s=641c1e7b4ed9d23edb12b8e7c95939822aa943d0", "upvote_ratio": 1.0, "id": "t3_jj91ge", "created_utc": 1603830081.0}
{"sub": "pytorch", "title": "PyTorch 1.7 released w/ CUDA 11, New APIs for FFTs, Windows support for Distributed training and more", "selftext": "nan", "upvote_ratio": 0.98, "id": "t3_jj8dbc", "created_utc": 1603828056.0}
{"sub": "pytorch", "title": "IDE for python (pytorch)", "selftext": "Hallo,\n\nI recently trying to migrate from **MATLAB** to **python**. I am trying to find a decent **IDE**. I work with **pytorch** mostly. I find **Spyder** very appealing due to variable explorer (reminds me MATLAB). However I read that **Visual Studio** is more widely used. Also I see that **jupyter** is good for the \"portability\". \n\nAny guidance here?", "upvote_ratio": 0.5, "id": "t3_jj2cub", "created_utc": 1603809651.0}
{"sub": "pytorch", "title": "Cannot move batch of images to gpu", "selftext": "Hello,\n\nI have a \"train\" method as shown below.  When I run it, I get an error that my input and weight type should be the same - which I understand as meaning that my network has been pushed to the GPU but my batch of images has not been.  Indeed, when I print out the device of my batch of images it says it is still on the CPU even though doing basically the same thing to the network moves it to the GPU.  Do you know what I am doing wrong here?\n\nThanks in advance!\n\n&amp;#x200B;\n\n    def train(net, trainset, testset, n, bs):\n        device = torch.device('cuda:0')\n        net.to(device)\n        o = optim.Adam(net.parameters(), lr=0.01)\n        trainloader = torch.utils.data.DataLoader(trainset, batch_size=bs)\n        testloader = torch.utils.data.DataLoader(testset, batch_size=bs)\n        \n        best_net = net\n        best_loss =  float('inf')\n        \n        #This is the epoch loop - one execution = one epoch\n        for e in range(n):\n            total_loss = 0\n            total_accuracy = 0\n            validation_loss = 0\n            validation_accuracy = 0\n            \n            #Will need gradient while training...\n            net.requires_grad = True\n            \n            #Get all batches...\n            for batch in trainloader:\n                images, labels = batch\n                images.to(device)\n                labels.to(device)\n                \n                for l in net.parameters():\n                    print(l.device)\n                print(images.device)\n                \n                #Get predictions from the network\n                preds = net(images)\n\noutput from print statements:\n\n    cuda:0\n    cuda:0\n    cuda:0\n    cuda:0\n    cuda:0\n    cuda:0\n    cuda:0\n    cuda:0\n    cuda:0\n    cuda:0\n    cpu\n\nError Message:\n\n    RuntimeError                              Traceback (most recent call last)\n    &lt;ipython-input-107-764051b26d04&gt; in &lt;module&gt;\n    ----&gt; 1 bnet = train(net, train_set, test_set, 10, 100)\n    \n    &lt;ipython-input-106-c6b2a1bc7e39&gt; in train(net, trainset, testset, n, bs)\n         30 \n         31             #Get predictions from the network\n    ---&gt; 32             preds = net(images)\n         33 \n         34             #Find the loss\n    \n    ~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\n        720             result = self._slow_forward(*input, **kwargs)\n        721         else:\n    --&gt; 722             result = self.forward(*input, **kwargs)\n        723         for hook in itertools.chain(\n        724                 _global_forward_hooks.values(),\n    \n    &lt;ipython-input-10-fd501e1e41e9&gt; in forward(self, t)\n         14 \n         15         #Layer 1\n    ---&gt; 16         t = self.conv1(t)\n         17         t = F.relu(t)\n         18         t = F.max_pool2d(t, kernel_size=2, stride=2)\n    \n    ~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\n        720             result = self._slow_forward(*input, **kwargs)\n        721         else:\n    --&gt; 722             result = self.forward(*input, **kwargs)\n        723         for hook in itertools.chain(\n        724                 _global_forward_hooks.values(),\n    \n    ~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py in forward(self, input)\n        417 \n        418     def forward(self, input: Tensor) -&gt; Tensor:\n    --&gt; 419         return self._conv_forward(input, self.weight)\n        420 \n        421 class Conv3d(_ConvNd):\n    \n    ~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py in _conv_forward(self, input, weight)\n        414                             _pair(0), self.dilation, self.groups)\n        415         return F.conv2d(input, weight, self.bias, self.stride,\n    --&gt; 416                         self.padding, self.dilation, self.groups)\n        417 \n        418     def forward(self, input: Tensor) -&gt; Tensor:\n    \n    RuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same\n\n&amp;#x200B;", "upvote_ratio": 1.0, "id": "t3_jirz7v", "created_utc": 1603762219.0}
{"sub": "pytorch", "title": "Loss flattens out", "selftext": "Hi all,\n\nI made a post on the Pytorch forums about my issue, please could someone help?\n\n[https://discuss.pytorch.org/t/loss-flattens-out/100699](https://discuss.pytorch.org/t/loss-flattens-out/100699)\n\n\\--------------------------------------------------\n\nI have worked on a couple of projects now where I have constructed a neural net for binary classification and something like this has happened. This leads me to believe it's something to do with how I am programming it rather than the data itself.\n\nI have tried all kinds of things, even posted here before but I'm not sure why. I did not recycle my own code or anything so I don't know how this problem persists across projects.\n\n&amp;#x200B;\n\nThe model is simple:\n\n`# A simple binary classification model`\n\n`class BinaryClassifier(nn.Module):`\n\n`def __init__(self, input_size, hidden_size, num_classes=1):`\n\n`super().__init__()`\n\n`self.input_size = input_size`\n\n`self.num_classes = num_classes`\n\n`self.linear1 = nn.Linear(input_size, hidden_size)`\n\n`self.linear2 = nn.Linear(hidden_size, num_classes)`\n\n`self.relu = nn.ReLU()`\n\n`def forward(self, x):`\n\n`x = self.linear1(x)`\n\n`x = self.relu(x)`\n\n`x = self.linear2(x)`\n\n`return x`\n\nI have set num\\_classes as 1 in order to get a probability that the label is 1. I wasn't sure how else to structure this binary classification.\n\nI use BCEWithLogitsLoss, Adam with a StepLR scheduler\n\n`model = BinaryClassifier(X_tensor.shape[1], 512)`\n\n`criterion = nn.BCEWithLogitsLoss()`\n\n`optimizer = torch.optim.Adam(model.parameters(), lr=lr)`\n\n`scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)`\n\nIt's worth mentioning here I only added the scheduler to fix the problem and it did improve the loss but not the flattening out effect.\n\nThis is how I train the model:\n\n`def sub_train_(model, dataloader):`\n\n`model.train()`\n\n`losses = list()`\n\n`for idx, (X, y) in enumerate(dataloader):`\n\n`out = model(X)`\n\n`loss = criterion(out, y.unsqueeze(1))`\n\n`optimizer.zero_grad()`\n\n`loss.backward()`\n\n`optimizer.step()`\n\n`losses.append(loss.item())`\n\n`return np.mean(losses), model`\n\n&amp;#x200B;\n\n`def train(model, trainloader, testloader, scheduler, n_epochs):`\n\n`best_model = model`\n\n`best_loss = math.inf`\n\n`ts = time.time()`\n\n`losses = list()`\n\n`for epoch in range(n_epochs):`\n\n`train_loss, model = sub_train_(model, trainloader)`\n\n`test_loss = sub_valid_(model, testloader)`\n\n`scheduler.step()`\n\n`losses.append(train_loss)`\n\n`if train_loss &lt; best_loss:`\n\n`best_loss = train_loss`\n\n`best_model = model`\n\n`print('Epoch: {}, train_loss: {}, test_loss: {}'.format(`\n\n`epoch, train_loss, test_loss`\n\n`))`\n\n`te = time.time()`\n\n`fig, ax = plt.subplots()`\n\n`ax.plot(range(n_epochs), losses)`\n\n[`plt.show`](https://plt.show)`()`\n\n`mins = int((te-ts) / 60)`\n\n`secs = int((te-ts) % 60)`\n\n`print('Training completed in {} minutes, {} seconds.'.format(mins, secs))`\n\n`return losses, best_model`\n\nAnd every time it yields a loss plot like this:\n\nhttps://preview.redd.it/ocwt26vg0iv51.png?width=457&amp;format=png&amp;auto=webp&amp;s=dcc9935fa17cc008a488d659ffc9a66dbda20069\n\n&amp;#x200B;\n\nWhat am I doing wrong here? How can I avoid this happening?\n\nThank you in advance to anyone who helps me out with this!", "upvote_ratio": 0.67, "id": "t3_jimfls", "created_utc": 1603743813.0}
{"sub": "pytorch", "title": "Pytorch hackathon went against its own rules, who should I contact?", "selftext": "For your reference, [this](https://pytorch2020.devpost.com/project-gallery) is the link to the hackathon.\n\nAccording to the [rules](https://imgur.com/a/oFyw8Ia), there are honorable mention prizes for those who are scoring 3rd to 8th. Although it is not a big deal to most, I find that people should be correctly awarded the right prize given the circumstance.\n\nEdit: As you can clearly see, there are no honorable mention prizes given to participants\n\nThus I asked twice, [once](https://imgur.com/a/pbAw9mF) in the official slack channel where I got straight up ignored, and on the [official](https://imgur.com/Zv5er2d) devpost q/a section, where the manager was irresponsive.\n\nThis is a literal shot in the dark but what should I do to solve this?? To whom do I even contact at this point?", "upvote_ratio": 0.43, "id": "t3_jhwdoe", "created_utc": 1603643866.0}
{"sub": "pytorch", "title": "Matrix-NMS Questions", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_jhui6a", "created_utc": 1603637243.0}
{"sub": "pytorch", "title": "How does one install custom binaries for the Knights Landing CPU architecture for PyTorch?", "selftext": "nan", "upvote_ratio": 0.5, "id": "t3_jhsy7w", "created_utc": 1603630792.0}
{"sub": "pytorch", "title": "Does DataLoader accept string values as labels? What values does it accept?", "selftext": "I'm trying to train a simple classifier with PyTorch, and in an attempt to do something other than just follow along a tutorial I am training it to classify lists into two categories: \"repeating\" and \"increasing\".\n\nSo I tried to build some code to work with DataLoader, as I'm generating these lists. It looks like this:\n\n    class ListDataset (Dataset):\n        def __init__(self, num_lists = num_data, verbose = 0):\n            x_data = []\n            y_data = []\n            # Generate the appropriate number of lists\n            for i in range (num_lists):\n                x, y = generate_list(verbose=verbose)\n                x_data.append(x)\n                y_data.append(y)\n            # Record, though I could have done this with less code\n            self.x_data = x_data\n            self.y_data = y_data\n            self.length = len(self.x_data)\n    \n            print(self.y_data)\n            print(self.y_data[0])\n\nIt's not working properly, when I try to call `label =` [`label.to`](https://label.to)`(self.device)` I get the error `AttributeError: 'tuple' object has no attribute 'to'` and printing the label before that gives something which looks like this: `(\"increasing\", \"repeating\" .... \"repeating\", \"increasing\")` The problem is not with this part of the code, as it was pretty much lifted from a tutorial. \n\nI'm sure that the problem is with the data code, but I'm not quite sure, is it that my labels are strings? What should they be? 1 and 0 ints?", "upvote_ratio": 1.0, "id": "t3_jhqlny", "created_utc": 1603617816.0}
{"sub": "pytorch", "title": "A new deep learning computer vision library!", "selftext": "Hi guys, I hope you are well and healthy!  I have put together a compact, concise, and customizable deep learning computer vision library called \"glasses\". \n\nGithub: [https://github.com/FrancescoSaverioZuppichini/glasses](https://github.com/FrancescoSaverioZuppichini/glasses)\n\nWebsite: [https://francescosaveriozuppichini.github.io/glasses-webapp/](https://francescosaveriozuppichini.github.io/glasses-webapp/)\n\nIt is the first beta so there are a lot of missing models and features that I will add in the future.\n\nI do computer vision for a living and I wanted to have a flexible easy to use tool for my daily work. Most of the current libraries are very badly written with tons of code repetition and not very easy to customize. Moreover, it is hard to understand how most models are implemented and I hope my library will make it easier for new people in the field.\n\nUnfortunately, I still haven't found a good place to host the pre-trained models so they are not available at the moment (any suggestions?)\n\nI also would like to know if anyone wants to help me out with some feedback or in the coding!", "upvote_ratio": 0.96, "id": "t3_jhc7od", "created_utc": 1603558200.0}
{"sub": "pytorch", "title": "Dynamic Sky Replacement and Harmonization in Videos", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_jh1uxv", "created_utc": 1603509829.0}
{"sub": "pytorch", "title": "scatter_add reduce output dimensions/shape", "selftext": "I have memory issues with the scatter\\_add function.\n\nThe standard implementation computes, e.g. self\\[i\\]\\[index\\[i\\]\\[j\\]\\[k\\]\\[m\\]\\]\\[k\\]\\[m\\] += src\\[i\\]\\[j\\]\\[k\\]\\[m\\], where the output, will have shape I x Index\\_Max x K x M. This is often a large tensor, causing memory issues. I then sum over certain dimensions., e.g. if choose dimension 0,3 it will then compute a tensor shape 1x Index\\_Max x K x 1.\n\nI am looking for an option, or workaround, to choose some of the output dimensions to have shape 1. In the example above, the scatter\\_add function would compute\n\nself\\[0\\]\\[index\\[i\\]\\[j\\]\\[k\\]\\[m\\]\\]\\[k\\]\\[0\\] += src\\[i\\]\\[j\\]\\[k\\]\\[m\\]\n\n(or ignore dimensions 0,3 altogether), not computing the entire output tensor.\n\nThank you for the suggestions!", "upvote_ratio": 1.0, "id": "t3_jgs94p", "created_utc": 1603476014.0}
{"sub": "pytorch", "title": "Is there something like torch.utils.data.TensorDataset(*tensors) for TensorFlow/Keras?", "selftext": "I am \"translating\" a notebook made in Pytorch to one made in Keras.  And they use that app to pack the data from a tensor into the dataset that will be used for the network. But I can't find something that fulfills that function. I would greatly appreciate the help! \n\nPytorch documentation says that torch.utils.data.TensorDataset (* tensors) does: \n\n\"Dataset wrapping tensors. \nEach sample will be retrieved by indexing Tensor a along the first dimension.\"\n\nThank you everybody!", "upvote_ratio": 1.0, "id": "t3_jgii9y", "created_utc": 1603438254.0}
{"sub": "pytorch", "title": "How to apply a safe softmax", "selftext": "I'm training a model that applies softmax across an axis, in the following way:\n\n x = F.softmax(x.float(), dim=-1)\n\nHowever, some rows in x are only -inf, leading to an output of NaN.  As such, I would like these rows to be outputted with something else, like a zero vector, for example.  So in affect, I want an if statement that only applies softmax when the values aren't all -inf.  How does one do this in pytorch?", "upvote_ratio": 1.0, "id": "t3_jgfloz", "created_utc": 1603425306.0}
{"sub": "pytorch", "title": "Experiment Logging with TensorBoard and wandb", "selftext": "Training a machine learning model is an iterative process. You first implement a baseline solution and measure its quality.  \nOften, quite a few experiments need to be performed before a good solution is obtained. Once you obtain a good solution, it is important that you have all the information necessary to reproduce the same result. That\u2019s why tracking the best hyperparameter set is so important.\n\nhttps://preview.redd.it/w2y64aca1ou51.jpg?width=600&amp;format=pjpg&amp;auto=webp&amp;s=14345263d51671f89f187cce474e92c1690b3881\n\nIt is also cumbersome without the right tools!  \nIn today's post, we will learn how to log your experiments like a pro using these two tools.\n\n1. **Tensorboard**\n2. **Weights and Biases Developer Tools**\n\nTo learn the details please check out the post below.\n\n[https://www.learnopencv.com/experiment-logging-with-tensorboard-and-wandb/](https://click.convertkit-mail.com/xmu2qe4ng2a6hk4exlbg/e0hph0unxznz08i8/aHR0cHM6Ly93d3cubGVhcm5vcGVuY3YuY29tL2V4cGVyaW1lbnQtbG9nZ2luZy13aXRoLXRlbnNvcmJvYXJkLWFuZC13YW5kYi8=)", "upvote_ratio": 0.5, "id": "t3_jg1wdc", "created_utc": 1603380973.0}
{"sub": "pytorch", "title": "Can someone explain problems of running multiple jobs on 1 gpu?", "selftext": "Say I wanted to run a hyperparameter search over a model. If I was running on a CPU theres options like GridSearchCV that allows me to parallelise this process over a single CPU, resulting in a speedup.\n\nHowever, situations like this don't seem to exist on a GPU. I guess I could spawn several workers on a GPU and send a job to each of them. However, people seem sceptical of this approach (https://discuss.pytorch.org/t/split-single-gpu/18651/8) saying that it will not result in a performance increase.\n\nCan someone explain to me why? I thought GPUs were \"good\" at parallelising ML problems. They can vectorise some problems and parallelise them, so why not my neural network? What am I missing here?", "upvote_ratio": 1.0, "id": "t3_jfxi2n", "created_utc": 1603364498.0}
{"sub": "pytorch", "title": "Image-Driven Furniture Style for Interactive 3D Scene Modeling", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_jfrvag", "created_utc": 1603336532.0}
{"sub": "pytorch", "title": "Size of LSTM input", "selftext": "I am trying to make LSTM network and I am having some problems. Let's say I have 600 sequences each has 90 elements. What should be the size of LSTM input? Documentation imply shape \\[600,1,90\\] but my LSTM doesn't seem to work and I am trying to find the issue.", "upvote_ratio": 1.0, "id": "t3_je621p", "created_utc": 1603128290.0}
{"sub": "pytorch", "title": "get(self, idx) vs __getitem__(self, idx) for custom datasets", "selftext": "Hi everyone,\nI'm confused about a matter regarding custom datasets and indexing. I create a custom dataset class inheriting from Dataset class. I recently noticed that I implemented the following method for getting samples:\n\ndef get(self, idx):\n   ...\n\nwhile people usually implement the following, as far as I've seen from the online tutorials:\n\ndef _ _ getitem _ _(self, idx):\n   ...\n\nIs there any difference between them? I tested both functions and they seem to have the same functionality. But I couldn't find anything on web to justify it.\n\nThank you very much.", "upvote_ratio": 1.0, "id": "t3_jdzo1y", "created_utc": 1603105793.0}
{"sub": "pytorch", "title": "Which state should i pass in siamese network?", "selftext": "I am using  embedding- lstm -  and confused in\n\n  lstm\\_out, (hidden, cell) = nn.LSTM()(..)  \n\n&amp;#x200B;\n\nwhat should i take in those of three in final layer, I am not including fully connected layer here Anybody help!!", "upvote_ratio": 1.0, "id": "t3_jdwxxu", "created_utc": 1603090818.0}
{"sub": "pytorch", "title": "Should I pass hidden state to LSTM with every input?", "selftext": "I am training LSTM network and wondering whether I should pass any hidden cell state along with input. Documentation doesn't say much about it, even in example they pass it in one case but not in the other. So, do I have to pass hidden cell state with every input or not? If yes then what does it change in training process?", "upvote_ratio": 1.0, "id": "t3_jdpk27", "created_utc": 1603059348.0}
{"sub": "pytorch", "title": "Need help in implementing a pytorch equivalent.", "selftext": "https://discuss.pytorch.org/t/how-to-implement-pytorch-equivalent-of-keras-kernel-weight-regulariser/99773?u=paganpasta\n\n\nPosting the forum link to the problem as it contains a description of the problem and keras code.\n\nAny help is appreciated.", "upvote_ratio": 0.8, "id": "t3_jdfx1m", "created_utc": 1603025964.0}
{"sub": "pytorch", "title": "HELP with encoder_decoder \\ seq2seq model ?", "selftext": "for reference i used this code right here : [https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/more\\_advanced/Seq2Seq/seq2seq.py](https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/more_advanced/Seq2Seq/seq2seq.py)\n\nit has been a WEEK since i am trying to do a seq2seq model in Pytorch. MOST the example uses NLP but i need TIME SERIES FORECASTING. i know the teory and enough practice but i still can't make the model works.  What should have been an easy task done in a day or two has turned into a nightmare week full of frustration where now i am at the point where i just wanna quit everything and go plant potato.\n\n&amp;#x200B;\n\nEdit. so i tried to remove the embedding layer and use Linear layer instead. and obviously use MSE loss instead of CrossEntropy.\n\nthe problem is in the decoder when you do target\\[0\\] wich should be the first word (&lt;sos&gt;), but in my case it should be the fist number of every batch . Ex suppose we have batch two so the  input is: (\\[1,2,3,4\\],\\[9,10,11,12\\]) and target is (\\[5,6,7,8\\],\\[13,14,15,16\\]). if i do target\\[0\\] i get\\[5\\] wich has shape 1x1 instead target\\[0\\] should be (\\[5,13\\]) wich has shape 1xbatch\\_size i need this shape for the LSTM in the decoder. obviously the same problem is for the next target\\[1\\] and the other number in the sequence.\n\nanother problem is the LSTM that require 3 dimensions : in the tutorial is used embedding  and the input form shape (batch\\_size,seq\\_length) goes to (batch\\_size,seq\\_length,embedding\\_features)  but i just do x.unsqueeze(0) is it the same thing or is gonna be a problem ?\n\nPS. my discord is  Bisd#9079", "upvote_ratio": 1.0, "id": "t3_jd14hu", "created_utc": 1602961731.0}
{"sub": "pytorch", "title": "Neural net spits out same number in testing", "selftext": "I trained my model and after some epochs loss was very low. I tried to to test it and it spits out the same number again and again, if I add `optimizer.step()` to the testing code it works fine. Where's the problem?\n\n&amp;#x200B;\n\nSorry for such basic question but I really can't think of any solution", "upvote_ratio": 1.0, "id": "t3_jczcyj", "created_utc": 1602955888.0}
{"sub": "pytorch", "title": "Groundbreaking research from UWashington researchers: Remove any background noise/voice when in a video call! (See video)", "selftext": "nan", "upvote_ratio": 0.67, "id": "t3_jcnndw", "created_utc": 1602903312.0}
{"sub": "pytorch", "title": "Cloud traning", "selftext": "I dont have nvidia gpu, what is best cloud environment where can i write scripts and train models.?", "upvote_ratio": 0.5, "id": "t3_jcbxtq", "created_utc": 1602862305.0}
{"sub": "pytorch", "title": "A trick for training on large batches", "selftext": "Hi everyone,\nSince I'm having memory issues for training on larger batches, I thought of a trick but I'm not sure it would work.\n\nSo, normally we do the following:\n\nFor each batch:\n\n    \u00acCalculate loss \n\n\n    \u00acBack prop\n\n\n    \u00acOptimizer step\n\n\n    \u00acOptimizer zero grad\n\n\nInstead, I want to do the following, for instance a batch size of 1024:\n\nSet batch size equal to 1. \n\n\nFor each batch:\n\n\n    \u00acCalculate loss \n\n\n    \u00acBack prop\n\n\n    If (batch_idx+1) % 1024 == 0:\n\n\n         \u00acOptimizer step\n\n\n         \u00acOptimizer zero grad\n\nWould that be equivalent to training on batches of 1024 samples, since I accumulate the gradients for 1024 samples and update once for every 1024 samples?\n\nThank you.", "upvote_ratio": 0.75, "id": "t3_jc97jl", "created_utc": 1602852617.0}
{"sub": "pytorch", "title": "Tutorial on using PyTorch to emulate guitar amps and effects in a plugin.", "selftext": "nan", "upvote_ratio": 0.97, "id": "t3_jaz1nu", "created_utc": 1602675583.0}
{"sub": "pytorch", "title": "Microsoft Joined pytorch, What about our privacy? GNU when?", "selftext": "I've been  die hard fan of Pytorch since i encounter it. I am linux user, although I am also not fan of microsoft or facebook, but anyway, I heard microsoft joined Pytorch?  what does it means? Do they track us or do survellience on us, and put our privacy at risk?\n\n&amp;#x200B;\n\nWe would be happy if Pytorch would fan of FSF community. \n\nThanks", "upvote_ratio": 0.5, "id": "t3_jax1ux", "created_utc": 1602665512.0}
{"sub": "pytorch", "title": "Pytorch vs tensorflow for job opurtunities/freelancing", "selftext": "I have seen some charts of the popularity of pytorch going up(maybe surpassing tensorflow in the future)  should I learn pytorch or tensorflow if I want to get a data scientist job / do freelancing ?\nOr does it not matter ?", "upvote_ratio": 0.88, "id": "t3_jarkh8", "created_utc": 1602640755.0}
{"sub": "Python", "title": "Demo a python app in a web browser", "selftext": "nan", "upvote_ratio": 0.75, "id": "t3_udzqyt", "created_utc": 1651165264.0}
{"sub": "Python", "title": "Build a Comment Toxicity Model with Deep Learning and Python", "selftext": "nan", "upvote_ratio": 0.67, "id": "t3_udzgw0", "created_utc": 1651164553.0}
{"sub": "Python", "title": "Let the computer test your Python GUI application", "selftext": "How to [write easy automated GUI tests](https://blog.rareschool.com/2022/04/let-computer-test-your-python-gui.html) for Python applications that use guizero.", "upvote_ratio": 0.33, "id": "t3_udyqv0", "created_utc": 1651162650.0}
{"sub": "Python", "title": "Did you know you can pull and analyze almost 1M time series economic indicators directly in python using the FRED api and pandas?", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_udydh5", "created_utc": 1651161687.0}
{"sub": "Python", "title": "Can i call myself a junior? What to do next?", "selftext": "my github - [https://github.com/leirons](https://github.com/leirons)\n\nFirstly i would like to say sorry for my English, still learning and dont want to use translator. I am major of  computer science from Ukraine  on 2nd course\n\nCan you rate my github and tell me if can consider myself as a junior developer and look for a job?\n\nMy stack: Everything with Python/FastAPI/Selenium/Aiogram, also used Django, Docker(just for packaging app, it's very simple and helpful),AWS(A litter knowledge) for simple deployment on EC2 or docker containers, Javascript/HTML/CSS/React - learning and thinking about a pet project but dont know what to do.\n\nI'm stuck in place, I have a desire to do something difficult to raise my knowledge as a programmer, but I have no idea what exactly, I don't want to do something that has already been done hundreds of times by others\n\nThanks for reading and answers.\n\nInteresting thing in Python(just found out, idk why i added it) ( does not matter to the text above )\n\n    some_dict = {}\n    some_dict[5.5] = \"JavaScript\"\n    some_dict[5.0] = \"Ruby\"\n    some_dict[5] = \"Python\"\n    \n    print(some_dict[5.0]) # == Python Not Ruby", "upvote_ratio": 0.67, "id": "t3_udxxrj", "created_utc": 1651160510.0}
{"sub": "Python", "title": "When Python can\u2019t thread: a deep-dive into the GIL\u2019s impact", "selftext": "nan", "upvote_ratio": 0.75, "id": "t3_udwq9a", "created_utc": 1651157307.0}
{"sub": "Python", "title": "Book Club: Kubeflow for machine learning with Holden Karau &amp; Adi Polak", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_udvp7y", "created_utc": 1651154478.0}
{"sub": "Python", "title": "Project sigstore (free software signing service) just released a library to sign and verify python packages", "selftext": "nan", "upvote_ratio": 0.74, "id": "t3_udvn1q", "created_utc": 1651154333.0}
{"sub": "Python", "title": "I would like to increase my python kills.", "selftext": "Hello. I enjoy coding in python and i know the basics and a bit of working with apis and stuff but i would like to further my knowledge but i do not know how to start and from where what do you all recommend. i mean skills not kills :( xd\n\n&amp;#x200B;\n\nThank you everyone for the help it means a lot", "upvote_ratio": 0.79, "id": "t3_udujsc", "created_utc": 1651151135.0}
{"sub": "Python", "title": "Riemann sums animations with Python", "selftext": "&amp;#x200B;\n\n[Left, midpoint, and right Riemann sums](https://i.redd.it/08w2yr2qc9w81.gif)\n\n[https://github.com/chicolucio/integrals](https://github.com/chicolucio/integrals)\n\nI've made this project to show how Riemann sums provide an easy way to approximate a definite integral.", "upvote_ratio": 1.0, "id": "t3_udt7f3", "created_utc": 1651146765.0}
{"sub": "Python", "title": "Announcement: Embed Interactive Jupyter Notebooks in Static Websites (Jekyll) for Free", "selftext": "Hi there,\n\nI'm Juergen, a web developer. Currently, I'm working on integrating Jupyter Notebooks in (static) webpages to make Notebooks live/interactive available using the Binder service. I've integrated the (Python/JS) package/library ***nbinteract*** with my Jekyll-based static website at [https://jekyll.one](https://jekyll.one).\n\nYou'll find in the menubar a dropdown **Live Examples**. Go for one of these sites. On these sites, use the menu  Learn/Experimental/Textbooks\" and open one of the pages in that dropdown. The pages integrate some nice Jupyter Notebooks using interactive widgets.\n\nIt would be very helpful if some of you had a look. I'm very thankful to hear your opinion!\n\nBTW: You can create a personal website in minutes if you're already on **Github** or **Netlify,** go for the **Rocketstart** button on [https://jekyll.one](https://jekyll.one) to create your web!\n\n&amp;#x200B;\n\nHappy Jekylling,\n\nJuergen", "upvote_ratio": 0.81, "id": "t3_udt0qs", "created_utc": 1651146093.0}
{"sub": "Python", "title": "How to Edit a bunch of json ?", "selftext": "&amp;#x200B;\n\ni have a question  :\n\n How to edit a bunch of  json files and delete \"eg. \"type\": \"png\" \" / replace the value  of \"name\" to be \"goodies\"  not  \"#5\"\n\n&amp;#x200B;\n\n { \n\n\"name\": \"#5\",\n\n \"image\": \"dope deep\",\n\n \"type\": \"png\",\n\n \"attributes\": \\[\n\n{\n\n\"trait\\_type\": \"Eyes\",\n\n\"value\": \"Side-eye\"\n\n},\n\n  \\],\n\n}\n\n&amp;#x200B;\n\ni think the function / code  will work like that :\n\n1- import /read and *parse* all json files\n\n2- make a loop to make edits (delete specific lines / replace values) to all files\n\n3- export json files with their same names", "upvote_ratio": 0.62, "id": "t3_udrusf", "created_utc": 1651141584.0}
{"sub": "Python", "title": "Hatch 1.0.0 - Modern, extensible Python project management", "selftext": "nan", "upvote_ratio": 0.94, "id": "t3_udpzri", "created_utc": 1651133163.0}
{"sub": "Python", "title": "2013 'Lost connection to MySQK server during server during query'", "selftext": " \n\nHello,\n\nI have the problem 2013 'Lost connection to MySQK server during server during query' which comes when I come to write a data to my remote DB located on pythonanywhere . has anyone already solved this problem ? Thank you.", "upvote_ratio": 0.5, "id": "t3_udpd99", "created_utc": 1651130424.0}
{"sub": "Python", "title": "Is it possible on Python?", "selftext": "Guys, i' m working on diploma work about french language in magrheb countries and I have a task to solve. I am nobody in programming and I don't know is it possible to solve with Python. The task: i need a tool that gatheres transformed french words from different forums, like Reddit, compares them with actual french words and sends these words to excel table.", "upvote_ratio": 0.57, "id": "t3_udp9w8", "created_utc": 1651130018.0}
{"sub": "Python", "title": "The SymPy/HackerRank DMCA Incident", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_udp1d8", "created_utc": 1651129050.0}
{"sub": "Python", "title": "HTML Processing Tools in Python [Documentation - made easy to read]", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_udomcp", "created_utc": 1651127331.0}
{"sub": "Python", "title": "Web Automation With Selenium And Python (making a court reservation bot)", "selftext": "nan", "upvote_ratio": 0.83, "id": "t3_udoa0e", "created_utc": 1651125906.0}
{"sub": "Python", "title": "A tool to seed your dev database with real data", "selftext": "A bunch of developers and myself have created [RepliByte](https://github.com/qovery/replibyte) - an open-source tool to seed a development database from a production database.\n\n## Features \u26a1\n\n- Support data backup and restore for PostgreSQL, MySQL and MongoDB\n- Replace sensitive data with fake data\n- Works on large database (&gt; 10GB) (read Design)\n- Database Subsetting: Scale down a production database to a more reasonable size\n- Start a local database with the prod data in a single command\n- On-the-fly data (de)compression (Zlib)\n- On-the-fly data de/encryption (AES-256)\n- Fully stateless (no server, no daemon) and lightweight binary\n- Use custom transformers\n\n## My motivation\n\nAs a developer, creating a fake dataset for running tests is tedious. Plus, it does not reflect the real-world data and painful to keep updated. If you prefer to run your app tests with production data. Then RepliByte is for you as well.\n\nAvailable for MacOSX, Linux and Windows.\n\nhttps://github.com/qovery/replibyte give a \u2b50 if you like it.", "upvote_ratio": 0.77, "id": "t3_udnnag", "created_utc": 1651123414.0}
{"sub": "Python", "title": "I made an ASCII jumpscare for anyone who wants to use it", "selftext": "it's not very good, but I hope you like :)\n\n`print(\"....................................................................................................\\n........................................................................................................................................................................................................\\n....................................................................................................\\n....................................................................................................\\n....................................................................................................\\n....................................................................................................\\n....................................................................................................\\n....................................................................................................\\n....................................................................................................\\n....................................................................................................\\n....................................................................................................\\n..............................................................:::^^::::..........:..................\\n.............................::::::........................::^lGJJG7!~::....::::::..................\\n...........................::^~~!!~^^::..................::^lYBEEEBPJ7~:::::^^^^~^..................\\n..........................:^~DKAWJJDH~^:................::^~JBEEEE&amp;E53~^^^^/7/~^::..................\\n.........................::^RJPV&amp;GGGGJ!::...............::^~JGEXXXXE57~^~J7JJ!!~:...................\\n..................:::::::::^LJGEEEEEEY!^:................::^~JBEEBPJ!~^~/YPPJ7~^:...................\\n..................:~^~7!^:::^~LPBEEEG7^:..................:::^YY!~~^^^^~JP5Y!~^.....................\\n...................:~l7YYG!:::^^~!!!~::......................::::::::^~V5YJ~~:......................\\n....................:^l7JY!~::::::...............................:::^lJ5YG~:........................\\n.....................:^~GY5J!^:..............:::.................::~Y55YG^^.........................\\n......................:~lYP557~:..........:::::::::::::::.......:^YJYY5G!:..........................\\n......................:::lGYP5YG~::.......::::::::::::::.....:^~GJPPPY7~::..........................\\n.........................:^l7YYP5J7~!^^~^.::::::^~^^^^::::^ll7BPEEBG5Y!^............................\\n..........................::^lG55YYP5JYE5YJGGPYGGE5YBBG7G5GEB5BG5G5Y7^:.............................\\n............................:^^l7JGGGBEE&amp;EEEG&amp;EGGGBGGG5YGG55EEGGP57~^:..............................\\n...............................:l7YPGEEEGPG5GGGEGG5PGG5YEEGPYEBYJG!.................................\\n.................................:^lGPG55EGYGBEPBBJJBGYYPYYBYPP!:::.................................\\n....................................~G77PB5BYBGGGEY5EGYPE5Y5Y77^....................................\\n....................................::^l7GGYYE5GE&amp;GG&amp;EY5EG57!~^:....................................\\n......................................::::~~lYll5EP5EGJGY7~^:::.....................................\\n..........................................::::::VFF-FFF-^:..........................................\\n..........................................::.....::.::!^............................................\\n..........................................::..........^:............................................\\n..........................................::..........::............................................\\n..........................................::........................................................\\n..........................................::........................................................\\n..........................................:.........................................................\\n..........................................:.........................................................\\n....................................................................................................\\n....................................................................................................\\n....................................................................................................\\n....................................................................................................\")`", "upvote_ratio": 0.36, "id": "t3_udk3yw", "created_utc": 1651111726.0}
{"sub": "Python", "title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!", "selftext": "Discussion of using Python in a professional environment, getting jobs in Python as well as ask questions about courses to further your python education!\n\n**This thread is not for recruitment, please see** r/PythonJobs **or the thread in the sidebar for that.**", "upvote_ratio": 0.72, "id": "t3_udhlpf", "created_utc": 1651104010.0}
{"sub": "Python", "title": "A package with a bunch of tools to work with JSON data more easily. - T4Json", "selftext": "Python's standard json package is pretty good and all... but it only serializes/deserializes JSON data. That's why I created T4Json - (tools 4 json). It can open up json data from a file path, URL/Endpoint or JSON String... and then do many different operations on it like flattening, changing values, changing keys, reading values, adding data, moving/copying pairs or values around, searching for keys within nested data... and much much more. To get a better idea of what it can do take a quick look at the [docs](https://cybergeek1943.github.io/t4json/). Hint: there are whole lot of things you can do with the `flatten()` method. Also you can use relative/absolute paths to navigate nested JSON data... just like in the file system.\n\n**Outline:**\n\n* Open up JSON data with the T4Json class.\n* Use methods to make changes.\n* Save changes.\n\nExample:\n\n    &gt;&gt;&gt; data = T4Json('test.json')\n    &gt;&gt;&gt; data.delete(r'scores\\\\names\\\\test')\n    &gt;&gt;&gt; data.add({'geek': 99}, to_path=r'scores\\\\names')\n    &gt;&gt;&gt; data.save()  # or save_as()\n\nSearch Example from this [URL](https://mdn.github.io/learning-area/javascript/oojs/json/superheroes.json):\n\n    &gt;&gt;&gt; data = T4Json('https://mdn.github.io/learning-area/javascript/oojs/json/superheroes.json')\n    &gt;&gt;&gt; data.search('powers')\n    [\"Radiation resistance\", \"Turning tiny\", \"Radiation blast\", \"Million tonnepunch\", \"Damage resistance\", \"Superhuman reflexes\", \"Immortality\", \"Heat Immunity\", \"Inferno\", \"Teleportation\", \"Interdimensional travel\"]\n\nThese example are only a little window into what t4json can do.\n\nI created it for myself because I wanted an easier way to work with JSON data. I was not planning to make a tool with so many feature but they just kind of came along as I worked on it... and then I thought about releasing to the public. So here it is. Hopefully someone can find it helpful.\n\nHappy coding to everyone! \ud83d\udc69\ud83c\udffd\u200d\ud83d\udcbb\ud83d\udc68\ud83c\udffd\u200d\ud83d\udcbb\ud83e\uddd1\ud83c\udffd\u200d\ud83d\udcbb\ud83d\ude42", "upvote_ratio": 0.76, "id": "t3_udg1q9", "created_utc": 1651099427.0}
{"sub": "Python", "title": "Lambda Functions + Map() Reduce() Filter()", "selftext": "nan", "upvote_ratio": 0.29, "id": "t3_ude784", "created_utc": 1651094355.0}
{"sub": "Python", "title": "Lots of modules aka files is this trouble in some manner I dont get ?", "selftext": "Hi, opinions based on direct experience might help me. Thanks in advance.  \n\n\nIm new to Python but very experienced with SW development. Having no problems whatsoever.  \n\n\nI want to be sure others who inherit my project can change it and make it do things they want..  \n\n\nSo I have carved it up into 'modules' each a .py file I never seem to see python stuff done liek this. I mean if you want to change somebody else program and you see a perfectly named files, its pretty smallish, its very enabling, huh ?  \n\n\n  \nAll modules have this 'on top'  \n\n\nimport os as os  \nimport sys as sys  \nimport tkinter as tk  \nimport datetime as dt  \nfrom tkinter import ttk  \nfrom tkinter import font  \nfrom PIL import Image as pl  \nfrom PIL import ImageTk as ii  \nfrom tkinter import filedialog as fd  \nfrom tkinter.messagebox import showinfo  \nfrom tokenize import maybe  \n\\# Imports. parts of this program  \nimport avMedia as av  \nimport forFun as ff  \nimport globalIdeas as gi  \nimport modalSimple as ms  \nimport screenBuilder1 as sb  \nimport windowLookie as wl  \nimport fileIo as io  \nimport flatEdit as fe  \nimport pickle as pk   \nimport sequenceContainer as sc   \nimport fractionScreens as fs   \nimport transformations as xf   \nimport configSetting as cs   \n\n\nIs this cool or am I missing something ? There must be a way a import and be nested, probably. This as above though is kind of clear. XX. is each moduie. I use it even if the method calls a method in its same file so it can be cut and pasted  if its apparent its in the wrong spot.  \n\n\nRegs\n\nYour Turn  \nmy ref 27 Apr 2021 23:00 Z Python advice getting on reddit", "upvote_ratio": 0.17, "id": "t3_uddgw4", "created_utc": 1651092446.0}
{"sub": "Python", "title": "Python Coding Intermediate: Python Classes, Methods and OOPs", "selftext": "nan", "upvote_ratio": 0.33, "id": "t3_udd6e9", "created_utc": 1651091692.0}
{"sub": "Python", "title": "Learn Python: Python Baby Steps", "selftext": "nan", "upvote_ratio": 0.33, "id": "t3_udd58y", "created_utc": 1651091604.0}
{"sub": "Python", "title": "Ideas for a new project?", "selftext": "Hello, my girlfriend and I want to do some Python project, since we never did anything together related to programming. She's so close to finish his degree on Data Science Engineering, and I'm on my third year of Computer Science. We both have done some big projects, so I would like to hear from you some cool ideas.\n\nThanks :)", "upvote_ratio": 0.74, "id": "t3_udcq5p", "created_utc": 1651090489.0}
{"sub": "Python", "title": "question about importing python file", "selftext": "hello,\n\n&amp;#x200B;\n\ni am in a class where i need to import three files (they are python files). my teacher has posted the python files on canvas for us to have, but I am having trouble actually importing the files. i have dowloaded anaconda so i have all the base files, and i am coding in spyder. whenever i attempt to import, i am met with a 'ModuleNotFoundError'. how can i fix this problem?", "upvote_ratio": 0.33, "id": "t3_udc5um", "created_utc": 1651089035.0}
{"sub": "Python", "title": "Mypy 0.950 Released", "selftext": "nan", "upvote_ratio": 0.88, "id": "t3_udbbnm", "created_utc": 1651086759.0}
{"sub": "Python", "title": "Python Inline Source Syntax Highlighting Using Type Annotations", "selftext": "nan", "upvote_ratio": 0.84, "id": "t3_udaqbu", "created_utc": 1651085223.0}
{"sub": "Python", "title": "Job search automation with Python", "selftext": "I just finished up a project that uses Python for job search automation on Indeed and blogged about it here: [https://coreybowndatascience.blog/2022/04/27/using-python-for-job-search-automation/](https://coreybowndatascience.blog/2022/04/27/using-python-for-job-search-automation/)", "upvote_ratio": 0.69, "id": "t3_uda3lb", "created_utc": 1651083524.0}
{"sub": "Python", "title": "Underscoring (or dunder-scoring) the importance of native type methods in Python", "selftext": "nan", "upvote_ratio": 0.4, "id": "t3_ud8sb3", "created_utc": 1651079996.0}
{"sub": "Python", "title": "How common is poorly organized python code in industry?", "selftext": "I've always loved python, tend to gravitate towards it with IoT projects or really anything that I don't want to work on for a super long time. It just works everywhere and is easy to manage for small -&gt; medium size projects.\n\nBecause of this I have been *dying* to use it in a professional setting and my call was finally answered. I was given a project that I needed to interface with. This project however is fucking awful. 8000+ lines of python 2.7. That's just in the main program, not even all the utility classes it uses. There's not a consistent spacing between functions/classes etc. I've dealt with this before in other languages like C# which is way easier to decipher since it has brackets for everything and Visual Studio has more tools. It takes me forever to understand what a function is doing because it's nested with 10+ if/else and try/except statements.\n\nHow common is this around industries and companies..? This isn't the first time I've been presented a file this big in my industry nor will it be the last.", "upvote_ratio": 0.94, "id": "t3_ud8cec", "created_utc": 1651078836.0}
{"sub": "Python", "title": "Python Is Now Top Programming Language \u2014 But Shouldn't Be", "selftext": "nan", "upvote_ratio": 0.21, "id": "t3_ud7nbo", "created_utc": 1651077044.0}
{"sub": "Python", "title": "Hackerforms - create interfaces for scripts straight from your Python code", "selftext": "nan", "upvote_ratio": 0.33, "id": "t3_ud6wuv", "created_utc": 1651075137.0}
{"sub": "Python", "title": "Python Cybersecurity \u2014 Build your own python tools (PortScanner, Visual Network Tracker and Anonymous FTP Scanner)", "selftext": "**Python Cybersecurity \u2014 PortScanner**\n\nBuild a simple Port Scanner using the Python Programming language. Port Scanner is an application designed to probe a server or host for open ports. Such an application may be used by administrators to verify security policies of their networks and by attackers to identify network services running on a host and exploit vulnerabilities.\n\n**Link**: [https://youtu.be/bH-3PuQC\\_n0](https://youtu.be/bH-3PuQC_n0)\n\n**Python Cybersecurity \u2014 Visual Network Tracker**\n\nDive into Network Traffic visualization using the Python programming language, Wireshark and Google Maps. This tutorial covers the implementation steps needed to take a file of network traffic and convert it into a visual presentation using Google Maps.\n\n**Link**: [https://youtu.be/xuNuy8n8u-Y](https://youtu.be/xuNuy8n8u-Y)\n\n**Python Cybersecurity \u2014 Anonymous FTP Scanner**\n\nBuild a simple FTP Scanner using the Python Programming language. Anonymous FTP is a means by which archive sites allow general access to their archives of information. These sites create a special account called \u201canonymous\u201d.\n\n**Link**: [https://youtu.be/BIZfRodSW9w](https://youtu.be/BIZfRodSW9w)", "upvote_ratio": 0.54, "id": "t3_ud6c1o", "created_utc": 1651073616.0}
{"sub": "Python", "title": "Ways to unpack a tuple with 1 element", "selftext": "Some functions always return a tuple, even when only returning a single value.\n\nWays I'm debating of assigning the element value to a variable:\n\n1. `hist = conn.query(hist_query).fetchone()[0]`\n2. `hist, *_ = conn.query(hist_query).fetchone()`\n3. `hist, = conn.query(hist_query).fetchone()`\n4. /u/LardPi `(hist, ) = conn.query(hist_query).fetchone()`\n5. /u/redditusername58 `[hist] = conn.query(hist_query).fetchone()`\n\nWhich would make the most sense to you when you came back and read it?\n\nI'm partial to 2, but I think 1 is more common in Python.\n\nAny other methods that are even more obvious?\n\n&amp;#x200B;\n\n**EDIT:** Thanks for the discussion on the explicitness of the result and new techniques!", "upvote_ratio": 0.8, "id": "t3_ud67zr", "created_utc": 1651073314.0}
{"sub": "Python", "title": "Create a Bluetooth LE repeater using Python on Raspberry pi to overcome the range limitation", "selftext": "nan", "upvote_ratio": 0.75, "id": "t3_ud66s4", "created_utc": 1651073221.0}
{"sub": "Python", "title": "How to Version Control your Django Project", "selftext": "Hey All,\n\nI wrote a small guide for how I Version Control my Django Projects. If you are interested I would love to get your feedback.  [https://builtwithdjango.com/blog/django-version-control](https://builtwithdjango.com/blog/django-version-control)\n\nIt is aimed at the people who are just starting out.\n\nTL;DR of the post is to use a good .gitignore \ud83d\ude03", "upvote_ratio": 0.5, "id": "t3_ud4xau", "created_utc": 1651069818.0}
{"sub": "Python", "title": "How ASCII video are made", "selftext": "nan", "upvote_ratio": 0.77, "id": "t3_ud4oj6", "created_utc": 1651069142.0}
{"sub": "Python", "title": "GitHub - dflook/python-minifier: Transform Python source code into it's most compact representation", "selftext": "nan", "upvote_ratio": 0.67, "id": "t3_ud2xcd", "created_utc": 1651064166.0}
{"sub": "Python", "title": "How to Make Pass Maps in Python", "selftext": "Howdy,\n\nBeginner: What project should I do?\nNot beginner: do what you are passionate about?\n\nThis is me finally finding a passion project: \n\nhttps://youtu.be/o1ZHIocdTEk", "upvote_ratio": 0.5, "id": "t3_ud29il", "created_utc": 1651062042.0}
{"sub": "Python", "title": "I have written a blog building Neural Network from scratch with Python and trained it on image data.", "selftext": "nan", "upvote_ratio": 0.93, "id": "t3_ud0cqi", "created_utc": 1651055155.0}
{"sub": "Python", "title": "Python Implementation of Gnome Sort, Child of Insertion sort and Bubble sort", "selftext": "nan", "upvote_ratio": 0.56, "id": "t3_ucyywi", "created_utc": 1651049129.0}
{"sub": "Python", "title": "Switch to Python Development from DBA", "selftext": "Hi All,  \n\n\nI am working as DBA from last 8 years, but want to switch my career into Python &amp; Django Development.\n\nI have made several projects in Python &amp; Django from last 3 years for work and as well as my hobby and passive income projects. I am very good at scripting, use REST APIs, cloud functions, CI/CD tools like Jenkins, GitHub-Actions, Ansible etc. for automations.  \nI have also made my personal Tech blog showcasing my work.\n\nI started applying for Python/Django roles, but I don't get even shortlisted anywhere.  \nEveryone is hesitant to hire a DBA for development roles.\n\nHow do I get a job in development? Thanks for helping in advance.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/77y4sxie51w81.png?width=992&amp;format=png&amp;auto=webp&amp;s=5741608caf23b77c8129e13b0dd1e6bce0e1264a", "upvote_ratio": 0.96, "id": "t3_ucykyo", "created_utc": 1651047290.0}
{"sub": "Python", "title": "Good ways to send and recieve REST request objects when ID of object is only sometimes sent or received?", "selftext": "Bit of a mouthful, but I have a flask restful server that I am using to serve data and do certain operations to a local computer. \n\nWhenever this REST server returns something from a the database, it returns the object with its ID (integer). When its a computed response, it does not send the ID.\n\nLikewise, when I request to get an individual resource, I ask by ID. When I request to create a resource, I need to not send the ID of itself and any nested objects.\n\nAt the moment I am using Marshmallow to serialise and deserialise, and the best I can do is set ID: Optional[int]. But doing this just sends ID over as None which does t really work nicely. \n\nI guess the question is, is there a good way to achieve what I am trying to do here? Seems like every single thing that has ever used a REST API will have faced this issue. What are your solutions?\n\nE: \n\nquick solution for those using marshmallow. Set load_only=True on the schema definition. This will make Marshmallow ignore the field when you dump it, but still pick it up on load. For the opposite use dump_only. Handy and quite clean in my opinion", "upvote_ratio": 0.77, "id": "t3_ucx5qc", "created_utc": 1651041127.0}
{"sub": "Python", "title": "We built the \"Netlify for Backend\" that runs on your AWS account!", "selftext": "nan", "upvote_ratio": 0.3, "id": "t3_ucwrpv", "created_utc": 1651039598.0}
{"sub": "Python", "title": "GitHub - GeeTransit/sphinx-better-subsection: Better your Sphinx section IDs", "selftext": "Hey all.\n\nI created a (imo) pretty cool Sphinx extension to make header permalinks use the label just before them. You can [see it in action here](https://geetransit.github.io/soundit/_generated/CHANGELOG/) (click on the \"#\" beside the headers).\n\nA label is the `.. _name` [syntax for to linking specific sections](https://www.sphinx-doc.org/en/master/usage/restructuredtext/roles.html#ref-role). As a quick example, the permalink on \"1.2.3 - 2022-04-26\" in the rST below would be `#v1-2-3` instead of `#id1` or whatever:\n\n    .. _v1.2.3\n    \n    1.2.3 - 2022-04-26\n    ------------------\n    - Made things cooler.\n\nTo use it, simply `pip install sphinx-better-subsection` and add `extensions += [\"sphinx_better_subsection\"]` to your `conf.py` file.\n\nAny feedback / feature requests are welcome :D\n\n[https://github.com/GeeTransit/sphinx-better-subsection](https://github.com/GeeTransit/sphinx-better-subsection)", "upvote_ratio": 0.78, "id": "t3_ucv4vl", "created_utc": 1651033286.0}
{"sub": "Python", "title": "Wednesday Daily Thread: Beginner questions", "selftext": "New to Python and have questions? Use this thread to ask anything about Python, there are no bad questions!\n\nThis thread may be fairly low volume in replies, if you don't receive a response we recommend looking at r/LearnPython or joining the Python Discord server at [https://discord.gg/python](https://discord.gg/python) where you stand a better chance of receiving a response.", "upvote_ratio": 0.63, "id": "t3_ucqd0q", "created_utc": 1651017610.0}
{"sub": "Python", "title": "Python Tips And Tricks \u2014 Using Built-In Database", "selftext": "nan", "upvote_ratio": 0.53, "id": "t3_ucnftn", "created_utc": 1651009071.0}
{"sub": "Python", "title": "AutoTwitchDrops. A minimalist bot that gets Twitch drops for you written in pure Python.", "selftext": "This is a very simple app that I went above and beyond to make as clean and properly structured/formatted as possible. The goal was to write and structure the code in the most pythonic, beautiful, eloquent way possible. Also overwatch 2 drops are tomorrow and I wanted to nab a beta key. It also works for all other twitch drops. All possible details and instructions are available in the repo thanks to a really nice readme template I just started using, but I'll add the basics here as well to save some people clicks :)\n\nrepo: [https://github.com/trevtravtrev/AutoTwitchDrops](https://github.com/trevtravtrev/AutoTwitchDrops)\n\nI'll start with the basics of how it works:\n\n    1) read a list of streamers from a text file\n    2) open a unique browser tab for each twitch streamer's stream\n    3) refresh each tab every x amount of seconds to start any streams that weren't yet started last refresh\n\nAnd how simple it is to use:\n\n    1) fill a text file with streamer twitch names (streamers.txt)\n    2) input a couple settings (settings.py)\n    3) run main.py\n\nLibraries/tools used:\n\n    - python 3.10\n    - selenium\n    - poetry\n    - mypy\n    - Pylint\n    - isort\n    - black\n\nNow the fun part. The time taken to write the core features took about **20 minutes**. The time taken correctly formatting the code (black), proper package/structure management (poetry), type hints (mypy), sorting imports (isort), errors/code smells (pylint), and a beautiful readme took about **6 hours.**\n\nIt works very well and I'm sharing for anyone that would like to use it and more importantly for feedback/critique.\n\n    1) how could the code/code structure be further improved?\n    2) are there any other magical tools/libraries I'm missing that I can add to my arsenal above?\n    3) any other tips/feedback is greatly appreciate\n\nI'm an experienced python dev and available to answer any questions as well :)\n\nrepo: [https://github.com/trevtravtrev/AutoTwitchDrops](https://github.com/trevtravtrev/AutoTwitchDrops)", "upvote_ratio": 0.83, "id": "t3_ucl9mz", "created_utc": 1651003133.0}
{"sub": "Python", "title": "FastAPI with SQLModel, Alembic and Authentication. Full course", "selftext": "nan", "upvote_ratio": 0.72, "id": "t3_uck9s7", "created_utc": 1651000434.0}
{"sub": "Python", "title": "Mocha - A simple open-source Continuous Profiling tool for Python!", "selftext": "nan", "upvote_ratio": 0.88, "id": "t3_uci6jw", "created_utc": 1650994883.0}
{"sub": "Python", "title": "Recover deleted/overwritten files with RecoverPy 1.5.0", "selftext": "&amp;#x200B;\n\nhttps://i.redd.it/6zy0hh7d8wv81.gif\n\nHi! I recently release RecoverPy v1.5.0 and I think I might give you some news.  \n\\-&gt; Repo: [https://github.com/PabloLec/RecoverPy](https://github.com/PabloLec/RecoverPy)  \n\\-&gt; What is it?  \nRecoverPy is a 100% Python tool to not only recover deleted but also overwritten files. I got the idea when I was quite new to some programming best practices, especially version control...  \nLong story short, I accidentally piped my output into my precious script... Just spent the day working on something and instead of typing myscript &gt; log, I typed log &gt; myscript, oh boy what a feeling.  \nI knew some tools to recover deleted files, but my problem was quite different, I didn't deleted the file (in system words, marked the file blocks as deleted/available), I just replaced it's content. Talk about an impostor syndrome.  \nAfter a long ride in the abysses of unix stackexchange, I found some dark combination of grep and dd command to search directly in your raw system partitions blocks and eventually recovered my file! But as the process was really slow and painful, I thought it might be a good idea to make a tool out of it. That's how RecoverPy was born.  \n\n\n  \n\\-&gt; 1.5.0  \nSince then, the tool has had quite some success. Especially in the hacker community (wasn't the initial intent but still). It even appeared in hakin9 magazine.  \nLast releases have been quite stable, lastly I mostly added QoL updates and better binary file search handling.  \nFeel free to have a look and tell me what you think of it! It's my biggest personal project and I'm beginning to be quite proud of my baby :)", "upvote_ratio": 0.95, "id": "t3_ucfids", "created_utc": 1650987787.0}
{"sub": "Python", "title": "Reloadium - Hot Reloading aka Edit and Continue for Python", "selftext": "&amp;#x200B;\n\nhttps://i.redd.it/fyscm8r8fvv81.gif\n\nMore details here: [https://github.com/reloadware/reloadium](https://github.com/reloadware/reloadium)\n\nUsing is very simple. Just edit your file and hit save (Ctrl-S).\n\nIf you guys use PyCharm then you can try out the plugin:\n\n[https://plugins.jetbrains.com/plugin/18509-reloadium](https://plugins.jetbrains.com/plugin/18509-reloadium)\n\nIt enables hot reloading capabilities in Python like changing code during debugging, fixing errors after exceptions occur, restarting current functions etc.", "upvote_ratio": 0.99, "id": "t3_ucbzry", "created_utc": 1650977925.0}
{"sub": "Python", "title": "Robyn - A Python web framework with a Rust runtime - crossed 200k installs on PyPi", "selftext": "Hi Everyone! \ud83d\udc4b\n\nI wrote this blog to celebrate 200k install of Robyn. This blog documents the journey of Robyn so far and sheds some light on the future plans of Robyn.\n\nI hope you all enjoy the read and share any feedback with me.\n\nBlog Link: [https://www.sanskar.me/hello\\_robyn.html](https://www.sanskar.me/hello_robyn.html)", "upvote_ratio": 0.96, "id": "t3_ucazjl", "created_utc": 1650974662.0}
{"sub": "Python", "title": "GitHub - LukasZahradnik/PyNeuraLogic: PyNeuraLogic lets you use Python to create Differentiable Logic Programs", "selftext": "nan", "upvote_ratio": 0.8, "id": "t3_uc9wc1", "created_utc": 1650970850.0}
{"sub": "Python", "title": "Broadcast dictionary: operate on all values of a dictionary without loops or dict comprehension", "selftext": "For a client, I had to do the same analysis for several datasets.\n\nThe existing codebase was full of things like this:\n\n    raw_data = {\n        \"apples\": load_data(\"apples\"),\n        \"pears\": load_data(\"pears\"),\n        ...\n    }\n    clean_data = {k: clean(df) for k, df in raw_data.items()}\n    \n    models = {}\n    for k, df in clean_data.items():\n        model = train_model(df)\n        models[k] = model\n\nTo make things easier, I have created the [Broadcast Dictionary](https://github.com/mariushelf/bcdict) package.\n\nIt allows to perform operations on all values of a dictionary like this:\n\n    pip install bcdict\n    from bcdict import BCDict\n    import bcdict\n    \n    keys = [\"apples\", \"pears\", \"bananas\"]\n    raw_data = bcdict.bootstrap(keys, load_data)\n    clean_data = raw_data.pipe(clean)\n    models = clean_data.pipe(train_model)\n\nYou can also access attributes of all values:\n\n    &gt;&gt;&gt; clean_data.shape\n    BCDict({'apples': (16, 5), 'pears': (18, 5), 'bananas': (12, 5)})\n\nOr call functions:\n\n    &gt;&gt;&gt; clean_data.head()\n    {'apples':       A     B     C     D  target\n    0  0.81  0.19  0.79  0.61    0.46\n    1  0.11  0.47  0.34  0.15    0.66\n    2  0.07  0.73  0.95  0.01    1.00,\n     'bananas':       A     B     C     D  target\n    0  0.72  0.82  0.36  0.11    0.95\n    1  0.41  0.53  0.85  0.69    0.75\n    2  0.22  0.55  0.71  0.24    0.18,\n     'pears':       A     B     C     D  target\n    0  0.63  0.34  0.07  0.32    0.34\n    1  0.74  0.65  0.90  0.48    0.13\n    2  0.72  0.77  0.57  0.78    0.50}\n\nSo if you work a lot with dictionaries the `bcdict` package could be useful for you.\n\nYou can check out the full documentation, [more examples](https://bcdict.readthedocs.io/en/stable/examples/examples.html) and even a full [`sklearn` pipeline](https://bcdict.readthedocs.io/en/stable/examples/train_test_evaluate.html) on [https://bcdict.readthedocs.io](https://bcdict.readthedocs.io).", "upvote_ratio": 0.79, "id": "t3_uc9efg", "created_utc": 1650968849.0}
{"sub": "Python", "title": "Would be nice to run a python module function with \"python -m mymod:func\"", "selftext": "Just saying, would be very nice to run a python module function with the same syntax used to define module scripts, directly from the command line. Is there any proposal of the kind?", "upvote_ratio": 0.77, "id": "t3_uc8k1r", "created_utc": 1650965309.0}
{"sub": "Python", "title": "OS Signal Handling in Python 3.x.x", "selftext": "nan", "upvote_ratio": 0.69, "id": "t3_uc67ax", "created_utc": 1650954660.0}
{"sub": "Python", "title": "Build your own Data Acquisition System (.csv file) using Python and Arduino", "selftext": "nan", "upvote_ratio": 0.79, "id": "t3_uc66x3", "created_utc": 1650954613.0}
{"sub": "Python", "title": "Top Checklist to Hire Python Developers", "selftext": "nan", "upvote_ratio": 0.25, "id": "t3_uc5rj2", "created_utc": 1650952860.0}
{"sub": "Python", "title": "Rate my game: Lightshift!", "selftext": "&amp;#x200B;\n\nhttps://preview.redd.it/htro0gk1wsv81.png?width=1085&amp;format=png&amp;auto=webp&amp;s=c149383c94b5abbc6dbffa3e1155b87aefd87854\n\nLast time, I made a post for my game, [AGILE](https://skysurfer-kon.itch.io/agile) and people said the controls were too hard. Most people couldn't even pass level 2 (there are 18 levels). To make this game user friendly with easy controls and good game feel, I need some people to playtest my game (you) because my friends keep postponing it. \n\n&amp;#x200B;\n\n**PLEASE NOTE:** \n\nMy game is not done and some parts of this game might not be in the final version. There are glitches and weird things I need to fix, so if you see terrible backface culling on some blocks or red blocks that serve no purpose, please do not give feedback about those as this game is the alpha build currently. The sounds are also not final and some are nonexistent. Also, P**lease do not take any assets or code**, because I spent many hours on everything (apart from sounds). The graphics are also the best possible, so if you are on battery mode it might lag a bit, unless you have a good pc.\n\n&amp;#x200B;\n\n**AFTER YOU PLAYED THE GAME:**\n\nAfter you played the game, please give me feedback on the vfx, controls, general difficulty of the game, and other things I could improve.\n\n&amp;#x200B;\n\n**CONTROLS:**\n\nmoving left &amp; right: left and right arrow keys\n\njump: Z\n\ndash: C\n\n&amp;#x200B;\n\n**DOWNLOAD:**\n\n[https://www.mediafire.com/file/gx1bmudwe6zsm1u/Lightshift.zip/file](https://www.mediafire.com/file/gx1bmudwe6zsm1u/Lightshift.zip/file)", "upvote_ratio": 0.6, "id": "t3_uc4gr1", "created_utc": 1650947761.0}
{"sub": "Python", "title": "Tuesday Daily Thread: Advanced questions", "selftext": "Have some burning questions on advanced Python topics? Use this thread to ask more advanced questions related to Python.\n\n**If your question is a beginner question we hold a beginner Daily Thread tomorrow (Wednesday) where you can ask any question! We may remove questions here and ask you to resubmit tomorrow.**\n\nThis thread may be fairly low volume in replies, if you don't receive a response we recommend looking at r/LearnPython or joining the Python Discord server at [https://discord.gg/python](https://discord.gg/python) where you stand a better chance of receiving a response.", "upvote_ratio": 0.5, "id": "t3_ubz9be", "created_utc": 1650931209.0}
{"sub": "Python", "title": "Anubis - Python Obfuscator", "selftext": "Project on Github [here](https://github.com/0sir1ss/Anubis)\n\nSo, I was looking around online and github when I realised that there were little python obfuscators. Now this makes sense since it's an interpreted language which isn't meant to be obfuscated. However, obfuscators still exist. First there's [pyarmor](https://pypi.org/project/pyarmor/) however that has been shown to be deobfuscated (still quite good). Then there's tools and github repositories like [pyminifier](https://pypi.org/project/pyminifier/) but sadly they don't do much to obfuscate the code ~~replace exec with print.~~ The best solution that does a solid job at obfuscating your code is [here](https://pyob.oxyry.com/), but if you want to purchase the offline CLI version it will cost you USD $1998.\n\nSo instead, I decided to create my own obfuscator. This includes a plethora of features such as junk code and custom encryption for one liners. It even includes the obfuscation you find at oxyry.\n\nYou can see the difference it makes from this source [here](https://github.com/0sir1ss/Anubis/blob/main/example/script.py) to this obfuscated one liner [here](https://github.com/0sir1ss/Anubis/blob/main/example/script-obf.py).\n\nAlso, I  have added an option to instead compile to an exe with [Nuitka](https://pypi.org/project/Nuitka/) incase you don't want to use the one line mode and distribute the .pyd file along with it.\n\nAs of now, Anubis contains:\n\n* Anti VM\n* Anti Debugger\n* Junk Code\n* Custom Encryption\n* Compile to exe with Nuitka", "upvote_ratio": 0.7, "id": "t3_ubxqt3", "created_utc": 1650926818.0}
{"sub": "Python", "title": "I used the speech_recognition library for many of my projects, but needed an offline capability so I switched to Vosk. Here's a quick video on how to install and use it. Before I invest too much time with it, is there a better offline speech recognition library?", "selftext": "nan", "upvote_ratio": 0.75, "id": "t3_ubwvez", "created_utc": 1650924418.0}
{"sub": "Python", "title": "Wyngman helps you better understand your AWS Cognito users", "selftext": "Every wished to subset users based on their creation date? The AWS Documentation for Cognito says you can only search users based on their standard attributes. Wyngman is a CLI Tool written in Python that helps you for the use-case like, **How many users did I gain in a given date range!**\n\nFeel free to contribute and provide feedback\n\nSource Code: [https://github.com/Razin-Tailor/wyngman](https://github.com/Razin-Tailor/wyngman)\n\nPypi: [https://pypi.org/project/wyngman/](https://pypi.org/project/wyngman/)\n\nMedium: [https://medium.com/@r42intailor/wyngman-helps-you-better-understand-your-aws-cognito-users-7e48c895c486](https://medium.com/@r42intailor/wyngman-helps-you-better-understand-your-aws-cognito-users-7e48c895c486)", "upvote_ratio": 0.67, "id": "t3_ubvzi8", "created_utc": 1650922030.0}
{"sub": "Python", "title": "Learning Python", "selftext": "This is my second day working on python. I'm 15 years old and I've always been good at maths and used to program a bit with \"blocks\" a couple years ago. Now I've been trying to familiarise myself with variables data structures and will soon move on more to functions. I've seen some suggestions but what resources do you guys recommend, (I prefer hands on learning).\n\nHeres a script i wrote today, if anyone has any ideas how i could add stuff and make this script more advanced, let me know! Thanks for the help in advance.\n\n\\#fruits\n\nbanana = 10\n\norange = 20\n\napple = 40\n\n\\#money\n\nmoney\\_ownership = int(input(\"How much money do you have? :\"))\n\n\\#Buy fruits\n\napple\\_ammount = int(input(\"How many apples would you like? : \"))\n\norange\\_ammount = int(input(\"How many oranges would you like? : \"))\n\nbanana\\_ammount = int(input(\"How many bananas would you like? : \"))\n\n\\#total cost\n\ntotal\\_cost = (apple\\_ammount \\* apple + orange\\_ammount \\* orange + banana\\_ammount \\* banana )\n\nprint(\"Your total cost is\", + total\\_cost)\n\n\\#how much money will be left\n\nmoney\\_left = money\\_ownership-total\\_cost\n\nprint(\"Youll have\", + money\\_left, \"left.\")\n\nif money\\_left &lt; 0:\n\nprint(\"sorry you do not have enough money\")\n\nelse:\n\nprint(\"Your balance is enough\")", "upvote_ratio": 0.65, "id": "t3_ubucj0", "created_utc": 1650917752.0}
{"sub": "Python", "title": "Succinct callable type hints", "selftext": "Sadly, [PEP 677](https://peps.python.org/pep-0677/) was [rejected](https://mail.python.org/archives/list/python-dev@python.org/message/NHCLHCU2XCWTBGF732WESMN42YYVKOXB/), so we get to continue suffering with `Callable` and `Protocol`.\n\nLook at this mess for typing a method that returns a method. It's to type hint part of Alembic's setup code.\n\n    class type_include_object(Protocol):\n        def __call__(self, object: Table, name: str, type_: str, reflected: Any, compare_to: Any) -&gt; bool: ...\n    \n    class type_include_schemas(Protocol):\n        def __call__(self, names: List[str]) -&gt; type_include_object: ...\n    \n    @dataclass\n    class type_metadata:\n        include_schemas: type_include_schemas\n\nThis is _crap_. I strongly dislike this code I've written. Please tell me there's a better, more succinct way to:\n\n1. Type hint a method or function\n2. Include parameter names and types", "upvote_ratio": 0.84, "id": "t3_ubrjze", "created_utc": 1650910467.0}
{"sub": "Python", "title": "Choosing a Python library", "selftext": "When you need to do something tricky, and you don't want to reinvent the wheel - [how do you choose a Python library](https://blog.rareschool.com/2022/04/choosing-python-library.html)", "upvote_ratio": 0.36, "id": "t3_ubqn2k", "created_utc": 1650908028.0}
{"sub": "Python", "title": "10% of the 666 most popular Python GitHub repos have f-string bugs (so 68 pull requests were made in 24 hours to fix them all)", "selftext": "nan", "upvote_ratio": 0.64, "id": "t3_ubkvrd", "created_utc": 1650892429.0}
{"sub": "Python", "title": "Introduction Python\u2019s Moto Library- Easily Mock out AWS Services", "selftext": "nan", "upvote_ratio": 0.54, "id": "t3_ubon6t", "created_utc": 1650902810.0}
{"sub": "Python", "title": "Image Processing Camera Suggestions?", "selftext": "Not sure if this is the right place but I've been playing around with ML and image processing in Python and I want to try and use a camera to stream a live feed that is easy to process. Does anyone have any good suggestions for a camera brand that is easy to interface with and receive images? I am trying to find something without a subscription or anything like that to use. Thanks.", "upvote_ratio": 0.75, "id": "t3_ubo3za", "created_utc": 1650901458.0}
{"sub": "Python", "title": "Giving the Python environment management ecosystem the old-fashioned Gordian Knot treatment. Plus: conda best practices and a fictional history of python env+dep management tools.", "selftext": "nan", "upvote_ratio": 0.82, "id": "t3_ubn40k", "created_utc": 1650898826.0}
{"sub": "Python", "title": "merge5audio - A simple GUI app that merges audio files", "selftext": "[https://github.com/arthtyagi/merge5audio](https://github.com/arthtyagi/merge5audio)\n\nI made this in an hour as a predecessor to my other audio-processing app called TRILLAUDIO (open-source and will be working on it this summer).\n\nOh, and it merges a folder of audio files in batches of 5. (helps with merging multiple stems of my projects if I want to. I'm referring to a music-production project here btw)\n\nLmk if it's acting up and I'll help you with it.", "upvote_ratio": 0.81, "id": "t3_ublodl", "created_utc": 1650894791.0}
{"sub": "Python", "title": "Learn Python in 3 Hours [DE]", "selftext": "nan", "upvote_ratio": 0.53, "id": "t3_ubl5x7", "created_utc": 1650893290.0}
{"sub": "Python", "title": "Flask -&gt; FastAPI", "selftext": "I'm happy with Flask and do not see the need to switch to FastAPI. Change my mind &amp; (ideally) point me to a nice tutorial that does just that in a demo project.", "upvote_ratio": 0.32, "id": "t3_ubiv0a", "created_utc": 1650885862.0}
{"sub": "Python", "title": "Text Summarization with Huggingface Transformers and Python", "selftext": "nan", "upvote_ratio": 0.63, "id": "t3_ubgv0x", "created_utc": 1650877984.0}
{"sub": "Python", "title": "GitHub - helblazer811/ManimML: ManimML is a project focused on providing animations and visualizations of common machine learning concepts with the Manim Community Library.", "selftext": "nan", "upvote_ratio": 0.71, "id": "t3_ubfsl4", "created_utc": 1650873375.0}
{"sub": "Python", "title": "Making a list of advanced topics in Python", "selftext": "I'm preparing for a technical interview. I failed the first one a week ago. I noticed the excercises were tagged as (advanced python). 2 coding questions, one from decorators and the other from SQL.\n\nI'm taking a month to prepare for my second chance at it. And I'm making a list of advanced topics/concepts in python. I don't want to be taken by surprise again.\n\nList comprehension\n\nAnonymous function\n\nDecorators\n\nGenerators\n\nException handling\n\nInheritance\n\nEncapsulation\n\nUnit testing\n\nRegex\n\nI need suggestions just in case I'm missing anything. I do practice coding excercises on hackerank.", "upvote_ratio": 0.75, "id": "t3_ubd7xt", "created_utc": 1650862809.0}
{"sub": "Python", "title": "Port scanning with Python", "selftext": "Lately I've been learning a bit about communicating over a network with Python and thought it might be fun to create a simple TCP port scanner. Had a lot of fun playing around with this on my home network. \n\nThe full project write up and code can be found [here](https://sheldonbarry.com/2022/04/24/port-scanning-with-python/).", "upvote_ratio": 0.69, "id": "t3_ubakvn", "created_utc": 1650853507.0}
{"sub": "Python", "title": "Python 3.11 Preview: Task and Exception Groups \u2013 Real Python", "selftext": "nan", "upvote_ratio": 0.96, "id": "t3_ub9elz", "created_utc": 1650849707.0}
{"sub": "Python", "title": "Monday Daily Thread: Project ideas!", "selftext": "Comment any project ideas beginner or advanced in this thread for others to give a try! If you complete one make sure to reply to the comment with how you found it and attach some source code! If you're looking for project ideas, you might be interested in checking out Al Sweigart's, [\"The Big Book of Small Python Projects\"](https://inventwithpython.com/bigbookpython/) which provides a list of projects and the code to make them work.", "upvote_ratio": 0.74, "id": "t3_ub7ugb", "created_utc": 1650844811.0}
{"sub": "Python", "title": "2-Button UI engine in MicroPython with \"Apps\" on a TTGO T-Display", "selftext": "https://youtu.be/wR3AkhD0nEg\n\nMore of a demo than anything at this point, but I've got working text entry, menus, loadable apps(any file named app_AppName.py is detected as an app).\n\nI also have sleep modes, and wakelock-like functionality(Including modem sleep), and the ability for one app to launch another, with arguments and return values.\n\nIn screen-off sleep mode, it uses 6mA without Wi-Fi(The USB chip and charge led takes some power, probably more like 0.5mA on battery).  With Wi-Fi it's around 7mA with spikes every few seconds.\n\nWith the screen on, you get 45mA with spikes up to 65mA.  Probably room to improve that. But I'm impressed by the low power capabilities of MP(Once you add some assorted pull requests from the internet).\n\nThe Calculator app actually just launches the text entry app, and evals the text you enter.\n\nThere's a stopwatch and a tally counter, and a settings menu where you can customize things like the colors, and set an app to auto-load as soon as it boots up.\n\nIn the future, I'll add password protection to prevent exiting the default app, and I'd like to eventually have some kind of mobile \"App store\" Android app for uploading new content, and maybe the ability to configure WiFi/MDNS/MQTT from the settings menu.\n\n\nThe original idea for this was to be a replacement for Logitech's Harmony remotes.  I think it would be awesome to have an open source home automation remote, that used an app-capable OS, that could also be a replacement for smart wall switches.\n\nAnother fun thing I'd like to do is some kind of programming feature, so you could edit the logic for a little robot just with a 2 or 3 button menu.\n\nIf MicroPython ever gets BLE bonding, making a wireless keyboard emulator for media or presentations might be another fun app.", "upvote_ratio": 0.92, "id": "t3_ub6l4g", "created_utc": 1650840886.0}
{"sub": "Python", "title": "In-Depth Analysis of Moonbirds NFTs using Python and Alchemy", "selftext": "nan", "upvote_ratio": 0.17, "id": "t3_ub5plk", "created_utc": 1650838236.0}
{"sub": "Python", "title": "Breaking Anti-Cheat With Electronics &amp; Python", "selftext": "nan", "upvote_ratio": 0.4, "id": "t3_ub4yc3", "created_utc": 1650836115.0}
{"sub": "Python", "title": "James Bond film details", "selftext": "    import random\n    import numpy\n    from statistics import mode\n    import matplotlib.pyplot as plt\n    import csv\n    import collections\n    \n    ##dictionary of films\n    from numpy import ndarray\n    \n    films_info_dic = {\n        'Films':[\n            {\n            \"Name\": \"Dr. No\",\n            \"Actor\":\"Sean Connery\",\n            \"Running Time\": 109,\n            \"Year\": \"1962\"\n            },\n    \n            {\n            \"Name\": \"From Russia with Love\",\n            \"Actor\":\"Sean Connery\",\n            \"Running Time\": 115,\n            \"Year\": \"1963\"\n            },\n    \n            {\n            \"Name\": \"Goldfinger\",\n            \"Actor\": \"Sean Connery\",\n            \"Running Time\": 110,\n            \"Year\": \"1964\"\n            },\n    \n            {\n                \"Name\": \"Thunderball\",\n                \"Actor\": \"Sean Connery\",\n                \"Running Time\": 130,\n                \"Year\": \"1965\"\n            },\n    \n            {\n                \"Name\": \"You Only Live Twice\",\n                \"Actor\": \"Sean Connery\",\n                \"Running Time\": 117,\n                \"Year\": \"1967\"\n            },\n    \n            {\n                \"Name\": \"On Her Majesty's Secret Service\",\n                \"Actor\": \"George Lazenby\",\n                \"Running Time\": 140,\n                \"Year\": \"1969\"\n            },\n    \n            {\n                \"Name\": \"Diamonds Are Forever\",\n                \"Actor\": \"Sean Connery\",\n                \"Running Time\": 120,\n                \"Year\": \"1971\"\n            },\n    \n            {\n                \"Name\": \"Live and Let Die\",\n                \"Actor\": \"Roger Moore\",\n                \"Running Time\": 121,\n                \"Year\": \"1973\"\n            },\n    \n            {\n                \"Name\": \"The Man with the Golden Gun\",\n                \"Actor\": \"Roger Moore\",\n                \"Running Time\": 125,\n                \"Year\": \"1974\"\n            },\n    \n            {\n                \"Name\": \"The Spy Who Loved Me\",\n                \"Actor\": \"Roger Moore\",\n                \"Running Time\": 125,\n                \"Year\": \"1977\"\n            },\n    \n            {\n                \"Name\": \"Moonraker\",\n                \"Actor\": \"Roger Moore\",\n                \"Running Time\": 126,\n                \"Year\": \"1979\"\n            },\n    \n            {\n                \"Name\": \"For Your Eyes Only\",\n                \"Actor\": \"Roger Moore\",\n                \"Running Time\": 127,\n                \"Year\": \"1981\"\n            },\n    \n            {\n                \"Name\": \"Octopussy\",\n                \"Actor\": \"Roger Moore\",\n                \"Running Time\": 131,\n                \"Year\": \"1983\"\n            },\n    \n            {\n                \"Name\": \"A View to a Kill\",\n                \"Actor\": \"Roger Moore\",\n                \"Running Time\": 131,\n                \"Year\": \"1985\"\n            },\n    \n            {\n                \"Name\": \"The Living Daylights\",\n                \"Actor\": \"Timothy Dalton\",\n                \"Running Time\": 130,\n                \"Year\": \"1987\"\n            },\n    \n            {\n                \"Name\": \"Licence to Kill\",\n                \"Actor\": \"Timothy Dalton\",\n                \"Running Time\": 133,\n                \"Year\": \"1989\"\n            },\n    \n            {\n                \"Name\": \"GoldenEye\",\n                \"Actor\": \"Pierce Brosnan\",\n                \"Running Time\": 128,\n                \"Year\": \"1995\"\n            },\n    \n            {\n                \"Name\": \"Tomorrow Never Dies\",\n                \"Actor\": \"Pierce Brosnan\",\n                \"Running Time\": 119,\n                \"Year\": \"1997\"\n            },\n    \n            {\n                \"Name\": \"The World Is Not Enough\",\n                \"Actor\": \"Pierce Brosnan\",\n                \"Running Time\": 128,\n                \"Year\": \"1999\"\n            },\n    \n            {\n                \"Name\": \"Die Another Day\",\n                \"Actor\": \"Pierce Brosnan\",\n                \"Running Time\": 133,\n                \"Year\": \"2002\"\n            },\n    \n            {\n                \"Name\": \"Casino Royale\",\n                \"Actor\": \"Daniel Craig\",\n                \"Running Time\": 144,\n                \"Year\": \"2006\"\n            },\n    \n            {\n                \"Name\": \"Quantum of Solace\",\n                \"Actor\": \"Daniel Craig\",\n                \"Running Time\": 106,\n                \"Year\": \"2008\"\n            },\n    \n            {\n                \"Name\": \"Skyfall\",\n                \"Actor\": \"Daniel Craig\",\n                \"Running Time\": 143,\n                \"Year\": \"2012\"\n            },\n    \n            {\n                \"Name\": \"Spectre\",\n                \"Actor\": \"Daniel Craig\",\n                \"Running Time\": 148,\n                \"Year\": \"2015\"\n            },\n    \n            {\n                \"Name\": \"No Time to Die\",\n                \"Actor\": \"Daniel Craig\",\n                \"Running Time\": 163,\n                \"Year\": \"2021\"\n            },\n    \n        ]\n    \n    }\n    \n    \n    ##print values of running time\n    times_list = []\n    \n    \n    ##Running Time list\n    for item in films_info_dic[\"Films\"]:\n        times=item[\"Running Time\"]\n        times_list.append(times)\n    \n    ##List of Actors\n    actor_list = []\n    for item in films_info_dic[\"Films\"]:\n        Actors=item[\"Actor\"]\n        actor_list.append(Actors)\n    \n    ##Remove duplicates\n    actor_list = list(dict.fromkeys(actor_list))\n    \n    def get_film_details(actor_name,decade):\n        films_actor_counter=0\n        films_decade_counter = 0\n        ##actors\n        for item in films_info_dic[\"Films\"]:\n            if item[\"Actor\"] == actor_name:\n                times = item[\"Running Time\"]\n                times_list.append(times)\n    \n                # Mean, median and mode of runnint times list\n                mean_time = numpy.mean(times_list)\n                median_time = numpy.median(times_list)\n                mode_time = mode(times_list)\n    \n                films_actor_counter=films_actor_counter+1\n    \n                ##write to notepad\n                with open((actor_name)+\".txt\", 'w') as f:\n                    f.write(f\"The mean running time for {actor_name} is {mean_time}\\n\")\n                    f.write(f\"The median running time for {actor_name} is {median_time}\\n\")\n                    f.write(f\"The mode running time for {actor_name} is {mode_time}\\n\")\n                    f.write(f\"{actor_name} starred in {films_actor_counter} number of films.\\n\")\n            ##decades\n            Film_Year=item[\"Year\"]\n            Film_decade=Film_Year[2]\n            Test_film=decade[2]\n    \n            if Film_decade == Test_film:\n                times = item[\"Running Time\"]\n                times_list.append(times)\n    \n                # Mean, median and mode of runnint times list\n                mean_time = numpy.mean(times_list)\n                median_time = numpy.median(times_list)\n                mode_time = mode(times_list)\n    \n                films_decade_counter=films_decade_counter+1\n    \n                ##write to notepad\n                with open((decade)+\"s.txt\", 'w') as f:\n                    f.write(f\"The mean running time for {decade}s is {mean_time}\\n\")\n                    f.write(f\"The median running time for {decade}s is {median_time}\\n\")\n                    f.write(f\"The mode running time for {decade}s is {mode_time}\\n\")\n                    f.write(f\"{decade}s had {films_decade_counter} number of films.\\n\")\n    \n    \n    ##call method for each actor\n    decade_list =[\"1960\", \"1970\", \"1980\", \"1990\", \"2000\", \"2010\", \"2020\"]\n    for actor,decade in zip(actor_list, decade_list):\n        get_film_details(actor,decade)\n    \n    \n    print(times_list)\n    print(actor_list)\n    \n    \n    \n    \n    #Mean, median and mode of runnint times list\n    mean_time=numpy.mean(times_list)\n    median_time=numpy.median(times_list)\n    mode_time=mode(times_list)\n    \n    print(f\"The mean running time is {mean_time}\")\n    print(f\"The median running is {median_time}\")\n    print(f\"The mode running is {mode_time}\")", "upvote_ratio": 0.43, "id": "t3_ub2fqt", "created_utc": 1650829006.0}
{"sub": "Python", "title": "Development of Desktop apps with Python", "selftext": "If you are developing desktop apps and are familiar with environment variables and shortcuts, did you realized that there are no packages that would make it easy for cross compatible and easy management of this stuff ?\n\nAfter searching and making my own package i decided to make it public so it can potentially help somebody with the same stuff i was struggling before, feel free to check it out:Github: [https://github.com/jiri-otoupal/pycrosskit](https://github.com/jiri-otoupal/pycrosskit)\n\nIf you would star my repo for the work I do, it would make my day much better :)  \nI will be glad if it will make your life easier, Cheers !", "upvote_ratio": 0.75, "id": "t3_ub2c8w", "created_utc": 1650828730.0}
{"sub": "Python", "title": "Weekly Code - Week 4: Digit Subtraction!", "selftext": "This week I decided to use OEIS entry A185107: difference of digits of the nth prime. This one is pretty exciting, and I think I may make a library featuring this Digit Subtraction and what not. It's fairly compelling. I don't know what I'd use it for, but I'm sure there'd be some kind of use out there.   \n\n\nThis week is documented here:\n\n[https://youtu.be/pHip9F5H8Zc](https://youtu.be/pHip9F5H8Zc)\n\n[https://github.com/F35H/WeeklyCode](https://github.com/F35H/WeeklyCode)  \n\n\nHere's the OEIS entry:  \n[https://oeis.org/A185107](https://oeis.org/A185107)", "upvote_ratio": 0.67, "id": "t3_ub1ppc", "created_utc": 1650826915.0}
{"sub": "Python", "title": "I made a game that let's you play any MIDI file with arrow keys!", "selftext": "Video Preview: [https://streamable.com/zhc909](https://streamable.com/zhc909)\n\nIt's like dance dance revolution but you can play any song you want, all you need is a midi file. You can specify which instruments you want to play on each difficulty in JSON files.\n\nSource code: [https://github.com/ravenkls/Midi-Arrow-Rush](https://github.com/ravenkls/Midi-Arrow-Rush)", "upvote_ratio": 1.0, "id": "t3_ub1p3g", "created_utc": 1650826868.0}
{"sub": "Python", "title": "Possible career in Python as a Bilingual", "selftext": "Hi! I am working right now as a bilingual for Spanish and English in a tech company. I don't have skills in tech such as programming etc. But just recently I decided to study Python to upskill myself and have greater opportunities. But I am not sure if there's a career such as a software developer where I could still use my skill as a Bilingual.\n\nHoping for your insight.\n\nThanks", "upvote_ratio": 0.44, "id": "t3_uazva3", "created_utc": 1650821777.0}
{"sub": "Python", "title": "I am intermediate, how to take python (programming skills) to next level?", "selftext": "About me: I know basic programming and problem solving. I already made 2 websites (basic todo-app) using python (Django) as backend and free web templates for frontend. I also made some basic projects with MySQL and python. Made some spammer bots with [selenium](https://selenium-python.readthedocs.io/). I worked with file creation and manipulation. Most of the times I use functions (I'm not comfy with classes)\n\nBut, at this stage, it feels `something's` off.. I don't know what.. but it feels like I'm stuck at this level.\n\n(*I don't have a job*)", "upvote_ratio": 0.83, "id": "t3_uazm1g", "created_utc": 1650821027.0}
{"sub": "Python", "title": "Set Types in Python \u2014 set, frozenset [Documentation - made easy to read]", "selftext": "nan", "upvote_ratio": 0.75, "id": "t3_uaz839", "created_utc": 1650819918.0}
{"sub": "Python", "title": "Game of life", "selftext": "Hi,\n\nI've build a small \"Game of Life\" project, using pygame for display, based first on Conway's rules, and then many other rules (\\~20 for now).\n\nIt's available on PyPI:\n\n[https://pypi.org/project/conway-pygame/](https://pypi.org/project/conway-pygame/)\n\nand on gitlab.com:\n\n[https://gitlab.com/frague59/conway](https://gitlab.com/frague59/conway)\n\nEnjoy !", "upvote_ratio": 0.8, "id": "t3_uaxzr5", "created_utc": 1650816367.0}
{"sub": "Python", "title": "The Python Graph Gallery", "selftext": "nan", "upvote_ratio": 0.97, "id": "t3_uaxm35", "created_utc": 1650815307.0}
{"sub": "Python", "title": "Useful tricks with pip install URL and GitHub", "selftext": "nan", "upvote_ratio": 0.57, "id": "t3_uawxuo", "created_utc": 1650813388.0}
{"sub": "Python", "title": "How to use MicroPython on Docker!", "selftext": "nan", "upvote_ratio": 0.84, "id": "t3_uawlsz", "created_utc": 1650812439.0}
{"sub": "Python", "title": "Speeding up python CLI's!", "selftext": "so yesterday I was trying to make a python CLI that needed to be fast.\n\nno matter what I did (`python -S`, pyinstaller, cx_freeze, bytecode compiling) it wouldnt be as fast as I needed it and some even made it slower like cx_freeze.\n\nwell I found a way to make running it much much faster (nearly instantaneous).\n\nby using an asynchronous socket server and using netcat as input, you can give input to the server and recieve output immediately, since the script is already running\n\nit works for windows and linux, but you may need cygwin on windows unless there is a built in timeout and netcat command I do not know about\n\nhow it works (on linux) is you define a bash function with this:\n`hithere(){ printf \"$@\" | timeout 0.1 nc 127.0.0.1 50200; }`\n\nand run the script: \n`python pyspeedtest.py`\n\nto send input you run `hithere Mum` and you get `hi, Mum`\n\npyspeedtest.py\n```\nimport pyspeed\n\nclass myhandler(pyspeed.pyspeed_handler):\n    def handle_request(self, *argv):\n        return f\"hi, {argv[1]}\"\n\nhandle = myhandler()\nhandle.run()\n```\n\npyspeed.py\n```python\nimport asyncio, socket\n\nclass pyspeed_handler:\n    async def run_server(self, address, port, handle):\n        server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        server.bind((address, port))\n        server.listen(8)\n        server.setblocking(False)\n        loop = asyncio.get_event_loop()\n        while True:\n            client, _ = await loop.sock_accept(server)\n            loop.create_task(handle(client))\n    async def handle_client(self, client):\n        loop = asyncio.get_event_loop()\n        request = None\n        try:\n            while request != 'quit':\n                request = (await loop.sock_recv(client, 255)).decode('utf8')\n                response = self.handle_request(*([''] + str(request).split(' '))) + '\\n'\n                await loop.sock_sendall(client, response.encode('utf8'))\n            client.close()\n        except BrokenPipeError:\n            pass\n    def handle_request(self, *argv):\n        return ''.join(argv)\n    def run(self, address=\"localhost\", port=50200):\n        asyncio.run(self.run_server(address, port, self.handle_client))\n```\n```", "upvote_ratio": 0.6, "id": "t3_uavrnn", "created_utc": 1650809995.0}
{"sub": "Python", "title": "ga-extractor - CLI tool for extracting Google Analytics data", "selftext": "Hi /r/Python,\n\nI've created a simple CLI tool in Python for extracting Google Analytics data. It can be handy if you want to retrieve some analytics data without dealing with Google's APIs.\n\nThe tool can also transform the data into more readable CSV output.\n\nPyPI package: https://pypi.org/project/ga-extractor/\n\nGitHub repository: https://github.com/MartinHeinz/ga-extractor\n\nFeedback is very much appreciated!", "upvote_ratio": 0.92, "id": "t3_uau4ci", "created_utc": 1650804648.0}
{"sub": "Python", "title": "Get total time spent watching movies logged on to Letterboxd", "selftext": "[Github Link \\[Letterboxd Movie Runtimes\\]](https://github.com/HighnessAtharva/Letterboxd-Movie-Runtimes)\n\nYou can now get the total time you have spent watching all the movies that you have logged on your Letterboxd Profile and export it to a CSV using this simple Python Script.\n\nCould not find a tool on the internet that did this for me so I built it myself. Enjoy :)", "upvote_ratio": 0.83, "id": "t3_uau2nl", "created_utc": 1650804488.0}
{"sub": "Python", "title": "Logging facility for Python - Documentation, made easy to read", "selftext": "nan", "upvote_ratio": 0.38, "id": "t3_uatomd", "created_utc": 1650803132.0}
{"sub": "Python", "title": "Building a Soccer Shot Map for Spain", "selftext": "Salve Jason and the Pythonauts!\n\nI've created a tutorial on building a shot map for soccer games. It uses data from Statsbompy and my own library of todofcpy. You can view it here:\n\n[https://www.youtube.com/watch?v=99FVmANPNXI](https://www.youtube.com/watch?v=99FVmANPNXI)", "upvote_ratio": 0.88, "id": "t3_uatfhp", "created_utc": 1650802234.0}
{"sub": "Python", "title": "borb vs fpdf2 - comparing 2 PDF generation libs: features &amp; benchmark", "selftext": "nan", "upvote_ratio": 0.81, "id": "t3_uasf5r", "created_utc": 1650798393.0}
{"sub": "Python", "title": "I made my first Discord Bot with Python!", "selftext": "Hello everyone! Over the past 2 weeks I have been working on Discord Bot using [discord.py](https://discord.py) and Python! Inspired by r/place I decided to make a simple bot where users and place pixels on a large canvas. Commands include:\n\n$add\\_pixel: Add a pixel to the grid at a specified coordinate and rgb color value\n\n$playback: Create a video showing the full canvas history\n\n$ban: Bans a user from placing new pixels (Admin only)\n\n$unban: Unbans a user allowing them to place pixels again (Admin only)\n\nOne of the reasons I started with project was to learn how to use the MongoDB database as well as learn discord.py Here is a small example of the bot being used on my server\n\nhttps://i.redd.it/m1ure9744gv81.gif\n\nIf you are interested feel free to add the bot to your server with this link: [https://discord.com/api/oauth2/authorize?client\\_id=964251008115019847&amp;permissions=116736&amp;scope=bot](https://discord.com/api/oauth2/authorize?client_id=964251008115019847&amp;permissions=116736&amp;scope=bot) Also DM me (ScriptLine Studios#8597) if you need help or run into issues!\n\nThanks everyone!", "upvote_ratio": 0.9, "id": "t3_uar1v9", "created_utc": 1650792599.0}
{"sub": "Python", "title": "Open source daily Capybara Website built with Python", "selftext": "Ever wondered \"If a random Capybara was assigned to Today, what Capybara would it be?\" Well wonder no more, [Capy.life](https://capy.life/) has your back!\n\n[Capy.life](https://Capy.life) is a free &amp; open source website built with Svelte &amp; Python, what also has a Discord, Matrix &amp; Twitter bot written in Python.\n\nThe Capybara submitting process uses perceptual hash to ensure two Capybara images aren't too much alike.\n\n# Source code\n\n* [Website](https://github.com/capylife/capyend) (Any PRs or Stars are appreciated)\n* [Twitter bot](https://github.com/capylife/flappycapy)\n* [Discord bot](https://github.com/capylife/capycord)\n* [Matrix bot](https://github.com/capylife/neocapy)\n\n# Previews\n\n[Home page](https://preview.redd.it/yuq91y504gv81.png?width=1668&amp;format=png&amp;auto=webp&amp;s=f7245094ddacf37ac8e0047f7854979cb35e40d8)\n\n[Admin Page](https://preview.redd.it/9bnjcl614gv81.png?width=1668&amp;format=png&amp;auto=webp&amp;s=fd72626f13a76e3bc8e2c6b450c9ff5c9975d26d)", "upvote_ratio": 0.63, "id": "t3_uar10e", "created_utc": 1650792493.0}
{"sub": "Python", "title": "GitHub - roniemartinez/browsers: Python library for detecting and launching browsers", "selftext": "nan", "upvote_ratio": 0.69, "id": "t3_uaqavx", "created_utc": 1650789380.0}
{"sub": "Python", "title": "What's your favorite GUI library and why? I'll start, mine is TKinter because its the first one I learned and I found it easy for basic display", "selftext": "nan", "upvote_ratio": 0.93, "id": "t3_uapobr", "created_utc": 1650786701.0}
{"sub": "Python", "title": "Python Selenium Tutorial #10 - Scrape Websites with Infinite Scrolling", "selftext": "nan", "upvote_ratio": 0.8, "id": "t3_uapb3n", "created_utc": 1650785148.0}
{"sub": "Python", "title": "Extracting WhatsApp messages from an iOS backup", "selftext": "nan", "upvote_ratio": 0.96, "id": "t3_ualvg6", "created_utc": 1650771297.0}
{"sub": "Python", "title": "Program to document code snippets and control under-development projects", "selftext": "Hey there,\n\nI wrote a **Tkinter** program specifically for developers based on two concepts:\n\n* Giving the developer the ability to document (their/others) knowledge and important **code snippets** in an easy, readable, and organized manner.\n* Grouping the under-development **projects in one place** for easy access and control, provided the directory path.\n\nPlease, feel free to have a look at the [Source Code](https://github.com/shehab-fekry/Developer-WorkSpace) and tell me what you think :)\n\nThere will be further features to be added.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/p94e69l4xdv81.png?width=899&amp;format=png&amp;auto=webp&amp;s=26e1359e3f2ac793765cfed2b05b9a3fa55c9c5a\n\nhttps://preview.redd.it/llglv803xdv81.png?width=899&amp;format=png&amp;auto=webp&amp;s=9bfc8a963025fdc9ecd7bc41829babdc6631ebac", "upvote_ratio": 0.64, "id": "t3_uaknmh", "created_utc": 1650766926.0}
{"sub": "Python", "title": "just want to bury", "selftext": " I do not speak English so well so I will write with the help of google translator, I started studying programming for now and I'm having difficulty understanding the makes something object-oriented and something variable I hope not to be talking wrong i realized that often the staff talks about symbols and numbers as whole and letters as variable is right if I think that way ?", "upvote_ratio": 0.33, "id": "t3_uak5ot", "created_utc": 1650765192.0}
{"sub": "Python", "title": "Pons, an async Ethereum RPC client library", "selftext": "I've been waiting a long time for async support in `web3`, and now that it started to appear, it only supports `asyncio` (while I use `trio` in my application), and is in general not quite finished. So I decided to write an RPC client of my own with convenient contract calls, simple structure (instead of a hundred levels of indirection in `web3`) and strictly typed. Still a lot of possible enhancements possible, but it is already useful (well, I use it, at least :).\n\nRepo: https://github.com/fjarri/pons\n\nA simple example:\n\n    import trio\n\n    from eth_account import Account\n    from pons import Client, HTTPProvider, AccountSigner, Address, Amount\n\n    async def main():\n\n       provider = HTTPProvider(\"&lt;your provider's https endpoint&gt;\")\n       client = Client(provider)\n\n       acc = Account.from_key(\"0x&lt;your secret key&gt;\")\n       signer = AccountSigner(acc)\n\n       async with client.session() as session:\n          my_balance = await session.eth_get_balance(signer.address)\n          print(my_balance)\n\n          another_address = Address.from_hex(\"0x&lt;some address&gt;\")\n          await session.transfer(signer, another_address, Amount.ether(1.5))\n\n          my_balance = await session.eth_get_balance(signer.address)\n          print(my_balance)\n\n\n    trio.run(main)\n\n\nMore in the [Tutorial](https://pons.readthedocs.io/en/latest/tutorial.html#tutorial) (not very extensive for now, but hopefully gives an idea of how to use it), and of course there's always the [API reference](https://pons.readthedocs.io/en/latest/api.html).\n\nI am sure there are a lot of usage scenarios I haven't even considered, so I would be especially grateful for complaints about this or that method/parameters/naming being inconvenient, counterintuitive, confusing, or out of place.", "upvote_ratio": 0.38, "id": "t3_uaj86y", "created_utc": 1650761986.0}
{"sub": "Python", "title": "Sunday Daily Thread: What's everyone working on this week?", "selftext": "Tell /r/python what you're working on this week! You can be bragging, grousing, sharing your passion, or explaining your pain. Talk about your current project or your pet project; whatever you want to share.", "upvote_ratio": 0.93, "id": "t3_uai5y5", "created_utc": 1650758410.0}
{"sub": "Python", "title": "Deciding what to use among Cython / Pypy / Numba", "selftext": "So I want to experiment and speed up my code. I have studied the basics of Cython and Numba but Pypy only has 10 year onld videos.\n\nWhat I have found:\n- Cython converts Python into C and makes the code useable in both Python and C\n- Numba directly converts Python into Machine code and is useful for Math operations (numpy)\n- Numba is JIT compiler\n- Both Cython and Numba don't support 3rd party libraries like Pandas and spacy..\n- Pypy is an implementation of Python. Normally the Python we use when we write python abc.exe in cmd is Cpython(not Cython).\n- Numba and Cython speed up the code a lot if the code is compatible... things like list don't work with Numba...\n\n\nWould be super helpful if someone can please explain the difference between Numba, Cython and Pypy and when to use which.\n\nEven pointing me to the resources would be great!\n\nThanks in advance.", "upvote_ratio": 0.82, "id": "t3_uafu40", "created_utc": 1650751007.0}
{"sub": "Python", "title": "Discussion: What is the most pythonic way to print an extra line break?", "selftext": "Which of these four equivalent methods do you prefer and why?\n\n    print('Beginning processing ...\\n')\n\nor\n\n    print('Beginning processing ...', '\\n')\n\nor\n\n    print('Beginning processing ...', end='\\n\\n')\n\nor\n\n    print('Beginning processing ...')\n    print()\n\nIs any of them more or less pythonic than another?", "upvote_ratio": 0.92, "id": "t3_uadbi3", "created_utc": 1650743421.0}
{"sub": "Python", "title": "A simple python library that can be used to run large Web3 queries on Ethereum blockchain concurrently as per Ethereum JSON-RPC specification.", "selftext": "A simple python library that can be used to run large Web3 queries on Ethereum blockchain concurrently as per Ethereum JSON-RPC specification.\n\nThe library provides a bare minimal framework for expressing raw JSON-RPC queries as described in the Ethereum Specification and execute them together either concurrently (off-chain on the client side) or together as a batch (JSON-RPC batch specification on-chain). This method greatly reduces the time required to run large queries sequentially and thus can be used for use-cases where we need to index large number of transactions happening on ethereum blockchain in a local database for faster Web2 queries.\n\nSource code: [GitHub](https://github.com/Narasimha1997/aio-eth)\n\nPyPi: [aio-eth](https://pypi.org/project/aio-eth/)", "upvote_ratio": 0.25, "id": "t3_uad9mx", "created_utc": 1650743270.0}
{"sub": "Python", "title": "Face detection algorithms comparison", "selftext": "I selected 5 ready-made  algorithms for face detection and compared them with each other by such  metrics as Precision, Recall, IOU and time on the dataset I marked up. I  am ready to accept your Pull Request with your solutions(algorithms)  and results!\n\nBlog post: [https://habr.com/ru/post/661671/](https://habr.com/ru/post/661671/)\n\nGitHub:  [https://github.com/wb-08/face-detection-algorithms-comparison](https://github.com/wb-08/face-detection-algorithms-comparison)", "upvote_ratio": 0.64, "id": "t3_uacer5", "created_utc": 1650740697.0}
{"sub": "Python", "title": "GitHub - plasma-umass/slipcover: Near Zero-Overhead Python Code Coverage", "selftext": "nan", "upvote_ratio": 0.93, "id": "t3_ua8sgx", "created_utc": 1650730255.0}
{"sub": "Python", "title": "Parking space counter created using OpenCV and Python", "selftext": "Hello!\n\nI created a simple two-step parking space counter:\n\n\\- first, you mark the positions of all parking spaces you are interested in using \"parking\\_space\\_picker.py\";\n\n\\- second, you run \"parking\\_space\\_counter.py\" to check if the parking space is vacant or not and count them.\n\n[RESULT](https://youtu.be/LERHWFmSSdM)\n\n[CODE](https://github.com/codegiovanni/Parking_space_counter)\n\n&amp;#x200B;\n\nVideo used in the code:\n\nTom Berrigan [https://www.youtube.com/watch?v=yojapmOkIfg&amp;list=LL&amp;index=10](https://www.youtube.com/watch?v=yojapmOkIfg&amp;list=LL&amp;index=10)\n\n&amp;#x200B;\n\nThe code is inspired by:\n\nMurtaza's Workshop - Robotics and AI [https://www.youtube.com/watch?v=caKnQlCMIYI](https://www.youtube.com/watch?v=caKnQlCMIYI)", "upvote_ratio": 0.95, "id": "t3_ua6xh2", "created_utc": 1650724972.0}
{"sub": "Python", "title": "Step by step explanation of Insertion Sort in Python", "selftext": "nan", "upvote_ratio": 0.54, "id": "t3_ua6x8j", "created_utc": 1650724949.0}
{"sub": "Python", "title": "Space Science: Autoencoder latent space visualization of asteroid spectra", "selftext": "Hey Everyone,\n\nLast time, I introduced Autoencoders (using Keras) to develop a deep learning architecture that learns a low-dimensional representation of asteroid reflectance spectra.\n\nAlthough I compressed the 49-dimensional spectra to only 2 dimensions, the results were quite fair. So... why did I compress it so ridiculously high? Well, a 2-D space can easily be visualized!\n\nAnd this visualization is being done today. In today's tutorial, we'll use Matplotlib for a static display of the data, and ipwidgets, to create an interactive widget within our notebook on Google Colab! Let's see whether our 25-fold compression leads to some proper latent space, where the asteroid classes can be distinguished:\n\nGitHub Link: [https://github.com/ThomasAlbin/Astroniz-YT-Tutorials/blob/main/%5BML1%5D-Asteroid-Spectra/12\\_dl\\_autoencoder\\_latent\\_space.ipynb](https://github.com/ThomasAlbin/Astroniz-YT-Tutorials/blob/main/%5BML1%5D-Asteroid-Spectra/12_dl_autoencoder_latent_space.ipynb)  \nYouTube Link: [https://www.youtube.com/watch?v=h26O2qbc5DA](https://www.youtube.com/watch?v=h26O2qbc5DA)\n\nThe next session will be the final one of the asteroid science project. There, we will create a higher dimensional latent space and apply some clustering algorithm to determine the number of asteroid classes from a data-scientific perspective. Stay tuned!\n\nThomas", "upvote_ratio": 0.81, "id": "t3_ua6sq9", "created_utc": 1650724581.0}
{"sub": "Python", "title": "What makes a good programmer?", "selftext": "I recently started a python course and I'm currently just focused on it but I feel like this is wrong and I'm missing something", "upvote_ratio": 0.54, "id": "t3_ua64gc", "created_utc": 1650722585.0}
{"sub": "Python", "title": "Python matplotlib and numpy New Playlist", "selftext": "nan", "upvote_ratio": 0.33, "id": "t3_ua2fpl", "created_utc": 1650709546.0}
{"sub": "Python", "title": "How do you manage conflicting packages in your requirements.txt ?", "selftext": "Hi,\n\nLet's say you have in your requirements.txt :\n\n    package_A\n    package_B\n    package_C\n    package_D\n\nbut `package_A` requires `some_dependency&lt;=1.5` and `package_B` requires `some_dependency&gt;=2.2` . How do you handle that (knowing that I might have tens of conflicting packages)?\n\nI don't think virtualenvs would be a good solution here since the project has one entry point and packages are imported in the same code ...\n\nThank you !:)", "upvote_ratio": 0.96, "id": "t3_ua2a7k", "created_utc": 1650708904.0}
{"sub": "Python", "title": "\"Community is essential to programmers\" - Eric Matthes", "selftext": "I've just started working my way through Eric Matthes' Python Crash Course. In his introduction he states, \" Community is essential to programers because programming isn't a solitary pursuit.... Having  a well connected community is critical in helping you solve problems, and the Python community is fully supportive of people like you who are learning python as your first programming language.\" \n\n&amp;#x200B;\n\nI've dabbled a bit in the basic front end languages and I'm currently playing around with Vue so I wouldn't say it's my first language. However, I did feel compelled to reach out to this community after reading that.   \n\n\nIf you have any advice for someone starting to pick up python, I'm happy to listen and learn.", "upvote_ratio": 0.84, "id": "t3_ua1z3n", "created_utc": 1650707604.0}
{"sub": "Python", "title": "10 examples of using Python for big data analysis", "selftext": "nan", "upvote_ratio": 0.73, "id": "t3_ua1kfl", "created_utc": 1650705802.0}
{"sub": "Python", "title": "MNE \u2014 Open-source Python package for exploring, visualizing, and analyzing human neurophysiological data: MEG, EEG, sEEG, ECoG, NIRS, and more", "selftext": "nan", "upvote_ratio": 0.97, "id": "t3_ua0faz", "created_utc": 1650700833.0}
{"sub": "Python", "title": "Python Tips and Tricks \u2014 Write Better Python Code", "selftext": "nan", "upvote_ratio": 0.61, "id": "t3_u9ziwb", "created_utc": 1650697016.0}
{"sub": "Python", "title": "freeCodeCamp: Gradio Course - Create User Interfaces for Machine Learning Models in Python", "selftext": "nan", "upvote_ratio": 0.83, "id": "t3_u9y93f", "created_utc": 1650691784.0}
{"sub": "Python", "title": "Call me naive, but would it not be possible to create a tool for python the auto adds type hints at run time?", "selftext": "I\u2019m going to learn python over the summer, but coming from Java &amp; c# in IDE\u2019s where casting etc can auto completed within the problem pane when making newb mistakes, and also knowing the pain of PHP runtime errors, I\u2019m hoping the dynamic experience will be smoother with python.\n\nI\u2019m ranting.. would this be feasible ? Just a thought.", "upvote_ratio": 0.17, "id": "t3_u9sdwa", "created_utc": 1650672030.0}
{"sub": "Python", "title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread", "selftext": "Found a neat resource related to Python over the past week? Looking for a resource to explain a certain topic?\n\nUse this thread to chat about and share Python resources!", "upvote_ratio": 1.0, "id": "t3_u9sdh3", "created_utc": 1650672009.0}
{"sub": "Python", "title": "I have multiple interdependent Python services &amp; modules for my work. I use conventional commits (changetype: scope: \u2026) allowing automated changelogs. I think I need to go monorepo and add the changed service to the commit structure (service: changetype: scope). Does this look like a good strategy?", "selftext": "nan", "upvote_ratio": 0.82, "id": "t3_u9rgzf", "created_utc": 1650669201.0}
{"sub": "Python", "title": "What is a good, pure Python alternative to lxml's objectify?", "selftext": "Reference: https://lxml.de/objectify.html\n\n&gt; Accessing the children of an XML element deploys object attribute access. If there are multiple children with the same name, slicing and indexing can be used. Python data types are extracted from XML content automatically and made available to the normal Python operators.", "upvote_ratio": 1.0, "id": "t3_u9qp2k", "created_utc": 1650666983.0}
{"sub": "Python", "title": "Proper launch of python packages", "selftext": "nan", "upvote_ratio": 0.75, "id": "t3_u9q8tx", "created_utc": 1650665684.0}
{"sub": "Python", "title": "Coding an Intelligent Battleship Agent", "selftext": "nan", "upvote_ratio": 0.5, "id": "t3_u9pn6u", "created_utc": 1650664027.0}
{"sub": "Python", "title": "rashell (Relational Algebra Shell)", "selftext": "Hi all.\n\nI've uploaded my project on Pypi. It is called **rashell** which stands for **R**elational **A**lgebra **S**hell. It provides a command line interface and a DSL to define, fill and query a relational model. This tool is intended for educational use only, to illustrate the underlying concepts of relational databases in a more interactive way. It can be installed via pip :\n\n    $ pip install rashell\n\nPlease refer to Readme on gitlab to know how to use it.\n\n[https://gitlab.com/skebir/rashell](https://gitlab.com/skebir/rashell)\n\n[https://pypi.org/project/rashell/](https://pypi.org/project/rashell/)\n\nI would like to have your opinion on it. Thank you in advance.", "upvote_ratio": 1.0, "id": "t3_u9p9g6", "created_utc": 1650662976.0}
{"sub": "Python", "title": "Does anyone know what editor this is or what sorts of editors have this feature?", "selftext": "[The editor is linked here](https://i.imgur.com/9HcadwR.gif)   \n \nBasically allowing you to similar parts to multiple lines of code by clicking and then hitting backspace?", "upvote_ratio": 0.5, "id": "t3_u9obb5", "created_utc": 1650660405.0}
{"sub": "Python", "title": "A screenful of advice about writing command-line tools in Python", "selftext": "nan", "upvote_ratio": 0.77, "id": "t3_u9m149", "created_utc": 1650654132.0}
{"sub": "Python", "title": "I've made a pure Python implementation of the QOI image format", "selftext": "[link to the code](https://github.com/SudoOmbro/qoi_converter)\n\nas said in the title, it's a pure python implementation of the [qoi format](https://qoiformat.org/), a lossless image compression methd that manages be 50x faster while encoding and 3x-4x while faster decoding than PNG.\n\nPython isn't known to be fast though, but this implementation is not too bad speed wise to be fair.\n\nMore info on how it was implemented in the README :)", "upvote_ratio": 1.0, "id": "t3_u9k3i3", "created_utc": 1650648791.0}
{"sub": "Python", "title": "Cache in asynchronous Python applications", "selftext": "nan", "upvote_ratio": 0.8, "id": "t3_u9ihvl", "created_utc": 1650644486.0}
{"sub": "Python", "title": "Car Wash Pattern: Parallelizing Non-Thread Safe and/or CPU-intensive Processes with Future Based Queue Centric Approach in Python", "selftext": "This article can be a useful guide for parallelizing non-thread-safe and CPU-bound processes, such as some machine learning models, for purposes such as model to web service conversion. While developing the solution, I tried to pay particular attention to the advantages, disadvantages and pitfalls of python. Maybe there is nothing new for the masters, but I think it is a neat resource for the enthusiasts.\n\n[https://medium.com/vlmedia-tech/parallelizing-non-thread-safe-and-or-cpu-intensive-processes-with-future-based-queue-centric-b2247bbcf231](https://medium.com/vlmedia-tech/parallelizing-non-thread-safe-and-or-cpu-intensive-processes-with-future-based-queue-centric-b2247bbcf231)", "upvote_ratio": 0.88, "id": "t3_u9hhmv", "created_utc": 1650641812.0}
{"sub": "Python", "title": "Searching a student apartment in Z\u00fcrich was too boring so I made a Telegram bot.", "selftext": "[GitHub link](https://github.com/bskdany/WokoWGZScraperBot)\n\nBasically anyone I know checks every day [woko.ch](https://woko.ch) and [wgzimmer.ch](https://wgzimmer.ch) for rooms and apartments that are being rented for students in Switzerland , the first to contact the seller usually gets the room. I don't have the time and will to do that.\n\nSo I made a bot that scrapes both those websites with requests and  BeautifulSoup. The bot collects the urls of the rented rooms and saves them in a txt, then it checks the website every minute for changes. If a new url is found then the room data is sent to me with the Telegram API. \n\nBoth those websites don't have a policy against scraping, but they do have some little bot protection.\n\nYeah that's it, I made this in two days and I'm hosting the bot on heroku.\n\nSuggestions are welcome (especially in security).", "upvote_ratio": 0.78, "id": "t3_u9g6jb", "created_utc": 1650638272.0}
{"sub": "Python", "title": "Common Python Anti-Patterns to watch out for", "selftext": "nan", "upvote_ratio": 0.81, "id": "t3_u9g5r7", "created_utc": 1650638203.0}
{"sub": "Python", "title": "How to Write a Python Script to Create and Update a Changelog", "selftext": "nan", "upvote_ratio": 0.5, "id": "t3_u9dy34", "created_utc": 1650631951.0}
{"sub": "Python", "title": "copilot getting creepy", "selftext": "hey, yesterday I was making an auto reply bot with telethon (+ copilot)\n\nI got a phone number as a suggestion in **plain text** , meaning that copilot does not have a 'personal info filter' or whatever.\n\nSomeone said that GitHub also used private repos to train copilot, well, let's hope that nobody gets a suggestion with my bybit api keys\n\nhttps://preview.redd.it/i8mw9t26s2v81.jpg?width=384&amp;format=pjpg&amp;auto=webp&amp;s=55d082b6ff58f8c9b1e527f424a72de9a88c3504\n\nhttps://preview.redd.it/q3mhee6ir2v81.jpg?width=1354&amp;format=pjpg&amp;auto=webp&amp;s=f2325cb1e6f144a9e6d571c3c86bae178e2ecfbd", "upvote_ratio": 0.88, "id": "t3_u9dobe", "created_utc": 1650631103.0}
{"sub": "Python", "title": "Python 3.11 Preview: Task and Exception Groups \u2013 Real Python", "selftext": "nan", "upvote_ratio": 0.92, "id": "t3_u9dhsy", "created_utc": 1650630515.0}
{"sub": "Python", "title": "Boihut bookstore(Ecommerce ) website made in Django", "selftext": "This was built as my first year university project. \n\n&amp;#x200B;\n\nhttps://preview.redd.it/7ar5lrkw72v81.png?width=1920&amp;format=png&amp;auto=webp&amp;s=fa8428c4ff54ca6ac149b89915153657b1aabe71\n\n&amp;#x200B;\n\nLive link : [https://boihut.biz](https://boihut.biz)\n\nGithub: [https://github.com/shaongitt/boihut](https://github.com/shaongitt/boihut)", "upvote_ratio": 0.9, "id": "t3_u9bpmd", "created_utc": 1650624350.0}
{"sub": "Python", "title": "Login and logout functionality in django framework", "selftext": "nan", "upvote_ratio": 0.65, "id": "t3_u97qgl", "created_utc": 1650607754.0}
{"sub": "Python", "title": "Friday Daily Thread: Free chat Friday! Daily Thread", "selftext": "Use this thread to talk about anything Python related! Questions, news,  projects and any relevant discussion around Python is permitted!", "upvote_ratio": 0.8, "id": "t3_u915sx", "created_utc": 1650585609.0}
{"sub": "Python", "title": "Step by step explanation of Bubble sort with python implementation", "selftext": "nan", "upvote_ratio": 0.88, "id": "t3_u90cgx", "created_utc": 1650583086.0}
{"sub": "Python", "title": "Intel 8051 microcontroller emulator", "selftext": "I made this app as a BE thesis. It's using Brython to make Python work in Chromium (Electron) and React with Material-UI for the front-end. It's supposed to be rendered in a fixed-sized Electron window, so it may not look the best in a browser.\n\n[https://github.com/estarq/i8051emu](https://github.com/estarq/i8051emu)", "upvote_ratio": 0.75, "id": "t3_u8xutf", "created_utc": 1650575884.0}
{"sub": "Python", "title": "dc_schema, a tiny library to generate JSON schema from python dataclasses", "selftext": "I wrote a small library for generating JSON schema from python dataclasses. I'm using pydantic a lot in my daily work, but wanted to understand JSON schema better myself and create a lightweight, focused solution for schema generation.  \n\nPosting here incase some of you are interested to try it out, maybe someone has some constructive feedback/review.\n\nhttps://github.com/Peter554/dc_schema", "upvote_ratio": 0.75, "id": "t3_u8x9pw", "created_utc": 1650574245.0}
{"sub": "Python", "title": "Cache in asynchronous Python applications", "selftext": "nan", "upvote_ratio": 0.67, "id": "t3_u8v01p", "created_utc": 1650567983.0}
{"sub": "Python", "title": "Is it bad practice to start with Jupyter Notebooks?", "selftext": "Nowadays whenever I start a new Python project I always start with a jupyter notebook to test snippets of code, to later add to my main script. \n\nShould I avoid doing this? Is this bad practice?", "upvote_ratio": 0.91, "id": "t3_u8tsd6", "created_utc": 1650564637.0}
{"sub": "Python", "title": "Python\u2019s Match-Case Is Too Slow (If You Don\u2019t Understand It)", "selftext": "nan", "upvote_ratio": 0.44, "id": "t3_u8t99e", "created_utc": 1650563193.0}
{"sub": "Python", "title": "Any suggestions for simple predictive modeling for class project using nba game data", "selftext": "Any suggestions for simple predictive modeling for class project using nba game data", "upvote_ratio": 0.6, "id": "t3_u8sby9", "created_utc": 1650560623.0}
{"sub": "Python", "title": "A magic hand using DXL and servo motors", "selftext": "I recorded this quick demo to show how topology detection can work with DXLs or servo motors. I developed a few lines of code, if you want to see, I can share them with you!", "upvote_ratio": 0.78, "id": "t3_u8qjgg", "created_utc": 1650555851.0}
{"sub": "Python", "title": "co-author.py - Creates \"Co-authored-by\" lines from usernames and issue/PR urls using GitHub REST API", "selftext": "I needed a quick way to credit the original PR authors while patching [my Alabaster fork](https://github.com/introt/alabester/releases/tag/0.7.22), so I wrote this little script to assist in the task.\n\nThe combination of using the REST API along with the no-reply email addresses seems novel in the co-authoring space, so I decided to share it - I hope someone finds it useful!\n\nFeel to fork and/or put it up on PyPi etc, the code's released under the MIT license.\n\nhttps://gist.github.com/introt/ad30bcbdf789aed5bba43082741c7769", "upvote_ratio": 0.6, "id": "t3_u8ol71", "created_utc": 1650550389.0}
{"sub": "Python", "title": "Just started making a Pok\u00e9mon wordle type game. Wish me luck, should be done by the end of two weeks", "selftext": "It's my first ever big project so I'm pretty excited :)", "upvote_ratio": 0.62, "id": "t3_u8of7m", "created_utc": 1650549900.0}
{"sub": "Python", "title": "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| - TQDM is a simple library for adding progress bars to your python code. I made a tutorial about it.", "selftext": "nan", "upvote_ratio": 0.67, "id": "t3_u8nzzr", "created_utc": 1650548669.0}
{"sub": "Python", "title": "Is everything worth solving?", "selftext": "When a bug takes ages to fix should I still keep trying or move on since I\u2019m a beginner should I not try to move on to learn something else and then maybe come back at some point? Thanks", "upvote_ratio": 0.62, "id": "t3_u8nfcq", "created_utc": 1650546950.0}
{"sub": "Python", "title": "Manage Encryption Key", "selftext": "I created a public Github repo for Creating, Encrypting, and Decrypting files called: [Manage Encryption Key](https://github.com/Moreless91/Manage-Encryption-Key)\n\n[Main Menu](https://preview.redd.it/bgde2ghlkvu81.png?width=306&amp;format=png&amp;auto=webp&amp;s=32be688c1e436dae386d3b3301745c4789afba18)\n\nIt's a CLI tool that I've needed for quite awhile for quickly creating a key for a new app or decrypting some older data from older projects.\n\nHere's some screenshots:\n\nEncrypting:  \n\n[Encrypting](https://preview.redd.it/lacxuedllvu81.png?width=325&amp;format=png&amp;auto=webp&amp;s=3a2b8e0eb3c39648619ec2bd38aba2aa83873a78)\n\nDecrypting:\n\n&amp;#x200B;\n\n[Decrypting](https://preview.redd.it/vqsewmbslvu81.png?width=333&amp;format=png&amp;auto=webp&amp;s=802a9c3e0b7041ce4830a5831517698b864981ee)\n\nYou can stay organized by adjusting the Settings option for storing your files quickly:\n\n[Settings](https://preview.redd.it/8jgd4ckulvu81.png?width=321&amp;format=png&amp;auto=webp&amp;s=48faf595e791bd713c78c390666150f83e79db02)\n\nTo setup, view the README on [https://github.com/Moreless91/Manage-Encryption-Key](https://github.com/Moreless91/Manage-Encryption-Key)\n\nI'm still new to python and programming in general. No formal education. Tear my code apart, please!!", "upvote_ratio": 0.7, "id": "t3_u8mozm", "created_utc": 1650544746.0}
{"sub": "Python", "title": "Know How to Create and Visualize a Decision Tree with Python", "selftext": "nan", "upvote_ratio": 0.72, "id": "t3_u8k0am", "created_utc": 1650535185.0}
{"sub": "Python", "title": "2022 Update: Understanding Best Practice Python Tooling by Comparing Popular Project Templates", "selftext": "I have just posted a 2022 update to my old blog post from 2020 -&gt; [https://medium.com/@jonas.r.kemper/2022-update-understanding-best-practice-python-tooling-by-comparing-popular-project-templates-5872602fe617?sk=a7ed50c63851d81093697c62b740396a](https://medium.com/@jonas.r.kemper/2022-update-understanding-best-practice-python-tooling-by-comparing-popular-project-templates-5872602fe617?sk=a7ed50c63851d81093697c62b740396a)  \n\n\nHope you like it! :)", "upvote_ratio": 0.91, "id": "t3_u8jfcb", "created_utc": 1650532690.0}
{"sub": "Python", "title": "Unpopular opinion: Matplotlib is a bad library", "selftext": "I work with data using Python a lot. Sometimes, I need to do some visualizations.  Sadly, matplotlib is the de-facto standard for visualization. The API of this library is a pain in the ass to work with. I know there are things like Seaborn which make the experience less shitty, but that's only a partial solution and isn't always easily available. Historically, it was built to imitate then-popular Matlab. But I don't like Matlab either and consider it's API and plotting capabilities very inferior to e.g. Wolfram Mathematica. Plus trying to port the already awkward Matlab API to Python made the whole thing double awkward, the whole library overall does not feel very Pythonic.\n\nPlease give a me better plotting libary that works seemlessly with Jupyter!", "upvote_ratio": 0.92, "id": "t3_u8j6fn", "created_utc": 1650531638.0}
{"sub": "Python", "title": "GitHub - aGIToz/PyInpaint: A lightweight image inpainting tool in python.", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_u8hao9", "created_utc": 1650523323.0}
{"sub": "Python", "title": "Data Operations Using Python Mitosheets", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_u8fj6a", "created_utc": 1650516403.0}
{"sub": "Python", "title": "Urwid wrapper for nvidia-smi", "selftext": "Hello. I wanted to share a simple urwid-based wrapper I wrote for nvidia-smi. I find that nvidia-smi takes up too much space in my terminal and I prefer graphical/bar based displays. These are not my GPUs - I am not that rich. Check it out here:\n\n[https://github.com/nec4/gpu-array](https://github.com/nec4/gpu-array)  \n\n\nhttps://i.redd.it/7af8spadasu81.gif\n\nIt's written entirely in Python, and was my first departure from using ncurses directly. After diving in, I have to save urwid is very slick.", "upvote_ratio": 0.76, "id": "t3_u8bx7e", "created_utc": 1650504587.0}
{"sub": "Python", "title": "Creating and API from scratch...", "selftext": "Hey guys, I just learned Django and React, I know my few things about python amd javascript. Ive done small projects using \"Django Rest Framework\".\n\nMy question is, can I build an API from scratch without using Django or DjangoRest Framework? \n\nIm curious how its done. All the google searches Ive done they give back answers usimg Django or flask.\n\nIf I had only python, postgres, javascript, html and css to work on...how can I build an API without frameworks? \n\nThe idea of creating one from scratch confuses me since Ive only built one using Django Rest Framework. \n\n\n\nAny guidance will help.", "upvote_ratio": 0.66, "id": "t3_u8ajpp", "created_utc": 1650500352.0}
{"sub": "Python", "title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!", "selftext": "Discussion of using Python in a professional environment, getting jobs in Python as well as ask questions about courses to further your python education!\n\n**This thread is not for recruitment, please see** r/PythonJobs **or the thread in the sidebar for that.**", "upvote_ratio": 0.72, "id": "t3_u8a5pq", "created_utc": 1650499209.0}
{"sub": "Python", "title": "Python Tutorial - How to create a Car Processor and Detector using Python?", "selftext": "Hey Everyone! I created a short Python Tutorial explaining how I created a Car Processor and Detector using Python.  \n[https://www.youtube.com/watch?v=ZXFS-uUNTcg](https://www.youtube.com/watch?v=ZXFS-uUNTcg)\n\nhttps://preview.redd.it/gjbek5zoqru81.png?width=2880&amp;format=png&amp;auto=webp&amp;s=149cf6b3e18c77d190c806686d588862ebcab56e", "upvote_ratio": 0.75, "id": "t3_u89kvw", "created_utc": 1650497426.0}
{"sub": "Python", "title": "Flask vs FastAPI for a microservice", "selftext": "Hello,\n\n&amp;#x200B;\n\nI'm going to build a microservice that processes images and does OMR on them, and I'm torn between using Flask or FastAPI.\n\n&amp;#x200B;\n\nI have used Flask in the past, but recently I have been using Nodejs, so the async nature of FastAPI will make it similar to Nodejs, plus I have read that it's better for making APIs.\n\n&amp;#x200B;\n\nWhich one do you think is more suitable in my case?", "upvote_ratio": 0.76, "id": "t3_u86ra7", "created_utc": 1650489404.0}
{"sub": "Python", "title": "Type hints immediately giving payoffs", "selftext": "[https://imgur.com/a/0NaPfxQ](https://imgur.com/a/0NaPfxQ)\n\nSo, I 'FORCED' myself to use typehints.   Flask seems to already priovide some motivation, so I went ahead and carried on.\n\nLOOK!  WOW!  PyCHARM!\n\nImmediately, I know that I'm not 'properly' creating a file path.  I can go back and fix this later, but look, joining a bunch of strings with '/' doesn't make an 'os.path'.   Nice warning!\n\nTypehints.... I'm becoming a fan.  It's slowing me down a little developing, but this is an obvious place a bug can happen, and I didn't really think about it.\n\nIt's not ready, but this is a custom-built hls/dash packager.   [https://github.com/flipmcf/CasterPak](https://github.com/flipmcf/CasterPak)   It's only like 2 days old, so be nice.", "upvote_ratio": 0.61, "id": "t3_u86q6j", "created_utc": 1650489316.0}
{"sub": "Python", "title": "Bloomberg just Open sourced Memray a memory profiler for Python", "selftext": "nan", "upvote_ratio": 0.98, "id": "t3_u84tjr", "created_utc": 1650484108.0}
{"sub": "Python", "title": "Python learning group?", "selftext": "Hello! I recently graduated college with a psychology degree but figuring out it may not be something I want to do with my life, I recently started learning programing languages, specificly Python.\n\nI'm currently doing okay. I'm understanding things so far but as it gets more advanced in the long run, I believe that learning with like minded people would increase the possibility of us seeing it through. \n\nDoes anyone want to form a study group, via discord or know of any??", "upvote_ratio": 0.76, "id": "t3_u81o93", "created_utc": 1650475476.0}
{"sub": "Python", "title": "Python's stability", "selftext": "My production code has been running for a while. For the first 24 hours, every metric, errors, logging, etc.. happened as expected. But the last 6 hours was a disaster. Even though the state was unchanged, some logs were not shown, some services were not called, etc... Have you guys faced this kind of instability before?", "upvote_ratio": 0.22, "id": "t3_u7zog4", "created_utc": 1650470137.0}
{"sub": "Python", "title": "I wrote an article on \"Packaging and Publishing Packages on PyPI\"", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_u7lr6r", "created_utc": 1650420710.0}
{"sub": "Python", "title": "How to write a Python3 wrapper library/module for a JSON REST API in 15 simple steps", "selftext": "nan", "upvote_ratio": 0.96, "id": "t3_u7vquv", "created_utc": 1650458946.0}
{"sub": "Python", "title": "37 Sixty Second Python Tutorials", "selftext": "Python 60 Second Videos: https://www.youtube.com/playlist?list=PL6lxxT7IdTxG5li13TmP0fvHaFRZaHi4d", "upvote_ratio": 0.8, "id": "t3_u7uhit", "created_utc": 1650454737.0}
{"sub": "Python", "title": "Gaming oriented Turtle fork", "selftext": "I've been learning python for around 6 months and I've made a fork of the turtle module called \"burtle\", its meant to make games with, but i added other cool stuff like a text box and click detection.\n\nPlease check it out and give feedback!\n\n[https://github.com/alannxq/Burtle](https://github.com/alannxq/Burtle)", "upvote_ratio": 0.64, "id": "t3_u7s0vo", "created_utc": 1650444648.0}
{"sub": "Python", "title": "Java Vs Python Comparison: Which Programming Language is Right for My Business?", "selftext": "Entrepreneurs are presently facing a challenge in establishing their enterprise app since they must make a difficult decision: which language to employ for their app development. Both Java and Python have advantages and disadvantages and certain commonalities. However, there are some distinctions between Java and Python.\n\nThe most difficult decision for entrepreneurs who wish to automate their businesses is to choose between the two master languages. You may have read about the differences between these two options, depending on your project's technical requirements. This blog provides eye-opening insights into the [Java vs Python](https://www.bacancytechnology.com/blog/java-vs-python) difference in enterprise applications which you can check out. \n\n&amp;#x200B;", "upvote_ratio": 0.31, "id": "t3_u7rvi0", "created_utc": 1650443966.0}
{"sub": "Python", "title": "Making the switch from academia to industry? R to Python?", "selftext": "I'm 32F and finally deciding to make the switch. The only snag is my experience in coding is all in R. I know there are lots of online Python courses but I'm looking for an intensive (couple of weeks maybe?)  course that would be accredited and sit well on my CV. I've done well for myself in academia, in terms of reputation in data viz, but I have no connections in industry and I'm not sure how to get in the door. Anyone else made the switcheroo?", "upvote_ratio": 0.88, "id": "t3_u7qtvf", "created_utc": 1650439255.0}
{"sub": "Python", "title": "Compiling Python programs with Pyinstaller", "selftext": "nan", "upvote_ratio": 0.73, "id": "t3_u7pu7w", "created_utc": 1650435048.0}
{"sub": "Python", "title": "Visualization of the 3x+1 problem using turtle graphics", "selftext": "&amp;#x200B;\n\n[My new screen saver](https://preview.redd.it/8zfb0b1p4mu81.png?width=3814&amp;format=png&amp;auto=webp&amp;s=a726b12997a7c6d3fad9a784e56550c878d192bd)\n\n[Source code](https://github.com/Spovis/collatz)\n\nThis is a visualization of the sequence generated by the Collatz conjecture. It generates some beautifully intricate graphs from some very simple logic. I had a lot of fun making this, maybe it will give some ideas to someone else.\n\nAny code critiques are welcome.", "upvote_ratio": 0.88, "id": "t3_u7ogas", "created_utc": 1650429653.0}
{"sub": "Python", "title": "Novel and small python projects and code snippets", "selftext": "Share your answers to these tasks in the comments. Feel free to ask for help.\n\n# Task 1 (Beginner) #\nCreate a class \u201cNumberSet\u201d that:\n- Inherits from the built in set class\n- That overrides __setitem__ to only allow numbers\n\nYou will learn about inheritance, \u201csuper\u201d, and sets\n\n# Task 2 (Intermediate) # \nCreate an NxN matrix where N is any odd number, where each cells value is the distance of that cell from the centre. \n- For examples 3x3 matrix, the \u201cmiddle\u201d cells value will be 0, but any outer cells value will be 1\n\nYou will learn about basic algorithmic logic, nested loops (or arrays ;) ) and a bit of basic math\n\n# Task 3 (Intermediate) #\nCreate a 100x100 pixel \u201cblue\u201d colour bitmap image from scratch (with no third party image libraries):\n- You will use this to understand the file format required https://en.wikipedia.org/wiki/BMP_file_format?wprov=sfti1\n- You will be working with binary data only\n\nYou will learn about file headers and formats, binary data, and image creation\n\n# Task 4 (Intermediate) #\nCreate a function decorator that runs the decorated function twice.\n\nYou will learn about nested functions, higher order functions, and decorators", "upvote_ratio": 0.75, "id": "t3_u7ls2t", "created_utc": 1650420783.0}
{"sub": "Python", "title": "How Python Enriched the Use of AI in Several Industries", "selftext": "Artificial intelligence and machine learning can be considered as the new backbones of the IT industry. While discussion over developing newer technologies to provide maximum safety continues, people innovate expanded abilities and capacities of artificial intelligence. [Artificial intelligence](https://en.wikipedia.org/wiki/Artificial_intelligence) went from being a part of science fiction to a daily need of people worldwide. AI's added abilities have successfully reduced manual labor to a great extent and almost perfected the accuracy level.\u00a0\n\nThe amount of data produced increases its volume day by day, and the size has now become impossible to handle manually. AI helps analyze the data of many organizations to predict their future outcome and plan growth strategies accordingly. For example, many online websites have now launched the feature of chatbots that utilizes AI. This helps to enhance the customer experience. This shows how AI and machine learning efficiently and accurately process huge volumes of data for drawing in more customers.\n\nForming Future Technologies\n\nThe rising volume of data is increasing the complexity of data proportionately. As data becomes more complex, analyzing it manually becomes tougher. This machine intelligence is utilized to analyze and process the data for accurate results, and there is no limit to data.\u00a0\n\nUsing artificial intelligence mainly enhances three aspects of any organization. They are:\n\n* Accuracy in predictions and insight production increases business efficiency.\n* It is a very low-cost venture instead of hiring many individuals at high salaries.\n* Finally, it increases the productivity of the organization.\n\nThis gives a precise reason why many companies use AI and machine learning to propel product development and improve the overall performance. Research has also discovered that using AI-laded technologies is the new trend in industrial transformation, particularly for enhancing the company. It has also concluded that within a few years, the companies that utilized AI in producing innovative products and processes are most likely to expand their grounds. To sum up, AI and machine learning produce better results while giving much smaller efforts.\u00a0\n\nThe Use of Python\n\n[Python](https://www.python.org/) checks off many boxes that result in being useful for AI and machine learning. Many features are present in Python, which makes it one of the best languages for these purposes. Thus, the knowledge of Python is important in the data science field as many industries use it to predict and analyze data. Python is open-source in nature, so AI development companies' achievements can be shared with the community.\u00a0\n\nSome industries that extensively use Python in expanding their businesses are:\n\n* Healthcare\n* Travel and transportation\n* Finance technology\n\nHealth Care\n\nHealth centers use data to scrutinize and diagnose their patients properly. The growing popularity of AI involvement in decrypting data leads to the rise in Python programmers' demand as it is the most popularly used program in the data science industry.\n\nTravel and Transportation\n\nPython and machine learning algorithms enable travel giants to predict airplane routes and behaviors easily. Therefore, implementing AI in the travel industry helps with customer efficiency, creating cost-effective budget plans, and setting the price details of new routes.\u00a0\n\nFinance Technology\n\nAI has also used its way in finance sectors to save from risks and frauds. Programming languages can detect any anomaly behavior and also helps in analyzing market behavior effectively.\u00a0\n\nTake\u00a0[**data science course**](https://360digitmg.com/data-science)\u00a0today and get your dream job.\u00a0 \u00a0\u00a0\n\n\ud83d\udcf7", "upvote_ratio": 0.2, "id": "t3_u7l4rv", "created_utc": 1650418780.0}
{"sub": "Python", "title": "Looks like I'm going to prom thanks to Python!", "selftext": "[https://github.com/Jah-On/prom-decider](https://github.com/Jah-On/prom-decider)", "upvote_ratio": 0.5, "id": "t3_u7l0ce", "created_utc": 1650418382.0}
{"sub": "Python", "title": "Learn how to Scraping Google and optimize search using google search operators", "selftext": "nan", "upvote_ratio": 0.33, "id": "t3_u7kzgt", "created_utc": 1650418303.0}
{"sub": "Python", "title": "Wednesday Daily Thread: Beginner questions", "selftext": "New to Python and have questions? Use this thread to ask anything about Python, there are no bad questions!\n\nThis thread may be fairly low volume in replies, if you don't receive a response we recommend looking at r/LearnPython or joining the Python Discord server at [https://discord.gg/python](https://discord.gg/python) where you stand a better chance of receiving a response.", "upvote_ratio": 0.88, "id": "t3_u7j74e", "created_utc": 1650412810.0}
{"sub": "Python", "title": "PyMailer: A small utility I wrote in Python3 to automate sending server notifications via an SMTP service", "selftext": "I do a lot of self-hosting and homelabbing, and I often found myself wishing that my various services and cron jobs had a simple way to send me notifications through a regular Gmail account. I know tools already exist to do this but I just wanted a dirt-simple utility that I could say pipe the output of a script in to and it would just automatically send it to me in an email.\n\nSo over the weekend I wrote [PyMailer](https://github.com/UltraChip/PyMailer) \\- a basic command-line utility written in Python3 that does exactly that. Now getting my server to notify me is as simple as adding\n\n    cat /some/log/file/or/something | pymailer\n\nto the end of a script. Configuration is accomplished by editing a JSON-esque config file - I tried to keep it as simple as possible.\n\nHonestly it's nothing all that special - I banged it out in a couple hours - but it was a fun excuse to learn how to use argparse (especially figuring out how to read in data from stdin) as well as getting to play with Python's MIME and SMTP modules. And even though this is one of the simplest projects I've done the past few years it's likely going to end up being one I actually use a lot, so I'm glad I made it.", "upvote_ratio": 0.5, "id": "t3_u7hk4i", "created_utc": 1650407953.0}
{"sub": "Python", "title": "I wrote a python script that you can copy into your projects for quickly logging information between your Terminal and Files", "selftext": "I\u2019ve been working on a bunch of different python scripts recently and I found myself constantly referring back to Python\u2019s built-in logging library for some basic debugging/logging setup. After doing this a few times I decided to sit down and write this script that you can quickly import into any existing python environment and quickly start logging information to either your terminal or a file. It\u2019s a short 200-ish .py file that you can just copy over into your project and instantly start using.\n\nSource can be found here: [https://github.com/henryriveraCS/logger](https://github.com/henryriveraCS/logger)\n\nLet me know what you think :\\^)", "upvote_ratio": 0.75, "id": "t3_u7gw5z", "created_utc": 1650406071.0}
{"sub": "Python", "title": "Is anyone taking the CS50p (for Python) currently in progress from Harvard?", "selftext": "Just wondering what your thoughts are on it, how it compares to the original CS50, etc?", "upvote_ratio": 0.85, "id": "t3_u7g0ix", "created_utc": 1650403719.0}
{"sub": "Python", "title": "Keylogger In Just 10 Lines Of Python", "selftext": "nan", "upvote_ratio": 0.69, "id": "t3_u7fczz", "created_utc": 1650401947.0}
{"sub": "Python", "title": "Dash is Deeper than Dashboards", "selftext": "nan", "upvote_ratio": 0.89, "id": "t3_u7emja", "created_utc": 1650399953.0}
{"sub": "Python", "title": "Program that counts lines of code", "selftext": "Posted this while back but its been changed quite a bit. At the time many of you had very constructive criticisms of my code that were incredibly helpful. Please do so again! This script runs on the command line and counts lines of code in a specified file or directory for files with a specified extension. Feel free to tear it apart :)\n\n[https://github.com/carterdugan/LineCounter](https://github.com/carterdugan/LineCounter)", "upvote_ratio": 0.5, "id": "t3_u7e96y", "created_utc": 1650398975.0}
{"sub": "Python", "title": "Just A Todo App", "selftext": "* **It's a Todo App that I wrote, and it's my first time developing a full-stack application with Flask. It's possible that it has multiple bugs, since I didn't thoroughly test it. Limiters or rate limitations are built into the routes and endpoints to avoid abuse. You can create or add additional Todos, but I haven't implemented** [**CRUD**](https://developer.mozilla.org/en-US/docs/Glossary/CRUD) **entirely yet, but I may do so in the future since I'm still learning fullstack development.**\n\nhttps://preview.redd.it/u3yovjldbju81.png?width=1366&amp;format=png&amp;auto=webp&amp;s=d39ecebde42df6e3dfa2d30ae47ce2e4690639d8\n\n* **Github :** [SecretsX - JustATodoApp](https://github.com/SecretsX/JustATodoApp)", "upvote_ratio": 0.6, "id": "t3_u7cwkw", "created_utc": 1650395385.0}
{"sub": "Python", "title": "Created Python Jobs (Backend and AI/ML) Website", "selftext": "Hey, Guys!\n\nI'm building a Job board that connects Python Developers (Backend and AI/ML Engineers) with Startups and Companies hiring for Python roles. Launching in 2 Weeks!\n\nHere is the link to keep in touch \u27a1 [pyhunt.com](https://www.pyhunt.com/) \n\nFeedbacks and Questions are welcome,\n\nThanks :)", "upvote_ratio": 0.73, "id": "t3_u7ccs4", "created_utc": 1650393952.0}
{"sub": "Python", "title": "UFC analysis with Python", "selftext": "I made the code in Python to get rankings of UFC fighters in Lightweight for period  2013-2022 from site [mma-stats.com](https://mma-stats.com), process them, and create bar chart race of these rankings. Code for my project is available on [https://github.com/SergeyZago/ufc\\_analysis/blob/main/UFC%20Lightweight.py](https://github.com/SergeyZago/ufc_analysis/blob/main/UFC%20Lightweight.py). Visual representation of result is available on  [https://youtu.be/cenZedT-B8o](https://youtu.be/cenZedT-B8o)", "upvote_ratio": 0.57, "id": "t3_u7c0jv", "created_utc": 1650393068.0}
{"sub": "Python", "title": "How to Build Countable Classes in Python", "selftext": "nan", "upvote_ratio": 0.63, "id": "t3_u7bdds", "created_utc": 1650391390.0}
{"sub": "Python", "title": "Snake Code", "selftext": "Snake Code:\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nimport time  \n\n\nimport pygame  \nfrom enum import Enum  \nimport random  \n\n\nclass Direction(Enum):  \nUP = 1  \n DOWN = 2  \n RIGHT = 3  \n LEFT = 4  \nwindow\\_width = 720  \nwindow\\_height = 720  \npygame.init()  \npygame.display.set\\_caption(\"Snake Game 1\")  \nwindow = pygame.display.set\\_mode((window\\_width, window\\_height))  \n\n\nrefresh\\_controller = pygame.time.Clock()  \n\n\nsnake\\_position = \\[250, 250\\]  \nsnake\\_body = \\[\\[250, 250\\],  \n \\[240, 240\\],  \n \\[230, 250\\]\\]  \n\n\nfood\\_position = \\[250, 250\\]  \n\n\nscale = 20  \nglobal score  \nscore = 0  \nglobal speed  \nspeed = 1  \ndef handle\\_keys(direction):  \nnew\\_direction = direction  \n for event in \\[e for e in pygame.event.get() if e.type == pygame.KEYDOWN\\]:  \n if event.key == pygame.K\\_UP and direction != Direction.DOWN:  \nnew\\_direction = Direction.UP  \n if event.key == pygame.K\\_DOWN and direction != Direction.UP:  \nnew\\_direction = Direction.DOWN  \n if event.key == pygame.K\\_RIGHT and direction != Direction.LEFT:  \nnew\\_direction = Direction.RIGHT  \n if event.key == pygame.K\\_LEFT and direction != Direction.RIGHT:  \nnew\\_direction = Direction.LEFT  \n return new\\_direction  \n\n\ndef move\\_snake(direction):  \n\n\nif direction == Direction.UP:  \nsnake\\_position\\[1\\] -= scale  \n if direction == Direction.DOWN:  \nsnake\\_position\\[1\\] += scale  \n if direction == Direction.LEFT:  \nsnake\\_position\\[0\\] -= scale  \n if direction == Direction.RIGHT:  \nsnake\\_position\\[0\\] += scale  \nsnake\\_body.insert(0, list(snake\\_position))  \n\n\ndef generate\\_new\\_food():  \nfood\\_position\\[0\\] = random.randint(5, ((window\\_height - 2) // scale)) \\* scale  \nfood\\_position\\[1\\] = random.randint(5, ((window\\_height - 2) // scale)) \\* scale  \n\n\ndef get\\_food():  \n global score  \n global speed  \n if abs(snake\\_position\\[0\\] - food\\_position\\[0\\]) &lt; 20 and abs(snake\\_position\\[1\\] - food\\_position\\[1\\]) &lt; 20:  \nscore += 1  \n speed += 2  \n generate\\_new\\_food()  \n else:  \nsnake\\_body.pop()  \n\n\ndef paint\\_hud():  \nfont = pygame.font.SysFont(\"Arial\", scale\\*2)  \nrender = font.render(f\"Score: {score}\", True, pygame.Color(255, 255, 255))  \nrect = render.get\\_rect()  \nwindow.blit(render, rect)  \npygame.display.flip()  \n\n\ndef repaint():  \nwindow.fill(pygame.Color(5, 0, 5))  \n for body in snake\\_body:  \npygame.draw.circle(window, pygame.Color(0, 255, 0), (body\\[0\\], body\\[1\\]), scale/2)  \npygame.draw.rect(window, pygame.Color(255, 0, 0), pygame.Rect(food\\_position\\[0\\]-scale/2, food\\_position\\[1\\]-scale/2, scale, scale/2))  \n\n\ndef game\\_over\\_message():  \nfont = pygame.font.SysFont('Arial', scale\\*3)  \nrender = font.render(f\"Score: {score}\", True, pygame.Color(255,255,255))  \nrect = render.get\\_rect()  \nrect.midtop = (window\\_width / 2, window\\_height / 2)  \nwindow.blit(render, rect)  \npygame.display.flip()  \ntime.sleep(2.5)  \npygame.quit()  \n exit(0)  \n\n\ndef game\\_over():  \n if snake\\_position\\[0\\] &lt; 0 or snake\\_position\\[0\\] &gt; window\\_width - 10:  \ngame\\_over\\_message()  \n if snake\\_position\\[0\\] &lt; 0 or snake\\_position\\[1\\] &gt; window\\_height - 10:  \ngame\\_over\\_message()  \n for blob in snake\\_body\\[1:\\]:  \n if snake\\_position\\[0\\] == blob\\[0\\] and snake\\_position\\[1\\] == blob\\[1\\]:  \ngame\\_over\\_message()  \n\n\ndef game\\_loop():  \ndirection = Direction.RIGHT  \n while True:  \ndirection = handle\\_keys(direction)  \nmove\\_snake(direction)  \nget\\_food()  \nrepaint()  \ngame\\_over()  \npaint\\_hud()  \npygame.display.update()  \nrefresh\\_controller.tick(speed)  \n\n\nif \\_\\_name\\_\\_ == \"\\_\\_main\\_\\_\":  \ngame\\_loop()", "upvote_ratio": 0.27, "id": "t3_u79ohy", "created_utc": 1650387027.0}
{"sub": "Python", "title": "Hello everyone I have been creating a boilerplate/template for FastAPI and PostgreSQL. Please have a look and give me a star if you like. #Python #FastAPI #PostgreSQL #Pytest", "selftext": "nan", "upvote_ratio": 0.5, "id": "t3_u79f99", "created_utc": 1650386342.0}
{"sub": "Python", "title": "Download Outlook Email Attachments Using Microsoft Graph API In Python[", "selftext": "Recently published a video covering how to download emails from your Outlook account with Microsoft Graph API using Python, and thought some of you might find the tutorial useful.  \n\n\nVideo Link: [https://youtu.be/UF-hc2nZV\\_A](https://youtu.be/UF-hc2nZV_A)", "upvote_ratio": 1.0, "id": "t3_u77n4y", "created_utc": 1650381620.0}
{"sub": "Python", "title": "78 Python data science practice problems in a single github repo including numpy, pandas, matplotlib, scipy, regex, pytorch", "selftext": "nan", "upvote_ratio": 0.98, "id": "t3_u77fce", "created_utc": 1650381050.0}
{"sub": "Python", "title": "QualityScaler 1.3.0 - Image/video upscaling &amp; enhancement Windows app", "selftext": "&amp;#x200B;\n\n[GUI](https://preview.redd.it/vtj3a827yhu81.png?width=1357&amp;format=png&amp;auto=webp&amp;s=9321b61c88261595c31d3512cef93af4cc0bcf39)\n\n[EXAMPLE](https://preview.redd.it/lbfejx48yhu81.png?width=2232&amp;format=png&amp;auto=webp&amp;s=13067052d8ed6364eff09d716746e37986cff7d1)\n\nItch -&gt;  [https://jangystudio.itch.io/qualityscaler](https://jangystudio.itch.io/qualityscaler)\n\nGithub -&gt;  [https://github.com/Djdefrag/QualityScaler/releases/tag/1.3.0](https://github.com/Djdefrag/QualityScaler/releases/tag/1.3.0)\n\n&amp;#x200B;\n\n**Update 1.3.0 - Auto tile-merge / speed and UI improv. (1.3.0)**\n\nNew\n\n* Automatic tiles and merge images and video frames to avoid Gpu VRam limitation\n* Gaussian filtering after upscale to avoid tiles and merging defects in images\n* New upscale factor x3\n\nUI\n\n* A new vertical left bar, now is much cleaner and uniform\n* All buttons now have same dimension\n* App title changed color and the background has been removed\n* Upscale/Stop buttons are bigger\n* Other general improvements\n\nBugfix/improvement\n\n* Removed unused functions\n* General code cleaning and improvements", "upvote_ratio": 1.0, "id": "t3_u76rf5", "created_utc": 1650379305.0}
{"sub": "Python", "title": "I made a football simulation/game entirely using Python Turtle Graphics", "selftext": "# Overview:\n\nAbout a year ago i made a project called as \"Python Football Game\" which was just a Pong game on green background and goal posts it felt like cheating so this time i made an entire football game with 11 players and other rules etc in python turtle with about *629 lines of code* you can have a look at it here:\n\n&amp;#x200B;\n\n[Working Of Simulation\\/Game](https://reddit.com/link/u75ydj/video/awyg0ye3thu81/player)\n\n[Python Football Simulation In Turtle](https://www.youtube.com/watch?v=7rRYpX5-9RI) (Alternative Link Youtube)\n\nYou can do all sorts of things from passing ball to other teammates to scoring goals to switching players (inspired by fifa like mechanics) with throwing ball to a teammate if it goes outside etc. The team that scores 3 goals first wins you play against CPU with enemy having its own Logical AI it's a fun experience.\n\n# Source Code:\n\n[Project Source Code Here](https://github.com/JackhammerYT/Football-Simulation)\n\n# Controls:\n\nw(up) ,  a(left) ,  d(right) , s(down) ,  q(north west) ,  e(north east) ,  z(south west) ,  x(south east) Space Bar (Change Player) ,  Mouse Click (Pass the ball onto clicked position)\n\n# Challenges:\n\nEven though with how simple it's design was there were few challenges i learned to overcome, the biggest one being Calling a function after a delay without using threads. This was a rather hard challenge to overcome because turtle is not a thread safe framework that is all the GUI operations need to run on main thread and it so happened that i required to call a GUI operation with delay i solved it by making my own FunctionInvoker class based on time module that invokes a function after a delay you can see it's structure in \"[FunctionInvoker.py](https://FunctionInvoker.py)\" file on github repo", "upvote_ratio": 0.81, "id": "t3_u75ydj", "created_utc": 1650377093.0}
{"sub": "Python", "title": "I was today years old when I found out that Python supports else clauses in try/excepts.", "selftext": "Their functionality is brilliant, too! I always used to use guard values and flags to find out if an exception happened.\n\nTotally surprised (and embarrassed), but IMO this is one of the reasons why writing in Python feels so nice.\n\nContext: I have been using python for 7 years (from 2nd year of college up to mid-PhD), and using it almost exclusively in the last 4. I did know about the `finally` clause, but I hardly ever use it.", "upvote_ratio": 0.94, "id": "t3_u75y3m", "created_utc": 1650377073.0}
{"sub": "Python", "title": "I wrote a Spotify alternative in Python", "selftext": "[Myuzi](https://gitlab.com/zehkira/myuzi) is a Spotify alternative for Linux. It's built entirely using Python. The interface uses GTK, and the streaming is handled by `youtube-dl` and Gstreamer.\n\n\nYou don't need an account to use Myuzi, and there are no payments or ads.\n\n\nThis is a prototype, so things will break a lot.\n\n\n[AUR](https://aur.archlinux.org/packages/myuzi) | [Source](https://gitlab.com/zehkira/myuzi) | [Donations](https://www.patreon.com/bePatron?u=65739770)", "upvote_ratio": 0.87, "id": "t3_u7446h", "created_utc": 1650371709.0}
{"sub": "Python", "title": "What are some of the main reasons I should switch to pytest from unittest?", "selftext": "nan", "upvote_ratio": 0.9, "id": "t3_u73xv2", "created_utc": 1650371161.0}
{"sub": "Python", "title": "Build a Web App with Pandas", "selftext": "nan", "upvote_ratio": 0.71, "id": "t3_u725g8", "created_utc": 1650364948.0}
{"sub": "Python", "title": "How To Install Kivy On macOS Monterey", "selftext": "nan", "upvote_ratio": 0.25, "id": "t3_u71x6r", "created_utc": 1650364046.0}
{"sub": "Python", "title": "Auto ML in Python \u2014 An Overview of the MLBox Package", "selftext": "nan", "upvote_ratio": 0.5, "id": "t3_u70gk9", "created_utc": 1650358000.0}
{"sub": "Python", "title": "Dynamic Logging for logging.Logger", "selftext": "I have created an extension of `logging.Logger` class to be able to easily &amp; dynamically log \"extra\" values. This module provides 2 main APIs - `log_extras()` decorator to log values from the decorated function arguments and `set_extras()` method to log static values.\n\n    ```\n    import dynamic_logger\n    import logging\n    logging.setLoggerClass(dynamic_logger.Logger)\n    \n    # Set-up log formatter with user-definied attributes\n    fmt = '[%(asctime)s] &lt;%(app)s&gt; [%(levelname)s] &lt;%(id)s&gt; &lt;%(customer_id)s&gt; --- %(message)s' # Note: format attributes in &lt;&gt; are user-definited\n    \n    # Load config\n    logging.basicConfig(format=fmt, datefmt='%d-%b-%y %H:%M:%S', level='INFO')\n    \n    applogger = logging.getLogger(__name__)\n    \n    @applogger.log_extras('id',int=0,customer_id='obj.customer_id') # Log value of 'id' and 'obj.customer_id'\n    def example_1(id=0,id2=0,obj=None):\n        applogger.info('This example shows how to log values from function arguments')\n    \n    if __name__ = \"__main__\":\n        example_1(id=123456,obj={\"customer_id\":777})\n    ```\n\nWill give below log entry\n\n    [2022-04-19 12:53:51,658] [INFO] &lt;123456&gt; &lt;customer_id:777&gt; --- This example shows how to log values from function arguments\n\nMore examples and code can be found [here](https://github.com/ajatkj/dynamic_logger).\n\nI hope this is useful for someone.", "upvote_ratio": 0.75, "id": "t3_u70a80", "created_utc": 1650357206.0}
{"sub": "Python", "title": "I developed a template for starting new Python projects! Features: Poetry, GitHub CI/CD, MkDocs, publishing to PyPi/Artifactory, Pytest, Tox, black and isort.", "selftext": "nan", "upvote_ratio": 0.94, "id": "t3_u7081n", "created_utc": 1650356962.0}
{"sub": "Python", "title": "Python Lambda Function in Simple Words", "selftext": "How to Use Lambda Function?  \nDifference Between Lambda and Def Function?  \nWhat are the benefits of the Lambda Function?  \n\n\nThe most important things you should know about the Lambda/Anonymous Function in Python by many examples.  \n\n\nTake a look at [my story](https://medium.com/p/e05171925c98) and leave comments!", "upvote_ratio": 0.5, "id": "t3_u6zh3e", "created_utc": 1650353719.0}
{"sub": "Python", "title": "Beginning a brand new set of ML foundations series starting with Pandas", "selftext": "Beginning a Hey all, \n\nI am starting a new set of ML video series on Youtube. I will be covering the topics very deep, the way I wish someone taught me. \n\nPandas is the first topic, promise to post one video every weekday. Please view and let me know your suggestions.\n\nLink :  [Pandas for Data Science](https://www.youtube.com/watch?v=t4yBm9agYWo&amp;list=PLFAYD0dt5xCxdKfIR3ZX3k07qrqEjUZSO&amp;index=2&amp;t=1s)", "upvote_ratio": 0.88, "id": "t3_u6yugv", "created_utc": 1650351107.0}
{"sub": "Python", "title": "Python Selenium Tutorial #9 - How to bypass/solve hCaptcha using 2captcha API", "selftext": "nan", "upvote_ratio": 0.69, "id": "t3_u6yo7z", "created_utc": 1650350404.0}
{"sub": "Python", "title": "Are you a person who loves reinventing a wheel ?", "selftext": "Just wondering, are you a person who loves remaking the most popular python library ?\n\nWhen people are learning how to create a Python web app using Flask, you are thinking \"I am going to make the library by myself.\" . When people are learning how to use Django + Gunicorn, you are thinking \"I am going to make all of the components by myself.\" .", "upvote_ratio": 0.75, "id": "t3_u6u7i6", "created_utc": 1650334907.0}
{"sub": "Python", "title": "From 30 to 11 Lines of Code: Revisiting Rock Paper Scissors in Python", "selftext": "nan", "upvote_ratio": 0.5, "id": "t3_u6t9yb", "created_utc": 1650332091.0}
{"sub": "Python", "title": "Is your company still letting you use Pycharm?", "selftext": "I really hate programming without Pycharm. Actually, I hate programming without Jetbrains as I use several of their applications for personal use and pay for the All-Products Pack. I noticed today that CORP does not have it in their approved software list any more. I still have it installed on my work machine and still use their stuff everyday at home. Is all okay with Jetbrains given the current thing?", "upvote_ratio": 0.76, "id": "t3_u6t4hn", "created_utc": 1650331639.0}
{"sub": "Python", "title": "Tuesday Daily Thread: Advanced questions", "selftext": "Have some burning questions on advanced Python topics? Use this thread to ask more advanced questions related to Python.\n\n**If your question is a beginner question we hold a beginner Daily Thread tomorrow (Wednesday) where you can ask any question! We may remove questions here and ask you to resubmit tomorrow.**\n\nThis thread may be fairly low volume in replies, if you don't receive a response we recommend looking at r/LearnPython or joining the Python Discord server at [https://discord.gg/python](https://discord.gg/python) where you stand a better chance of receiving a response.", "upvote_ratio": 0.88, "id": "t3_u6rcpp", "created_utc": 1650326409.0}
{"sub": "Python", "title": "Indeed Job Scraper", "selftext": "What's happening in the job market?\n\nI published a new python script that pulls new jobs from an indeed advanced query and stores to a datasource. \n\nThe attributes for each job include the posting title, description, pay, and url.\n\nI have another project I am working on where I am training an NLP model on a large dataset with Prodigy. If there is interest I can share that as well. Tune in!\n\nhttps://github.com/hazondata/webscraping", "upvote_ratio": 0.7, "id": "t3_u6r6xy", "created_utc": 1650325935.0}
{"sub": "Python", "title": "DRF application for Authentication Using metamask", "selftext": "Hey Guys,  \nSo recently I wrote a drf application for authentication using metamask extension for a one click login. This app uses simple-JWT for managing and creating tokens and so is compatible with djoser as well.\n\nI am planning to create a more detailed documentation, add more testing and create a sample-app for  demonstrations. I would really appreciate it if you test it out and tear it apart if you want.  Feel free to contribute in any way possible.  \nAlso followed this tutorial for the basics [https://www.toptal.com/ethereum/one-click-login-flows-a-metamask-tutorial](https://www.toptal.com/ethereum/one-click-login-flows-a-metamask-tutorial)", "upvote_ratio": 0.86, "id": "t3_u6nv18", "created_utc": 1650316814.0}
{"sub": "Python", "title": "Build a Twitter Bot with Python, Tweepy and the Twitter API", "selftext": "nan", "upvote_ratio": 0.55, "id": "t3_u6mmt6", "created_utc": 1650313555.0}
{"sub": "Python", "title": "Program Raspberry Pi Pico with Python https://medium.com/@needablackcoffee/program-raspberry-pi-pico-with-python-6a410192dd07", "selftext": "nan", "upvote_ratio": 0.36, "id": "t3_u6lo7q", "created_utc": 1650311084.0}
{"sub": "Python", "title": "Masonite Permission - Masonite Framework.", "selftext": "To all the Masonite Framework developers/engineers, the ACL package that I created is now production ready. Give it a try, and feedback.\n\n[https://github.com/yubarajshrestha/masonite-permission](https://github.com/yubarajshrestha/masonite-permission)", "upvote_ratio": 0.5, "id": "t3_u6kxwt", "created_utc": 1650309124.0}
{"sub": "Python", "title": "Python implementation of recursive insertion sort.", "selftext": "nan", "upvote_ratio": 0.76, "id": "t3_u6khdk", "created_utc": 1650307932.0}
{"sub": "Python", "title": "Top down Minecraft.. clone? Written in python and pygame.", "selftext": "[https://youtu.be/-riH-R-1EfY](https://youtu.be/-riH-R-1EfY)\n\nIgnore my terrible voice.\n\nGithub: [https://github.com/Electro-Corp/top-down-minecraft-ripoff](https://github.com/Electro-Corp/top-down-minecraft-ripoff)", "upvote_ratio": 0.33, "id": "t3_u6ib28", "created_utc": 1650302268.0}
{"sub": "Python", "title": "User login system with JWT and FastAPI", "selftext": "In this series so far, we have set up our environment and created a user. \n\nNow we are going to let them login, we do so by sending them a JWT token which is then required in subsequent requests to access protected resources. \n\nRead this 4th post in this series to know how: [https://santoshk.dev/posts/2022/tdd-approach-to-create-an-authentication-system-with-fastapi-part-4/](https://santoshk.dev/posts/2022/tdd-approach-to-create-an-authentication-system-with-fastapi-part-4/)", "upvote_ratio": 0.78, "id": "t3_u6gvm7", "created_utc": 1650298419.0}
{"sub": "Python", "title": "Are there any task queue libraries with the ergonomics of FastAPI", "selftext": "## django is to FastAPI as celery is to `???`\n\nWe currently use FastAPI as our backend and Celery as our task/job queue. But I find it really clunky. Not that there's anything *wrong* with the Django/Celery way of doing things, but it doesn't jive with me. Everything is that old school loosey-goosey python style with kwargs and dynamic magic everywhere. It's sync so I need separate helper functions for all my IO work in the API, then another set for those in tasks. And I'm stuck with pass-by-import of all my IO, rather than DI, so I have to monkeypatch everything to unit test.\n\nOh and I know about background tasks, this needs to be a distributed task queue with separate agents.\n\nIs there a task queue library out there with:\n\n- static types, full coverage ideally\n- Depends() style dependency injection, or similar\n- async native\n\nI've looked at RQ/ARQ and they are close to what I want but not quite.\n\nAlternatively, are there any tips to get a better experience with Celery in the way of async, typing, and DI? I think it's getting better with the typing, but the lack of DI frustrates my unit test writing and makes it more convoluted.\n\nThanks!", "upvote_ratio": 0.72, "id": "t3_u6g9bf", "created_utc": 1650296839.0}
{"sub": "Python", "title": "Which IDE do you use?", "selftext": "I'm new to CS in general, and our intro class used Wing Personal. Is there something about a basic IDE that holds me back? I'm not very educated on IDE's and have no idea if they're all the same. I've had no problems with Wing so far, I really like the tab structure.", "upvote_ratio": 0.69, "id": "t3_u6g4jk", "created_utc": 1650296486.0}
{"sub": "Python", "title": "Python Tips and Tricks \u2014 Write Better Python Code", "selftext": "nan", "upvote_ratio": 0.5, "id": "t3_u6fzb2", "created_utc": 1650296100.0}
{"sub": "Python", "title": "PyQt hello world", "selftext": "nan", "upvote_ratio": 0.43, "id": "t3_u6fiqo", "created_utc": 1650294880.0}
{"sub": "Python", "title": "What is the usual tech stacks paths for Python developers going into the world?", "selftext": "I have a year left in Uni and I really like Python so I'd like to be prepared properly.", "upvote_ratio": 0.6, "id": "t3_u6egcg", "created_utc": 1650292090.0}
{"sub": "Python", "title": "If you could start your Python journey over with what you know know what practices would you force your past self to implement?", "selftext": "This could also apply to programming in general or even best practices for communicating code to non technical team members/management.", "upvote_ratio": 0.86, "id": "t3_u6c6p8", "created_utc": 1650285569.0}
{"sub": "Python", "title": "Why do people still pay and use matlab having python numpy and matplotlib?", "selftext": "nan", "upvote_ratio": 0.94, "id": "t3_u6bcgc", "created_utc": 1650282874.0}
{"sub": "Python", "title": "I have written a blog to create a Perceptron(Single Neuron) and then trained it on Cat and Dog image data. All from scratch in python.", "selftext": "nan", "upvote_ratio": 0.56, "id": "t3_u6adlb", "created_utc": 1650279439.0}
{"sub": "Python", "title": "Ping With Python", "selftext": "nan", "upvote_ratio": 0.52, "id": "t3_u68m93", "created_utc": 1650272377.0}
{"sub": "Python", "title": "3d Game Engine in Python", "selftext": "Have any of you guys experimented with 3d game engines in python like ursina, Panda3d  etc... I want to prototype some ideas but I'm worried about running into platform specific issues after completing most of the development. I can obviously learn Unity or Unreal but I would like to know if viable options are there in python", "upvote_ratio": 0.92, "id": "t3_u6695b", "created_utc": 1650262365.0}
{"sub": "Python", "title": "Why are some static type-checking functionalities implemented as identity functions?", "selftext": "For example, `typing.cast` returns the argument unchanged. Obviously, this has no effect on the program at runtime. If this is the case, why can we not just tell the static type checker (mypy) that we want the argument to be treated as some type *without* having to add a function call? Why can\u2019t this override/cast be in a comment of some sort?", "upvote_ratio": 0.82, "id": "t3_u64dfw", "created_utc": 1650255058.0}
{"sub": "Python", "title": "Monday Daily Thread: Project ideas!", "selftext": "Comment any project ideas beginner or advanced in this thread for others to give a try! If you complete one make sure to reply to the comment with how you found it and attach some source code! If you're looking for project ideas, you might be interested in checking out Al Sweigart's, [\"The Big Book of Small Python Projects\"](https://inventwithpython.com/bigbookpython/) which provides a list of projects and the code to make them work.", "upvote_ratio": 0.81, "id": "t3_u5zpzh", "created_utc": 1650240011.0}
{"sub": "Python", "title": "I wrote a program to play Minecraft in your Windows command line / Linux console", "selftext": "Video demonstration: https://youtu.be/P1d04is-wQQ\nSource code: https://github.com/louis-e/cli-screenview", "upvote_ratio": 0.93, "id": "t3_u5ykrh", "created_utc": 1650236464.0}
{"sub": "Python", "title": "Type safe Django app, Part 3", "selftext": "nan", "upvote_ratio": 0.7, "id": "t3_u5xql9", "created_utc": 1650233988.0}
{"sub": "Python", "title": "r/Place Data Visualization w/ Python + Blender (open source project)", "selftext": "I've created a [GitHub repository](https://github.com/ChrisCrossCrash/r-place-blender) containing everything you need to create beautiful 3D renders of the [r/Place](https://www.reddit.com/r/Place/) 2022 canvas (scroll down for more info).\n\n[Python Logo \\(r\\/Place 2022\\)](https://preview.redd.it/bm0m1c1vv5u81.jpg?width=1920&amp;format=pjpg&amp;auto=webp&amp;s=c813de013e50fe3fa91cd954478cc3fdff25cb66)\n\n[Blender Logo \\(r\\/Place 2022\\)](https://preview.redd.it/l7w8d9wet5u81.jpg?width=800&amp;format=pjpg&amp;auto=webp&amp;s=45fc8f698e489d0c2b2b8bd8f9c1cf3d8d1420a9)\n\n[Whatever this is \\(r\\/Place 2022\\)...](https://preview.redd.it/fmbxz1xms5u81.png?width=3840&amp;format=png&amp;auto=webp&amp;s=f721632bb9e10af682fdbb01b8ca2f64403afa30)\n\n[Start of r\\/Place 2022](https://preview.redd.it/f7ckdpshs5u81.png?width=3840&amp;format=png&amp;auto=webp&amp;s=9654241197449ccb064a01d32a4d69d93aba53bb)\n\n[\\\\\"Darth Plagueis the Wise \\(r\\/Place 2017\\)\\\\\"](https://preview.redd.it/ete7b05vr5u81.png?width=1920&amp;format=png&amp;auto=webp&amp;s=6fc618bf43ce3cb1476a58620c7bcb92063a38c4)\n\n[Rainbows \\(r\\/Place 2017\\)](https://preview.redd.it/tiz5v32ts5u81.png?width=1920&amp;format=png&amp;auto=webp&amp;s=b07375a5843038d3bed97040688aaa9e52003599)\n\n[Green Lattice \\(r\\/Place 2017\\)](https://preview.redd.it/9jrmj6c6t5u81.png?width=1920&amp;format=png&amp;auto=webp&amp;s=282051e36bd9bbddc2d83dc97656100cdc4c2743)\n\n[r-place.blend open in Blender](https://preview.redd.it/101fu8vzv5u81.jpg?width=1921&amp;format=pjpg&amp;auto=webp&amp;s=c3bfd90fc93d3a69ca88de3ffbf5d2080a9ed607)\n\nI've prepared a [GitHub repository](https://github.com/ChrisCrossCrash/r-place-blender) with everything you need to create beautiful 3D renders of the r/Place 2022 canvas. It's an open source project with an MIT license.\n\nI recently posted a [Star Wars timelapse I created earlier](https://www.reddit.com/r/blender/comments/u0gtvs/a_long_time_ago_in_a_subreddit_far_far_away/). I also created one for the [Michigun memorial](https://www.reddit.com/r/blender/comments/u178pc/michigun_memorial_request/) artwork, which somebody requested. The linked GitHub repository contains an improved version of the code and `.blend` file used to generate those renders.\n\nLet me know if you need help or notice any mistakes. I'm super excited to see what other people can do with this!", "upvote_ratio": 0.8, "id": "t3_u5xfgt", "created_utc": 1650233080.0}
{"sub": "Python", "title": "Quickly create online forms for your python scripts", "selftext": "nan", "upvote_ratio": 0.65, "id": "t3_u5x46b", "created_utc": 1650232184.0}
{"sub": "Python", "title": "Relational Algebra interpreter powered by Python", "selftext": "I made a RA interpreter in Python, [Pireal](https://github.com/centaurialpha/pireal).\n\nSmall story: In my database class (back in 2015), I could not use a software made for Windows (WinRDBI), so I set out to make a free and cross-platform alternative. Taking advantage of what I wanted to understand how the compilers and interpreters work, I wrote an interpreter for Pireal.\n\nToday the project is used at my university, but I had never been promoted. I hope it serves more people. And of course, any can collaborate to improve it.", "upvote_ratio": 0.7, "id": "t3_u5w63g", "created_utc": 1650229365.0}
{"sub": "Python", "title": "Mean population of US States with graph", "selftext": "    import random\n    import numpy\n    from statistics import mode\n    import matplotlib.pyplot as plt\n    import csv\n    import collections\n    \n    ##dictionary of States and populations\n    from numpy import ndarray\n    \n    #states_info_dic = {\n    #    \"California\": 39538223,\n    #    \"Texas\": 29145505,\n    #    \"Florida\": 21538187,\n    #    \"New York\": 20201249,\n    #    \"Pennsylvania\": 13002700,\n     #   \"Illinois\": 12812508,\n     #   \"Ohio\":\t11799448,\n     #   \"Georgia\": 10711908,\n     #   \"North Carolina\": 10439388,\n    ##    \"Michigan\": 10077331,\n    #    \"New Jersey\":8882190,\n    #    \"Virginia\":\t8535519,\n     #   \"Washington\":7614893,\n     #   \"Arizona\":\t7278717,\n    #    \"Massachusetts\":6949503,\n    #    \"Tennessee\":6833174,\n     #   \"Indiana\":\t6732219,\n     #   \"Missouri\":\t6137428,\n     #   \"Maryland\":\t6045680,\n     #   \"Wisconsin\":5822434\n    #}\n    \n    states_info_dic = {\n        \"Cal\": 39538223,\n        \"Tex\": 29145505,\n        \"Flo\": 21538187,\n        \"NY\": 20201249,\n        \"Pen\": 13002700,\n        \"Ill\": 12812508,\n        \"Ohio\":\t11799448,\n        \"Geo\": 10711908,\n        \"NC\": 10439388,\n        \"Mich\": 10077331,\n        \"NJ\":8882190,\n        \"Vir\":\t8535519,\n        \"Was\":7614893,\n        \"Ari\":\t7278717,\n        \"Mass\":6949503,\n        \"Ten\":6833174,\n        \"Ind\":\t6732219,\n        \"Miss\":\t6137428,\n        \"Mary\":\t6045680,\n        \"Wis\":5822434\n    }\n    \n    \n    ##print values of states\n    population = list(states_info_dic.values())\n    \n    print(population)\n    \n    ##round population\n    round_up_list =[]\n    for num in population:\n        round_up_int=num/1000000\n        round_up_list.append(round(round_up_int))\n    \n    #Mean, median and mode of list\n    mean1=numpy.mean(population)\n    median1=numpy.median(population)\n    mode1=mode(round_up_list)\n    \n    print(f\"The mean of the population is {mean1}\")\n    print(f\"The median of the population is {median1}\")\n    print(f\"The mode of the population is {mode1}\")\n    \n    \n    \n    \n    \n    \n    print(round_up_list)\n    \n    ##Grpah population information\n    # x-coordinates of left sides of bars\n    left = states_info_dic.keys()\n    \n    # heights of bars\n    height = states_info_dic.values()\n    \n    \n    # plotting a bar chart\n    plt.bar(left, height, width=0.8, color=['green'])\n    \n    # naming the x-axis\n    plt.xlabel('States')\n    # naming the y-axis\n    plt.ylabel('Populations')\n    # plot title\n    plt.title('U.S States populations')\n    \n    # function to show the plot\n    plt.show()", "upvote_ratio": 0.33, "id": "t3_u5v91y", "created_utc": 1650226634.0}
{"sub": "Python", "title": "Weekly Algorithm Project: Middle Square Method", "selftext": "Recently I started  a weekly algorithm project just to help me study new algorithms with a flair of good practice. This week I decided to focus on Python and the \"Middle Square Method\" a PRNG produced by John von Neuman in 1949. I further reiterated two attempted improvements to the algorithm that come from a couple of papers just last month: one using a Weyl Sequence and another using the Weyl Sequence with a Counter - both were by Bernard Widynski.   \n\n\nTesting them was interesting as there appeared to be barely a difference between the two new iterations, however, both were much more stable than the original work by von Neuman. Typically with the newer algorithms I'd gather a standard deviation roaming around 4-8 x 10\\^15-18. Obviously that's fairly in line with modern standards. The original method had a lower deviation, however, I found that one harder to test perhaps because I should have a used a different algorithm.  \n\n\nFull documentation can be found here if anyone is interested. For GitHub, it is under KNOWNALGO/W03.  \n\n\n[https://youtu.be/O53ihWtoEGk](https://youtu.be/O53ihWtoEGk)  \n\n\n[https://github.com/F35H/WeeklyCode](https://github.com/F35H/WeeklyCode)  \n\n\nAny word on improvements \\[don't get me with \"four space only\"\\] would be appreciated. Although, I'm fairly certain I could have done much better with the original method probably implementing the \"bit-shift\" way of doing it Bernard used at the very least.  \n\n\nMight as well link it here, here are the two papers:  \n\n\n[https://arxiv.org/pdf/1704.00358.pdf](https://arxiv.org/pdf/1704.00358.pdf)  \n\n\n[https://arxiv.org/pdf/2004.06278.pdf](https://arxiv.org/pdf/2004.06278.pdf)", "upvote_ratio": 0.63, "id": "t3_u5twnz", "created_utc": 1650222631.0}
{"sub": "Python", "title": "Simple android game with online functionality", "selftext": "Hi everyone!\n\nTopics covered in this project are:\n\n* game made using the Kivy Python module\n* compiling with buildozer (and spec file) to get an .aab file\n* using the 'kv' language\n* using Kivy to switch between screens, display graphs, etc.\n* online functionality (e.g. leaderboard) using firebase\n* firebase rules\n* app splash screen\n\n&amp;#x200B;\n\nshowcase video: [https://youtu.be/PFNS4bGSocQ](https://youtu.be/PFNS4bGSocQ)\n\ngithub:  [https://github.com/Contraposite/PrimeFactorization](https://github.com/Contraposite/PrimeFactorization)\n\nbuildozer method I followed:  [Creating an AAB for python apps using Buildozer (github.com)](https://gist.github.com/Guhan-SenSam/35c5ed7da254a7c0141e6a8b6101eb33)\n\nplay store listing:  [Prime Factorizer Game - Apps on Google Play](https://play.google.com/store/apps/details?id=com.jf.primefactorizer)\n\nThis is my first app, which I wrote in Python and the 'kv' language, using Kivy, compiled to create an android app bundle using buildozer, and successfully uploaded to the Google Play Store.\n\nThere is a reasonable amount of support for Kivy but not too much for using it with firebase, so I hope this helps some people trying to make something similar. My github link includes my buildozer spec file and a text file of my firebase rules.\n\nThe premise of my game is simple: you're shown a target number, and you need to use the on-screen buttons to input the prime factors of that number, in order of smallest to largest (e.g. for target number 66, the prime factors are 2, 3, and 11, so you would press '2 3 1 1'. You are timed, and each time you complete a target, you get a new one. Target numbers get larger with time so that there is some difficulty progression.\n\nMost of the functionality is shown in the linked video above, but some notable things are that you can create an online profile or play fully offline, the game shows you a variety of stats and graphs of your past play data, there are search functions to find points shown on the graphs, there is an online scoreboard, with a graph showing the leaderboard entries overlapped. You can also view the stats and graphs of other online players.\n\nThere are two glitches that I ran into with Kivy:\n\n* kivy-garden scatterplot graphs sometimes have a graphical issue where the data points disappear and are not redrawn. The workaround was to redraw the graph with a slightly different size (I added a numpy random number times a negligibly small number to the size I actually needed).\n* in the kivy textboxes, if the user types an '@' sign while the font size is large, it can crash the app. Definitely one of the most bizarre glitches I've ever encountered, but I just kept the font size small to avoid it.\n\nI tried to add sounds for button feedback, but there was too much of a delay and it wasn't very good, so I've taken it out and plan to add sounds sometime in the future with another method to avoid the delay.", "upvote_ratio": 0.75, "id": "t3_u5tqje", "created_utc": 1650222127.0}
{"sub": "Python", "title": "Use python to optimize the rebar lost in construction.", "selftext": "Hello everyone, I'm a python beginner. I want to improve my coding skill so I try to use python to help my work.The following problem is bar cut list. It use in construction business. The rebar length is 10 meter each and We have to cut to length follow the order.We have to find the sequence of each Rebar Mark.I.E. We should cut bar No. 439(7350mm) then use it's scrap as rebar No.410 (2650mm) or We might cut bar no.443(6950mm) then no.410(2650mm) this would left the scrap 400mm.The less scrap on rebar the less rebar We have to order.The goal is to minimize rebar scrap on bar cut list.How should I create a logic follow this Idea?I have attached the link to csv file in case you want to try it.Thanks in advance.  \n[https://drive.google.com/file/d/1vxJXX8jmEoumwA08x8UtVyBCjSDpyldu/view?usp=sharing](https://drive.google.com/file/d/1vxJXX8jmEoumwA08x8UtVyBCjSDpyldu/view?usp=sharing)\n\nhttps://preview.redd.it/tm23n32zp4u81.png?width=450&amp;format=png&amp;auto=webp&amp;s=252b11688408922e06d0192bc52f2a415319124f", "upvote_ratio": 0.93, "id": "t3_u5sisg", "created_utc": 1650218540.0}
{"sub": "Python", "title": "Gupshup - Chat in the terminal", "selftext": "Gupshup is a terminal-based application inspired by discord!\n\ngithub: [https://github.com/kraanzu/gupshup/](https://github.com/kraanzu/gupshup/)", "upvote_ratio": 0.79, "id": "t3_u5q6jl", "created_utc": 1650211690.0}
{"sub": "Python", "title": "Funny bot I made", "selftext": "[https://youtu.be/XrYadm3y\\_e4](https://youtu.be/XrYadm3y_e4)", "upvote_ratio": 0.36, "id": "t3_u5pjy6", "created_utc": 1650209914.0}
{"sub": "Python", "title": "Writing Better Django Queries", "selftext": "A blog that explains how to write better Django Queries by both performance and Memory wise.\n\n[https://delliganesh.dev/tech/the-one-with-better-django-queries/](https://delliganesh.dev/tech/the-one-with-better-django-queries/)", "upvote_ratio": 0.45, "id": "t3_u5o1aq", "created_utc": 1650205312.0}
{"sub": "Python", "title": "PySGI, the library for creating web servers", "selftext": "Still under development, the PySGI library can manage HTTP routes, requests and responses simply and quickly. Help me by giving this repository a star!\n\n[PySGI GitHub ](https://github.com/jaedsonpys/pysgi)", "upvote_ratio": 0.72, "id": "t3_u5npcy", "created_utc": 1650204250.0}
{"sub": "Python", "title": "How to create an image out of text using Python and Glide OpenAI library ? [tutorial]", "selftext": "  \n\nhttps://preview.redd.it/2q5yox16e3u81.png?width=1280&amp;format=png&amp;auto=webp&amp;s=0e06a88407034c5e567443a881a43e59684fb1ef\n\nHi,\n\nThis is a nice and fun Python tutorial that enables to produce an image out of a simple text \n\nThis is effect is based on Python and the amazing Glide library (based on OpenAI's )\n\nThe outcome is impressive. \n\nYou can find the link for the video tutorial here: https://youtu.be/DKEOmRBgPe8\n\nYou can find Python instructions file here : [https://github.com/feitgemel/Python-Code-Cool-Stuff/blob/master/Glide/glide-Install-instructions.txt](https://github.com/feitgemel/Python-Code-Cool-Stuff/blob/master/Glide/glide-Install-instructions.txt)  \n\n\nEnjoy\n\nEran\n\n&amp;#x200B;\n\n\\#python #opencv #Glide  #OpenAI", "upvote_ratio": 0.77, "id": "t3_u5n88a", "created_utc": 1650202661.0}
{"sub": "Python", "title": "Python Script EXE detected as virus in VT", "selftext": "Hey Everybody,   \n\n\nI'm creating from a python script, with Pyinstaller ,an exe file, but when I insert it to Virus total for testing it detects it as a virus in many viruse detections softwares.   \n\n\nI have tried to solve it with nukita, several guides I found but nothing worked.   \n\n\nAnyone has any advice?", "upvote_ratio": 0.75, "id": "t3_u5m3x5", "created_utc": 1650198722.0}
{"sub": "Python", "title": "Toolchain recommendations for a production-quality monorepo?", "selftext": "Hi,\n\nI\u2019m not exactly a newcomer to Python, but I\u2019ve been using C++ pretty much exclusively for all of my professional projects. I am starting a new project in Python where we are going to be using a monorepo and I am looking for some recommendations on what people use for tooling.\n\nBuild system: I would like something to help orchestrate tests, generate coverage reports, handle running a typechecker in CI, do dependency analysis to figure out what changed in a commit, build Docker images and package cloud functions. It looks like the leading candidates are Bazel and Pants? Which does the community prefer?\n\nTypechecker: Which do people use? A balance of speed, thoroughness of warnings and integration into editors/build systems seems like the driving factors here.\n\nDependency management: I am familiar with virtual environments, but to me this seems at odds with a monorepo where the virtual environment will grow boundlessly. Is there any tooling to help here?\n\nDocumentation: one of the main points of a monorepo is to ease the reuse of code, so docs are important. I am familiar enough with sphinx to know it is a pain in the butt. Are there any easy-to-use alternatives that can produce comparable quality?\n\nAny other tools that make a big difference in refactoring and shifting errors to the left?\n\nThanks!", "upvote_ratio": 0.86, "id": "t3_u5kkxh", "created_utc": 1650192403.0}
{"sub": "Python", "title": "Stack Game made in pygame", "selftext": "GitHub - [https://github.com/Vaibhav521/Pygame/tree/main/stack](https://github.com/Vaibhav521/Pygame/tree/main/stack)\n\nin-game footage \n\n&amp;#x200B;\n\n[stack game](https://reddit.com/link/u5ilqk/video/fzhstiy6t1u81/player)", "upvote_ratio": 0.57, "id": "t3_u5ilqk", "created_utc": 1650183508.0}
{"sub": "Python", "title": "Python Cybersecurity\u2014 Network Tracking using Wireshark and Google Maps", "selftext": "nan", "upvote_ratio": 0.72, "id": "t3_u5ibzu", "created_utc": 1650182335.0}
{"sub": "Python", "title": "Windows 11 design on Python", "selftext": "Hello. This is my project to styling Win32 applications on Python. I try to make the application look in Fluent design with Mica.\n\nIf someone is interested, you can join or just look. \ud83d\ude0a\n\n[witalihirsch/QTWin11: QT Theme for Win32 apps on Windows 11 (github.com)](https://github.com/witalihirsch/QTWin11)\n\n[Example](https://preview.redd.it/q9tcxjjgz0u81.jpg?width=1078&amp;format=pjpg&amp;auto=webp&amp;s=060b7e0390e85cf641b6b4c8141eb61dbf4f3d15)\n\nhttps://preview.redd.it/qgd7ykywc1u81.jpg?width=346&amp;format=pjpg&amp;auto=webp&amp;s=0057bb4e4d4da836fdfe1384894b7f039bef5e89\n\nhttps://preview.redd.it/cw3qmuywc1u81.jpg?width=344&amp;format=pjpg&amp;auto=webp&amp;s=1e324ad9ac28484d38be37a154e8ae997a685f69\n\nhttps://preview.redd.it/z8rkcwywc1u81.jpg?width=345&amp;format=pjpg&amp;auto=webp&amp;s=84f73edc619d65c26998b44051bcfd0e0b630889\n\nhttps://preview.redd.it/1hwt01zwc1u81.jpg?width=345&amp;format=pjpg&amp;auto=webp&amp;s=2a0e0dcbf7ece6a400eb6d784fc5e11cba0b98b5\n\nhttps://preview.redd.it/fldjh6zwc1u81.jpg?width=186&amp;format=pjpg&amp;auto=webp&amp;s=35ffbee447fb622c0d3a75e22872bdf6a34c6232", "upvote_ratio": 0.97, "id": "t3_u5gcj7", "created_utc": 1650173660.0}
{"sub": "Python", "title": "I watched programmer who said for every beginners don't learn python because only jobs you can use python with are data science and machine learning etc.. and these jobs are not for beginners, so you will not work and get money with python as beginner. is it true ?", "selftext": "nan", "upvote_ratio": 0.35, "id": "t3_u5fqtp", "created_utc": 1650171251.0}
{"sub": "Python", "title": "Edge detection project", "selftext": "I'm in grade 12 and planning on doing a project on something edge detection related. What can I do somewhat unique?", "upvote_ratio": 0.75, "id": "t3_u5dr4w", "created_utc": 1650164059.0}
{"sub": "Python", "title": "They say Python is the easiest language to learn, that being said, how much did it help you learn other languages? Did any of you for instance try C++ but quit, learn Python, and then back to C++?", "selftext": "nan", "upvote_ratio": 0.94, "id": "t3_u5c9et", "created_utc": 1650158940.0}
{"sub": "Python", "title": "Sunday Daily Thread: What's everyone working on this week?", "selftext": "Tell /r/python what you're working on this week! You can be bragging, grousing, sharing your passion, or explaining your pain. Talk about your current project or your pet project; whatever you want to share.", "upvote_ratio": 0.92, "id": "t3_u5amsg", "created_utc": 1650153610.0}
{"sub": "Python", "title": "Arrowdantic 0.1.0 released", "selftext": "Hi,\n\nI am announcing the release of [arrowdantic](https://pypi.org/project/arrowdantic/):\n\nArrowdantic is a small Python library backed by a\n[mature Rust implementation](https://github.com/jorgecarleitao/arrow2) of Apache Arrow that can interoperate with\n* [Parquet](https://parquet.apache.org/)\n* [Apache Arrow](https://arrow.apache.org/) and \n* [ODBC](https://en.wikipedia.org/wiki/Open_Database_Connectivity) (databases).\n\nIt has a similar performance and higher safety (e.g. no segfaults) than pyarrow.\n\nIt supports reading from and writing to ODBC compliant databases at\nlikely similar performance as [`turbodbc`](https://turbodbc.readthedocs.io/en/latest/) and it does not require conda to install.\n\nThis package is particularly suitable for environments such as AWS Lambda - it takes 13M of disk space, compared to 82M taken by pyarrow.\n\n## Features\n\n* declare and access Arrow-backed arrays (integers, floats, boolean, string, binary)\n* read from and write to Apache Arrow IPC file\n* read from and write to Apache Parquet\n* read from and write to ODBC-compliant databases (e.g. postgres, mongoDB)\n\nIt is not intended for OLAP type of queries (Pandas, Polars and many others serve this purpose better).\n\nThere is still work to support more types (e.g. datetimes, python dictionaries), etc, but the gist is there - a small utility to lower the barrier to interoperate the Arrow format with Parquet, Arrow and ODBC.\n\nLooking forward for feedback!", "upvote_ratio": 0.87, "id": "t3_u58xni", "created_utc": 1650148309.0}
{"sub": "Python", "title": "Slowpoke: A tool to convert Spotify and Youtube songs to slow and reverb", "selftext": "[https://github.com/newpolygons/SlowPoke](https://github.com/newpolygons/SlowPoke)\n\nLittle tool I worked on this weekend. Hope you guys like it. Currently setup to allow you to control the speed of the song.", "upvote_ratio": 0.8, "id": "t3_u58rf6", "created_utc": 1650147774.0}
{"sub": "Python", "title": "Is it me learning python that there are many different ways to write it? I learn one way to write variable then next lesson shows me functions to show the same outcome. Or am I\u2019m looking at this too soon and something else suppose to go along with it?", "selftext": "nan", "upvote_ratio": 0.62, "id": "t3_u57opq", "created_utc": 1650144578.0}
{"sub": "Python", "title": "Space Science: Asteroid spectra meet Autoencoders", "selftext": "Hey Pythonistas,\n\nIn my [previous videos](https://www.youtube.com/playlist?list=PLNvIBWkEdZ2gagAcgm44cplgSvQ_Cmvbv) I explained and showed the \"nature\" of asteroid spectra. Based on their major 4 classes, I created a Support-Vector-Machine classifier as well as a Keras-based neural network to distinguish between these classes ([GitHub repo](https://github.com/ThomasAlbin/Astroniz-YT-Tutorials/tree/main/%5BML1%5D-Asteroid-Spectra); yes... I should have numbered my scripts starting at 01, 02, 03, ...).\n\nClassifying imbalanced data is really interesting, and it is also somehow exciting to write Conv1D networks, since it is rarely seen (most use Conv2D in their tutorials, since most use images to classify data).\n\nBut what would be more interesting than writing a neural network classifier? Well, data classes can be artificial constructs. Especially \"space spectra\" (asteroid, mass spectra of dust particles in space, or spectra of so-called active galactic nuclei), are not always 100% distinguishable and have transitional classes between major classes.\n\nSo... can we classify our asteroid spectra in a data-driven way? Well, that's the goal of my last 3 video sessions, starting today with the very first one. The idea:\n\n\\- Creating an Autoencoder to reconstruct asteroid spectra\n\n\\- Displaying and inspecting the latent space of these spectra\n\n\\- Applying some clustering algorithm on the latent space to determine a data science based number of potential asteroid classes\n\nStarting with the Autoencoder part I'd like to share my most recent tutorial:\n\n[YouTube Link](https://www.youtube.com/watch?v=UsiY28qjBK8&amp;list=PLNvIBWkEdZ2gagAcgm44cplgSvQ_Cmvbv&amp;index=13)\n\n[GitHub Link](https://github.com/ThomasAlbin/Astroniz-YT-Tutorials/blob/main/%5BML1%5D-Asteroid-Spectra/11_dl_autoencoder_reconstruction.ipynb)\n\nI hope you'll like the final steps of this *Space Science with Python* project. I am honestly thinking to put everything in a small scientific paper :).\n\nCheers,\n\nThomas", "upvote_ratio": 0.73, "id": "t3_u57cdr", "created_utc": 1650143542.0}
{"sub": "Python", "title": "What are your useful packages to help scraping (besides requests, beautifulsoup etc)?", "selftext": "What are your go-to packages to include in python scraping scripts?\n\nBesides the basics (requests/urlib(2), beautifulsoup/lxml, selenium, scrapy).  What are the packages you include in your scripts?\n\nFor me it's:\n\n* click\n* requests-cache\n* retry/backoff\n* fake-useragent", "upvote_ratio": 0.8, "id": "t3_u578lt", "created_utc": 1650143213.0}
{"sub": "Python", "title": "Shipping command line programs with a python package", "selftext": "I already know this inherently sounds like a bad idea, but I'd love some genuine, constructive technical advice on the following scenario.\n\nI have a python package that is used by others at work, and one of the features I need to add is the ability to upload binary files (\\~1GB) from local to a cloud object storage, in this case Azure Data Lake.\n\nThe Azure Data Lake python client does have the ability to upload data, but it is significantly slower than their recommended command line bulk copy tool AzCopy.\n\n**Can I get feedback on this approach:**\n\n\\-pip install the python package\n\n\\-the python package already has a command line entry point so the user would type something like\n\n    &lt;package_name&gt; setup externals\n\nWhich would then run a setup script which would install azcopy to the users $HOME path and create a symlink to their PATH for the binary to run.\n\n\\-Then python would use it via subprocess.\n\n**For me the negatives I see are:**\n\n\\-Having to write handling code for the setup script to fail gracefully.\n\n\\-Possibly having to host the already installed binaries on a fileshare somewhere so URL changes and version changes don't break the installer\n\n\\-Having an extra step if someone else clones the repo, pip installs the requirements, they have one more step before they can recreate.\n\n**Positives are:**\n\n\\-AzCopy is way faster than the python sdk and doesn't have to upload objects one at a time.\n\n\\-I can extend this to use RClone eventually to make it cloud storage agnostic, and have it work with S3 as well\n\n&amp;#x200B;\n\nHas anyone ever done anything like this? Am I being oblivious to any major issues here?", "upvote_ratio": 0.6, "id": "t3_u53wpo", "created_utc": 1650133545.0}
{"sub": "Python", "title": "Do you think AI/machine learning is a very important field to get into for future relevance?", "selftext": "I am just starting to learn about it this week and thinking it is really going to be integral in the years to come right?\n\nPerhaps as important if not more than the information age we are in now. More probably when skynet goes online and the war between humans/ai begins.\n\nSo, good to get onboard now.\n\nI think my last great decision was learning coding around 2016 and this is going to be another great one in terms of time management and where to focus your efforts.\n\nNot only yes but interested to hear what fields it will be relevant in? All?\n\nEDIT: seems this is going off what I intended. I am not actually looking for a new career path. I am more just interested in knowing what future implementations will/might be as it becomes more ubiquitous, just speculating what that might be.", "upvote_ratio": 0.68, "id": "t3_u51zk3", "created_utc": 1650128123.0}
{"sub": "Python", "title": "CLI macro for Windows", "selftext": "Hey everyone, I built a cli program for windows that can open files,websites, create temporary python enviroments. This is kind of my first full scale project that really eases my workflow.I would love to get suggestions on this program. :)\n\n[https://github.com/Sidharth-S/do](https://github.com/Sidharth-S/do)", "upvote_ratio": 0.73, "id": "t3_u51ud3", "created_utc": 1650127715.0}
{"sub": "Python", "title": "Python Virtual Environments: A Primer", "selftext": "nan", "upvote_ratio": 0.96, "id": "t3_u51l5b", "created_utc": 1650126979.0}
{"sub": "Python", "title": "A lightweight way to profile Python Code non-intrusively", "selftext": "Latency:\n\n[https://github.com/oracle-samples/hiq/blob/henry\\_dev/hiq/examples/paddle/demo.ipynb](https://github.com/oracle-samples/hiq/blob/henry_dev/hiq/examples/paddle/demo.ipynb)\n\nMemory:\n\n[https://github.com/oracle-samples/hiq/blob/main/hiq/examples/paddle/demo\\_memory.ipynb](https://github.com/oracle-samples/hiq/blob/main/hiq/examples/paddle/demo_memory.ipynb)\n\n&amp;#x200B;\n\n[Latency Graph](https://preview.redd.it/qgc7upex3xt81.png?width=512&amp;format=png&amp;auto=webp&amp;s=87a0a0cbf34e2ee4fd560d5c0768179ec5d95d76)\n\n&amp;#x200B;\n\nMore details at: [https://github.com/oracle-samples/hiq](https://github.com/oracle-samples/hiq)", "upvote_ratio": 0.59, "id": "t3_u51h29", "created_utc": 1650126645.0}
{"sub": "Python", "title": "How to make annoy any interviewer with the fizzbuzz challenge", "selftext": "I was bored so I decided to solve the fizzbuzz in an obnoxious way. Hope you enjoy (:\n\n`output = [\"fizzbuzz\", \"buzz\", \"fizz\"]`  \n`iterList = lambda x: [x%i for i in [15, 5, 3]]`  \n`fizzBuzz = [output[iterList(x).index(0)] if 0 in iterList(x) else x for x in range(1, 101)]`  \n`print(fizzBuzz)`\n\n`[1, 2, 'fizz', 4, 'buzz', 'fizz', 7, 8, 'fizz', 'buzz', 11, 'fizz', 13, 14, 'fizzbuzz', 16, 17, 'fizz', 19, 'buzz', 'fizz', 22, 23, 'fizz', 'buzz', 26, 'fizz', 28, 29, 'fizzbuzz', 31, 32, 'fizz', 34, 'buzz', 'fizz', 37, 38, 'fizz', 'buzz', 41, 'fizz', 43, 44, 'fizzbuzz', 46, 47, 'fizz', 49, 'buzz', 'fizz', 52, 53, 'fizz', 'buzz', 56, 'fizz', 58, 59, 'fizzbuzz', 61, 62, 'fizz', 64, 'buzz', 'fizz', 67, 68, 'fizz', 'buzz', 71, 'fizz', 73, 74, 'fizzbuzz', 76, 77, 'fizz', 79, 'buzz', 'fizz', 82, 83, 'fizz', 'buzz', 86, 'fizz', 88, 89, 'fizzbuzz', 91, 92, 'fizz', 94, 'buzz', 'fizz', 97, 98, 'fizz', 'buzz']`\n\nEdit: yes I know the title is messed", "upvote_ratio": 0.66, "id": "t3_u4zs3s", "created_utc": 1650121763.0}
{"sub": "Python", "title": "Live flame graph rendering in the terminal", "selftext": "Austin TUI 1.2.0 has just been released, with the new live flame graph mode.\n\n[https://github.com/P403n1x87/austin-tui](https://github.com/P403n1x87/austin-tui)\n\nThis is what it looks like in VS Code. The graph can be paused and files opened directly in the editor with a Ctrl + Click on the path for further inspection. The VS Code extension is also available for more insight into performance\n\n[https://marketplace.visualstudio.com/items?itemName=p403n1x87.austin-vscode](https://marketplace.visualstudio.com/items?itemName=p403n1x87.austin-vscode)\n\nhttps://i.redd.it/5kh8pn811wt81.gif", "upvote_ratio": 0.77, "id": "t3_u4x741", "created_utc": 1650113643.0}
{"sub": "Python", "title": "\"Safe\" way to install Python 3 on MBA M1", "selftext": "Hi. I just want to ask if there's something like a \"safe\" way to install Python 3 on Macbook Air m1. I'm learning Python right now, and MBA has Python 2.7 something version.", "upvote_ratio": 0.82, "id": "t3_u4uji2", "created_utc": 1650103050.0}
{"sub": "Python", "title": "Recover deleted and overwritten files with RecoverPy 1.5.0", "selftext": "&amp;#x200B;\n\nhttps://i.redd.it/qg7rwesb1vt81.gif\n\nHi! I recently release RecoverPy v1.5.0 and I think I might give you some news.\n\n**-&gt; Repo:** [https://github.com/PabloLec/RecoverPy](https://github.com/PabloLec/RecoverPy)  \n\n\n# What is it?\n\nRecoverPy is a 100% Python tool to not only recover deleted but also overwritten files.  \nI got the idea when I was quite new to some programming best practices, especially version control...  \n\n\nLong story short, I accidentally piped my output into my precious script... Just spent the day working on something and instead of typing myscript &gt; log, I typed log &gt; myscript, oh boy what a feeling.\n\nI knew some tools to recover deleted files, but my problem was quite different, I didn't deleted the file (in system words, marked the file blocks as deleted/available), I just replaced it's content. Talk about an impostor syndrome.\n\nAfter a long ride in the abysses of unix stackexchange, I found some dark combination of grep and dd command to search directly in your raw system partitions blocks and eventually recovered my file! But as the process was really slow and painful, I thought it might be a good idea to make a tool out of it. That's how RecoverPy was born.  \n\n\n# 1.5.0\n\nSince then, the tool has had quite some success. Especially in the hacker community (wasn't the initial intent but still). It even appeared in hakin9 magazine.\n\nLast releases have been quite stable, lastly I mostly added QoL updates and better binary file search handling.\n\nFeel free to have a look and tell me what you think of it! It's my biggest personal project and I'm beginning to be quite proud of my baby :)", "upvote_ratio": 0.93, "id": "t3_u4u8mb", "created_utc": 1650101676.0}
{"sub": "Python", "title": "Python Web Frameworks", "selftext": "nan", "upvote_ratio": 0.5, "id": "t3_u4u85t", "created_utc": 1650101619.0}
{"sub": "Python", "title": "Python for complete begginers Medium", "selftext": "I wrote this cool article about python for complete begginers about 2 months ago and I thought I'd share it. In this article you will learn about:\n1) what coding is , what python is\n2) The advantages of python\n3) How to download and install python\n4) What an ide is\n5) Maths in python\n6) Variables\n\nHere is the link : https://4rkal.medium.com/python-for-beginners-f1df170bcc08", "upvote_ratio": 0.35, "id": "t3_u4tsp2", "created_utc": 1650099636.0}
{"sub": "Python", "title": "GitHub - AlexEidt/ASCII-Video: Blazing fast ASCII Image/Video Renderer.", "selftext": "nan", "upvote_ratio": 0.95, "id": "t3_u4rpyc", "created_utc": 1650090494.0}
{"sub": "Python", "title": "Run python (and sql) with dbt", "selftext": "nan", "upvote_ratio": 0.72, "id": "t3_u4mwco", "created_utc": 1650072702.0}
{"sub": "Python", "title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread", "selftext": "Found a neat resource related to Python over the past week? Looking for a resource to explain a certain topic?\n\nUse this thread to chat about and share Python resources!", "upvote_ratio": 0.86, "id": "t3_u4l9am", "created_utc": 1650067210.0}
{"sub": "Python", "title": "i created my own music player in Python3", "selftext": "i always wanted an offline music player that was low on ram, and functioned properly, and for the love of god i could not find one so i created mine.\n\ni would be glad if you tested it, here's the github link:\n\n[https://github.com/AvivHamagniv69/music-player](https://github.com/AvivHamagniv69/music-player)", "upvote_ratio": 0.79, "id": "t3_u4l7re", "created_utc": 1650067079.0}
{"sub": "Python", "title": "How to build a disease prediction service", "selftext": "nan", "upvote_ratio": 0.83, "id": "t3_u4jf15", "created_utc": 1650061429.0}
{"sub": "Python", "title": "TL;DR: Dictionary Comprehension + Early\\Late Binding of Lambdas in Python is Mental", "selftext": "Hello,\n\nI found a behaviour I couldn't explain when using lambdas in a dictionary comprehension so I decided to post a little Intermediate Showcase to inform you about my struggles and give you a solution, in the end (no spoilers)\n\nI like to minify python code in my spare time to relax and it often takes some iterations for each different function or class, alongside those iterations I was golfing a function in a progress bar script I wrote a very long time ago, this is the brief description of what the function should have done: `The function has to take a list of n elements and color them green, yellow or red according to a given condition.`\n\nThis was the last *functioning* function in the process of being golfed:\n\n    def paint(total, done, elements):\n        \n        color_str  = lambda x, y: f'{x}{y}\\033[00m'\n    \n        colors = {'r'   : lambda x: color_str('\\033[91m',x),\n                  'g' : lambda x: color_str('\\033[92m',x),\n                  'y': lambda x: color_str('\\033[93m',x)\n                  }['g' if total == done else 'y' if total/2 &lt;= done else 'r']\n    \n        return [*map(colors,elements)]\n\nI thought, that's easy, this has to be equivalent to this golfed function:\n\n    def paint(total, done, elements):\n        return [*map({'rgy'[i]:lambda x:f'\\033[9{i+1}m{x}\\033[00m' for i in range(len('rgy'))\n               }['g' if total == done else 'y' if done &lt;= total/2 else 'r'],elements)]\n\n*Until it's not*, in fact, while the first function outputs a correct response, the last one is **somehow** stuck on the color yellow, so I thought the dictionary comprehension wasn't working properly and thus I tried:\n\n    dict_comp = {'rgy'[i]:lambda x:f'\\033[9{i+1}m{x}\\033[00m' for i in range(len('rgy'))}\n    print(dict_comp)\n\nThis prompted what I thought it would prompt, a dictionary with `r`,`g` and `y` as keys and a list of lambdas in different memory locations `{'r': &lt;function &lt;dictcomp&gt;.&lt;lambda&gt; at 0x0000018D3E986200&gt;, 'g': &lt;function &lt;dictcomp&gt;.&lt;lambda&gt; at 0x0000018D3E9860E0&gt;, 'y': &lt;function &lt;dictcomp&gt;.&lt;lambda&gt; at 0x0000018D3E986050&gt;}` so I tried to print all the key and lambda pairs with a test for each lambda:\n\n    for key, lmbd in dict_comp.items():\n        print(key, lmbd(f'Testing {key}'))\n\nThis was the output:\n\n[Horrific Minion-Colored Results](https://preview.redd.it/o216rv5pjrt81.png?width=139&amp;format=png&amp;auto=webp&amp;s=eb4882d1c3114792c06656ac59204a08f297a790)\n\nI decided to remove the slash to check the text value of the lambda and the correct progression of `i` and this why I noticed the problem: each lambda stored the last value of `i`!\n\n    dict_comp = {'rgy'[i]:lambda x:f'033[9{i+1}m{x}\\033[00m' for i in range(len('rgy'))}\n    for key, lmbd in dict_comp.items():\n        print(key, lmbd(f'Testing {key}'))\n    \n    &gt;&gt; r 033[93mTesting r # the value after the square bracket is 93 (it's supposed to be 91)\n    &gt;&gt; g 033[93mTesting g # the value after the square bracket is 93 (it's supposed to be 92)\n    &gt;&gt; y 033[93mTesting y # the value after the square bracket is 93\n\nHere's the whole code in order to try both functions:\n\n    from time import sleep\n    \n    def paint(total, done, elements):\n        \n        color_str  = lambda x, y: f'{x}{y}\\033[00m'\n    \n        colors     =   {'r'   : lambda x: color_str('\\033[91m',x),\n                        'g' : lambda x: color_str('\\033[92m',x),\n                        'y': lambda x: color_str('\\033[93m',x)\n                        }['g' if total == done else 'y' if total/2 &lt;= done else 'r']\n    \n        return [*map(colors,elements)]\n    \n    \n    # UNCOMMENT THIS IN ORDER TO TEST THE FUNCTION DOWN BELOW\n    \"\"\"\n    def paint(total, done, elements):\n        return [*map({'rgy'[i]:lambda x:f'\\033[9{i+1}m{x}\\033[00m' for i in range(len('rgy'))\n               }['g' if total == done else 'y' if done &lt;= total/2 else 'r'],elements)]\n    \"\"\"\n    \n    def CustomProgressBar(task, completeness) -&gt; None:\n        size = 100 // 5\n        empty  = size - completeness//5\n        fill = size - empty\n        \n        percent  = f'{completeness:&gt;3}% '\n        \n        filler = f'{\"\u2550\"*fill}'\n        isComplete = fill==size\n    \n        progress_bar, percent  = paint(size,fill, [filler, percent])\n        progress_bar+=f'{\"\u2500\"*empty}'\n    \n        print(f'\\r{task:&lt;25}{percent}{progress_bar}',\n                end='\\n' if isComplete else '')\n    \n    \n    \n    for i in range(101):\n        sleep(0.05)\n        CustomProgressBar('Range 0-100', i)\n\nI then asked myself:\n\n&gt;Why does this happen? Isn't each anonymous function different?\n\nAfter some scavenger hunt in StackOverFlow (thanks you Stack for the duplicate question tootip) I found out that the issue might be in [late binding in functions and lambdas in particular](https://stackoverflow.com/questions/3431676/creating-functions-in-a-loop), so the answer, apparently, is that even if the functions are stored in a different memory location as part of the dictionary, the lambda function captures the ***NAME*** of the variable, not the ***VALUE*** of the variable and assigns the value after the dict comprehension is called, so each value becomes the last value in the loop, as the friendliest guy on StackOverFlow explains [here](https://stackoverflow.com/questions/70862614/how-does-python-dict-comprehension-work-with-lambda-functions-inside).\n\nAnd so, the solution was there, after several hours of head scratches and articles about early and late bind  and (anonymous) functions in loop variables inside loops, I had to tie (or late-bind, if you will) the value name of the variable in the lambda function to the  value of the variable in the loop.\n\nThis means I had to create another value in the lambda and that the value HAS to be the last in the lambda function because it will be created as a keyword argument and lambdas respect the rule of \\*args first, \\*\\*kwargs last.\n\nSo, the solution was finally here:\n\n    def paint(total, done, elements):\n        return [*map({'rgy'[i]:lambda x, y=i+1:f'\\033[9{y}m{x}\\033[00m'for i in range(len('rgy'))}['g' if total == done else 'y' if done &gt;= total/2 else 'r'],elements)]\n\nIf you have any question, you find this interesting, you have any feedback, you want to talk about Python or you just want to send me death threats because I wrote a long-ass post about lambdas and how they work in loops, HMU or comment down below.\n\nHave fun, and happy easter:\n\n[Easter Bunny](https://preview.redd.it/c2qo8krnmrt81.png?width=564&amp;format=png&amp;auto=webp&amp;s=354dfac6c9adada4c4e5a57d531b073aa4967968)", "upvote_ratio": 0.76, "id": "t3_u4j0jn", "created_utc": 1650060242.0}
{"sub": "Python", "title": "Learn Python from Scratch to Advance with Detailed Hands-on", "selftext": "nan", "upvote_ratio": 0.42, "id": "t3_u4hgpj", "created_utc": 1650055769.0}
{"sub": "Python", "title": "Archimedes Spiral: Converting old-school BASIC to Python", "selftext": "A little bit retro, a little bit Python. This is pretty much a direct conversion of the BASIC code to Python. Perhaps you can make it even better!\n\nhttps://goto10.substack.com/p/archimedes-spiral\n\nhttps://i.imgur.com/eHdCPrK.jpg", "upvote_ratio": 0.9, "id": "t3_u4h485", "created_utc": 1650054739.0}
{"sub": "Python", "title": "Creating an HTTPS Lambda Endpoint without API Gateway", "selftext": "nan", "upvote_ratio": 0.57, "id": "t3_u4gkfx", "created_utc": 1650053153.0}
{"sub": "Python", "title": "Python module for Notion", "selftext": " \n\nNotionpy is a python module that helps you integrate notion with your programme\\\\workflow utilizing notion's API, the module helps you create, retrieve and update pages or databases with ease\n\nSomeone will ask \"what is the difference between this module and the other plenty of modules out there !?\", as far as I have seen searching the web, this is the most versatile, user-friendly one\n\nyou can start using it by typing in your terminal :\n\n    pip install auto-py-notion \n\nGithub repo :\n\n[https://github.com/kareemmahlees/NotionPy.git](https://github.com/kareemmahlees/NotionPy.git)\n\nNote :\n\nthis module is still basic stage with fairly simple functionality, so all your suggestions, issues, and contributions are very welcome", "upvote_ratio": 0.6, "id": "t3_u4da5o", "created_utc": 1650044005.0}
{"sub": "Python", "title": "How to build a RSS from scraping using Python", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_u4d17u", "created_utc": 1650043321.0}
{"sub": "Python", "title": "Python, Flask, Elasticsearch - front controller and API documentation - Part 3 Tutorial", "selftext": "Hi, the 3d article devoted to the theme: \u201cHow to work with ElasticSearch, Python and Flask\u201d already ready for reading. Here will speak about package dependencies we are going to use, some project structure aspects, controller, REST API, flask\\_apispec package and response/request models. All details are here: [\"Symfony, elasticsearch - front controller and api documentation\"](https://sergiiblog.com/python-flask-elasticsearch-front-controller-and-api-documentation/). Have a pleasant reading.", "upvote_ratio": 0.4, "id": "t3_u4aplz", "created_utc": 1650036900.0}
{"sub": "Python", "title": "Geometry Calculation", "selftext": "nan", "upvote_ratio": 0.25, "id": "t3_u4a3sa", "created_utc": 1650035177.0}
{"sub": "Python", "title": "How To Build Your Own Crypto News Aggregator [Streamlit]", "selftext": "nan", "upvote_ratio": 0.23, "id": "t3_u49yxj", "created_utc": 1650034812.0}
{"sub": "Python", "title": "A simple email app", "selftext": "I made a simple email app in python\n\n [gocrazygh/emailapp: A simple email app in python (github.com)](https://github.com/gocrazygh/emailapp)", "upvote_ratio": 0.67, "id": "t3_u48ux2", "created_utc": 1650031662.0}
{"sub": "Python", "title": "Fun project to notify my boss if I\u2019m at my desk or not each morning.", "selftext": "I made a script that utilizes opencv to help me identify if I\u2019m at my desk or not and then let my boss know. Made a fun video on how it turned out. \n\n[Video](https://youtu.be/AV7qLsYnOWY)", "upvote_ratio": 0.91, "id": "t3_u48iqw", "created_utc": 1650030683.0}
{"sub": "Python", "title": "Running Python in the Browser with WebAssembly", "selftext": "nan", "upvote_ratio": 0.72, "id": "t3_u47p54", "created_utc": 1650028107.0}
{"sub": "Python", "title": "2 Use Cases of Python Pre-commit Hooks to Tidy Up Your Git Repositories", "selftext": "nan", "upvote_ratio": 0.64, "id": "t3_u47kk6", "created_utc": 1650027739.0}
{"sub": "Python", "title": "Like httpie? Might need to like it again...", "selftext": "A great Python project, [HTTPie](https://github.com/httpie/httpie) recently lost all of its Github stars due to an easy-to-make mistake. [Read more at their blog](https://httpie.io/blog/stardust).\n\nI enjoy [HTTPie](https://github.com/httpie/httpie) as a cURL-like command line tool for interacting with APIs and other web resources. A very clever UI, and a good example of using [rich](https://rich.readthedocs.io/) and [requests](https://docs.python-requests.org/).\n\nYou may want to consider helping them restore or even increase their online community, sadly lost due to this error. You can star and/or watch the repo at https://github.com/httpie/httpie", "upvote_ratio": 0.96, "id": "t3_u46vhe", "created_utc": 1650025435.0}
{"sub": "Python", "title": "Firedm repos no longer exist.", "selftext": "FireDM is a python open source (Internet Download Manager) with multi-connections, high speed engine, it downloads general files and videos from youtube and tons of other streaming websites .\n\nYesterday I tried to go to the GitHub page to download the latest release but the repo returns a 404 which means it does not exist. The pypi package still exists and here's the link:\n\nhttps://pypi.org/project/FireDM/\n\nThe project page clearly shows that the repository is inaccessible.No forks of it even exist on Github. I can't seem to find any news or complaints anywhere on the internet about this so I was wondering is there some announcement I missed?\nAnd to my last question, is there a way we can save the project through pypi and create forks of it?\n\nEdit : my package manager still has a copy(Manjaro and AUR)", "upvote_ratio": 0.95, "id": "t3_u44t8y", "created_utc": 1650017535.0}
{"sub": "Python", "title": "Vitrix - An open source FPS video game coded in Python!", "selftext": "Vitrix is a fully open source video game coded in Python! It makes use of Ursina Engine and TKinter for its GUIs and has with prebuilt releases that come bundled with a Python binary and all necessary libraries preinstalled!\n\nEven though Vitrix is still in the early stages of its development, it is still perfectly playable and has actively maintained code and a wiki. Me being the developer, I'm not very good with any of the arts, so anybody who can contribute textures, models or sounds is much appreciated. Vitrix still has much development to go, so anybody who helps will be welcomed.\n\n&amp;#x200B;\n\nWant to see one of your ideas in Vitrix someday?\n\nRecommend me ideas: [https://github.com/ShadityZ/Vitrix/discussions/24](https://github.com/ShadityZ/Vitrix/discussions/24)\n\nApply to become a developer: [https://github.com/ShadityZ/Vitrix/discussions/26](https://github.com/ShadityZ/Vitrix/discussions/26)\n\n&amp;#x200B;\n\nYou can find the Vitrix github repository here: [https://github.com/ShadityZ/Vitrix](https://github.com/ShadityZ/Vitrix)\n\nHere at some images:\n\n[https://user-images.githubusercontent.com/543577/163570397-c4736068-199e-4527-a998-e18309e9c49c.png](https://user-images.githubusercontent.com/543577/163570397-c4736068-199e-4527-a998-e18309e9c49c.png)\n\n[https://imgur.com/a/PLKDG4L](https://imgur.com/a/PLKDG4L)\n\n&amp;#x200B;\n\nContributions open, ShadityZ", "upvote_ratio": 0.9, "id": "t3_u43oxi", "created_utc": 1650012550.0}
{"sub": "Python", "title": "Python custom formatting", "selftext": "nan", "upvote_ratio": 0.88, "id": "t3_u43mf5", "created_utc": 1650012218.0}
{"sub": "Python", "title": "Good GUI builder for Python similar to Java\u2019s Eclipse Windowbuilder", "selftext": "Hi guys! I\u2019m a Java developer migrating to Python, and I\u2019m trying to find a good GUI builder that is similar to Java\u2019s Eclipse windowbuilder. I have tried PyQy5 and it\u2019s Qt Designer which is very good and very close to windowbuilder in terms of interface, but the only problem is that the Qt Designer doesn\u2019t work dynamically with the IDE like the windowbuilder: all Qt Designer\u2019s ui file need to be translated manually to py file and then modified. This is really bad for efficiency since everything added and modified in the Qt Designer need to be saved and translated again and manually copied from the new py file to the main py file, and all objects\u2019 code\u2019s locations need to be manually located (Unlike in windowbuider where you double click the object in the builder and you will be automatically redirected to the related code) \nSince Python is such a popular language and so many people use it to develop complicated GUI softwares, I guess there has to be a GUI builder that is more efficient and similar to Java\u2019s windowbuilder", "upvote_ratio": 0.76, "id": "t3_u40lpu", "created_utc": 1649999500.0}
{"sub": "Python", "title": "Airflow 2.3.0 - Dynamic Tasks", "selftext": "Coming Soon in Airflow 2.3.0 - First-class support for \"Dynamic Tasks\". This is feature is called \"Dynamic Task Mapping\"\n\nThe wait for the most requested feature of Apache Airflow is almost over !! \n\nNo longer hacking over creating dynamic tasks, with Dynamic Task Mapping, Airflow will allow users to create a number of tasks at runtime based upon current data, rather than having to know in advance how many tasks would be needed.\n\nhttps://twitter.com/kaxil/status/1514745136680419335?s=21&amp;t=suoW11Re4Ew2cooN4FbhEw", "upvote_ratio": 0.74, "id": "t3_u3vzb1", "created_utc": 1649983938.0}
{"sub": "Python", "title": "Collection(s) of counter-intuitive Python behaviour", "selftext": "This is kind of a shot in the blue: Some time ago I found a  (GitHub?) repo that collected unexpected/surprising Python behaviour. Ofc this depends on ones knowledge but it is stuff that is counter-intuitive if you don't know it or the internals.\n\nThe first example was [internal caching of -5...256](https://www.reddit.com/r/Python/comments/18leav/python_integer_range_5_256_and_identity_comparison/) and all the consequences of that (ids, is), \u2026\n\nIf you don't know that particular repo, do you know similar behaviour? I found this [Python oddities talk](https://treyhunner.com/python-oddities/) but that wasn't it.\n\nSecondary question: What would be a good source for this (edit: finding oddities/inconsistencies)? I enjoyed to learn what's new by reading the release notes &lt;3. I found looking/searching through [bugs.python.org](https://bugs.python.org/) quite interesting, because you can follow a discussion about aspectes of the language properties/behaviour, same idea why I skimmed PEPs and their discussion.\n\nHowever, there is always something one does not know and I want to learn all of that. For that reason I am keeping a file in which I document parts of the language that I am not that used to, and then I mostly play around with `ptpython` and try stuff which I try to use later on if it seems like a good fit.\n\nI guess well fitting flairs are discussion, help, resource, \u2026 but since I can't really decide I am choosing help. after all I want help to find a resource which could provide material for a discussion.", "upvote_ratio": 0.89, "id": "t3_u3vd2g", "created_utc": 1649981969.0}
{"sub": "Python", "title": "Friday Daily Thread: Free chat Friday! Daily Thread", "selftext": "Use this thread to talk about anything Python related! Questions, news,  projects and any relevant discussion around Python is permitted!", "upvote_ratio": 0.57, "id": "t3_u3uzei", "created_utc": 1649980810.0}
{"sub": "Python", "title": "Non developer code review", "selftext": " Hello, I am not a developer and barely work with software for my job but have been self taught ever since reading \"Automate the boring stuff with python\" four years ago. One of the biggest issues that I run into by being self taught and not working with software is I am never really certain if I am doing things correctly or if there are better ways. I have found a bunch of people on fiver to do code reviews but as I don't have a complete project, I'm not sure if that is the best path to take for a code review. For the most part I believe I am past the point of having to following tutorials, but not far enough to really make a full project from start to finish. To summaries my questions:\n\n1. What are the best places/communities for non developer code review?\n2. What are the best places/ways to learn how to construct a project from start to finish?\n\nHere is my github to assess my level of knowledge (FYI not much lol)\n\n[https://github.com/daedalus23/Configuration](https://github.com/daedalus23/Configuration)", "upvote_ratio": 0.67, "id": "t3_u3u8om", "created_utc": 1649978547.0}
{"sub": "Python", "title": "For those familiar with SpeechRecognition module, does it collect user data?", "selftext": "I have a question, and it may be dumb but I haven't been able to find an answer online, but does the SpeechRecognition module, specifically CMU Sphinx or Google Speech Recognition collect user data?  \n\n\nI am a fan of having privacy and try to leave as minimal of a digital footprint when possible and have been wondering since I started using SpeechRecognition about the data collection. I'd like to avoid having to deploy my own NLP, TTS, and STT models if I can to achieve privacy for the project I am working on that involves this stuff.", "upvote_ratio": 0.6, "id": "t3_u3tp8v", "created_utc": 1649976942.0}
{"sub": "Python", "title": "Version 2.1.1 of YFrake library just released!", "selftext": "I'm happy to let you guys know that the YFrake library now implements caching, speeding up consecutive identical requests to the same endpoints even more! If you are into fintech and use stock market data from Yahoo Finance, give YFrake a try!  \nIt's easy to use, it's way faster than yfinance, has more data endpoints, is thoroughly tested, is fully documented and can even run as a server to forward data to other applications!\n\nYou can check it out on GitHub at: [https://github.com/aspenforest/yfrake](https://github.com/aspenforest/yfrake)  \nThe docs are available on: [https://yfrake.readthedocs.io/](https://yfrake.readthedocs.io/)", "upvote_ratio": 0.6, "id": "t3_u3taxa", "created_utc": 1649975777.0}
{"sub": "Python", "title": "Ready-Made Mobile App UI for Serverless Python", "selftext": "I made a tool for myself to wrap Python code in a mobile app (chat interface) so that I can let my non-technical friends try any programs I write. Curious if you guys would be interested too. \n\nI like coding in Python, but I didn't like how my non-technical friends and family couldn't try my python apps. So I made a lightweight frontend for iOS and Android using Flutter and a lightweight web app to fill in a serverless Python function. \n\nThis is how it works:\n\n1. User uses the mobile app to send a message. Optionally with an attachment, like an image. This message is sent to serverless function as a JSON object. \n2. Serverless function starter code is just print(msg) and return empty\\_message. But when I want to write an app, I do some operations on msg, and return a JSON in a certain schema. \n3. Mobile app gets the JSON back from serverless, and I render it to user according to the JSON schema. \n\nI thought I'd try a bunch of consumer app ideas, but I actually ended up mostly making a bunch of other tools for myself. Like when I send a hard-coded message like \"redis flushall\", I make the backend flush redis cache for another app that I made. \n\nI'm not really utilizing my own tool well, so I'm curious whether anyone here would be interested in using it to make some consumer tools. I can't publicly open it, cuz I made it for myself and there's no robust authentication to prevent people from running spam code on the serverless infra. But I'd love to send link and placeholder auth to some people if you are interested and leave a comment! (Would love to know what kind of apps you'd build!). \n\nIf a lot of people are interested, I'll work on protecting it behind some auth, or let people pay for their own compute cost (for example, log in to their AWS; and automatically set up a VM to run the python code you write using my web IDE)!", "upvote_ratio": 0.5, "id": "t3_u3s8ji", "created_utc": 1649972674.0}
{"sub": "Python", "title": "Run Multiple Functions in Parallel in Python3", "selftext": "nan", "upvote_ratio": 0.13, "id": "t3_u3rxoa", "created_utc": 1649971834.0}
{"sub": "Python", "title": "I'm organizing a hackathon!", "selftext": "Someone interested in build something next weekend?\n\n\\-&gt; workby.io/hackathon", "upvote_ratio": 0.38, "id": "t3_u3r7bu", "created_utc": 1649969801.0}
{"sub": "Python", "title": "Financial portfolio optimization for scikit-learn enthusiasts", "selftext": "Scikit-portfolio is a Python package designed to introduce **data scientists and machine learning engineers** to the problem of **optimal portfolio allocation in finance**. The main idea of scikit-portfolio is to provide many well-known portfolio optimization methods with an easily accessible **scikit-learn inspired** set of API.\n\nYou can optimize your portfolio starting from a pandas Dataframe with the prices or returns and simply compute the MinVol vanilla portfolio to get the optimal weights as:\n\n`MinimumVolatility().fit(prices).weights_`\n\nWhen your portfolio optimization method depends on a number of hyperparameters you can simply perform a `GridSearchCV` as in machine-learning algorithms, and present the grid of hyperparameters, which for most methods are already encoded in the class.\n\n```\nprices_train, prices_test = train_test_split(prices, test_size=0.3, shuffle=False)\nptf_model = MaxSharpe()\nbest_model = GridSearchCV(\n    estimator=ptf_model,\n    param_grid=ptf_model.grid_parameters(),\n    cv=KFold(5),\n    scoring=sharpe_ratio_scorer\n).fit(prices_train)\n```\n\nIt implements Omega Ratio and MAD efficient frontier as well as a number of portfolio hyperparameters search methods for the optimization in backtesting settings, using the same methods as in classical machine-learning model selection method.\n\nAdditionally, I've implemented ensemble portfolios, like the **Michaud Resampled Efficient Frontier** that builds an optimal portfolio based on the average of many efficient frontiers based on random perturbations of the expected returns.\nWe not only support the classical MeanVariance efficient frontier, but many kind of efficient frontiers, with different definitions of risk and satisfaction.\n\nThis is for example how you build an ensemble of estimators of the maximum Sharpe ratio portfolio:\n```\nprices = load_tech_stock_prices()\n# create a Maximum sharpe ratio portfolio estimator to be fed to resampled frontier meta-estimator\nptf = MaxSharpe(\n    returns_data=False,\n    risk_free_rate=0.0,\n    frequency=252,\n    rets_estimator=MeanHistoricalLinearReturns()\n)\nensemble = MichaudResampledFrontier(\n    ptf_estimator=ptf,\n    rets_estimator=MeanHistoricalLinearReturns(), \n    risk_estimator=SampleCovariance(),\n    n_iter=512,\n    n_jobs=-1\n).fit(prices)\n```\n\n**Documentation**\n[https://scikit-portfolio.github.io/scikit-portfolio/](https://scikit-portfolio.github.io/scikit-portfolio/)\n\n**Source code**\n[https://github.com/scikit-portfolio/scikit-portfolio](https://github.com/scikit-portfolio/scikit-portfolio)\n\nThe project requires some help in the documentation, while it is already pretty stable in the API and bugfix.\n\n\n![mad_frontier](https://scikit-portfolio.github.io/scikit-portfolio/imgs/mad_efficient_frontier.svg)", "upvote_ratio": 0.69, "id": "t3_u3qq2x", "created_utc": 1649968463.0}
{"sub": "Python", "title": "Parts of the Standard Library that are considered to be bad practice / un-Pythonic ?", "selftext": "I had an argument today about the using the `_empty` object from the `inspect` module to denote an item not found in a lookup, rather than just returning None (as None could well be a value in our data), or erroring (as too many try/excepts make code more difficult to read, and failing to find something in our system is not actually an error anyway). \n\nAside from the idea of using a private attribute of another module, the other person says it's \"not Pythonic\" to use smaller custom types like this, as it's too close to something like `typedef` in c++ (never mind that Python has `namedtuple` for almost the exact same purpose anyway, but I digress).\n\nI argued that it's a ready-made and easily legible solution to the problem, and regardless, if it's good enough for the standard library then it should be good enough for us. \n\nWhile I think I'm right in this case, I know the last point is a very dogmatic way of looking at things.\n\nIt got me thinking - are there any notable parts of the language's standard modules, that would be considered a poor or incorrect use of the language if you were to use them in production?", "upvote_ratio": 0.92, "id": "t3_u3p62s", "created_utc": 1649964161.0}
{"sub": "Python", "title": "GUI, CLI and library for remote controlling Philips Android TVs", "selftext": "Some time ago I was trying to find out if there's a way to programmatically control my TV's Ambilight feature. It turns out Philips Android-powered TVs have a pretty extensive API and there's no decent library or program to make use of it! I found this state of affairs unacceptable and decided to be the change I want to see in the world. Then I proceeded to create a library, CLI, and finally GUI utilizing this API:\n\n* [PhilipsTV GUI](https://github.com/bcyran/philipstv-gui) \\- a GUI, obviously\n* [philipstv](https://github.com/bcyran/philipstv) \\- CLI and a lib\n\nThe features include emulating pressing TV remote keys, listing and changing channels and applications, and of course controlling Ambilight.\n\n[All of PhilipsTV GUI functionality in one image](https://preview.redd.it/vkp8gfnkzit81.png?width=1241&amp;format=png&amp;auto=webp&amp;s=14a40b658da14951ad22c1e59e07ec98074547b4)\n\nIt would be pretty cool if someone found that useful because I spent quite a lot of time on this and actually... I don't even need this.", "upvote_ratio": 0.94, "id": "t3_u3m6ar", "created_utc": 1649955769.0}
{"sub": "Python", "title": "I used a new dataframe library (polars) to wrangle the one of the largest housing price databases. Code in post", "selftext": "nan", "upvote_ratio": 0.94, "id": "t3_u3m0qp", "created_utc": 1649955374.0}
{"sub": "Python", "title": "A live 45-minutes session on the fundamentals of observability, OpenTelemetry, and distributed tracing with microservices' messaging systems (Kafka, RabbitMQ, etc)", "selftext": "Hi everyone, we're running another live OpenTelemetry and observability fundamentals session - Wednesday, April 20 at 11 AM PDT.\n\nYou will learn how to instrument your message brokers and apps to capture traces with OpenTelemetry.\n\nThis session is at no cost and vendor-neutral.\n\nYou can expect in this session: 45 minutes of core concepts, how to deploy it yourself hands-on + Q&amp;A.\n\nIf you are interested in observability, OpenTelemetry, and tracing - join!\n\nRegister here [https://www.aspecto.io/opentelemetry-fundamentals/messaging-systems/](https://www.aspecto.io/opentelemetry-fundamentals/messaging-systems/?utm_source=post&amp;utm_medium=reddit&amp;utm_campaign=r-python-opentelemetry-fundamentals-messaging-systems)", "upvote_ratio": 0.55, "id": "t3_u3kzsh", "created_utc": 1649952550.0}
{"sub": "Python", "title": "Using Python how to connect to multiple Bluetooth LE devices and transfer data between them.", "selftext": "nan", "upvote_ratio": 0.64, "id": "t3_u3j2d6", "created_utc": 1649947229.0}
{"sub": "Python", "title": "Running Rich's Inspect in bashrc", "selftext": "I really like [Rich's Inspect method](https://rich.readthedocs.io/en/stable/introduction.html#rich-inspect) such that I want to create a shortcut in my bashrc. I tried thiese and it didn't work.\n\n    alias rinspect='python -c \"from rich import inspect; import '$1'; inspect('$1')\"'\n    \n    alias r2inspect='python -c 'import sys; from rich import inspect; import sys.argv[1]; inspect(sys.argv[1])'\n\nBasically, I want to run it like this:\n\n    $ rinspect datetime\n\nWhat am I missing here?", "upvote_ratio": 0.84, "id": "t3_u3imay", "created_utc": 1649945996.0}
{"sub": "Python", "title": "how to handle huge amount of data for a web app ?", "selftext": " \n\nhey guys, i'm working on a project where i'm importing data from google sheets using sheets API weekly, so that each week a new sheet is added to the spreadsheet and then the data is automatically collected in one new sheet to be used as input for my dashboard that i used streamlit to build, i'm planning now to deploy it on heroku, but i came to ci/cd part which i didnt get tbh, and i,m wondering how my app would perform when the data would be larger maybe after 1 year, whaat to do then , is there any solution to handle that big data from the start ?", "upvote_ratio": 0.7, "id": "t3_u3gs01", "created_utc": 1649940505.0}
{"sub": "Python", "title": "mdiff - generating diff with block move detection", "selftext": "[mdiff](https://github.com/m-matelski/mdiff) is a package for comparing and generating diff for input sequences. It can detect sequence elements displacements (i.e. line in text have been moved up or down).\n\n# Sequence Matcher\n\nExample:\n\n    from mdiff import HeckelSequenceMatcher\n    \n    a = ['line1', 'line2', 'line3', 'line4', 'line5']\n    b = ['line1', 'line3', 'line2', 'line4', 'line6']\n    sm = HeckelSequenceMatcher(a, b)\n    opcodes = sm.get_opcodes()\n    \n    for tag, i1, i2, j1, j2 in opcodes:\n        print('{:7}   a[{}:{}] --&gt; b[{}:{}] {!r:&gt;8} --&gt; {!r}'.format(tag, i1, i2, j1, j2, a[i1:i2], b[j1:j2]))\n\nOutput:\n\n    equal     a[0:1] --&gt; b[0:1]  ['line1'] --&gt; ['line1']\n    move      a[1:2] --&gt; b[2:2]  ['line2'] --&gt; []\n    equal     a[2:3] --&gt; b[1:2]  ['line3'] --&gt; ['line3']\n    moved     a[1:1] --&gt; b[2:3]         [] --&gt; ['line2']\n    equal     a[3:4] --&gt; b[3:4]  ['line4'] --&gt; ['line4']\n    replace   a[4:5] --&gt; b[4:5]  ['line5'] --&gt; ['line6']\n\n# Text Diff\n\nGenerating diff for input texts with (optional) similar lines changes detection.\n\n    from mdiff import diff_lines_with_similarities, CompositeOpCode\n    \n    a = 'line1\\nline2\\nline3\\nline4\\nline5'\n    b = 'line1\\nline3\\nline2\\nline4\\nline6'\n    a_lines, b_lines, opcodes = diff_lines_with_similarities(a, b, cutoff=0.75)\n    \n    # Just printing diff on a line level, and nested diff on a character level for similar lines\n    for opcode in opcodes:\n        tag, i1, i2, j1, j2 = opcode\n        print('{:7}   a_lines[{}:{}] --&gt; b_lines[{}:{}] {!r:&gt;10} --&gt; {!r}'.\n              format(tag, i1, i2, j1, j2, a_lines[i1:i2], b_lines[j1:j2]))\n        if isinstance(opcode, CompositeOpCode) and opcode.children_opcodes:\n            for ltag, li1, li2, lj1, lj2 in opcode.children_opcodes:\n                print('\\t{:7}   a_lines[{}][{}:{}] --&gt; b_lines[{}][{}:{}] {!r:&gt;10} --&gt; {!r}'\n                      .format(ltag, i1, li1, li2, j1, lj1, lj2, a_lines[i1][li1:li2], b_lines[j1][lj1:lj2]))\n\nOutput:\n\n    equal     a_lines[0:1] --&gt; b_lines[0:1]  ['line1'] --&gt; ['line1']\n    move      a_lines[1:2] --&gt; b_lines[2:2]  ['line2'] --&gt; []\n    equal     a_lines[2:3] --&gt; b_lines[1:2]  ['line3'] --&gt; ['line3']\n    moved     a_lines[1:1] --&gt; b_lines[2:3]         [] --&gt; ['line2']\n    equal     a_lines[3:4] --&gt; b_lines[3:4]  ['line4'] --&gt; ['line4']\n    replace   a_lines[4:5] --&gt; b_lines[4:5]  ['line5'] --&gt; ['line6']\n    \tequal     a_lines[4][0:4] --&gt; b_lines[4][0:4]     'line' --&gt; 'line'\n    \treplace   a_lines[4][4:5] --&gt; b_lines[4][4:5]        '5' --&gt; '6'\n\n# App\n\n**mdiff** provides simple [CLI tool](https://github.com/m-matelski/mdiff#cli-tool) and [GUI app](https://github.com/m-matelski/mdiff#standalone-gui-application) for comparing files and texts. It can be used for visualising and testing different diff algorithms (also Python built-in difflib.SequenceMatcher).\n\nhttps://preview.redd.it/tu88e3lhrgt81.png?width=1211&amp;format=png&amp;auto=webp&amp;s=45b602c8fed5b89d0dfc1228f889c44a1bca397f", "upvote_ratio": 0.86, "id": "t3_u3glpx", "created_utc": 1649939930.0}
{"sub": "Python", "title": "HackerRank or Leetcode?", "selftext": "In which one do you guys prefer to practice, and why?", "upvote_ratio": 0.85, "id": "t3_u3ghm3", "created_utc": 1649939555.0}
{"sub": "Python", "title": "Running your scheduled Python tasks on Heroku? You can now natively monitor them! \ud83d\udc7e\ud83d\udc7e\ud83d\udc7e", "selftext": "Hi devs!\n\nDo you, like so many others, use one-off dynos to run your scheduled tasks on Heroku?\n\nDo you feel like one-off dynos running in the background are kind of invisible?\n\nHave a look at the [One-off Dyno Metrics Heroku add-on](https://elements.heroku.com/addons/one-off-metrics)!\n\nThis Heroku add-on plots the execution times, throughput, concurrency, and dyno events of your one-off dynos. It also provides threshold alerting, allowing you to monitor important stuff. You will know exactly how your one-off dynos behave, and what is going wrong.\n\nThe add-on is about to become generally available. Last chance to install during the beta and get full access for a month after launch!", "upvote_ratio": 0.81, "id": "t3_u3gaab", "created_utc": 1649938900.0}
{"sub": "Python", "title": "DataSpell 2022.1 Released", "selftext": "nan", "upvote_ratio": 0.64, "id": "t3_u3d9w5", "created_utc": 1649928154.0}
{"sub": "Python", "title": "Using Python to create large scale SEM campaigns in minutes.", "selftext": " Join our next webinar on Wednesday 20/04/2022 at 12 PM CET.  \nOur next guest will be Elias Dabbas, the creator of advertools. He'll share how he's using Python to create large scale SEM campaigns in minutes using advertools.\n\nTopics covered:  \n\\- Generating Keywords for SEM Campaigns  \n\\- Creating Ads Using Long Descriptive Text  \n\\- Creating Ads on a Large Scale\n\n Register here: [https://www.linkedin.com/events/usingpythontocreatelargescalese6918192144046338048/about/](https://www.linkedin.com/events/usingpythontocreatelargescalese6918192144046338048/about/) \n\n[https://github.com/eliasdabbas/advertools](https://github.com/eliasdabbas/advertools)", "upvote_ratio": 0.4, "id": "t3_u39d7x", "created_utc": 1649911596.0}
{"sub": "Python", "title": "Python as a career", "selftext": "I am an Engineer and have a little background in coding using R. \n\nIs it possible I can learn Python to make it a side hustle? \n\nI welcome your advice. Thanks", "upvote_ratio": 0.82, "id": "t3_u379ba", "created_utc": 1649904315.0}
{"sub": "Python", "title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!", "selftext": "Discussion of using Python in a professional environment, getting jobs in Python as well as ask questions about courses to further your python education!\n\n**This thread is not for recruitment, please see** r/PythonJobs **or the thread in the sidebar for that.**", "upvote_ratio": 0.63, "id": "t3_u343wx", "created_utc": 1649894414.0}
{"sub": "Python", "title": "Learn how to write clean Python code with this free ebook", "selftext": "Hi!\n\nI wrote [Cleaner Python](https://ezzeddin.gumroad.com/l/cleaner-python) with the intention to help developers who are getting started in writing clean code in Python.\n\nI'd love to hear your feedback and please let me know if you have any questions.", "upvote_ratio": 0.58, "id": "t3_u32r51", "created_utc": 1649890439.0}
{"sub": "Python", "title": "Chances of getting a job for smb. learned python at home", "selftext": "Hey folks! I hope it\u2019s the place that I can post this question. Let me give you a brief info about my background. \n\nI\u2019ve studied economics and worked in international sales. After my gf got a job from a Swedish company we decided to move there. We will be there by August if everything went as we want. So I was thinking of learning software to change my career path. \n\nDuring pandemic I learnt SQL but than didn\u2019t develop myself. So now I started with python which I think easier than many languages. But what I am wondering is if I can get a job or not. I know that it depends on what and how much I learnt and etc. but what you guys think or recommend? \n\nHonestly speaking, I think I can get a job as an intern or a junior. Especially after Covid the demand for developers increased rapidly. So I am not pessimistic about this idea. After python I am planning to learn JavaScript too. \n\nAnyway. I am waiting for your answers and valuable feedbacks!", "upvote_ratio": 0.6, "id": "t3_u32l5z", "created_utc": 1649889953.0}
{"sub": "Python", "title": "What are your most wanted quality indicators for Python code?", "selftext": "Dear,\n\nI'm trying to list the good and bad practices regarding Python code to write a document to help the teams to assess progress points.\n\nFor example in good practices:\n\n* Comments\n* Typing\n* Docstrings\n* Sphinx documentation\n* Readable variable names\n* Git\n* CI/CD\n* Unit Tests\n* ...\n\nFor pollutions:\n\n* Deprecated code with no warning\n* Commented code everywhere\n* Full paths\n* ...\n\nI would like to know what are your own pain points and what you like, or hate, when you inherit from someone else code.\n\nThanks", "upvote_ratio": 0.71, "id": "t3_u31bkw", "created_utc": 1649886437.0}
{"sub": "Python", "title": "Alarm-Clock made with Python and Kivy", "selftext": "Hi! I'm here to share with you a little project I developed as a hobby last year: an alarm-clock made with Python and Kivy! I'd appreciate feedbacks :3\n\n[https://github.com/v0di/alarm-clock](https://github.com/v0di/alarm-clock)", "upvote_ratio": 1.0, "id": "t3_u2zz5r", "created_utc": 1649882880.0}
{"sub": "Python", "title": "PyCharm 2022.1 released", "selftext": "nan", "upvote_ratio": 0.96, "id": "t3_u2vp01", "created_utc": 1649871165.0}
{"sub": "Python", "title": "The fastest tool for querying large JSON files is written in Python! (benchmark)", "selftext": "[spyql](https://github.com/dcmoura/spyql) is a tool (and python lib) for querying and transforming data. It is fully written in Python.\n\nIn the [latest benchmark](https://colab.research.google.com/github/dcmoura/spyql/blob/master/notebooks/json_benchmark.ipynb), spyql outperformed all other tools, including jq, one of the most popular tools written in C.\n\nHere is one example extracted from the benchmark that shows spyql achieving the lowest processing time while keeping memory requirements low when the dataset size is &gt;= 100MB.\n\n&amp;#x200B;\n\n[Processing time and memory requirements vs size of input JSON data](https://preview.redd.it/ejgy2k0gwbt81.png?width=1315&amp;format=png&amp;auto=webp&amp;s=782958571e0a5a7309484011fbc0d7c1c9da5015)\n\nIMO, these results might questions some preconceived opinions about Python\u2019s performance and interpreted languages in general.\n\nThe benchmark is very easy to reproduce without installing any software since it runs on a [google colab notebook](https://colab.research.google.com/github/dcmoura/spyql/blob/master/notebooks/json_benchmark.ipynb).\n\nHappy to hear your thoughts!\n\n**UPDATE 2022/04/22**\n\nThank you all for your feedback. The benchmark was updated and the fastest tool is **NOT** written in Python. Here are the highlights:\n\n* Added ClickHouse (written in C++) to the benchmark: I was unaware that the clickhouse-local tool would handle these tasks. ClickHouse is now the fastest (together with OctoSQL);\n* OctoSQL (written in Go) was updated as a response to the benchmark: updates included switching to fastjson, short-circuiting LIMIT, and eagerly printing when outputting JSON and CSV. Now, OctoSQL is one of the fastest and memory is stable;\n* SPyQL (written in Python) is now third: SPyQL leverages orjson (Rust) to parse JSONs, while the query engine is written in Python. When processing 1GB of input data, SPyQL takes 4x-5x more time than the best, while still achieving up to 2x higher performance than jq (written in C);\n* I removed Pandas from the benchmark and focused on command-line tools. I am planning a separate benchmark on Python libs where Pandas, Polars and Modin (and eventually others) will be included.\n\nThis benchmark is a living document. If you are interested in receiving updates, please subscribe to the following issue: [https://github.com/dcmoura/spyql/issues/72](https://github.com/dcmoura/spyql/issues/72)\n\nThank you!", "upvote_ratio": 0.92, "id": "t3_u2v858", "created_utc": 1649869896.0}
{"sub": "Python", "title": "Ohio State University Researchers Develop SAT2LoD2: An Open-Source Python Tool For 3D Landscape Modelling Using Satelite Imagery", "selftext": "3D landscape modeling has seen a rise in its popularity and applications in recent years. It has countless applications in the fields of civil engineering, earth sciences, military applications, and many others. Geometric 3D models are typically developed using the city geography markup language (CityGML), and the Level-of-Detail (LoD) building model is the preferred model for building 3D models using CityGML.\u00a0\n\nThe use of Satellite imagery for landscape modeling provides the advantage of covering a wide area and is low cost. However, developing LoD2 models using satellite imagery remains a big challenge. Building models in such a way involves complex steps demanding heuristics-based approaches and ML-based detection paradigms.\n\nIn a recent paper, researchers at the Ohio State University propose a [SAT2LoD2 ](https://arxiv.org/pdf/2204.04139v1.pdf)to facilitate the development of 3D landscape models. SAT2LoD2 is an open-source, python-based GUI-enabled software that takes the satellite images as inputs and returns LoD2 building models as outputs. The software also has the feature of taking road networks and custom maps as additional inputs for better results.\n\n[Continue Reading](https://www.marktechpost.com/2022/04/13/ohio-state-university-researchers-develop-sat2lod2-an-open-source-python-tool-for-3d-landscape-modelling-using-satelite-imagery/)\n\nPaper: https://arxiv.org/pdf/2204.04139v1.pdf\n\nGithub: https://github.com/gdaosu/lod2buildingmodel", "upvote_ratio": 1.0, "id": "t3_u2v7m9", "created_utc": 1649869856.0}
{"sub": "Python", "title": "typeforce: Make mypy more effective", "selftext": "\n**Typeforce** is a CLI tool that enriches your Python environment with type annotations, empowering [mypy](https://mypy.readthedocs.io/en/stable/).\n\nIn particular:\n\n+ Generates `py.typed` for annotated packages.\n+ Installs missed stub files and plugins.\n\nhttps://github.com/orsinium-labs/typeforce", "upvote_ratio": 0.79, "id": "t3_u2sbgj", "created_utc": 1649862123.0}
{"sub": "Python", "title": "`nme` - package to simplify data persistence when upgrading data structures", "selftext": "I would like to share my python package `nme`. This package is for simplifying loading data from the older versions of code. It was initially created for [napari](https://napari.org/) plugins for improving science reproducibility, but I think that it may be useful for other projects. \n\nCode is here: https://github.com/Czaki/nme \nDocumentation: https://nme.readthedocs.io/en/latest/?badge=latest\n\nCurrently, it supports `json` and `cbor2` as backends for serialization. \n\nI'm open to any improvement suggestions.", "upvote_ratio": 0.5, "id": "t3_u2s3uy", "created_utc": 1649861576.0}
{"sub": "Python", "title": "Why does the Development Company use the Python web framework?", "selftext": "nan", "upvote_ratio": 0.13, "id": "t3_u2rvxb", "created_utc": 1649860950.0}
{"sub": "Python", "title": "Minimalist dependency injection in Python", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_u2pow9", "created_utc": 1649854549.0}
{"sub": "Python", "title": "I made a video about Cross Validation using python's sklearn package. An extremely important yet often overlooked topic in machine learning.", "selftext": "nan", "upvote_ratio": 0.36, "id": "t3_u2pmet", "created_utc": 1649854324.0}
{"sub": "Python", "title": "Python 3.11 is Coming! Here\u2019s How It Fares Against Python 3.10", "selftext": "nan", "upvote_ratio": 0.8, "id": "t3_u2orpk", "created_utc": 1649851479.0}
{"sub": "Python", "title": "Do you have hundreds of old and embarrassing tweets? Here's a script to delete them all.", "selftext": "I made a Python script to delete old tweets. Given a date, it'll delete all the tweets before that date. Personally, I had hundreds of tweets between my friends talking about hot boys in high school that I had forgotten all about but were all so public \ud83e\udd26\ud83c\udffb\u200d\u2640\ufe0f.\n\nGive it a try: [https://github.com/yaylinda/delete-tweets](https://github.com/yaylinda/delete-tweets)\n\nFeel free to make suggestions or improvements!", "upvote_ratio": 0.91, "id": "t3_u2o1al", "created_utc": 1649848859.0}
{"sub": "Python", "title": "Here is a script that turns your pc off when a download is finished", "selftext": "Here is a script that turns off your computer when a game on steam has finished downloading.\n\n    import os\n    import time\n    \n    while os.listdir('*insert your own steam downloading folder here*') != []:\n        print('Download in progress..')\n        time.sleep(5)\n    \n    print('Folder empty. Downloads complete.')\n    os.system(\"shutdown /s /t 1\")", "upvote_ratio": 0.86, "id": "t3_u2ln8f", "created_utc": 1649838771.0}
{"sub": "Python", "title": "Gotchas of early-bound function argument defaults in Python", "selftext": "nan", "upvote_ratio": 0.72, "id": "t3_u2iknk", "created_utc": 1649825890.0}
{"sub": "Python", "title": "Why do some functions have the arguments outside and some inside? e.g: len(example) vs example.upper() ?", "selftext": "This is the case too in other languages right?", "upvote_ratio": 0.82, "id": "t3_u2gg0a", "created_utc": 1649818670.0}
{"sub": "Python", "title": "A python framework for unstructured data processing", "selftext": "Unstructured data is information that is not arranged according to a predefined schema or data model. Image, text, video, and nested JSON are the most common types of unstructured data we collected in real-world applications.\n\nWe have just released [Towhee 0.6](https://github.com/towhee-io/towhee), a framework for doing ML jobs over unstructured data. Our latest release includes [`DataCollection`](https://towhee.readthedocs.io/en/branch0.6/data_collection/get_started.html), a new user-centric method-chaining API that enables rapid development and prototyping of unstructured data applications.\n\n`DataCollection` is designed to behave as a python list or generator, with some enhancement features such as `method-chaining coding style`, `parallel execution`, and `exception handling`. Here is a short example of image animation:\n\n```python\nimport towhee\n\ntowhee.glob['path']('./test.png') \\\n      .image_decode['path', 'origin']() \\\n      .img2img_translation.animegan['origin', 'facepaintv2'](model_name = 'facepaintv2') \\\n      .img2img_translation.animegan['origin', 'hayao'](model_name = 'hayao') \\\n      .img2img_translation.animegan['origin', 'paprika'](model_name = 'paprika') \\\n      .img2img_translation.animegan['origin', 'shinkai'](model_name = 'shinkai') \\\n      .select['origin', 'facepaintv2', 'hayao', 'paprika', 'shinkai']() \\\n      .show()\n```\n\n[`image_decode`](https://towhee.io/image-decode/cv2) and [`img2img_translation.animegan`](https://towhee.io/img2img-translation/animegan) are predefined `operator`s from [towhee hub](https://towhee.io). We already have nearly a hundred operators, officially maintained or contributed by our users.\n\nYou can check the result from [the tutorial](https://github.com/towhee-io/towhee/blob/main/tutorials/anime_style_transformer.ipynb). \n\nDocumentation for DataCollection is available [here](https://towhee.readthedocs.io/en/branch0.6/data_collection/get_started.html). We will be releasing code examples and tutorials using the new API in the upcoming weeks.\n\nWould appreciate some feedback and contribution :)", "upvote_ratio": 0.67, "id": "t3_u2g5ha", "created_utc": 1649817729.0}
{"sub": "Python", "title": "Do people generally write Sphinx API documentation using autoapi or manually?", "selftext": "(this might not be the best subreddit, but I didn't see a Sphinx subreddit, if there's a better place, let me know)\n\nI'm working on writing my first distributable python package and I'm trying to be very meticulous about the documentation, making a readthedocs page for it. I'm very new to using Sphinx, and I'm confused on the landscape of projects that have autogenerated API documentation vs. those that involve manually written documentation. \n\n\nI have been able to get the `autoapi` extension to work, but the documentation it generates feels very boilerplate, and I want to go in and reformat some of the stuff it does, which I think it's possible to essentially \"convert\" a project from auto to manual. So maybe I should autogenerate it at first and then convert it for later changes. I have some concerns about that approach though.\n\n\n1. What if I add more stuff to the API later, or update the way certain functions work? Would I just need to do those small changes by hand? I feel like if I reverted to the autogeneration it would overwrite my manual changes (maybe I'm wrong though).\n\n2. I actually prefer to code at the same time that I am documenting, so that the documentation is not an afterthought, but in that case I would want to do the manual changes at the same time as when I code, so maybe autogeneration is a bad idea for me. But this particular project is collaborative and I think I'm probably the only one in my group who *enjoys* the documentation, so I want to be sure it's super easy for future collaborators.\n\nAny thoughts at all would be greatly appreciated! I seem to be unable to find answers to a general best practices question like this", "upvote_ratio": 0.81, "id": "t3_u2dfzz", "created_utc": 1649809416.0}
{"sub": "Python", "title": "Wednesday Daily Thread: Beginner questions", "selftext": "New to Python and have questions? Use this thread to ask anything about Python, there are no bad questions!\n\nThis thread may be fairly low volume in replies, if you don't receive a response we recommend looking at r/LearnPython or joining the Python Discord server at [https://discord.gg/python](https://discord.gg/python) where you stand a better chance of receiving a response.", "upvote_ratio": 1.0, "id": "t3_u2cz7f", "created_utc": 1649808018.0}
{"sub": "Python", "title": "My first working code piece!", "selftext": "print ('What is the temperature today?')\n\nimport random\n\nTemperature = random.randint(10,40)\n\nprint(Temperature) \n\nprint ('Degrees')\n\n&amp;#x200B;\n\nif Temperature &gt; 24:\n\nprint ('Its a hot day,')\n\nprint ('Make sure to drink some water!')\n\n&amp;#x200B;\n\nif Temperature &lt;24:\n\nprint ('Its not to hot today!')\n\n&amp;#x200B;\n\nif Temperature == 24:\n\nprint ('Its a hot day,')\n\nprint ('Make sure to drink some water!')  \n\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nI am happy that it works!", "upvote_ratio": 0.6, "id": "t3_u2ccc8", "created_utc": 1649806110.0}
{"sub": "Python", "title": "Name a better learning resource than Schafer Corey, I'll wait", "selftext": "I am really amazed by Schafer Corey on [YouTube](https://www.youtube.com/c/Coreyms/videos) especially since I am not the the type of guy that enjoys watching videos to learn, I am honestly in awe with his teaching skills and it inspires me to write blogs. I will be very curious to see if you guys have other high quality content.\nI am well aware that you won't become proficient just by watching his videos but his tutorials get straight to the point and you understand the concept and you can build new things!", "upvote_ratio": 0.87, "id": "t3_u2b3r9", "created_utc": 1649802620.0}
{"sub": "Python", "title": "5 months of python, Beginning an AlgoBot, TEAR MY CODE to Shred to make me better.", "selftext": "nan", "upvote_ratio": 0.88, "id": "t3_u27epy", "created_utc": 1649792143.0}
{"sub": "Python", "title": "python programming quick look", "selftext": "nan", "upvote_ratio": 0.2, "id": "t3_u25pul", "created_utc": 1649787655.0}
{"sub": "Python", "title": "TIL Dropbox started, client and server, with Python and even hired the creator of Python", "selftext": "nan", "upvote_ratio": 0.4, "id": "t3_u255tq", "created_utc": 1649786229.0}
{"sub": "Python", "title": "2 Level Security system (Arduino Keypad + Face recognition ) using Python", "selftext": "nan", "upvote_ratio": 0.91, "id": "t3_u24o2f", "created_utc": 1649784906.0}
{"sub": "Python", "title": "fstring.help: Python f-string guide", "selftext": "nan", "upvote_ratio": 0.54, "id": "t3_u21wcl", "created_utc": 1649777711.0}
{"sub": "Python", "title": "20 Python Interview Questions To Challenge Your Knowledge", "selftext": "nan", "upvote_ratio": 0.44, "id": "t3_u218g8", "created_utc": 1649775942.0}
{"sub": "Python", "title": "Announcing Quart-DB", "selftext": "[Quart-DB](https://github.com/pgjones/quart-db) is a Quart extension that provides managed connection(s) to postgresql database(s).\n\nOnce initialised it will, by default, add a connection on `g` for every request usable via `g.connection`. Alternatively connections (and transactions) can be managed directly and explicitly.\n\nThe queries can be constructed using named `:name` parameters or `$1` positional parameters.\n\nQuart-DB uses [asyncpg](https://github.com/MagicStack/asyncpg) to manage the connections and [buildpg](https://github.com/samuelcolvin/buildpg) to parse the named parameter bindings.\n\n    from quart import g, Quart, websocket\n    from quart_db import QuartDB\n    \n    app = Quart(__name__)\n    db = QuartDB(app, url=\"postgresql://user:pass@localhost:5432/db_name\")\n    \n    @app.get(\"/&lt;int:id&gt;\")\n    async def get_count(id: int):\n        result = await g.connection.fetch_val(\n            \"SELECT COUNT(*) FROM tbl WHERE id = :id\",\n            {\"id\": id},\n        )\n        return {\"count\": result}\n    \n    @app.post(\"/\")\n    async def set_with_transaction():\n        async with g.connection.transaction():\n            await db.execute(\"UPDATE tbl SET done = $1\", [True])\n            ...\n        return {}\n    \n    @app.get(\"/explicit\")\n    async def explicit_usage():\n         async with db.connection() as connection:\n             ...", "upvote_ratio": 0.81, "id": "t3_u20nde", "created_utc": 1649774365.0}
{"sub": "Python", "title": "I wrote a tutorial on how to use pytest to write good Python code. I hope somebody finds it useful!", "selftext": "[https://github.com/rhayes777/workshop](https://github.com/rhayes777/workshop)", "upvote_ratio": 0.97, "id": "t3_u206vp", "created_utc": 1649773093.0}
{"sub": "Python", "title": "Shades: a module to make art with python", "selftext": "Over the last year or so, I've been developing a python  library to make it easier for me to make maths-ey generative art in python.\n\nI shared stuff a while back while I was still developing, and finally got stuff in a state where I'm pretty happy with things, so thought I'd share here just in case any other python fans might like it.\n\n[Here's a link if you're interested!](https://github.com/benrutter/Shades)  \n\n\nhttps://preview.redd.it/a9obpj02v3t81.png?width=2000&amp;format=png&amp;auto=webp&amp;s=4cd73bba9cab4e875772a32091c3ef49057372bb", "upvote_ratio": 0.97, "id": "t3_u1zzlq", "created_utc": 1649772545.0}
{"sub": "Python", "title": "Minesweeper but without any play-ability", "selftext": "This is the basic layout of a minesweeper field fished\n\n&amp;#x200B;\n\n    import curses\n    from curses import wrapper\n    from random import randint\n    \n     \n    \n    \n    field = [\n        [\"#\", \"#\", \"#\", \"#\", \"#\", \"#\", \"#\", \"#\", \"#\"],\n        [\"#\", \"#\", \"#\", \"#\", \"#\", \"#\", \"#\", \"#\", \"#\"],\n        [\"#\", \"#\", \"#\", \"#\", \"#\", \"#\", \"#\", \"#\", \"#\"],\n        [\"#\", \"#\", \"#\", \"#\", \"#\", \"#\", \"#\", \"#\", \"#\"],\n        [\"#\", \"#\", \"#\", \"#\", \"#\", \"#\", \"#\", \"#\", \"#\"],\n        [\"#\", \"#\", \"#\", \"#\", \"#\", \"#\", \"#\", \"#\", \"#\"],\n        [\"#\", \"#\", \"#\", \"#\", \"#\", \"#\", \"#\", \"#\", \"#\"],\n        [\"#\", \"#\", \"#\", \"#\", \"#\", \"#\", \"#\", \"#\", \"#\"],\n        [\"#\", \"#\", \"#\", \"#\", \"#\", \"#\", \"#\", \"#\", \"#\"]\n    ]\n    \n    \n    def print_maze(maze, stdscr,):\n        BLUE = curses.color_pair(1)\n        RED = curses.color_pair(2)\n    \n        bomb_Location = bomb_location()\n    \n        for i, row in enumerate(maze):\n            for j, value in enumerate(row):\n                \n                if (i,j) in bomb_Location:\n                    stdscr.addstr(i,j*2, 'x', RED)\n                else:\n                    \n                    amount = find_amount_bomb(field,i,j,bomb_Location)\n    \n                    stdscr.addstr(i,j * 2, amount, BLUE)\n    \n    \n    def bomb_location():\n        bombLocation = []\n    \n        for x in range(25):\n            bomb_loc = bomb()\n            bombLocation.append(bomb_loc)\n        return bombLocation\n    \n    \n    def find_amount_bomb(field, row, col, bomb_loc=[]):\n        \n        \n        bombs_nearby = 0\n    \n        neighbours = find_neigbours(field,row,col)\n    \n        for neighbour in neighbours:\n            if neighbour in bomb_loc:\n                bombs_nearby += 1\n        \n        return str(bombs_nearby)\n    \n    \n    \n    def find_neigbours(field,row,col):\n        neighbors = []\n        \n        if row &gt; 0 and col &lt; len(field[0]):\n            neighbors.append((row -1, col +1))\n    \n        if row &gt; 0 and col &gt; 0:\n            neighbors.append((row -1, col -1))\n    \n        if row &lt; len(field) and col &lt; len(field[0]):\n            neighbors.append((row +1, col +1))\n    \n        if row &lt; len(field) and col &gt; 0:\n            neighbors.append((row +1, col -1))     \n    \n        if row &gt; 0: # up\n            neighbors.append((row -1, col))\n    \n        if row &lt; len(field): # down\n            neighbors.append((row + 1, col))\n    \n        if col &gt; 0: # left\n            neighbors.append((row, col - 1))\n    \n        if col &lt; len(field[0]): # right \n            neighbors.append((row, col + 1))\n    \n        return neighbors \n    \n    \n    def bomb():\n        i = randint(0,8)\n        j = randint(0,8) \n        return i,j\n    \n    \n    def main(stdscr):\n        curses.init_pair(1, curses.COLOR_CYAN, curses.COLOR_BLACK)\n        curses.init_pair(2, curses.COLOR_RED, curses.COLOR_BLACK)\n    \n        stdscr.clear()\n        print_maze(field,stdscr) \n        stdscr.refresh() \n    \n        stdscr.getch()\n    \n    wrapper(main) \n\nidk if this is intermediate or beginner sorry if it is beginner", "upvote_ratio": 0.5, "id": "t3_u1zxp1", "created_utc": 1649772396.0}
{"sub": "Python", "title": "Python News: What's New From March 2022? \u2013 Real Python", "selftext": "nan", "upvote_ratio": 0.75, "id": "t3_u1yevy", "created_utc": 1649768051.0}
{"sub": "Python", "title": "Python code guidelines for unified, streamlined development", "selftext": "In Python programming, there are many things that developers have to consider and keep in mind when writing code. Those issues and practices differ from company to company and from team to team. At Evrone, we created our own collection of guidelines for Python, in order to build a common denominator for writing code within the company.\n\n[Read the guidelines here!](https://evrone.com/python-guidelines)", "upvote_ratio": 0.6, "id": "t3_u1xzt3", "created_utc": 1649766739.0}
{"sub": "Python", "title": "In addition to a properly formatted and stylish data presentation, we will also apply other helpful functionality, such as adding data to the database.", "selftext": "[https://medium.com/codex/how-to-add-new-rows-into-relational-tables-effortlessly-f6685a60eef2](https://medium.com/codex/how-to-add-new-rows-into-relational-tables-effortlessly-f6685a60eef2)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/7c47g2x903t81.png?width=875&amp;format=png&amp;auto=webp&amp;s=26f50b234e17feb3a3a269582a59e174016461b3", "upvote_ratio": 0.5, "id": "t3_u1wky9", "created_utc": 1649762014.0}
{"sub": "Python", "title": "Scraping Google Finance Ticker in Python", "selftext": "While programming is kinda easier than a stock market, you can do things programmatically, for example scraping Google Finance Ticker data in Python. \n\nHere's a working example to do exactly that, plus basic usage of Nasdaq API which Google is using, among [other data providers which Google Finance uses that you can find under Google's Disclaimer](https://www.google.com/intl/en_UA/googlefinance/disclaimer/). \n\nA gist to the same code below: https://gist.github.com/dimitryzub/a5e30389e13142b9262f52154cd56092\n\nFull code and [example in the online IDE](https://replit.com/@DimitryZub1/Scrape-Google-Finance-Ticker-Quote-in-Python#main.py):\n\n```python\nimport nasdaqdatalink\nimport requests, json, re\nfrom parsel import Selector\nfrom itertools import zip_longest\n\ndef scrape_google_finance(ticker: str):\n    params = {\n        \"hl\": \"en\" # language\n        }\n\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.60 Safari/537.36\",\n        }\n\n    html = requests.get(f\"https://www.google.com/finance/quote/{ticker}\", params=params, headers=headers, timeout=30)\n    selector = Selector(text=html.text)\n    \n    # where all extracted data will be temporary located\n    ticker_data = {\n        \"ticker_data\": {},\n        \"about_panel\": {},\n        \"news\": {\"items\": []},\n        \"finance_perfomance\": {\"table\": []}, \n        \"people_also_search_for\": {\"items\": []},\n        \"interested_in\": {\"items\": []}\n    }\n    \n    # current price, quote, title extraction\n    ticker_data[\"ticker_data\"][\"current_price\"] = selector.css(\".AHmHk .fxKbKc::text\").get()\n    ticker_data[\"ticker_data\"][\"quote\"] = selector.css(\".PdOqHc::text\").get().replace(\" \u2022 \",\":\")\n    ticker_data[\"ticker_data\"][\"title\"] = selector.css(\".zzDege::text\").get()\n    \n    # about panel extraction\n    about_panel_keys = selector.css(\".gyFHrc .mfs7Fc::text\").getall()\n    about_panel_values = selector.css(\".gyFHrc .P6K39c\").xpath(\"normalize-space()\").getall()\n    \n    for key, value in zip_longest(about_panel_keys, about_panel_values):\n        key_value = key.lower().replace(\" \", \"_\")\n        ticker_data[\"about_panel\"][key_value] = value\n    \n    # description \"about\" extraction\n    ticker_data[\"about_panel\"][\"description\"] = selector.css(\".bLLb2d::text\").get()\n    ticker_data[\"about_panel\"][\"extensions\"] = selector.css(\".w2tnNd::text\").getall()\n    \n    # news extraction\n    if selector.css(\".yY3Lee\").get():\n        for index, news in enumerate(selector.css(\".yY3Lee\"), start=1):\n            ticker_data[\"news\"][\"items\"].append({\n                \"position\": index,\n                \"title\": news.css(\".Yfwt5::text\").get(),\n                \"link\": news.css(\".z4rs2b a::attr(href)\").get(),\n                \"source\": news.css(\".sfyJob::text\").get(),\n                \"published\": news.css(\".Adak::text\").get(),\n                \"thumbnail\": news.css(\"img.Z4idke::attr(src)\").get()\n            })\n    else: \n        ticker_data[\"news\"][\"error\"] = f\"No news result from a {ticker}.\"\n\n    # finance perfomance table\n    if selector.css(\".slpEwd .roXhBd\").get():\n        fin_perf_col_2 = selector.css(\".PFjsMe+ .yNnsfe::text\").get()           # e.g. Dec 2021\n        fin_perf_col_3 = selector.css(\".PFjsMe~ .yNnsfe+ .yNnsfe::text\").get()  # e.g. Year/year change\n        \n        for fin_perf in selector.css(\".slpEwd .roXhBd\"):\n            if fin_perf.css(\".J9Jhg::text , .jU4VAc::text\").get():\n                perf_key = fin_perf.css(\".J9Jhg::text , .jU4VAc::text\").get()   # e.g. Revenue, Net Income, Operating Income..\n                perf_value_col_1 = fin_perf.css(\".QXDnM::text\").get()           # 60.3B, 26.40%..   \n                perf_value_col_2 = fin_perf.css(\".gEUVJe .JwB6zf::text\").get()  # 2.39%, -21.22%..\n                \n                ticker_data[\"finance_perfomance\"][\"table\"].append({\n                    perf_key: {\n                        fin_perf_col_2: perf_value_col_1,\n                        fin_perf_col_3: perf_value_col_2\n                    }\n                })\n    else:\n        ticker_data[\"finance_perfomance\"][\"error\"] = f\"No 'finence perfomance table' for {ticker}.\"\n    \n    # \"you may be interested in\" results\n    if selector.css(\".HDXgAf .tOzDHb\").get():\n        for index, other_interests in enumerate(selector.css(\".HDXgAf .tOzDHb\"), start=1):\n            ticker_data[\"interested_in\"][\"items\"].append(discover_more_tickers(index, other_interests))\n    else:\n        ticker_data[\"interested_in\"][\"error\"] = f\"No 'you may be interested in` results for {ticker}\"\n    \n    \n    # \"people also search for\" results\n    if selector.css(\".HDXgAf+ div .tOzDHb\").get():\n        for index, other_tickers in enumerate(selector.css(\".HDXgAf+ div .tOzDHb\"), start=1):\n            ticker_data[\"people_also_search_for\"][\"items\"].append(discover_more_tickers(index, other_tickers))\n    else:\n        ticker_data[\"people_also_search_for\"][\"error\"] = f\"No 'people_also_search_for` in results for {ticker}\"\n        \n\n    return ticker_data\n\n\ndef discover_more_tickers(index: int, other_data: str):\n    \"\"\"\n    if price_change_formatted will start complaining,\n    check beforehand for None values with try/except and set it to 0, in this function.\n    \n    however, re.search(r\"\\d{1}%|\\d{1,10}\\.\\d{1,2}%\" should make the job done.\n    \"\"\"\n    return {\n            \"position\": index,\n            \"ticker\": other_data.css(\".COaKTb::text\").get(),\n            \"ticker_link\": f'https://www.google.com/finance{other_data.attrib[\"href\"].replace(\"./\", \"/\")}',\n            \"title\": other_data.css(\".RwFyvf::text\").get(),\n            \"price\": other_data.css(\".YMlKec::text\").get(),\n            \"price_change\": other_data.css(\"[jsname=Fe7oBc]::attr(aria-label)\").get(),\n            # https://regex101.com/r/BOFBlt/1\n            # Up by 100.99% -&gt; 100.99%\n            \"price_change_formatted\": re.search(r\"\\d{1}%|\\d{1,10}\\.\\d{1,2}%\", other_data.css(\"[jsname=Fe7oBc]::attr(aria-label)\").get()).group()\n        }\n\n\nscrape_google_finance(ticker=\"GOOGL:NASDAQ\")\n\n# outputs a JSON string\n```\n\n\nA basic example of retrieving time-series data using Nasdaq API:\n\n```python\nimport nasdaqdatalink\n\ndef nasdaq_get_timeseries_data():\n    nasdaqdatalink.read_key(filename=\".nasdaq_api_key\")\n    # print(nasdaqdatalink.ApiConfig.api_key) # prints api key from the .nasdaq_api_key file\n\n    timeseries_data = nasdaqdatalink.get(\"WIKI/GOOGL\", collapse=\"monthly\") # not sure what \"WIKI\" stands for\n    print(timeseries_data)\n\nnasdaq_get_timeseries_data()\n```\n\nOutputs a `pandas` `DataFrame`:\n\n```lang-none\n                Open     High      Low    Close      Volume  Ex-Dividend  Split Ratio    Adj. Open    Adj. High     Adj. Low   Adj. Close  Adj. Volume\nDate                                                                                                                                                  \n2004-08-31   102.320   103.71   102.16   102.37   4917800.0          0.0          1.0    51.318415    52.015567    51.238167    51.343492    4917800.0\n2004-09-30   129.899   132.30   129.00   129.60  13758000.0          0.0          1.0    65.150614    66.354831    64.699722    65.000651   13758000.0\n2004-10-31   198.870   199.95   190.60   190.64  42282600.0          0.0          1.0    99.742897   100.284569    95.595093    95.615155   42282600.0\n2004-11-30   180.700   183.00   180.25   181.98  15384600.0          0.0          1.0    90.629765    91.783326    90.404069    91.271747   15384600.0\n2004-12-31   199.230   199.88   192.56   192.79  15321600.0          0.0          1.0    99.923454   100.249460    96.578127    96.693484   15321600.0\n...              ...      ...      ...      ...         ...          ...          ...          ...          ...          ...          ...          ...\n2017-11-30  1039.940  1044.14  1030.07  1036.17   2190379.0          0.0          1.0  1039.940000  1044.140000  1030.070000  1036.170000    2190379.0\n2017-12-31  1055.490  1058.05  1052.70  1053.40   1156357.0          0.0          1.0  1055.490000  1058.050000  1052.700000  1053.400000    1156357.0\n2018-01-31  1183.810  1186.32  1172.10  1182.22   1643877.0          0.0          1.0  1183.810000  1186.320000  1172.100000  1182.220000    1643877.0\n2018-02-28  1122.000  1127.65  1103.00  1103.92   2431023.0          0.0          1.0  1122.000000  1127.650000  1103.000000  1103.920000    2431023.0\n2018-03-31  1063.900  1064.54   997.62  1006.94   2940957.0          0.0          1.0  1063.900000  1064.540000   997.620000  1006.940000    2940957.0\n\n[164 rows x 12 columns]\n```\n\nA line-by-line tutorial: https://serpapi.com/blog/scrape-google-finance-ticker-quote-data-in-python/", "upvote_ratio": 0.84, "id": "t3_u1vwze", "created_utc": 1649759567.0}
{"sub": "Python", "title": "Gufo Ping - The Python asyncio Ping library", "selftext": "[Gufo Ping](https://pypi.org/project/gufo-ping/) is the Python asyncio Ping library. Besides the clean and simple interface is the highly-efficient raw sockets manipulation library implemented in the [Rust](https://rust-lang.org/) language with [PyO3](https://pyo3.rs/) wrapper.\n\nPinging is simple:\n\nSend one echo request and await for reply:\n\n    ping = Ping()\n    rtt = await ping.ping(\"127.0.0.1\")\n\nSend a series of requests and await replies:\n\n    ping = Ping()\n    async for rtt in ping.iter_rtt(\"127.0.0.1\", count=5):\n        print(rtt)\n\nGufo Ping is fast, allowing to monitor 100 000+ hosts at once.\n\nGufo Ping is the part of the [Gufo Stack](https://gufolabs.com/products/gufo-stack/)  - the battle-proven technologies which drive the [NOC](https://getnoc.com/)", "upvote_ratio": 0.75, "id": "t3_u1vbxb", "created_utc": 1649757278.0}
{"sub": "Python", "title": "Create a Bluetooth LE repeater using Python to overcome the range limitation when transferring data", "selftext": "nan", "upvote_ratio": 0.88, "id": "t3_u1u01z", "created_utc": 1649751499.0}
{"sub": "Python", "title": "Natural syntax for units in Python", "selftext": "nan", "upvote_ratio": 0.88, "id": "t3_u1tgt0", "created_utc": 1649749144.0}
{"sub": "Python", "title": "What the Decorators in Plain Words | Python", "selftext": "What is a decorator in Python?  \nWhy and how should I use it?  \nHow can I simplify the debugging of decorators?  \nHow can I decorate functions with parameters?  \nHow can I pass arguments to a decorator?  \n\n\n[This article](https://medium.com/@vlad.bashtannyk/what-the-decorators-in-plain-words-python-b600623ea497) will provide you answers to all these questions in plain words with lots of examples! Check it out now!", "upvote_ratio": 0.69, "id": "t3_u1tglr", "created_utc": 1649749121.0}
{"sub": "Python", "title": "Instagram likes predi tor using decision tree", "selftext": "nan", "upvote_ratio": 0.4, "id": "t3_u1qkkd", "created_utc": 1649738057.0}
{"sub": "Python", "title": "'Python is like a toy programming language compared to C++'", "selftext": "nan", "upvote_ratio": 0.14, "id": "t3_u1qbdq", "created_utc": 1649736750.0}
{"sub": "Python", "title": "Porting from Windows to Linux: Python vs. Powershell?", "selftext": "From a search, it looks like this question comes up every couple of months and, like everything else in IT, the answer is \"It depends.\" So, here are my depends...\n\nTL;DR - My shop is running database servers on Windows with maintenance and health check scripts written in DOS .BAT files. The company plans to migrate to cloud servers running Linux using the same database software (DB2 LUW). I want to update our scripts to a new scripting language prior to migrating and port to the new platforms when the time comes. In terms of porting between platforms, and getting a team of DBAs used to a new scripting language, would you go with Python or Powershell? Python seems easier to learn and implement, but I think PS might port better. Your thoughts?\n\n---\n\nMy shop is running database servers on Windows (not SQL Server) with the intent to migrate to cloud servers running Linux at some point in the future yet to be determined. Our database maintenance scripts are DOS .BAT files. That alone is reason enough to rewrite in a better language and, while we're at, we'll build in intelligence, restartability, health checks, logging. Not a lot of activity from the scripts themselves as they mostly run utilities and not many will run in a day or at once.\n\nMy plan is to convert the DOS .BAT files to PS now and port to Linux when/if we migrate.  However, I'm seeing a lot about Python and looked over some code. The pros I see for it is it seems easy to pick up and not too fussy. I also see comments on its speed and low resource consumption. Those same comments also note that speed and resources aren't an issue as long as concurrency is low, as it is in my case.\n\nHow is Python in porting between OSes? In Powershell, you have to code path names using the .net objects in order to get OS-independent code. Is there a similar mechanism in Python or will things like drive names and path separator characters cause issues?\n\nAnother example in PS is 'sort'.  If you use the 'sort' command, it will invoke the command for that OS which may behave differently. To get OS-independence, you have to use the actual Powershell command Sort-Object.\n\nAre there similar work-arounds in Python for path names? Does Python have issues with system commands that are named the same in both OSes? Are there any other porting gotchas?\n\nEDIT to answer common questions: The DB software is DB2 from IBM. There's a mainframe version, which we run on z/OS, and an LUW version, where LUW stands for Linux/Unix/Windows. Although it runs on Windows, industry standard for DB2 LUW is to run on Linux or AIX. We're currently running DB2 LUW on Windows Servers. Job scheduling is handled on the mainframe with remote triggers to the servers to run the scripts. The scripts are running utilities like reorgs, backups, restoring backups to other servers and then sending completion or failed status back to the mainframe scheduler.", "upvote_ratio": 0.9, "id": "t3_u1ogz1", "created_utc": 1649730644.0}
{"sub": "Python", "title": "Tuesday Daily Thread: Advanced questions", "selftext": "Have some burning questions on advanced Python topics? Use this thread to ask more advanced questions related to Python.\n\n**If your question is a beginner question we hold a beginner Daily Thread tomorrow (Wednesday) where you can ask any question! We may remove questions here and ask you to resubmit tomorrow.**\n\nThis thread may be fairly low volume in replies, if you don't receive a response we recommend looking at r/LearnPython or joining the Python Discord server at [https://discord.gg/python](https://discord.gg/python) where you stand a better chance of receiving a response.", "upvote_ratio": 0.6, "id": "t3_u1lfq8", "created_utc": 1649721611.0}
{"sub": "Python", "title": "SQLite Database with Python", "selftext": "nan", "upvote_ratio": 0.5, "id": "t3_u1k58n", "created_utc": 1649717865.0}
{"sub": "Python", "title": "AI Aimbot Python Tutorial", "selftext": "Been working on this for a while. Hope this helps inspire, motivate, &amp; educate my fellow programmers.  https://youtu.be/ilsn-TvryyA", "upvote_ratio": 0.66, "id": "t3_u1ibo5", "created_utc": 1649712969.0}
{"sub": "Python", "title": "The Python on Microcontrollers Newsletter - free, open source, no spam ever", "selftext": "# Interested in Python? Especially on small devices?\n\nWith the **Python on Microcontrollers newsletter**, you get all the latest information in one place!\n\nThe Python on Microcontrollers newsletter is the place for the latest news involving Python on hardware. It arrives Tuesday morning with all the week\u2019s happenings.\n\n**Catch all the\u00a0weekly news on** [**Python for Microcontrollers**](https://www.adafruitdaily.com/)\u00a0with\u00a0[adafruitdaily.com](https://www.adafruitdaily.com/).\n\n&gt;This\u00a0*ad-free, spam-free*\u00a0weekly email is filled with\u00a0**CircuitPython**,\u00a0**MicroPython**, and\u00a0**Python**\u00a0information that you may have missed, all in one place!  \nYou get a summary of all the software, events, projects, and the latest hardware worldwide once a week, no ads!\n\nEnsure you catch the weekly Python on Hardware roundup\u2013 you can cancel anytime\u00a0**\u2013** [**try our spam-free newsletter today**](https://www.adafruitdaily.com/)**!**\n\n[**https://www.adafruitdaily.com/**](https://www.adafruitdaily.com/)", "upvote_ratio": 0.57, "id": "t3_u1hqnr", "created_utc": 1649711457.0}
{"sub": "Python", "title": "Introduction to Streamlit and Streamlit Components", "selftext": "Streamlit is an open-source app framework for Machine Learning and Data Science teams.\n\nIn this article we will show you how to build Streamlit apps and custom **Streamlit** Components, with the end goal of implementing Auth0 **authentication**.\n\n[Read more\u2026](https://auth0.com/blog/introduction-to-streamlit-and-streamlit-components/?utm_source=reddit&amp;utm_medium=sc&amp;utm_campaign=streamlit)", "upvote_ratio": 0.25, "id": "t3_u1f4tt", "created_utc": 1649704784.0}
{"sub": "Python", "title": "Free Python 3 Course", "selftext": "Updated Link...(not sure what went wrong with old link)\n\n[https://www.udemy.com/course/python-three-from-beginner-to-pro/?couponCode=E74A9ED7BF27445AE778](https://www.udemy.com/course/python-three-from-beginner-to-pro/?couponCode=E74A9ED7BF27445AE778)\n\n&amp;#x200B;\n\nI created a Python course for beginners. The part that I really worked a lot on was functions, scope, closures and decorators. I always found these topics a bit hard for beginners.The other section that has a lot of material is OOP: classes, instances, properties, instance methods, class methods, inheritance and the MRO(method resolution order).Applications include web development using a backend SQL DB and of course numpy and pandas.", "upvote_ratio": 0.67, "id": "t3_u1f1se", "created_utc": 1649704559.0}
{"sub": "Python", "title": "Python Tutorial Snippet - How to create a Stock Trading News Alert Application?", "selftext": "Python Tutorial Snippet - How to create a Stock Trading News Alert Application?  \n[https://www.youtube.com/watch?v=60Z7Fl0Ddag](https://www.youtube.com/watch?v=60Z7Fl0Ddag)\n\nhttps://preview.redd.it/6xw1gq36yxs81.png?width=2880&amp;format=png&amp;auto=webp&amp;s=6b312526559942d33fc91ac5737151e949a8b442", "upvote_ratio": 0.67, "id": "t3_u1e1vm", "created_utc": 1649700883.0}
{"sub": "Python", "title": "Crypto toolkit I wrote", "selftext": "Crypto toolkit I'm working on. Features are:\n\n* Convert crypto currencies into fiat or other currencies\n* Check current prices\n* List some top decentralized exchanges\n* Get ***today's*** info on coins\n* And more\n\nhttps://github.com/Waxxx333/cryptkit", "upvote_ratio": 0.31, "id": "t3_u1ddpi", "created_utc": 1649699112.0}
{"sub": "Python", "title": "Basic how to load/read and show images in Python (OpenCV)", "selftext": "nan", "upvote_ratio": 0.25, "id": "t3_u1daza", "created_utc": 1649698916.0}
{"sub": "Python", "title": "Open-source library that takes an AI model as input and produces an optimized version that runs much faster in inference", "selftext": "nan", "upvote_ratio": 0.79, "id": "t3_u1bbjw", "created_utc": 1649693636.0}
{"sub": "Python", "title": "python code for snake game | How to make snake game in Python", "selftext": "nan", "upvote_ratio": 0.33, "id": "t3_u1ap07", "created_utc": 1649691916.0}
{"sub": "Python", "title": "My first PIP package based on subprocesses", "selftext": "I never thought that one day I will release a pip package, last summer I was just learning the basics, and now I write public packages and serverless applications for AWS Lambda. Python is so great, friendly to learn, and the opportunities to develop with Python are countless. \n\nHere is the project: [https://pypi.org/project/cmagick/](https://pypi.org/project/cmagick/)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/2cgh9kqn3xs81.png?width=1364&amp;format=png&amp;auto=webp&amp;s=6383f2111fee0be5d2bc0cbefea4a19bf86f7c20", "upvote_ratio": 0.94, "id": "t3_u1a9ob", "created_utc": 1649690654.0}
{"sub": "Python", "title": "Is there any platform to share scripts, import and run them easily?", "selftext": "nan", "upvote_ratio": 0.37, "id": "t3_u19ok4", "created_utc": 1649689110.0}
{"sub": "Python", "title": "Question on heapq design - why no maxheap implementation?", "selftext": "I am working through grokking the coding interview and decided to use python due to it's readability and overall simplicity in its syntax. \n\nThis morning I started working on the 'two heaps' algorithms. It struck me as a bit odd that python or the writers of the heapq library decided to make all implementations of heap minheaps rather than adding some additional APIs for maxheaps. \n\nMaybe it's just me, but I find it a bit hard to reason through programs that make use of maxheaps. Having to remember to push a value multiplied by -1 and then do the same for retrieval feels a bit un-intuitive, but maybe it's just me. \n\nDoes anyone know of the reasoning behind not implementing them separately and adding a thin layer to the maxheaps to avoid having to do this? I'm mostly just curious if there was any discussion around it when heapq was created but haven't been able to find anything yet.", "upvote_ratio": 0.81, "id": "t3_u1858d", "created_utc": 1649684785.0}
{"sub": "Python", "title": "This Week Two Intermediate Articles - On Dunder Methods and Python with Docker / Docker-Compose", "selftext": "* [Dunder Methods in Python:  The Ugliest Awesome Sauce](https://codesolid.com/dunder-methods-in-python-the-ugliest-awesome-sauce/)  \nImplementing several dunder methods, along with design considerations for when they make sense or not.  Includes a tool for enumerating the existing dunder methods on an object with their help strings.\n* [How to Use Docker and Docker Compose with Python](https://codesolid.com/how-to-use-docker-with-python/)  \nIncludes a simple docker container for Flask and a full Django plus Postgres starter stack using Docker Compose.", "upvote_ratio": 0.86, "id": "t3_u16r3b", "created_utc": 1649680593.0}
{"sub": "Python", "title": "Monitor your Cluster Stack with Telegraf, InfluxDB and Grafana", "selftext": "We recently worked on monitoring our HPC stack which runs SLURM workload where we utilized telegraf, influxdb, and grafana. The idea is to ssh into the node which provides some status of the entire cluster, take collect data, parse that collected raw data and write to influxdb. For visualization of the collected data grafana is utilized. I think this would be a good read for anyone looking for data collection, data analysis, and engineering on the infrastructure.\n\nPlease give us a star if the repository has helped you learn something.  \n\n\nRepo: [https://github.com/bethgelab/slurm-monitoring-public](https://github.com/bethgelab/slurm-monitoring-public)\n\nThanks", "upvote_ratio": 0.8, "id": "t3_u15q0w", "created_utc": 1649677179.0}
{"sub": "Python", "title": "Microservices in 10 minutes - Minos tutorial", "selftext": "Hello everyone! We wanted to share the last tutorial that we have created to show how to create a project with a microservice architecture (with an API, event broker, discovery...) and its first microservice, in \\~10 minutes.\n\nThis is a very quick overview, but we hope that it will help you understand how to create much more complex projects.\n\nIf you have any doubts, don't hesitate to contact us at [Gitter](https://gitter.im/minos-framework/community) or at [Github](https://github.com/minos-framework/minos-python)!\n\n[https://www.youtube.com/watch?v=ZYair128ITg](https://www.youtube.com/watch?v=ZYair128ITg)", "upvote_ratio": 0.89, "id": "t3_u15nc9", "created_utc": 1649676917.0}
{"sub": "Python", "title": "Low Code Python has Arrived", "selftext": "nan", "upvote_ratio": 0.48, "id": "t3_u15h0k", "created_utc": 1649676263.0}
{"sub": "Python", "title": "QualityScaler 1.2.0 - Image/video upscaling &amp; enhancement app", "selftext": "&amp;#x200B;\n\n[GUI](https://preview.redd.it/1vms35d5ivs81.jpg?width=1372&amp;format=pjpg&amp;auto=webp&amp;s=13bb87852a28d95fea0a111cdd8f1f7da3e8c670)\n\nItch -&gt; [https://jangystudio.itch.io/qualityscaler](https://jangystudio.itch.io/qualityscaler)\n\nGithub -&gt; [https://github.com/Djdefrag/QualityScaler/releases/tag/1.2.0](https://github.com/Djdefrag/QualityScaler/releases/tag/1.2.0)\n\n&amp;#x200B;\n\n**Update 1.2.0**\n\n Bugfix / perf. improvement / UI changes\n\nNew\n\n* A new wonderful handmade icon :D\n\nBugfix/improvement\n\n* Fixed the problem of displaying error messages correctly\n* Library import improvements\n* Other bugfix &amp; code cleaning\n\nUI changes\n\n* Changed \"QualityScaler\" title position and background - to make space for new features ;) -\n* Other little changes", "upvote_ratio": 0.89, "id": "t3_u146jl", "created_utc": 1649671292.0}
{"sub": "Python", "title": "Rmse-Mse-Linear regression mpdel-What is RMSE and MSE in linear regression models?-InsideAIML", "selftext": "nan", "upvote_ratio": 0.57, "id": "t3_u12cxr", "created_utc": 1649663520.0}
{"sub": "Python", "title": "Ideal Coding Bootcamp", "selftext": "Hi everyone \ud83d\udc4b\n\nWhat would your ideal coding bootcamp experience look like? \n\nLet\u2019s say you were a beginner and had zero experience in programming. You find an affordable 6 week python course. What would you expect to walk away with and what do you think your next steps would be?", "upvote_ratio": 0.6, "id": "t3_u10b4g", "created_utc": 1649654868.0}
{"sub": "Python", "title": "Selenium with Python for Beginners + Sample website you can probe without violating our Terms of Service", "selftext": "- [Selenium With Python](https://www.practiceprobs.com/problemsets/selenium-with-python/)\n- [Selenium Playground](https://seleniumplayground.practiceprobs.com/)", "upvote_ratio": 0.96, "id": "t3_u0uwsq", "created_utc": 1649636354.0}
{"sub": "Python", "title": "Monday Daily Thread: Project ideas!", "selftext": "Comment any project ideas beginner or advanced in this thread for others to give a try! If you complete one make sure to reply to the comment with how you found it and attach some source code! If you're looking for project ideas, you might be interested in checking out Al Sweigart's, [\"The Big Book of Small Python Projects\"](https://inventwithpython.com/bigbookpython/) which provides a list of projects and the code to make them work.", "upvote_ratio": 0.88, "id": "t3_u0ujvk", "created_utc": 1649635212.0}
{"sub": "Python", "title": "Learning GUI for Git", "selftext": "nan", "upvote_ratio": 0.5, "id": "t3_u0sygl", "created_utc": 1649630335.0}
{"sub": "Python", "title": "Discord log bot", "selftext": "nan", "upvote_ratio": 0.5, "id": "t3_u04kxn", "created_utc": 1649545363.0}
{"sub": "Python", "title": "How To Make A Good Github Repository For Your Python Projects", "selftext": "nan", "upvote_ratio": 0.77, "id": "t3_u05eqn", "created_utc": 1649548116.0}
{"sub": "Python", "title": "Can someone recommend me ball python names pls", "selftext": "I'm getting a ball python and I want its name to be a pun related to the programming language, but I'm coming up mostly empty.\n\n&amp;#x200B;\n\nI was thinking of naming it pip, but I feel like there are better names I can't think of.", "upvote_ratio": 0.82, "id": "t3_u04y52", "created_utc": 1649546544.0}
{"sub": "Python", "title": "IndicatorManagement v0.4.0 - Management of mathematical/financial indicators", "selftext": "Hello. I am a Python developer and I am making my first Python PyPI module project.\n\nThe library's name is \"indicator-management\"; It's about management of mathematical/financial indicators.\n\nThe benefit of this library is that you can handle very large amount of data because this library does not store the whole data at once, instead it loads the data whenever the calculation is needed.\n\nThe source code and details are available at [here](https://github.com/McDic/IndicatorManagement).\n\nThis library is still under pre-alpha development. Your feedback and interest is appreciated!\n\n[Example usage with matplotlib](https://i.redd.it/r1vdpr865rs81.gif)", "upvote_ratio": 0.82, "id": "t3_u0opx3", "created_utc": 1649618139.0}
{"sub": "Python", "title": "What is the best practice for injecting configuration into a python application", "selftext": "I am working on a Flask App. I have a configuration class defined which has configs for Dev, Qa, Prod and Test. I followed the common practice of reading the config file only once and initialising the app with the config data\n\napp.config.from\\_object(config object)\n\nNow I need to access urls defined in the config class in other places. I tried accessing these configs using the current app proxy. But I realised that I also have to access these class outside the request/ app context as I have celery tasks which access them.\n\nOne approach is to pass this config as a variable to every class it is required, which I dont prefer. Another option is to annotate the config class as singleton and create the config object at every place where I need them.  I also came across this library called Dependency\\_Injector. [https://python-dependency-injector.ets-labs.org/](https://python-dependency-injector.ets-labs.org/) This seems a bit heavy weight for my use case though. I am looking forward to know how other solve this problem\n\nEdit: I should add that I am already using environment variables. I create. the config object based on the value of the env vars.", "upvote_ratio": 0.76, "id": "t3_u0j5rn", "created_utc": 1649602241.0}
{"sub": "Python", "title": "Exploring data in CockroachDB with Python and Pandas in DataStation", "selftext": "nan", "upvote_ratio": 0.5, "id": "t3_u0i6he", "created_utc": 1649599224.0}
{"sub": "Python", "title": "Space Science with Python - Autoencoders (concept)", "selftext": "Coders!\n\nI keep up my weekly tutorial sessions (some asked, whether I could increase it to 2 videos or more per week... But I have too many things to do... let's see how it continues in the long run!).\n\nAnyway, what did we do in the last couple of weeks? Using Python on Google Colab we:\n\n- Downloaded asteroid spectra + their corresponding class (whether it is e.g. a stony or iron obejct)\n- Parsed, cleaned and enriched the data\n- Created an interactive spectrum visualization tool in Colab\n- Conducted a Machine Learning experiment using scikit-learn and their SVM implementation\n- Created a neural network with Keras and optimized its architecture with Keras-Tuner to classify our data\n\nSo are we done?\n\nTheoretically yes. But I would like to conclude the asteroid part with an unsupervised ML algorithm: Autoencoders! Using this neural network architecture + some clustering algorithms I'd like to show how one can create an unsupervised classification method.\n\nBut what are Autoencoders exactly, and how do they work?\n\nWell to split up theory and coding a little bit, I created a small \"concept\" video on Autoencoders, so that we can start coding next time (knowing what we want to do and what to expect).\n\nI am not a CGI expert or big YouTuber. It's more \"seminar-like\" and I'd like to know your opinion on this, whether it's useful, or not.\n\nLink: https://youtu.be/ET441nffKjU\n\nSo what will we do next with Python, Keras and the asteroid data?\n\n- Next tutorial: creating an Autoencoder using Keras (we won't use Keras-Tuner, to keep things simple). We'll also check the \"reconstruction\" power of the Autoencoder that compresses the 49-dimensional spectra into 2 dimensions\n\n- Afterwards: how does the latent space look like? We'll create an interactive Jupyter Viewer to visualise the latent space and color the spectrum class corresponding latent values to determine whether we have a \"class separation\" or not\n\n- Last video of the entire project: Rebuild an Autoencoder with a larger latent space, and applying Gaussian Mixture Models (GMMs) to determine the number of possible classes using the Bayesian Information Criterion (BIC)\n\nWell... Afterwards a new Space + Python project will start :)\n\nHope you guys like it. I am looking forward to suggestions and ideas!\n\nSee you next week!\n\nThomas", "upvote_ratio": 0.86, "id": "t3_u0hr50", "created_utc": 1649597837.0}
{"sub": "Python", "title": "How do you pronounce libraries with `py` in the name?", "selftext": "[this comment](https://reddit.com/r/Python/comments/u04y52/_/i454o57/?context=1) got me thinking about how each developer likes to pronounce package names\n\nI have always pronounced `numpy` as \u201cnum-pie\u201d, but some people I\u2019ve worked with have been adamant it is \u201cnum-pee\u201d\n\nhow do you pronounce \u201cpy\u201d in names? (even if it\u2019s solely in your head)", "upvote_ratio": 0.92, "id": "t3_u0g45w", "created_utc": 1649591920.0}
{"sub": "Python", "title": "A Brief Introduction to PyQt", "selftext": "nan", "upvote_ratio": 0.7, "id": "t3_u0f053", "created_utc": 1649587047.0}
{"sub": "Python", "title": "YouTube version in case you missed it live: troubleshooting Python applications on Kubernetes (hunting down memory leaks, running cpu profilers, and using non-breaking debuggers)", "selftext": "nan", "upvote_ratio": 0.8, "id": "t3_u0etrg", "created_utc": 1649586186.0}
{"sub": "Python", "title": "Python Selenium Tutorial #8 - Read, Block &amp; Mock Requests using Selenium Wire", "selftext": "nan", "upvote_ratio": 0.89, "id": "t3_u0c8yx", "created_utc": 1649574107.0}
{"sub": "Python", "title": "Desktop stereo system made with Python", "selftext": "&amp;#x200B;\n\nhttps://reddit.com/link/u0c60e/video/fawvxu65gns81/player\n\nI absolutely love music, especially when coding. For a while now, I've been wanting to have a stound graphical analyzer to make y coding sessions more fun, so I decided to make my own. Here's the [code](https://github.com/BrickSigma/Desktop-stereo). I haven't documented the code yet or added any README file to it, but I will soon.\n\nThis was a lot of fun to make because I learnt a lot of new things about digital audio processing, like their format, and Fourier transformations, which I'm in love with now. This was also my first attempt at editing a video for public presentation, so I hope I've done justice to it.\n\nThanks for reading and have an amazing day!", "upvote_ratio": 0.85, "id": "t3_u0c60e", "created_utc": 1649573774.0}
{"sub": "Python", "title": "Short Rock, Paper, Scissors game.", "selftext": "What do you thin about it, can it get any shorter?\n\n[https://github.com/sat1ss/The-shortest-Rock-Paper-Scissors-game](https://github.com/sat1ss/The-shortest-Rock-Paper-Scissors-game)", "upvote_ratio": 0.71, "id": "t3_u0byeh", "created_utc": 1649572827.0}
{"sub": "Python", "title": "r/AskScience flair classifier using Praw and Fasttext", "selftext": "I wanted to learn how to use get data from reddit  and came across the praw library, so I decided to create a fasttext nlp model which classifies what flair a post should be on r/AskScience (because they have purely text posts and each question is flaired). Currently it works with the top 10 flairs, but I plan to add some improvements to it later on to include all flairs and perhaps other subreddits too. \n\nlink: [https://github.com/arnavkartikeya/RedditFlairClassifier](https://github.com/arnavkartikeya/RedditFlairClassifier)", "upvote_ratio": 0.75, "id": "t3_u085ro", "created_utc": 1649557796.0}
{"sub": "Python", "title": "HiQ - A Modern Observability System", "selftext": "HiQ([https://github.com/oracle-samples/hiq](https://github.com/oracle-samples/hiq)) is a declarative, non-intrusive, dynamic and transparent tracking system for both monolithic application and distributed system. It brings the runtime information tracking and optimization to a new level without compromising with speed and system performance, or hiding any tracking overhead information. HiQ applies for both I/O bound and CPU bound applications.\n\nTo explain the four features, declarative means you can declare the things you want to track in a text file, which could be a JSON, YAML or even CSV, and no need to change program code. Non-intrusive means HiQ doesn\u2019t requires to modify original python code. Dynamic means HiQ supports tracing metrics featuring at run time, which can be used for adaptive tracing. Transparent means HiQ provides the tracing overhead and doesn\u2019t hide it no matter it is huge or tiny.\n\nIn addition to latency tracking, HiQ provides memory, disk I/O and Network I/O tracking out of the box. The output can be saved in form of normal line by line log file, or HiQ tree, or span graph.\n\n## Installation\n\npip install hiq-python\n\n## Documentation\n\n**HTML**: \ud83d\udcf7[ HiQ Online Documents](https://hiq.readthedocs.io/en/latest/index.html)  \n**PDF**: Please check \ud83d\udcf7[ HiQ User Guide](https://github.com/oracle-samples/hiq/blob/main/hiq/docs/hiq.pdf).\n\n## Jupyter NoteBook\n\n### Add Observability to PaddleOCR\n\n* [Latency](https://github.com/oracle-samples/hiq/blob/henry_dev/hiq/examples/paddle/demo.ipynb)\n* [Memory](https://github.com/oracle-samples/hiq/blob/main/hiq/examples/paddle/demo_memory.ipynb)\n\n### Add Observability to AlexNet\n\n* [Latency](https://github.com/oracle-samples/hiq/blob/main/hiq/examples/onnxruntime/demo.ipynb)\n* [Intrusive](https://github.com/oracle-samples/hiq/blob/main/hiq/examples/onnxruntime/demo_intrusive.ipynb)\n\n## Examples\n\nPlease check \ud83d\udcf7[ examples](https://github.com/oracle-samples/hiq/blob/main/hiq/examples) for usage examples.", "upvote_ratio": 0.83, "id": "t3_u07er0", "created_utc": 1649555126.0}
{"sub": "Python", "title": "Have/do any of you make a side hustle web scraping/procuring data with Python? Everyone that talks about it online in tandem with \u201cmake $ with Python\u201d never says how much they made made doing this.", "selftext": "nan", "upvote_ratio": 0.81, "id": "t3_u06k0b", "created_utc": 1649552099.0}
{"sub": "Python", "title": "Sunday Daily Thread: What's everyone working on this week?", "selftext": "Tell /r/python what you're working on this week! You can be bragging, grousing, sharing your passion, or explaining your pain. Talk about your current project or your pet project; whatever you want to share.", "upvote_ratio": 0.89, "id": "t3_u05lzc", "created_utc": 1649548811.0}
{"sub": "Python", "title": "What nontechnical piece of advice have you received that has changed how you work as a developer?", "selftext": "For example, Ive heard before automating something, you shouldn\u2019t ask yourself if it can be done but should it be done. \n\nIt\u2019s an intuitive, slap your forehead concept but one that was eye opening", "upvote_ratio": 0.89, "id": "t3_u0431h", "created_utc": 1649543792.0}
{"sub": "Python", "title": "Tips for Python debugging in Vim", "selftext": "What is your workflow and which plugins do you use, if any ?", "upvote_ratio": 0.75, "id": "t3_u03d4y", "created_utc": 1649541591.0}
{"sub": "Python", "title": "An easy to use PGP tool", "selftext": "Good morning to everyone, after my unsuccessful attempts at using PGP software, I decided to create my own one in python, as simple as I possibly could, to make it easier on myself should I ever need to use again this PGP process or should someone decide to use it.\n\nI'm a student, an amateur python programmer and I love challenges, so I took upon the challenge of creating my first tool with a GUI, it's my first time using any of these python libraries and posting anything publicly....\n\nI give you [EZPZ-PGP](https://github.com/HandImpersonator/EZPZ-PGP) (name could do some work, I know):\n\n[Tool menu](https://preview.redd.it/equy9g1wjks81.png?width=322&amp;format=png&amp;auto=webp&amp;s=ef57c2ce60e7c20edb4fb4f7a65dc2ee6993b4c4)\n\n[Folders created](https://preview.redd.it/nxec6ovyjks81.png?width=887&amp;format=png&amp;auto=webp&amp;s=27d55d3a47f937a9a2c18f26567293e16cb2c0ec)\n\n[Key location](https://preview.redd.it/4ywhr3n0kks81.png?width=885&amp;format=png&amp;auto=webp&amp;s=6979992268aa7458c90f4ac08abbc948387d54da)\n\n[Message encryption test](https://preview.redd.it/x2pe8gb2kks81.png?width=479&amp;format=png&amp;auto=webp&amp;s=74224b5cc3766fa1444e7e2d831279bda143de9d)\n\n[Message decryption test](https://preview.redd.it/364ye2n5kks81.png?width=479&amp;format=png&amp;auto=webp&amp;s=71385a74a8eee2463031ace4c578f3b6b7d83a15)\n\nI really have no idea how much simpler to make this tool, I have included some [mildly straightforward instructions](https://github.com/HandImpersonator/EZPZ-PGP#what-the-tool-can-do) for ease of use. It can create PGP keypairs (PGPY), encrypt, decrypt, sign and verify signatures on messages and files (PGPY).\n\nCompletely open source, I want to share my personal tool with the world with the hope of getting some feedback on it and see if people like it a bit.\n\nI'd like to work on translating the tool to other languages with latin alphabet, I'll upload soon to github a file with all the text needed to be translated to other languages. PM me if you decided to help translate the tool to your language so I can update the tool and include your name/account in the github credits!\n\n&amp;#x200B;\n\nThank you, looking forward to your feedback.", "upvote_ratio": 0.82, "id": "t3_u02g7d", "created_utc": 1649538795.0}
{"sub": "Python", "title": "I released a game made with Pygame!", "selftext": "Over the past week I challenged myself to make a game in Pygame and this was the result. Everything was made by me except the music!\n\nhttps://i.redd.it/kp5kik7kdks81.gif\n\nDownload the game here - [https://scriptline-studios.itch.io/planyt](https://scriptline-studios.itch.io/planyt)", "upvote_ratio": 0.96, "id": "t3_u01qvx", "created_utc": 1649536656.0}
{"sub": "Python", "title": "[Challenge] print \"Hello World\" without using W and numbers in your code", "selftext": "To be more accurate: without using w/W, **'** (apostrophe) and numbers.  \nEdit: try to avoid \"ord\", there are other cool tricks \n\n[https://platform.interway.ai/#/get/play\\_/ch/hello\\_\\[w09\\]orld](https://platform.interway.ai/#/get/play_/ch/hello_[w09]orld)\n\nDisclaimer: I built it, and I plan to write a post with the most creative python solutions", "upvote_ratio": 0.9, "id": "t3_u01kmr", "created_utc": 1649536115.0}
{"sub": "Python", "title": "Threading in Python: The Complete Guide", "selftext": "nan", "upvote_ratio": 0.81, "id": "t3_u01bcj", "created_utc": 1649535326.0}
{"sub": "Python", "title": "Has anyone applied to a job that requires a bachelors degree but doesn\u2019t have one themselves and got the job??", "selftext": "I\u2019m 22 years old and have an associates right now, I\u2019m working on by bachelors but I HATE SCHOOL. \n\nI have been learning python for a couple of months now an have a general understanding of it. I was just looking at entry level job and most of these require a bachelors degree, I really want to know if anyone has gotten any of those jobs without a bachelors??", "upvote_ratio": 0.79, "id": "t3_u00pmy", "created_utc": 1649533491.0}
{"sub": "Python", "title": "How to correctly install Python applications &amp; libraries from PyPI", "selftext": "nan", "upvote_ratio": 0.54, "id": "t3_tzyv6j", "created_utc": 1649527919.0}
{"sub": "Python", "title": "Using Nuitka to Speed Python Code", "selftext": "I am playing with Nuitka and following this [link](https://ao.ms/how-to-package-a-python-app-using-nuitka/). I was under the impression that compiled program should run faster but that is not the case here?\n\nRunning the Python code,\n\n    $ time python test1.py\n    h&lt;z`3C337E|$Oe2@\n    \n    real\t0m0.095s\n    user\t0m0.024s\n    sys\t0m0.013s            \n\nThen running the standalone code compiled via Nuitka,\n\n    $ time ./test1.bin\n    +&gt;PAGZ$OHlVK/.5\n    \n    real\t0m0.191s\n    user\t0m0.031s\n    sys\t0m0.014s\n\nThe standalone code runs slower but shouldn't it have run faster since it is a complied?", "upvote_ratio": 0.6, "id": "t3_tzy00o", "created_utc": 1649525369.0}
{"sub": "Python", "title": "Open Source Rhythmic Midi Generator for all Major Keys in Python", "selftext": "This program uses some relatively simple python logic to generate random chord progressions in every major key! The progressions are then broken up into rhythmic subunits, which are also semi-random.\n\nThe midi files need a specified directory to be outputted to. Where to do this can be found within the code.\n\nAny and all feedback is appreciated!\n\nLink to the code: [https://github.com/prod-emdub/midirhythm/tree/main](https://github.com/prod-emdub/midirhythm/tree/main)\n\n&amp;#x200B;\n\nHere is an example of a randomly generated progression in F major:\n\n&amp;#x200B;\n\nhttps://preview.redd.it/usbt91cd5js81.png?width=3541&amp;format=png&amp;auto=webp&amp;s=eb5513343eb5bccf4bdda2ad40eac07ba1355af6", "upvote_ratio": 0.91, "id": "t3_tzwra2", "created_utc": 1649521697.0}
{"sub": "Python", "title": "Build a Site Connectivity Checker in Python \u2013 Real Python", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_tzwhkh", "created_utc": 1649520902.0}
{"sub": "Python", "title": "I made an R6 Strat Roulette discord bot in python!", "selftext": "Hey I'm fairly new to python this is one of my first actual projects! If you want to talk to me about it or give me some suggestions add me on discord: Axkkzy#7992 :)  \n\n\n[https://github.com/Axkkzy/R6-Strat-Roulette-Bot](https://github.com/Axkkzy/R6-Strat-Roulette-Bot)", "upvote_ratio": 0.83, "id": "t3_tzulmw", "created_utc": 1649515365.0}
{"sub": "Python", "title": "Free Python Course Inquiry", "selftext": " I had a quick question regarding a coursera class I am currently taking for simple\u00a0python programming (it's offered for free and it looks like the course is laid out fairly well).\n\nHowever, after week 1 (so a few hours I've invested), it appears to be a course that may have been recycled/offered to newbies from original posting date of several years ago and not within the past 2 years (I found comments from students reviewing the\u00a0course from 2016). \n\nThe teaching style is a little inflexible at times (jumping around functions, assignments a bit), but I want to get through the course because of the value of learning to work with Python\n\nIs there value in this particular program based on the age of the course or do I need to restart a similar\u00a0course not older than a certain amount of years? They are working with python 3.4 btw\n\n&amp;#x200B;\n\nThanks for any feedback/recommendations!", "upvote_ratio": 0.33, "id": "t3_tzra4g", "created_utc": 1649503690.0}
{"sub": "Python", "title": "Python client for Crunchbase's REST API", "selftext": "Hi, I recently needed to use [Crunchbase](https://www.crunchbase.com/)'s REST API in a project but couldn't find a well-maintained python client for it. I started writing one and decided to open-source it. This is my first open-sourced project. Your feedback, improvements, and suggestions will be appreciated.\n\n[https://pypi.org/project/py-crunchbase-api/](https://pypi.org/project/py-crunchbase-api/)\n\nThanks.", "upvote_ratio": 1.0, "id": "t3_tzq3q7", "created_utc": 1649498486.0}
{"sub": "Python", "title": "What do you guys think of this book?", "selftext": "nan", "upvote_ratio": 0.87, "id": "t3_tzpk9y", "created_utc": 1649496037.0}
{"sub": "Python", "title": "custom-literals A module implementing custom literal suffixes using pure Python", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_tzp8qe", "created_utc": 1649494611.0}
{"sub": "Python", "title": "Python \u2014 Network Tracking using Wireshark and Google Maps", "selftext": "nan", "upvote_ratio": 0.91, "id": "t3_tzp4n9", "created_utc": 1649494069.0}
{"sub": "Python", "title": "A Hitomezashi pattern generator I made in python!", "selftext": "I made a Hitomezashi stitch pattern generator fully in python after watching the Numberphile video a long time ago. I used the pygame module to do it.\n\nSource Code- [https://github.com/Topkinsme/Hitomezashi-Stitch-Pattern-Generator/blob/main/main.py](https://github.com/Topkinsme/Hitomezashi-Stitch-Pattern-Generator/blob/main/main.py)  \n\n\nhttps://reddit.com/link/tzp0f4/video/sorpps5wsgs81/player", "upvote_ratio": 0.97, "id": "t3_tzp0f4", "created_utc": 1649493503.0}
{"sub": "Python", "title": "When default __new__ function should be overwritten?", "selftext": "I recently learnt about diffrence beetween __new__ and __init__ functions, but I cannot find usecase of writing my own __new__ method.", "upvote_ratio": 0.83, "id": "t3_tzmyvu", "created_utc": 1649484444.0}
{"sub": "Python", "title": "Pygame Tutorial - Menus and Buttons!", "selftext": "nan", "upvote_ratio": 0.4, "id": "t3_tzjdxt", "created_utc": 1649471029.0}
{"sub": "Python", "title": "Internship skills:", "selftext": "I\u2019m pretty new to python. All i\u2019ve done is take a dual enrollment class at gt for it and the farthest we covered in the course is project oriented learning. I understand how to do the basic problems in the class like given an array of movies and their gross profit, sort them from greatest profit to least profit. Simple things like that. What can i do to learn how to actually apply this stuff to the point that i would be useful in an internship??", "upvote_ratio": 0.75, "id": "t3_tzh04s", "created_utc": 1649462928.0}
{"sub": "Python", "title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread", "selftext": "Found a neat resource related to Python over the past week? Looking for a resource to explain a certain topic?\n\nUse this thread to chat about and share Python resources!", "upvote_ratio": 0.75, "id": "t3_tzgu3e", "created_utc": 1649462409.0}
{"sub": "Python", "title": "Shortening common parts of code", "selftext": "Title.\n\nIn JavaScript there are things like the ternary operator to reduce code size and \"improve\" code quality, I'm wondering if there is much in python aside from the obvious which is lambda", "upvote_ratio": 0.5, "id": "t3_tze4s6", "created_utc": 1649454239.0}
{"sub": "Python", "title": "I created a library for teacher task automation", "selftext": "I created a library of utilities I've used as a teacher to automate various tasks (mail merges, interacting with google classroom, generating rubrics). I hope that it might lower the barrier of entry for teachers with some python knowledge to get started with using programming to ease the repetitive aspects of our work.\n\nComments, criticism, and code review are much appreciated!\n\nhttps://pypi.org/project/teacherhelper/\n\nhttps://teacherhelper.jackdevries.com/\n\nhttps://github.com/jdevries3133/teacher_helper", "upvote_ratio": 0.94, "id": "t3_tzb9yc", "created_utc": 1649446042.0}
{"sub": "Python", "title": "Using Github to host local images permenently on the web.", "selftext": "When you create an issue or add an image on github, you can actually upload images to the github server without any external apis. We can use this feature to mimic Github's behavior when uploading an image and upload any local image to the web. Because github has to store all images permenently (or images on READMEs or issues could change), we also do not have to worry about uploaded images expiring.\n\nI have been using this feature (although not automatically) for a long time to host images for my own website, and finally managed to automate this process.\n\nHonestly, as a beginner in programming, hosting images for websites for free is a pain in the ass and I hope this will help people learn more about websites.\n\n[Github link](https://github.com/0ev/github-issue-image-upload)", "upvote_ratio": 0.38, "id": "t3_tzb541", "created_utc": 1649445662.0}
{"sub": "Python", "title": "Wordle in command line", "selftext": "you can wide the dictionary if you want \ud83d\ude09  \n\n\n    from os import system, name\n    import re\n    import random\n    \n    # define our clear function\n    def clear():\n      \n        if name == 'nt': # for windows\n            system('cls')\n        else: # for mac and linux(here, os.name is 'posix')\n            system('clear')\n    \n    #show the rules of the game and descripcion\n    def menu():\n    \n        print(\"\"\"\n        __        __            _ _         ____\n        \\ \\      / /__  _ __ __| | | ___   / ___| __ _ _ __ ___   ___\n         \\ \\ /\\ / / _ \\| '__/ _` | |/ _ \\ | |  _ / _` | '_ ` _ \\ / _ \\\\\n          \\ V  V / (_) | | | (_| | |  __/ | |_| | (_| | | | | | |  __/\n           \\_/\\_/ \\___/|_|  \\__,_|_|\\___|  \\____|\\__,_|_| |_| |_|\\___|\n        Try to guess the word, we lend you some clues when you assert.\n        After each guess, the color of the tiles will change to show how close your guess was to the word \n        \"\"\")\n    \n    def show_words(array_words,guessword):\n        \n        for word in array_words:\n            list_blocks = \"\"\n            for i,letter in enumerate(word):\n                if letter in guessword:\n                    if word[i] == guessword[i]:\n                        block = \"\ud83d\udfe9\"\n                    else:\n                        block = \"\ud83d\udfe8\" \n                else:\n                    block = \"\ud83d\udd32\" \n    \n                list_blocks += block\n            \n            print(word.replace(\"\", \" \")[1: -1])\n            print(list_blocks)\n    \n    \n    if __name__ == \"__main__\":\n        dictionary = [\"ninja\",\"great\",\"witch\",\"grown\",\"space\",\"stone\",\"earth\",\"extra\",\"entry\",\"slice\",\"shine\",\"sharp\",\"eager\",\"ebony\",\"penny\"]\n        guessword = random.choice(dictionary)\n        lenword = len(guessword)\n        word_guessed = False\n        array_words = []\n    \n    \n    while word_guessed == False:\n        clear()\n        menu()\n        show_words(array_words,guessword) \n        try:\n            word = input(f\"Hit some word of {lenword} length here:\")\n            \n            if len(word) != lenword:\n                raise ValueError(f\"it must be {lenword} length word!\")  \n            elif not re.search(r\"[a-zA-Z]{\"+str(lenword)+\"}\",word):\n                raise ValueError(f\"it must be only alfabetical characters!\")  \n            elif word == guessword:\n                print(word.replace(\"\", \" \")[1: -1])\n                print(\"\ud83d\udfe9\ud83d\udfe9\ud83d\udfe9\ud83d\udfe9\ud83d\udfe9\")\n                print(\"Awesome! \ud83c\udf89\ud83c\udf89\ud83c\udf89\")\n                word_guessed = True\n            else:\n                array_words.append(word)\n                \n        except ValueError as e:\n            print(e)\n            a = input(\"Press 'enter' to continue\")", "upvote_ratio": 0.46, "id": "t3_tz8uhj", "created_utc": 1649439253.0}
{"sub": "Python", "title": "so im 30 switching careers from automotive to computers started college for computer science learning python 3 but i feel its going kinda slow learning so much though so i figured I would ask the more knowledgeable people on what i can do in my free time to grow my knowledge more", "selftext": "nan", "upvote_ratio": 0.68, "id": "t3_tz7tcm", "created_utc": 1649436416.0}
{"sub": "Python", "title": "Preferred way to connect to a database", "selftext": "Is it a particular ORM, do you use a particular ODBC library, embracing the bleeding edge of speed with Apache Arrow? \n\nWhat satisfies your data source access needs and why?", "upvote_ratio": 0.78, "id": "t3_tz7aj5", "created_utc": 1649434965.0}
{"sub": "Python", "title": "Unix Command Cheat Sheet for Busy Developers", "selftext": "nan", "upvote_ratio": 0.67, "id": "t3_tz62m7", "created_utc": 1649431679.0}
{"sub": "Python", "title": "Scrape Google Play Search Apps in Python", "selftext": "Hey guys, just in case anyone wants to scrape Google Play Store App Search \ud83d\udc40\n\nFull code: \n\n```python\nfrom bs4 import BeautifulSoup\nfrom serpapi import GoogleSearch\nimport requests, json, lxml, re, os\n\n\ndef bs4_scrape_all_google_play_store_search_apps(\n                                          query: str, \n                                          filter_by: str = \"apps\",\n                                          country: str = \"US\"):\n    # https://docs.python-requests.org/en/master/user/quickstart/#passing-parameters-in-urls\n    params = {\n        \"q\": query,     # search query\n        \"gl\": country,  # country of the search. Different country display different apps.\n        \"c\": filter_by  # filter to display list of apps. Other filters: apps, books, movies\n    }\n\n    # https://docs.python-requests.org/en/master/user/quickstart/#custom-headers\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.79 Safari/537.36\",\n    }\n\n    html = requests.get(\"https://play.google.com/store/search\", params=params, headers=headers, timeout=30)\n    soup = BeautifulSoup(html.text, \"lxml\")\n\n    apps_data = []\n\n    for app in soup.select(\".mpg5gc\"):\n        title = app.select_one(\".nnK0zc\").text\n        company = app.select_one(\".b8cIId.KoLSrc\").text\n        description = app.select_one(\".b8cIId.f5NCO a\").text\n        app_link = f'https://play.google.com{app.select_one(\".b8cIId.Q9MA7b a\")[\"href\"]}'\n        developer_link = f'https://play.google.com{app.select_one(\".b8cIId.KoLSrc a\")[\"href\"]}'\n        app_id = app.select_one(\".b8cIId a\")[\"href\"].split(\"id=\")[1]\n        developer_id = app.select_one(\".b8cIId.KoLSrc a\")[\"href\"].split(\"id=\")[1]\n        \n        try:\n            # https://regex101.com/r/SZLPRp/1\n            rating = re.search(r\"\\d{1}\\.\\d{1}\", app.select_one(\".pf5lIe div[role=img]\")[\"aria-label\"]).group()\n        except:\n            rating = None\n        \n        thumbnail = app.select_one(\".yNWQ8e img\")[\"data-src\"]\n        \n        apps_data.append({\n            \"title\": title,\n            \"company\": company,\n            \"description\": description,\n            \"rating\": float(rating) if rating else rating, # float if rating is not None else rating or None\n            \"app_link\": app_link,\n            \"developer_link\": developer_link,\n            \"app_id\": app_id,\n            \"developer_id\": developer_id,\n            \"thumbnail\": thumbnail\n        })        \n\n    print(json.dumps(apps_data, indent=2, ensure_ascii=False))\n    \nbs4_scrape_all_google_play_store_search_apps(query=\"maps\", filter_by=\"apps\", country=\"US\")\n\n\n\n\ndef serpapi_scrape_all_google_play_store_apps():\n    params = {\n        \"api_key\": os.getenv(\"API_KEY\"),  # your serpapi api key\n        \"engine\": \"google_play\",          # search engine\n        \"hl\": \"en\",                       # language\n        \"store\": \"apps\",                  # apps search\n        \"gl\": \"us\",                       # contry to search from. Different country displays different.\n        \"q\": \"maps\"                       # search qeury\n    }\n\n    search = GoogleSearch(params)  # where data extracts\n    results = search.get_dict()    # JSON -&gt; Python dictionary\n\n    apps_data = []\n\n    for apps in results[\"organic_results\"]:\n        for app in apps[\"items\"]:\n            apps_data.append({\n                \"title\": app.get(\"title\"),\n                \"link\": app.get(\"link\"),\n                \"description\": app.get(\"description\"),\n                \"product_id\": app.get(\"product_id\"),\n                \"rating\": app.get(\"rating\"),\n                \"thumbnail\": app.get(\"thumbnail\"),\n                })\n\n    print(json.dumps(apps_data, indent=2, ensure_ascii=False))\n```\n\nOutput from DIY solution:\n\n```json\n[\n  {\n    \"title\": \"Google Maps\",\n    \"company\": \"Google LLC\",\n    \"description\": \"Real-time GPS navigation &amp; local suggestions for food, events, &amp; activities\",\n    \"rating\": 3.9,\n    \"app_link\": \"https://play.google.com/store/apps/details?id=com.google.android.apps.maps\",\n    \"developer_link\": \"https://play.google.com/store/apps/dev?id=5700313618786177705\",\n    \"app_id\": \"com.google.android.apps.maps\",\n    \"developer_id\": \"5700313618786177705\",\n    \"thumbnail\": \"https://play-lh.googleusercontent.com/Kf8WTct65hFJxBUDm5E-EpYsiDoLQiGGbnuyP6HBNax43YShXti9THPon1YKB6zPYpA=s128-rw\"\n  },\n  {\n    \"title\": \"Google Maps Go\",\n    \"company\": \"Google LLC\",\n    \"description\": \"Get real-time traffic, directions, search and find places\",\n    \"rating\": 4.3,\n    \"app_link\": \"https://play.google.com/store/apps/details?id=com.google.android.apps.mapslite\",\n    \"developer_link\": \"https://play.google.com/store/apps/dev?id=5700313618786177705\",\n    \"app_id\": \"com.google.android.apps.mapslite\",\n    \"developer_id\": \"5700313618786177705\",\n    \"thumbnail\": \"https://play-lh.googleusercontent.com/0uRNRSe4iS6nhvfbBcoScHcBTx1PMmxkCx8rrEsI2UQcQeZ5ByKz8fkhwRqR3vttOg=s128-rw\"\n  },\n  {\n    \"title\": \"Waze - GPS, Maps, Traffic Alerts &amp; Live Navigation\",\n    \"company\": \"Waze\",\n    \"description\": \"Save time on every drive. Waze tells you about traffic, police, crashes &amp; more\",\n    \"rating\": 4.4,\n    \"app_link\": \"https://play.google.com/store/apps/details?id=com.waze\",\n    \"developer_link\": \"https://play.google.com/store/apps/developer?id=Waze\",\n    \"app_id\": \"com.waze\",\n    \"developer_id\": \"Waze\",\n    \"thumbnail\": \"https://play-lh.googleusercontent.com/muSOyE55_Ra26XXx2IiGYqXduq7RchMhosFlWGc7wCS4I1iQXb7BAnnjEYzqcUYa5oo=s128-rw\"\n  }, ... other results\n]\n```\n\nFull blog post with step-by-step explanation: https://serpapi.com/blog/scrape-google-play-search-apps-in-python/", "upvote_ratio": 0.8, "id": "t3_tz621l", "created_utc": 1649431634.0}
{"sub": "Python", "title": "Rock-Paper-Scissors-Lizard-Spock Game", "selftext": "    from random import randint\n    # create a list of play options\n    options = [\"Rock\", \"Paper\", \"Scissors\", \"Lizard\", \"Spock\"]\n    \n    play = True\n    \n    while play == True:\n    \n        computer = options[randint(0, 4)]\n        user_input = input(\"Please select; Rock, Paper, Scissors, Lizard or Spock\\n\")\n        u = user_input.lower()\n    \n        player = u.capitalize()\n    \n        print(\"Player: \", player)\n        print(\"Computer: \", computer)\n    \n        ##Tie\n        if player == computer:\n            print(\"Tie!\")\n    \n        ##Rock\n        elif player ==\"Rock\":\n            if computer == \"Paper\":\n                print(\"You lose!\", computer, \"covers\", player)\n            elif computer == \"Scissors\":\n                print(\"You win!\", player, \"smashes\", computer)\n            elif computer == \"Lizard\":\n                print(\"You win!\", player, \"crushes\", computer)\n            elif computer == \"Spock\":\n                print(\"You lose!\", computer, \"vaporizes\", player)\n    \n    \n        ##Paper\n        elif player ==\"Paper\":\n            if computer == \"Scissors\":\n                print(\"You lose!\", computer, \"cuts\", player)\n            elif computer ==\"Rock\":\n                print(\"You win!\", player, \"covers\", computer)\n            elif computer == \"Lizard\":\n                print(\"You lose!\", computer , \"eats\", player)\n            elif computer == \"Spock\":\n                print(\"You win!\", player, \"disproves\", computer)\n    \n        ##Scissors\n        elif player == \"Scissors\":\n            if computer == \"Paper\":\n                print(\"You win!\", player, \"cuts\", computer)\n            elif computer == \"Rock\":\n                print(\"You lose!\", computer, \"crushes\", player)\n            elif computer ==\"Lizard\":\n                print(\"You win!\", player, \"decapitates\", computer)\n            elif computer == \"Spock\":\n                print(\"You lose!\", computer, \"smashes\", player)\n    \n        ##Lizard\n        elif player == \"Lizard\":\n            if computer ==\"Rock\":\n                print(\"You lose!\", computer, \"crushes\", player)\n            elif computer ==\"Paper\":\n                print(\"You win!\", player, \"eats\", computer)\n            elif computer == \"Scissors\":\n                print(\"You lose!\", computer, \"decapitates\", player)\n            elif computer == \"Spock\":\n                print(\"You win!\", player, \"\", computer)\n    \n        ##Spock\n        elif player == \"Spock\":\n            if computer == \"Rock\":\n                print(\"You win!\", player, \"vaporizes\", computer)\n            elif computer == \"Paper\":\n                print(\"You lose!\", computer, \"disproves\", player)\n            elif computer == \"Scissors\":\n                print(\"You win!\", player, \"smashes\", computer)\n            elif computer == \"Lizard\":\n                print(\"You lose!\", computer, \"poisons\", player)\n    \n    \n        print(\"Would you like to play again? \\n\")\n        answer =input()\n    \n        if answer.lower() ==\"y\" or answer.lower() ==\"yes\":\n            play == True\n        else:\n            break\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nThis is a game I made a while back, link; [https://github.com/WillPhillipsCVdemo/Rock-Paper-Scissors-Lizard-Spock/blob/master/Game.py](https://github.com/WillPhillipsCVdemo/Rock-Paper-Scissors-Lizard-Spock/blob/master/Game.py)", "upvote_ratio": 0.73, "id": "t3_tz41db", "created_utc": 1649425971.0}
{"sub": "Python", "title": "MicroPython pro-tip: Use WebREPL within your IDE", "selftext": "The repetitive process of editing your code in Thonny IDE, switching over to the WebREPL window, re-selecting your edited code file, re-sending it to your ESP32, and going back to Thonny to fix any bugs can get cumbersome and slow.\n\nHere's a visual tutorial on how to use WebREPL within Thonny. Hope it helps you!\n\n[https://bhave.sh/micropython-webrepl-thonny/](https://bhave.sh/micropython-webrepl-thonny/)", "upvote_ratio": 0.5, "id": "t3_tz3knl", "created_utc": 1649424548.0}
{"sub": "Python", "title": "br4nch 2.0 - Currently in development!", "selftext": "After recieving alot of helpful feedback and feature requests, I have decided to start the development of version 2.0 for br4nch!\n\n*Please visit:* [*https://br4nch.com*](https://br4nch.com) *for helpful links.*\n\n&amp;#x200B;\n\n**Here are some of the upcoming features:**\n\n* Renewing alot of arguments and crucial functions. \u26a0\ufe0f\n* Adding a new 'load.json' function that imports a complete json data file.\n* Adding a new 'export.json' function that exports a branch to a json data file.\n* Adding a new 'list.branches' function.\n* Adding a new 'list.nodes' function.\n* Adding an improved algorithm to search for new updates for br4nch.\n* Adding support for more python versions.\n* Updating documentation.\n* Rewritting alot of other functions.\n\n# \ud83d\udc99\ud83d\udc9b", "upvote_ratio": 0.6, "id": "t3_tz38cy", "created_utc": 1649423481.0}
{"sub": "Python", "title": "Favorite Python Web Framework", "selftext": "Django\nFastAPI\nFlask\nMasonite\nBottle\n\u2026other?\n\nAlso discuss why", "upvote_ratio": 0.93, "id": "t3_tz2v7b", "created_utc": 1649422377.0}
{"sub": "Python", "title": "Does clean code equal \"Workplace\" code", "selftext": "I am a beginner at python and have been following Angela Yu's 100 days of python. I commented on a post regarding my Coffee Machine code been some 400 lines while hers was around 150 lines with the same functionilty.\n\n&amp;#x200B;\n\nThis led me to ask this question of, if code works does it matter if it's clean and pythonic.\n\n&amp;#x200B;\n\nI will reference these two tic toe games below:\n\n&amp;#x200B;\n\n18000 lines (no adherence to DRY but simple to understand)\n\n[https://github.com/asweigart/my\\_first\\_tic\\_tac\\_toe/blob/9f38b04e857426c5a5b80919ad0b5fce0947c022/tictactoe.py](https://github.com/asweigart/my_first_tic_tac_toe/blob/9f38b04e857426c5a5b80919ad0b5fce0947c022/tictactoe.py)\n\n&amp;#x200B;\n\n1 line (I was shocked that such a thing was even possible, considering my tic tac toe code was around 500 lines with no AI. This code is in my opinion way way to complex and one would probably be working in the realm of datasciences/ML to understand such a thing)\n\n[https://www.reddit.com/r/Python/comments/9ozfeq/tictactoe\\_in\\_one\\_line\\_python35/](https://www.reddit.com/r/Python/comments/9ozfeq/tictactoe_in_one_line_python35/)\n\n&amp;#x200B;\n\nBoth programs work, but if one was given only the option of picking from the two, which would be preffered in the workplace? In my opinion, the 18000 line code would be acceptable as it allows all team members, novice or not to atleast have some understanding.\n\n&amp;#x200B;\n\nEDIT: Better late than never, but I realised I have been referencing my tic tac toe code of 500 lines vs the 1 and 18000 liners but havent posted it for comparison.\n\nSo here it is (I do acknowledge that a tic tac toe game has no relevance to the workplace, but is the code structure I used somewhere along the lines of \"hirable\".):\n\n[https://pastebin.com/GBCRiGWe](https://pastebin.com/GBCRiGWe)", "upvote_ratio": 0.5, "id": "t3_tyztcm", "created_utc": 1649410916.0}
{"sub": "Python", "title": "Creating 3D Maps with Python (feat.Mapbox)", "selftext": "**Creating 3D Maps with Python (feat.Mapbox)**  \n[https://wooiljeong.github.io/python/mapboxgl\\_map/](https://wooiljeong.github.io/python/mapboxgl_map/)\n\nUsing WGL provided by Mapbox with Python, I visualized apartment prices in Seoul, Korea on a 3D map. The description of the implementation method is in Korean, but the code is also recorded, so there is no difficulty in understanding it.  \n\n\nhttps://preview.redd.it/qcnyr389t9s81.png?width=919&amp;format=png&amp;auto=webp&amp;s=d9e1997e4f95119926a04aaa09492a11a3c2be60", "upvote_ratio": 0.57, "id": "t3_tyzbab", "created_utc": 1649408660.0}
{"sub": "Python", "title": "Backgammon Game", "selftext": "A game of backgammon. Written with Python and Cython. I'm interested in artificial intelligence so this program has an ai and can play against itself. It uses the monte-carlo tree search algorithm.\n\nhttps://preview.redd.it/xhkqm66ej8s81.png?width=802&amp;format=png&amp;auto=webp&amp;s=60007f6b59c5471f70b98234be65aa6a9201a551\n\n[https://github.com/Tracing/python-backgammon](https://github.com/Tracing/python-backgammon)", "upvote_ratio": 0.67, "id": "t3_tyvodx", "created_utc": 1649393370.0}
{"sub": "Python", "title": "Best 20 Python Program only for Beginners", "selftext": "The Python programming language is one of the most used languages in the world, especially in data analytics. There are many different types of Python projects that you can do. Some of the most common Python project ideas are listed in this blog.\n\nSource Code: [https://www.myguideinfo.com/search/label/python-projects](https://www.myguideinfo.com/search/label/python-projects)\n\n**The 20 Best Python Projects only for Beginners**\n\n1. Finding Mean, Median, Mode in Python without libraries\n2. Program to Find LCM of two numbers in Python\n3. Find Duplicate Values using Python\n4. Python Program to Check Prime Number\n5. Find Greater Number Using If Function In Python\n6. Taking Multiple User's Input Using While Loop in Python\n7. Create a Digital Clock In Python With Source Code\n8. Create To-Do List Using Python with Source Code\n9. Python Program to Display Calendar with Source Code\n10. Create a Password Generator In Python with Source Code\n11. Python Program to Print all Prime Numbers in an Interval\n12. Python Program to Find the Sum of Natural Numbers\n13. Python Program to Find the Factorial of a Number\n14. Calculate the Area of a Triangle in Python\n15. Python Program to Create a Countdown Timer\n16. Python Program to add two Matrices, Transpose, and Multiply\n17. Contact Management Project In Python With Source Code\n18. Billing System Project In Python With Source Code\n19. Vehicle Inventory System In Python With Source Code\n20. Library Management System In Python with Source Code\n\nSource Code: [https://www.myguideinfo.com/search/label/python-projects](https://www.myguideinfo.com/search/label/python-projects)", "upvote_ratio": 0.54, "id": "t3_tyvlpg", "created_utc": 1649393079.0}
{"sub": "Python", "title": "I'm 13, trying to learn Python.", "selftext": "Where/what do you think I should start, learn first, or do you just have any tips?\n\nAlso, make sure what ever you're suggesting is free. Please.", "upvote_ratio": 0.79, "id": "t3_tyu7kl", "created_utc": 1649388175.0}
{"sub": "Python", "title": "Friday Daily Thread: Free chat Friday! Daily Thread", "selftext": "Use this thread to talk about anything Python related! Questions, news,  projects and any relevant discussion around Python is permitted!", "upvote_ratio": 0.71, "id": "t3_tyqfk8", "created_utc": 1649376010.0}
{"sub": "Python", "title": "With the very little Python experience I have, I coded this little \u201cgame\u201d of uno.", "selftext": "[link to the python code](https://www.onlinegdb.com/Z0no-dI4B)", "upvote_ratio": 0.81, "id": "t3_tymxkf", "created_utc": 1649365805.0}
{"sub": "Python", "title": "Automate Investopedia stock simulator with Investopedia-bot", "selftext": "I made this [program](https://github.com/bassel27/Investopedia-Bot) which allows you to automate Investopedia and compare the stocks you're interested in to take better decisions. What do you think?", "upvote_ratio": 0.6, "id": "t3_tymrnf", "created_utc": 1649365341.0}
{"sub": "Python", "title": "Abandoned Docker Library?", "selftext": "Apologies if this isn't the right forum to raise this. I'm hoping folks here might have some insight or can point me in the right direction. I have a build tool that relies on the official Docker library for Python:\n\n[https://github.com/docker/docker-py](https://github.com/docker/docker-py)\n\nIt seems like this library has been mostly abandoned by Docker. There hasn't been any new commits for almost 6 months, there are a large number of issues and pull requests that appear to be languishing, and the code owners seem to have sparse activity on GitHub.\n\nAnyone know what gives? Is Docker abandoning the Python library since docker compose is being refactored into GoLang?", "upvote_ratio": 0.83, "id": "t3_tylcez", "created_utc": 1649361461.0}
{"sub": "Python", "title": "Getting started with Python - programming in Python 3.", "selftext": "nan", "upvote_ratio": 0.22, "id": "t3_tyl0o2", "created_utc": 1649360548.0}
{"sub": "Python", "title": "I built an all-in-one Python Web and AI/ML Resources Website", "selftext": "Hey there,\n\nI just built a Python Resources Website. Divided into two main Pages (Web Backend and AI/Machine Learning) for these Resources, the goal is to simplify the search for some of the best React resources such as:\n\n**1) For Python/Backend:**\n\n\u2022 Django, Flask, FastAPI Articles\n\n\u2022 Django, Flask, FastAPI Forums latest Discussions (Reddit)\n\n\u2022 Django, Flask, FastAPI YouTube Channels Videos\n\n\u2022 Django, Flask, FastAPI Websites\n\n\u2022 Django, Flask, FastAPI E-books, Snippets\n\n\u2022 *Job Opportunities (soon)*\n\n&amp;#x200B;\n\n**2) For Python/AI/ML:**\n\n\u2022 AI/ML Articles\n\n\u2022 AI/ML Forums latest Discussions (Reddit)\n\n\u2022 AI/ML YouTube Channels Videos\n\n\u2022 AI/ML Websites\n\n\u2022 AI/ML E-books, Snippets\n\n\u2022 *Job Opportunities (soon)*\n\n&amp;#x200B;\n\n**Here is the Link \ud83d\udc49** [**helloPython**](https://hellopython.vercel.app/)\n\n&amp;#x200B;\n\nFeel free to give some feedback, I'd like to keep pon working on this project because I love the Python Industry ;) \n\nThank You!", "upvote_ratio": 0.57, "id": "t3_tyi792", "created_utc": 1649352775.0}
{"sub": "Python", "title": "Send me suggestions", "selftext": "Hello, I am 18 and I have started to code last year. I want to check my code (if it is good or not, send me suggestions ;) ) with this example of the ticktacktoe game in the terminal.\n\nAvailable here : [https://github.com/FortisCodis/PyTicTacToe](https://github.com/FortisCodis/PyTicTacToe)\n\nSorry for my bad english I am french...\n\nThanks", "upvote_ratio": 0.17, "id": "t3_tyhe6o", "created_utc": 1649350514.0}
{"sub": "Python", "title": "Build a simple bank management system in python with MySQL", "selftext": "nan", "upvote_ratio": 0.2, "id": "t3_tyh72s", "created_utc": 1649349924.0}
{"sub": "Python", "title": "Video Introduction to pandas Library in Python", "selftext": "nan", "upvote_ratio": 0.38, "id": "t3_tygobz", "created_utc": 1649348450.0}
{"sub": "Python", "title": "Documentation is highly valued, but often overlooked", "selftext": "Hi all, In 2017, Github (opensourcesurveyorg) conducted a survey on open source projects, and we believe that in 2022, some issues are still relevant.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/p3nop6uhq4s81.png?width=914&amp;format=png&amp;auto=webp&amp;s=6093f8b9dd8d814002c2b01bff334ecc55f85758\n\nIt shows the importance of documentation in an Open source project and how frustrating it can be if it\u2019s not a priority. Some important things, in my opinion:\n\n1. When you encounter documentation problems, help a maintainer and open a change request to improve them.\n2. Licenses are by far the most important type of documentation for users and contributors.\n3. Documentation helps build engaged communities.\n4. When communicating about a project, use language that is clear and accessible to people who are not born English or do not read English fluently.\n\nI work for an open-source project; please feel free to comment if you have any tips for improving documentation.", "upvote_ratio": 0.69, "id": "t3_tygibq", "created_utc": 1649347990.0}
{"sub": "Python", "title": "Maze Creator", "selftext": "I created a website to create and play with mazes.\n\nWebsite Link: https://desolate-mountain-91027.herokuapp.com/home/\n\nGitHub Link: https://github.com/ShouvikGhosh2048/MazeCreator\n\nI would like feedback on the website and code.", "upvote_ratio": 0.5, "id": "t3_tyg1cd", "created_utc": 1649346669.0}
{"sub": "Python", "title": "Add machine learning to your apps easily with Google's MediaPipe and Python (Beginner's Guide)", "selftext": "Hey everyone!\n\n**I just released** [**this beginner's guide**](https://www.assemblyai.com/blog/mediapipe-for-dummies/) **to using MediaPipe in Python**. MediaPipe provides really easy-to-use APIs for common ML tasks like hand recognition, face tracking, object detection, and more! You can use it to add ML to apps for things like sign language recognition!\n\nHere's a video of how it performs extracting 3D pose data from a video:\n\n&amp;#x200B;\n\nhttps://reddit.com/link/tyftwn/video/w02ochs5n4s81/player\n\nLet me know what you think!", "upvote_ratio": 0.82, "id": "t3_tyftwn", "created_utc": 1649346097.0}
{"sub": "Python", "title": "Friends and I are tired of online tutorials so we\u2019re running a cohort for learning Python with competitive team games", "selftext": "Hi, we\u2019re a group of 4 friends who are working on something we think is cool but want to hear what you think! We\u2019re super early in working on this - if you want this to exist, register interest on [https://delta-academy.xyz](https://delta-academy.xyz/) :)\n\nThe best experiences we had when learning Python were working on projects and hackathons. We also benefited from being in cohorts of learners (e.g. at University) - the friends we made have often lasted.\n\nWe want to combine these two elements - live competitive coding games with a cohort of fellow Python learners. The cohort will be \\~20 people. It\u2019s the format we wanted but couldn\u2019t find online - so we\u2019re creating it!\n\nWe would have to charge for running cohorts (not least to cover the prizes!), but haven\u2019t figured out how much yet - just want to know first if this is something anyone wants!\n\nWe made a short video that hopefully explains everything in 1 min.\n\nhttps://reddit.com/link/tye97u/video/hbsy8qs594s81/player\n\nReally keen to hear feedback. :)", "upvote_ratio": 0.9, "id": "t3_tye97u", "created_utc": 1649341649.0}
{"sub": "Python", "title": "Palmette JS | Python and other language template generator.", "selftext": "Hi everyone,\n\nI recently made this CLI app that can generate a lot of templates (many in python) of different programming  language. Can I ask for a review? Is this a good idea or nah?\n\n(You can install it with \"npm install -g palmette-js\")\n\n[https://github.com/PalmetteJS/Palmette-js](https://github.com/PalmetteJS/Palmette-js)", "upvote_ratio": 0.67, "id": "t3_tycthh", "created_utc": 1649337218.0}
{"sub": "Python", "title": "Making a face-controlled keyboard (Python OpenCV + MediaPipe)", "selftext": "nan", "upvote_ratio": 0.33, "id": "t3_tycls0", "created_utc": 1649336539.0}
{"sub": "Python", "title": "Rtree 1.0 released", "selftext": "nan", "upvote_ratio": 0.83, "id": "t3_tyb398", "created_utc": 1649331385.0}
{"sub": "Python", "title": "How to summarize text with Python and machine learning", "selftext": "Summarization is a very common task that many developers would like to  automate. For example wouldn't it be nice to automatically create a  summary of each blog article you're writing? Or automatically summarize  documents for your employees? Tons of good applications exist.\n\nIn this article I'm showing how easy it is to perform advanced text summarization in Python thanks to Transformers and Bart Large CNN:\n\n[https://nlpcloud.io/how-to-summarize-text-with-python-and-machine-learning.html](https://nlpcloud.io/how-to-summarize-text-with-python-and-machine-learning.html?utm_source=reddit&amp;utm_campaign=la5u8885-fd8e-21eb-ca80-5242ac13d5ja)\n\nPlease don't hesitate to ask questions if you have any!", "upvote_ratio": 0.81, "id": "t3_tyazf7", "created_utc": 1649330969.0}
{"sub": "Python", "title": "extreqs: parsing package extras from a requirements.txt", "selftext": "I found myself writing the same logic in a couple of `setup.py` scripts so wrote a package to do it: https://pypi.org/project/extreqs/\n\nBroadly, it parses your `extras_require` dict from your `requirements.txt` file using special comments rather than having to define the same thing twice.\n\nAs noted in the documentation, there are situations where you wouldn't want to use this, most commonly for libraries. requirements.txt and package dependencies have different purposes in that case: requirements.txt provides a full, (somewhat) reproducible environment for CI and other developers to get an environment with hard versions and all of your linters etc., where package dependencies provide a minimal, permissive environment to allow as many people to use your library as possible. extreqs is primarily designed for applications (web backends, CLIs etc) which have optional extra functionality.\n\n(N.B. not a beginner but it's not a complicated enough codebase to merit anything else...)", "upvote_ratio": 0.67, "id": "t3_tyapmu", "created_utc": 1649329932.0}
{"sub": "Python", "title": "GitHub - corpnewt/ProperTree: Cross platform GUI plist editor written in python.", "selftext": "nan", "upvote_ratio": 0.5, "id": "t3_ty8hba", "created_utc": 1649320332.0}
{"sub": "Python", "title": "How To Learn Python", "selftext": "nan", "upvote_ratio": 0.27, "id": "t3_ty8fp5", "created_utc": 1649320121.0}
{"sub": "Python", "title": "I made a game from scratch in 48 hours for the Ludum Dare with Python", "selftext": "I made all of the code, artwork, sfx, etc. solo during the 48 hours of the Ludum Dare as required by the rules.\n\nhttps://i.redd.it/acex0iv0y1s81.gif\n\nThe game (and its source) are available here:\n\n[https://dafluffypotato.itch.io/explont](https://dafluffypotato.itch.io/explont)\n\nI also [livestreamed almost all of the development](https://www.youtube.com/playlist?list=PLX5fBCkxJmm1is3hgaBi037MEGFjtHJex) and created a [timelapse](https://youtu.be/4cgjYlH2g9g).\n\n&amp;#x200B;\n\nhttps://preview.redd.it/eat9owgny1s81.png?width=631&amp;format=png&amp;auto=webp&amp;s=5cae8fd8386d01c91135a834685bbaeecb97413a", "upvote_ratio": 0.97, "id": "t3_ty6yvn", "created_utc": 1649313627.0}
{"sub": "Python", "title": "Dockersh : A shell for docker commands with autocomplete", "selftext": "Hi everyone,\n\nI was learning Docker and felt that it was hard to remember all the commands and various arguments. I also kept forgetting the names of my containers &amp; images so I decided to build a CLI for Docker using Prompt Toolkit. Hopefully it will be useful for those of you learning &amp; working with devops. Let me know how I can improve. \n\nThe project can be found at [https://github.com/solamarpreet/dockersh](https://github.com/solamarpreet/dockersh)", "upvote_ratio": 0.67, "id": "t3_ty6f0a", "created_utc": 1649311338.0}
{"sub": "Python", "title": "Continuous feedback and the 'definition of done'", "selftext": "Hey! I wrote a post about what I see is sometimes the short fallings of 'definition of done'. Wonder whether you guys agree with it and have been dealing with such issues?\n\n[https://betterprogramming.pub/youre-never-done-by-definition-c04ac77c616b](https://betterprogramming.pub/youre-never-done-by-definition-c04ac77c616b)", "upvote_ratio": 0.5, "id": "t3_ty5lnw", "created_utc": 1649308203.0}
{"sub": "Python", "title": "What is the best Python -&gt; direct executable package / compiler today? (April,2022)", "selftext": "I need to create a few applications that I want to distribute as executables, the options I'm aware of are:\n\nNuitka\n\npyinstaller\n\npy2exe\n\npy2app\n\nWhat else is out there, and what is the BEST?\n\nI'm looking to create a couple of cli's using click, and a couple of GUI's using DearPyGui", "upvote_ratio": 0.93, "id": "t3_ty36vb", "created_utc": 1649299895.0}
{"sub": "Python", "title": "Pong game in just 14 lines.", "selftext": "    import pygame \n    pygame.init()\n    win = pygame.display.set_mode((800, 600))\n    clock = pygame.time.Clock()\n    rects = [pygame.Rect(0, 0, 20, 60), pygame.Rect(780, 0, 20, 60), pygame.Rect(390, 290, 20, 20)]\n    ball_vel = [5, 5]\n    while clock.tick(60) and not pygame.QUIT in [event.type for event in pygame.event.get()]:\n        keys = pygame.key.get_pressed()\n        rects = rects[0].move(0, (keys[pygame.K_s] - keys[pygame.K_w]) * 5).clamp(win.get_rect()), rects[1].move(0, (keys[pygame.K_DOWN] - keys[pygame.K_UP]) * 5).clamp(win.get_rect()), rects[2].move(*ball_vel)\n        rects[2].topleft = (390, 290) if rects[2].x &lt; 0 or rects[2].right &gt; 800 else rects[2].topleft\n        ball_vel[1], ball_vel[0] = -ball_vel[1] if rects[2].y &lt; 0 or rects[2].bottom &gt; 600 else ball_vel[1], -ball_vel[0] if rects[2].collidelist(rects[:-1]) != -1 else ball_vel[0]\n        win.fill((0, 0, 0))        \n        [[pygame.draw.rect, pygame.draw.ellipse][1 if rect == rects[2] else 0](win, (255, 255, 255), rect) for rect in rects]\n        pygame.display.update()", "upvote_ratio": 0.83, "id": "t3_ty15no", "created_utc": 1649293551.0}
{"sub": "Python", "title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!", "selftext": "Discussion of using Python in a professional environment, getting jobs in Python as well as ask questions about courses to further your python education!\n\n**This thread is not for recruitment, please see** r/PythonJobs **or the thread in the sidebar for that.**", "upvote_ratio": 0.67, "id": "t3_txzwcj", "created_utc": 1649289609.0}
{"sub": "Python", "title": "I made the game Wordle in Python", "selftext": "This game is somewhat similar to hangman.\n\nYou try to guess a random 5 letter word and the game will tell you the if the placement of each letter is correct or not. Example:\n\n```\nO - correct place\n? - right letter, wrong place\nX - letter is present in word\n\nWord we're looking for: hello\nYour word (input): house\n\nOutput: O?XX?\n```\nThis continues until you guess the word or give up. \n\nYou can look at the code here: [https://github.com/Nextross/Wordle-python](https://github.com/Nextross/Wordle-python)\n\nAny feedback is welcomed!", "upvote_ratio": 0.72, "id": "t3_txvx5q", "created_utc": 1649278243.0}
{"sub": "Python", "title": "The last Python 3.11 alpha (3.11.0a7) is available", "selftext": "nan", "upvote_ratio": 0.95, "id": "t3_txuukh", "created_utc": 1649275361.0}
{"sub": "Python", "title": "Continuous Feedback over code", "selftext": "Hey everyone! We made something! I wanted to get your feedback on our new open-source platform - Digma. Digma provides observability \ud83d\udd2d over code and makes it relevant to feature development.  It gleans code-relevant insights from OpenTelemetry and other sources and provides in-code feedback \ud83d\udcbb.  [https://github.com/digma-ai/digma](https://github.com/digma-ai/digma) Python is our first supported language. I'm the author of Digma and am really keen to hear your thoughts!", "upvote_ratio": 0.57, "id": "t3_txrs93", "created_utc": 1649267039.0}
{"sub": "Python", "title": "Blog API built with FastAPI, MySQL, SQLAlchemy, and Alembic", "selftext": "nan", "upvote_ratio": 0.8, "id": "t3_txrlz4", "created_utc": 1649266567.0}
{"sub": "Python", "title": "A practical introduction solving differential equations numerically", "selftext": "nan", "upvote_ratio": 0.83, "id": "t3_txq7d2", "created_utc": 1649262787.0}
{"sub": "Python", "title": "Configpile: a modern, typed argparse replacement", "selftext": "I started documenting astronomy code written by a PhD student, and wanted a command-line parsing library that would be self documenting.\n\nThus [ConfigPile](https://denisrosset.github.io/configpile/) was born! It's based on dataclasses with annotated types.\n\nSample code (imports omitted):\n\n    @dataclass(frozen=True)\n    class Calc(Config):\n        \"\"\"\n        A simple calculator\n        \"\"\"\n\n        #: First number to add\n        x: Annotated[float, Param.store(parsers.float_parser, short_flag_name=\"-x\")]\n\n        #: Second number to add\n        y: Annotated[float, Param.store(parsers.float_parser, short_flag_name=\"-y\")]\n\n    c = Calc.from_command_line_()\n    print(f\"{c.x} + {c.y} = {c.x+c.y}\")\n\nRunning this with \"-h\", you can a nice usage help automatically generated (through a legacy ArgumentParser). The same can feed the [sphinx-argparse](https://sphinx-argparse.readthedocs.io/en/latest/) extension to include documentation in the project web pages.\n\nConfigpile is based on modern Python, is written in mostly functional style, has user-friendly error reporting which accumulates errors instead of bailing out immediately, supports environment variables and INI files.\n\nI'd be super grateful for comments, especially about the documentation and the Python style.\n\nFollow a tutorial here: https://denisrosset.github.io/configpile/tutorial/index.html\n\nLearn about the main concepts here: https://denisrosset.github.io/configpile/concepts/index.html\n\nThe package can be easily installed using `pip install configpile`\n\n(Note: many edits for formatting)", "upvote_ratio": 0.86, "id": "t3_txq6ch", "created_utc": 1649262713.0}
{"sub": "Python", "title": "Show the songs that you are listening to on Telegram. (Like Discord's \"Listening to\") [Recently updated]", "selftext": "nan", "upvote_ratio": 0.5, "id": "t3_txp8kf", "created_utc": 1649260175.0}
{"sub": "Python", "title": "UNPHAT method for designing apps", "selftext": "I found this to be very useful when thinking about the most important part of writing apps: understanding what to solve and how. It's from https://news.ycombinator.com/item?id=30931831. \n\nNext time you find yourself Googling some cool new technology to (re)build your architecture around, I urge you to stop and follow\u00a0UNPHAT\u00a0instead:\n\nDon\u2019t even start considering solutions until you\u00a0Understand\u00a0the problem. Your goal should be to \u201csolve\u201d the problem mostly within the\u00a0problem\u00a0domain, not the solution domain.\n\neNumerate\u00a0multiple candidate solutions. Don\u2019t just start prodding at your favorite!\n\nConsider a candidate solution, then\u00a0read the\u00a0Paper\u00a0if there is one.\n\nDetermine the\u00a0Historical context\u00a0in which the candidate solution was designed or developed.\n\nWeigh\u00a0Advantages\u00a0against disadvantages.\u00a0Determine what was de-prioritized\u00a0to achieve what\u00a0was\u00a0prioritized.\n\nThink!\u00a0Soberly and humbly ponder how well this solution fits your problem.\u00a0What fact would need to be different for you to change your mind?\u00a0For instance, how much smaller would the data need to be before you\u2019d elect\u00a0not\u00a0to use Hadoop?", "upvote_ratio": 0.67, "id": "t3_txoq2p", "created_utc": 1649258798.0}
{"sub": "Python", "title": "I made a small tool to apply \"color palette restriction\" on images !", "selftext": "I got inspired by /r/place and I wanted to exercice my python skills so I tried to make a tool to redraw images with a specific color palette (/r/place color palette)\n\nHere are some examples :\n\n[Reference image](https://preview.redd.it/3p4us69n7xr81.jpg?width=494&amp;format=pjpg&amp;auto=webp&amp;s=0535c1eed010716f053549eec31a88d4453f2f6e)\n\n[Color palette restriction without dithering](https://preview.redd.it/63df44kt7xr81.png?width=494&amp;format=png&amp;auto=webp&amp;s=341497026bb4dd39dc63718cf792ba1e1f3030ef)\n\n[Color palette restriction with  Floyd-Steinberg dithering](https://preview.redd.it/xccpwquw7xr81.png?width=494&amp;format=png&amp;auto=webp&amp;s=8156a2f09de8b2a58c92f72ef90a5b017a1df2a7)\n\nIt can also work with any color palette provided !\n\nAlso, I am using PIL to read and draw on images.\n\n&amp;#x200B;\n\nYou can find the source code here :  [https://github.com/nrbt25/image-dithering/tree/master](https://github.com/nrbt25/image-dithering/tree/master) ", "upvote_ratio": 0.97, "id": "t3_txnuzx", "created_utc": 1649256498.0}
{"sub": "Python", "title": "YAML: The Missing Battery in Python", "selftext": "nan", "upvote_ratio": 0.88, "id": "t3_txmfli", "created_utc": 1649252420.0}
{"sub": "Python", "title": "Programming projects for Physics in Python", "selftext": "So I am looking for some projects to do in Physics using Python ,when I searched for the same in Quora ,it was too hard to even comprehend .\n\nI am ready to do some reading if it is needed ,I don't like learning Python just through some courses ,I think that learning by creating projects will be a good motivation.\n\nI have knowledge of Calc 1,2,3 a bit of Linear Algebra ,basics of solving diff equations .\n\nI am taking Classical Mechancis and Electromagnetism .\n\nAny suggestion ,I am aiming to score an internship with this project .\n\nThank you for reading my post .\n\nHave a nice day", "upvote_ratio": 0.78, "id": "t3_txm6zl", "created_utc": 1649251713.0}
{"sub": "Python", "title": "hello this could be an english vocabulary question, what does comprehension mean from 'List Comprehension'? I already learned that is like set builder notation", "selftext": "nan", "upvote_ratio": 0.61, "id": "t3_txk1nm", "created_utc": 1649244748.0}
{"sub": "Python", "title": "Scrape Naver Related Search Results with Python", "selftext": "\nFull DIY code:\n\n```python\nimport requests, json\nfrom parsel import Selector  # https://parsel.readthedocs.io/\n\n# https://docs.python-requests.org/en/master/user/quickstart/#passing-parameters-in-urls\nparams = {\n    \"query\": \"minecraft\",  # search query\n    \"where\": \"web\"         # web results. works with nexearch as well\n}\n\n# https://docs.python-requests.org/en/master/user/quickstart/#custom-headers\nheaders = {\n    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.87 Safari/537.36\",\n}\n\nhtml = requests.get(\"https://search.naver.com/search.naver\", params=params, headers=headers, timeout=30)\nselector = Selector(html.text)\n\nrelated_results = []\n\n# https://www.programiz.com/python-programming/methods/built-in/enumerate\nfor index, related_result in enumerate(selector.css(\".related_srch .keyword\"), start=1):\n    keyword = related_result.css(\".tit::text\").get().strip()\n    link = f'https://search.naver.com/search.naver{related_result.css(\"a::attr(href)\").get()}'\n\n    related_results.append({\n        \"position\": index,    # 1,2,3..\n        \"title\": keyword,\n        \"link\": link\n    })\n\n\nprint(json.dumps(related_results, indent=2, ensure_ascii=False))\n```\n\nOutputs:\n\n```json\n[\n  {\n    \"position\": 1,\n    \"title\": \"\ub9c8\uc778\ud06c\ub798\ud504\ud2b8\",\n    \"link\": \"https://search.naver.com/search.naver?where=nexearch&amp;query=%EB%A7%88%EC%9D%B8%ED%81%AC%EB%9E%98%ED%94%84%ED%8A%B8&amp;ie=utf8&amp;sm=tab_she&amp;qdt=0\"\n  },\n  {\n    \"position\": 2,\n    \"title\": \"minecraft \ub73b\",\n    \"link\": \"https://search.naver.com/search.naver?where=nexearch&amp;query=minecraft+%EB%9C%BB&amp;ie=utf8&amp;sm=tab_she&amp;qdt=0\"\n  },\n  {\n    \"position\": 3,\n    \"title\": \"craft\",\n    \"link\": \"https://search.naver.com/search.naver?where=nexearch&amp;query=craft&amp;ie=utf8&amp;sm=tab_she&amp;qdt=0\"\n  },\n  {\n    \"position\": 4,\n    \"title\": \"mine\",\n    \"link\": \"https://search.naver.com/search.naver?where=nexearch&amp;query=mine&amp;ie=utf8&amp;sm=tab_she&amp;qdt=0\"\n  },\n  {\n    \"position\": 5,\n    \"title\": \"mojang\",\n    \"link\": \"https://search.naver.com/search.naver?where=nexearch&amp;query=mojang&amp;ie=utf8&amp;sm=tab_she&amp;qdt=0\"\n  }\n]\n```\n\nAlternative solution using [Naver Related results API](https://serpapi.com/naver-related-results) from SerpApi:\n\n```python\nfrom serpapi import NaverSearch\nimport os, json\n\nparams = {\n    # https://docs.python.org/3/library/os.html#os.getenv\n    \"api_key\": os.getenv(\"API_KEY\"),  # your serpapi api key\n    \"engine\": \"naver\",                # search engine to parse results from\n    \"query\": \"minecraft\",             # search query\n    \"where\": \"web\"                    # web results\n}\n\nsearch = NaverSearch(params)          # where data extraction happens\nresults = search.get_dict()           # JSON -&gt; Python dictionary\n\nrelated_results = []\n\n# iterate over \"related_results\" and extract position, title and link\nfor related_result in results[\"related_results\"]:\n    related_results.append({\n        \"position\": related_result[\"position\"],\n        \"title\": related_result[\"title\"],\n        \"link\": related_result[\"link\"]\n    })\n\nprint(json.dumps(related_results, indent=2, ensure_ascii=False))\n```\n\nOutputs:\n\n```json\n[\n  {\n    \"position\": 1,\n    \"title\": \"\ub9c8\uc778\ud06c\ub798\ud504\ud2b8\",\n    \"link\": \"https://search.naver.com?where=nexearch&amp;query=%EB%A7%88%EC%9D%B8%ED%81%AC%EB%9E%98%ED%94%84%ED%8A%B8&amp;ie=utf8&amp;sm=tab_she&amp;qdt=0\"\n  },\n  {\n    \"position\": 2,\n    \"title\": \"minecraft \ub73b\",\n    \"link\": \"https://search.naver.com?where=nexearch&amp;query=minecraft+%EB%9C%BB&amp;ie=utf8&amp;sm=tab_she&amp;qdt=0\"\n  },\n  {\n    \"position\": 3,\n    \"title\": \"craft\",\n    \"link\": \"https://search.naver.com?where=nexearch&amp;query=craft&amp;ie=utf8&amp;sm=tab_she&amp;qdt=0\"\n  },\n  {\n    \"position\": 4,\n    \"title\": \"mine\",\n    \"link\": \"https://search.naver.com?where=nexearch&amp;query=mine&amp;ie=utf8&amp;sm=tab_she&amp;qdt=0\"\n  },\n  {\n    \"position\": 5,\n    \"title\": \"mojang\",\n    \"link\": \"https://search.naver.com?where=nexearch&amp;query=mojang&amp;ie=utf8&amp;sm=tab_she&amp;qdt=0\"\n  }\n]\n```\n\nThe difference between DIY and API solution is that you don't need to create the parser from scratch, maintain it, or figure out how to bypass blocks from Naver.\n\nFull blog post: https://serpapi.com/blog/scrape-naver-related-search-results-with-python/", "upvote_ratio": 0.4, "id": "t3_txif8z", "created_utc": 1649238341.0}
{"sub": "Python", "title": "Extending pylint with plugins", "selftext": "nan", "upvote_ratio": 0.5, "id": "t3_txfsuz", "created_utc": 1649226686.0}
{"sub": "Python", "title": "A Practical Introduction To Web Scraping With Python", "selftext": "nan", "upvote_ratio": 0.96, "id": "t3_txf65f", "created_utc": 1649224130.0}
{"sub": "Python", "title": "Python email sender", "selftext": " Hi everyone, I have created a script that sends an email to whoever I want using data retrieved from an API (I chose News API, and it will send me the top 20 news of the day).\n\n[https://github.com/jrodriigues/news-email-sender](https://github.com/jrodriigues/news-email-sender)\n\nTake a look, let me know what you think, and how to improve it!", "upvote_ratio": 0.6, "id": "t3_txd85l", "created_utc": 1649217069.0}
{"sub": "Python", "title": "I made my first ever programming assignment into a youtube tutorial", "selftext": "nan", "upvote_ratio": 0.64, "id": "t3_txbulk", "created_utc": 1649212511.0}
{"sub": "Python", "title": "Termtyper - Typing in terminal is fun !", "selftext": "Termtyper is a TUI (Text User Interface) typing application that provides you a great feel with typing with a lot of options to tweak!\n\nIt is highly inspired by [monkeytype](https://monkeytype.com/)\n\nIt is built on top of [textual](https://github.com/Textualize/textual) which provides the UI for the application\n\ngithub: [https://github.com/kraanzu/termtyper](https://github.com/kraanzu/termtyper)\n\n&amp;#x200B;\n\nhttps://reddit.com/link/txbsdd/video/ojo8k2o3mtr81/player", "upvote_ratio": 0.83, "id": "t3_txbsdd", "created_utc": 1649212316.0}
{"sub": "Python", "title": "Wednesday Daily Thread: Beginner questions", "selftext": "New to Python and have questions? Use this thread to ask anything about Python, there are no bad questions!\n\nThis thread may be fairly low volume in replies, if you don't receive a response we recommend looking at r/LearnPython or joining the Python Discord server at [https://discord.gg/python](https://discord.gg/python) where you stand a better chance of receiving a response.", "upvote_ratio": 1.0, "id": "t3_tx8uqj", "created_utc": 1649203209.0}
{"sub": "Python", "title": "Why and how to use conda?", "selftext": "I'm a data scientist and my main is python. I use quite a lot of libraries picked from github. However, every time I see in the readme that installation should be done with conda, I know I'm in for a bad time. Never works for me. \n\nEven installing conda is stupid. I'm sure there is a reason why there is no \"apt install conda\"... \n\nWhy use conda? In which situation is it the best option? Anyone can help me see the light?", "upvote_ratio": 0.9, "id": "t3_tx73j8", "created_utc": 1649198092.0}
{"sub": "Python", "title": "Online coding video tutorials - Opinions", "selftext": "Hello everyone,\n\nI am currently creating some space science + Python [YouTube tutorials](https://youtube.com/c/Astroniz). In these videos I do some coding and provide some explanations on what I am doing, the science behind it etc.\n\nNow I was wondering whether this is a good approach at all. I really like creating some content on YouTube (no I am not a professional one with my few 100 subscribers), and I am eager to improve it steadily. However I thought that \"live coding\" may take too much time and isn't easy to follow. My idea is to explain e.g., science related Jupyter Notebook cells in the videos to shorten the length. The code is provided on GitHub anyway, so a viewer could follow my tutorial (and thoughts) more easily. The coding would be reduced to a minimum and the focus could move to the \"non coding\" part.\n\nAny ideas or comments would be appreciated!\n\nThomas", "upvote_ratio": 0.6, "id": "t3_tx6k1q", "created_utc": 1649196551.0}
{"sub": "Python", "title": "Rubik's Cube Model in Python using OOP", "selftext": "I've been thinking for a while about developing a model to simulate a Rubik's cube in Python. I wanted to make use of object-oriented programming, and this project was the perfect excuse to get down to work. \ud83d\udcbb\u2328\ufe0f\ud83d\udd96\n\nhttps://carlosgrande.me/rubiks-cube-model/", "upvote_ratio": 0.84, "id": "t3_tx6gfb", "created_utc": 1649196238.0}
{"sub": "Python", "title": "What happened between SciPy creater Travis Oliphant and Enthought?", "selftext": "I was listening to his interview on the [Lex Friedman podcast](https://open.spotify.com/episode/2U2AkSSuAzmZi7nmGzJboW?si=28e8c7d915e04f75) and he mentioned that he is no longer friends with the founder of Enthought because he started Anaconda. I can't find any details online, does anyone know the story?", "upvote_ratio": 0.7, "id": "t3_tx67bv", "created_utc": 1649195579.0}
{"sub": "Python", "title": "My first package to pypi. Connpy: Network connection manager and automation module", "selftext": "Hi!\n\nSo a little bit of background, I'm a network engineer (ccie), with 10+ years of experience in networking, and i'm a really lazy guy so usually i try to automate and script everything. long time ago i created my bash connection manager and been using it since. A month ago i decided it was time to upgrade it to python and add the automation function i always wanted, and that is how connpy was born.\n\nI'm not a programmer so it may have some issues! let me know.\n\n[https://github.com/fluzzi/connpy/](https://github.com/fluzzi/connpy/) &lt;- here is the link, it also have some documentation it should be easy to use. It's created for linux as it's what i use everyday but i did some testing on macos and it should work.\n\n## connection manager\n\nFirst its the connection manager, it adds the commands conn/connpy to shell, and you can add all the devices you manage easily:\n\n- conn --add server1\n\nyou can add folders and subfolders to organize your devices. i use to work with a lot of clients so i have like 400+ nodes divided in multiples folders.\n\n- conn --add \"@office\"  \n- conn --add \"@servers@office\"\n- conn --add server2@servers@office\n- conn --add server3@servers@office\n- conn --add router1@office\n\nwhen you are creating a new node to connect, it allows you to refer profiles, this way you can manage passwords and other information in 1 place and use it in multiple nodes.\n\n- conn profile --add officeuser\n\nthen you reference inside the node configuration using \"@officeuser\".  \n\nyou can use profiles to send multiple passwords in case you use 1 or more jumphosts.\n\nOnce the nodes are created you just connect to them without passwords.\n- conn server1\n- conn server2@servers@home\n- conn server\n\n## automation module\n\nthis is the new feature, once you created your nodes you can use them with the automation module\n\n- import connpy\n- config = connpy.configfile()\n- server1 = config.getitem(\"server1@office\")\n- router1 = config.getitem(\"router@office\")\n- server = connpy.node(\"server1\",**server1,config = config)\n- router = connpy.node(\"router1\",**router1,config = config)\n- print(router.run([\"term len 0\", \"show run\"]))\n- if server.test(\"ls -la\", \"folder to find\"):\n-   print(\"folder found\")\n- else:\n-   print(\"missing folder\")\n\nYou can also run in multiple devices at the same time using class nodes. (more in the documentation)\n\nHope someone find it useful!!\nthanks!", "upvote_ratio": 0.89, "id": "t3_tx604l", "created_utc": 1649195030.0}
{"sub": "Python", "title": "\"Bicycle or Metro\": My first interactive web app using Dash/Flask.", "selftext": "I collected data from the Google Direction API and built an interactive app comparing travel duration times between Bicycle and Public Transportation in Santiago, Chile.\n\nEach specific route to a point in the map can be accessed by clicking on the destination point, and the relevant information will be displayed.\n\nThe app is deployed on Heroku (forgive the slow response): [http://biciometro.herokuapp.com](http://biciometro.herokuapp.com) . The website is in Spanish, but its features should be easy to understand.\n\n**Tools:**\n\n* For the GeoJSON grid, I used this [grid creator](https://cityofaustin.github.io/geojson-grid/#). I used Pandas, Dash, Plotly, NumPy and Mapbox. Everything is coded in Python.\n* Source code for the app: [https://github.com/mirkosimunovic/biciometro](https://github.com/mirkosimunovic/biciometro)\n* Please show support with a Github star   if you like the project. Thank you!", "upvote_ratio": 0.8, "id": "t3_tx4xj4", "created_utc": 1649192222.0}
{"sub": "Python", "title": "Create a timelapse of any canvas area and any timeframe of r/place", "selftext": "[(Github) Source code and download](https://github.com/gislerro/rplace-cropped-timelapse-creator)\n\nConfiguration in the config.yaml file:\n\n* range of images to create timelapse for [(archive of snapshots here)](https://rplace.space/combined/)\n* granularity of frames (take every n-th image - determines speed of timelapse)\n* top-left coordinates of the canvas timelapse\n* width &amp; height of the canvas\n* mp4 output dimensions &amp; name\n\n&amp;#x200B;\n\n[Example](https://reddit.com/link/tx3x67/video/vgytvsfpprr81/player)", "upvote_ratio": 0.74, "id": "t3_tx3x67", "created_utc": 1649189564.0}
{"sub": "Python", "title": "Applications of Python", "selftext": "Hey! Anyone who is interested in learning about the applications of python in the field of automation should check out [this](https://medium.com/@Nick_27/automation-using-python-66ec75a6b0ba) article. \n\nIt really encourages you to learn more about the language!", "upvote_ratio": 0.3, "id": "t3_tx0enu", "created_utc": 1649180149.0}
{"sub": "Python", "title": "Learning Python", "selftext": "Python just makes so much freaking sense! LOVING IT!!", "upvote_ratio": 0.8, "id": "t3_tx0cns", "created_utc": 1649179997.0}
{"sub": "Python", "title": "Running a live 45-minutes session on the fundamentals of observability, OpenTelemetry, and distributed tracing with microservices messaging systems (Kafka, RabbitMQ, etc)", "selftext": "Hi everyone, we're running another live OpenTelemetry and observability fundamentals session -  Wednesday, April 20 at 11 AM PDT.\n\nYou will learn how to instrument your message brokers and apps to capture traces with OpenTelemetry.\n\nThis session is at no cost and vendor-neutral.\n\nYou can expect in this session: 45 minutes of core concepts, how to deploy it yourself hands-on + Q&amp;A.\n\nIf you are interested in observability, OpenTelemetry, and tracing - join!\n\nRegister here [https://www.aspecto.io/opentelemetry-fundamentals/messaging-systems/](https://www.aspecto.io/opentelemetry-fundamentals/messaging-systems/?utm_source=post&amp;utm_medium=reddit&amp;utm_campaign=r-python-opentelemetry-fundamentals-messaging-systems)", "upvote_ratio": 0.4, "id": "t3_tx097t", "created_utc": 1649179745.0}
{"sub": "Python", "title": "As of today, how well does Anaconda run on M1 MacBook Pro?", "selftext": "Last year, there was an article from the developer that they were working on M1 version of the software suite. However, I don't seem to hear any news after that. Are they still working on it? I recall there were some compatibility issues with some packages last year. How well does it run on M1 MacBook Pro as of today? Are Anaconda and its packages fully compatible with M1 Mac now? ", "upvote_ratio": 0.75, "id": "t3_tww564", "created_utc": 1649168665.0}
{"sub": "Python", "title": "Attending my first PyCon US 2022 (SLC) - tips?", "selftext": "What recommendations would you have for someone new to this conference?\n\nI am new to programming and a beginner learning python (self-taught). Hoping to network, learn about new ideas to get my excited, and do some running in a new place.", "upvote_ratio": 0.91, "id": "t3_twr5wj", "created_utc": 1649151796.0}
{"sub": "Python", "title": "Reason to go from Python3.9 to 3.10 ?", "selftext": "I don't find and real advantages and all i have to do works fine on 3.9.\n\nChange my mind.", "upvote_ratio": 0.39, "id": "t3_twq7my", "created_utc": 1649147520.0}
{"sub": "Python", "title": "I'm presenting live in 6 hours at Microsoft Reactor online about troubleshooting Python applications, especially on Kubernetes. (Part 2 of the Python on Kubernetes series.) Let me know your questions in advance", "selftext": "nan", "upvote_ratio": 0.89, "id": "t3_twpt1m", "created_utc": 1649145681.0}
{"sub": "Python", "title": "Python Firebird-driver 1.4.3 released", "selftext": "nan", "upvote_ratio": 0.56, "id": "t3_twpopx", "created_utc": 1649145186.0}
{"sub": "Python", "title": "How to Execute SQL Queries in Python and R Tutorial", "selftext": "nan", "upvote_ratio": 0.5, "id": "t3_twpo9w", "created_utc": 1649145127.0}
{"sub": "Python", "title": "Community for Python app developers", "selftext": "Hey, I'm curious if there is a dedicated community of Python developers who are making apps with frontend interface. (As opposed to data analysis or training ML models). \n\nFor example, developers who are using Streamlit or Anvil. (Are there any other big ones?)\n\nDo you know any such communities?", "upvote_ratio": 0.58, "id": "t3_twpbuw", "created_utc": 1649143631.0}
{"sub": "Python", "title": "Python and the Truth", "selftext": "Hi folks,\n\nExcited to share some notes on Python's Truth values, their internals, and possible applications.\n\n[https://towardsdatascience.com/python-and-the-truth-90cf08380246](https://towardsdatascience.com/python-and-the-truth-90cf08380246)\n\nThanks and happy reading!", "upvote_ratio": 0.5, "id": "t3_twoqza", "created_utc": 1649141265.0}
{"sub": "Python", "title": "Why is Python becoming indispensable in IoT Industry?", "selftext": " IoT Applications are usually pre-built SaaS (software-as-a-service)  applications. These can present and analyze captured IoT sensor data to  businesses via dashboards. They use machine learning algorithms and  analyze huge amounts of connected sensor data in the cloud. Real-time  IoT alerts and dashboards provide you visibility into key performance  indicators, statistics for the meantime between failures, and other  details.", "upvote_ratio": 0.31, "id": "t3_two0m1", "created_utc": 1649138209.0}
{"sub": "Python", "title": "I got a little problem with my visualization program.", "selftext": "I find a great code of visualizing the PSO algorithm in 2D. And I tried to change it into 3D.I've done the majority part of it, but now the particles just don't update, they superpose over the old ones.\n\n[Figure shows like this](https://preview.redd.it/fhiagfvecnr81.png?width=1102&amp;format=png&amp;auto=webp&amp;s=6ed3c223f26d9407aecc242b2facc2e23458e16f)\n\n&amp;#x200B;\n\nHere is the code link\uff1a\n\n[https://linkode.org/#LsQHmirehTrbP1pqXEI2Q5](https://linkode.org/#LsQHmirehTrbP1pqXEI2Q5)", "upvote_ratio": 0.5, "id": "t3_twm7ke", "created_utc": 1649131590.0}
{"sub": "Python", "title": "Visualize Differential Equations", "selftext": "[https://www.youtube.com/watch?v=gH47pQHeKDE](https://www.youtube.com/watch?v=gH47pQHeKDE)\n\nNormalize for your highest order derivative and plug in your initial conditions to numerically solve any differential equation", "upvote_ratio": 0.58, "id": "t3_twjqds", "created_utc": 1649123664.0}
{"sub": "Python", "title": "Use nested GitHub Action Runners to reduce costs by up to 50% for multiple jobs in one workflow", "selftext": "nan", "upvote_ratio": 0.9, "id": "t3_twhorh", "created_utc": 1649117806.0}
{"sub": "Python", "title": "Tuesday Daily Thread: Advanced questions", "selftext": "Have some burning questions on advanced Python topics? Use this thread to ask more advanced questions related to Python.\n\n**If your question is a beginner question we hold a beginner Daily Thread tomorrow (Wednesday) where you can ask any question! We may remove questions here and ask you to resubmit tomorrow.**\n\nThis thread may be fairly low volume in replies, if you don't receive a response we recommend looking at r/LearnPython or joining the Python Discord server at [https://discord.gg/python](https://discord.gg/python) where you stand a better chance of receiving a response.", "upvote_ratio": 0.84, "id": "t3_twha21", "created_utc": 1649116810.0}
{"sub": "Python", "title": "Predicting the Champions League with Python! (21/22)", "selftext": "nan", "upvote_ratio": 0.67, "id": "t3_twgvyu", "created_utc": 1649115763.0}
{"sub": "Python", "title": "I published my first tutorial on YouTube: Insertion sort explained + Python code", "selftext": "nan", "upvote_ratio": 0.63, "id": "t3_twe9kh", "created_utc": 1649109341.0}
{"sub": "Python", "title": "I made use of Zoho Sheets and used it as a database engine", "selftext": "I started working on this yesterday and have just finished rewriting it into classes and publishing it as a PyPI project :)\n\nThis module uses Zoho Sheets (which is completely free to use) as a way to store data.\nI'm keen on the development of ZohoDB.py and aiming to keep improving it's performance in order to make it a kinda production-ready module :)\n\nWould love to hear your feedback on this one\nhttps://github.com/oddmario/zohodb.py", "upvote_ratio": 0.86, "id": "t3_tw954f", "created_utc": 1649096725.0}
{"sub": "Python", "title": "Create python logo on r/place", "selftext": "I in the r/place we can create a python logo on 384,1618", "upvote_ratio": 0.33, "id": "t3_tw8p3f", "created_utc": 1649095625.0}
{"sub": "Python", "title": "Voice Controlled Switch Using Arduino &amp; Python", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_tw8ace", "created_utc": 1649094644.0}
{"sub": "Python", "title": "Python f-strings Are More Powerful Than You Might Think", "selftext": "nan", "upvote_ratio": 0.93, "id": "t3_tw69i5", "created_utc": 1649089724.0}
{"sub": "Python", "title": "Book for python network programming", "selftext": "Hey folks, can you please recommend a good book(s) for network programming? Something that would cover subjects from low level TCP/IP, to SMTP/SSH/HTTP/etc libraries? Thanks in advance!", "upvote_ratio": 1.0, "id": "t3_tw5tru", "created_utc": 1649088639.0}
{"sub": "Python", "title": "Head-first Kubernetes - A hands-on tutorial for beginners", "selftext": "nan", "upvote_ratio": 0.85, "id": "t3_tw50n4", "created_utc": 1649086671.0}
{"sub": "Python", "title": "Active Learning in Machine Learning Explained", "selftext": "nan", "upvote_ratio": 0.57, "id": "t3_tw4o2v", "created_utc": 1649085795.0}
{"sub": "Python", "title": "Can my IT department tell (without any sortve deep dive) that I'm using a python script to login?", "selftext": "I have to do timesheets for my crew every morning, and it takes about twenty minutes a day. I've long since thought I could automate this, and write a little proof of concept and it worked. \n\nIt would however involve using pyautogui to enter my login information including password into our time management software. Do you think this could get me into shit with the IT department?", "upvote_ratio": 0.95, "id": "t3_tw2ozp", "created_utc": 1649080757.0}
{"sub": "Python", "title": "StackSocial Discount. Are their courses any good?", "selftext": "I saw that they were offering a 'complete Python certification bundle' for $35 'reduced from $2k' if you can believe that. \n\nI'm beginner to intermediate - written some Flask websites and programmed some automated stuff, so I was wondering if anyone would recommend this sort of thing to get into Python programming professionally?", "upvote_ratio": 0.43, "id": "t3_tw2ihc", "created_utc": 1649080260.0}
{"sub": "Python", "title": "Rock, Paper, Scissors", "selftext": "LMK what I can do differently. Thanks!\n\n&amp;#x200B;\n\nrunning = True  \nwhile running:  \n player1 = input(\" (Player1) Rock, Paper, or Scissors:\\\\n &gt;\")  \n player2 = input(\" (Player 2) Rock, Paper, or Scissors:\\\\n &gt;\")  \n   \n if player1.lower() == \"rock\" and player2.lower() == \"scissors\":  \n print(\"Player 1 Wins!\")  \n if player2.lower() == \"rock\" and player1.lower() == \"scissors\":  \n print(\"Player 1 Wins!\")  \n if player1.lower() == \"paper\" and player2.lower() == \"scissors\":  \n print(\"Player 2 Wins!\")  \n if player2.lower() == \"paper\" and player1.lower() == \"scissors\":  \n print(\"Player 1 Wins!\")  \n if player1.lower() == \"rock\" and player2.lower() == \"paper\":  \n print(\"Player 2 Wins\")  \n if player2.lower() == \"rock\" and player1.lower() == \"paper\":  \n print(\"Player 1 Wins\")  \n playing = input(\"Do you want to keep playing? y/n \\\\n &gt; \")  \n if playing.lower() == \"n\":  \n break  \n if playing.lower() == \"y\":  \n running = True", "upvote_ratio": 0.6, "id": "t3_tw0wkp", "created_utc": 1649075628.0}
{"sub": "Python", "title": "Solving and Animating the 3D Double Pendulum in Python: Sympy for algebra, Scipy for numerically solving differential equations, and vpython for 3D animation", "selftext": "nan", "upvote_ratio": 0.86, "id": "t3_tw0eck", "created_utc": 1649073978.0}
{"sub": "Python", "title": "ConfigParser - manage user-editable settings for your Python programs", "selftext": "nan", "upvote_ratio": 0.43, "id": "t3_tvzwqb", "created_utc": 1649072412.0}
{"sub": "Python", "title": "Scraping Naver Videos in Python", "selftext": "Full code: \n\n```python\nimport requests, os, json\nfrom parsel import Selector\n\n\ndef parsel_scrape_naver_videos():\n    params = {\n        \"query\": \"minecraft\",  # search query\n        \"where\": \"video\"       # video results\n    }\n\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.87 Safari/537.36\",\n    }\n\n    html = requests.get(\"https://search.naver.com/search.naver\", params=params, headers=headers, timeout=30)\n    selector = Selector(html.text)  # very similar to bs4, except parsel supports Xpath \n\n    video_results = []\n\n    for video in selector.css(\".video_bx\"):\n        # https://parsel.readthedocs.io/en/latest/usage.html#using-selectors\n        title = video.css(\".text::text\").get()\n        link = video.css(\".info_title::attr(href)\").get()\n        thumbnail = video.css(\".thumb_area img::attr(src)\").get()\n        channel = video.css(\".channel::text\").get()\n        origin = video.css(\".origin::text\").get()\n        video_duration = video.css(\".time::text\").get()\n        views = video.css(\".desc_group .desc:nth-child(1)::text\").get()\n        date_published = video.css(\".desc_group .desc:nth-child(2)::text\").get()\n\n        video_results.append({\n            \"title\": title,\n            \"link\": link,\n            \"thumbnail\": thumbnail,\n            \"channel\": channel,\n            \"origin\": origin,\n            \"video_duration\": video_duration,\n            \"views\": views,\n            \"date_published\": date_published\n        })\n    \n    print(json.dumps(video_results, indent=2, ensure_ascii=False))\n```\n\nPart of the output:\n\n```json\n[\n  {\n    \"title\": \" : \ud83c\udf32 How to build Survival Wooden Base (#3)\",\n    \"link\": \"https://www.youtube.com/watch?v=n6crYM0D4DI\",\n    \"thumbnail\": \"https://search.pstatic.net/common/?src=https%3A%2F%2Fi.ytimg.com%2Fvi%2Fn6crYM0D4DI%2Fmqdefault.jpg&amp;type=ac612_350\",\n    \"channel\": \"\uc18c\ud53c Sopypie\",\n    \"origin\": \"Youtube\",\n    \"video_duration\": \"24:06\",\n    \"views\": \"671\",\n    \"date_published\": \"4\uc77c \uc804\"\n  },\n  {\n    \"title\": \"\ub9c8\uc778\ud06c\ub798\ud504\ud2b8 \ubb34\ud55c\uc21c\ud658 \uc774\ub860 (\",\n    \"link\": \"https://www.youtube.com/watch?v=kQ7wyG9mShQ\",\n    \"thumbnail\": \"https://search.pstatic.net/common/?src=https%3A%2F%2Fi.ytimg.com%2Fvi%2FkQ7wyG9mShQ%2Fmqdefault.jpg&amp;type=ac612_350\",\n    \"channel\": \"TV\ube14\ub8e8\uc704\ud0a4\",\n    \"origin\": \"Youtube\",\n    \"video_duration\": \"01:44\",\n    \"views\": \"9\ub9cc\",\n    \"date_published\": \"2022.02.15.\"\n  } ... other results\n]\n```\n\nBlog post link if you need code explanation: https://serpapi.com/blog/scrape-naver-video-results-with-python/", "upvote_ratio": 0.5, "id": "t3_tvzlgk", "created_utc": 1649071317.0}
{"sub": "Python", "title": "Creating A Modern Python Development Environment", "selftext": "nan", "upvote_ratio": 0.71, "id": "t3_tvyvov", "created_utc": 1649068761.0}
{"sub": "Python", "title": "Different types of implementation of polymorphism", "selftext": "Hi All,\n\nI am reading about OOP in python and wonder if we implement ` __add__`, `__sub__` etc, it looks to me thst we are doing operator overloding. So my question is, that is it correct to think like that and does this implementation falls under AdHoc polymorphism in python.\n\nThanks\nPradeep", "upvote_ratio": 0.6, "id": "t3_tvx5ee", "created_utc": 1649061795.0}
{"sub": "Python", "title": "CaptchaCracker - Deep Learning-based Captcha Recognizer", "selftext": "Hello!\n\nI made a python open source project.\n\n**Github Repository:** [https://github.com/WooilJeong/CaptchaCracker](https://github.com/WooilJeong/CaptchaCracker)\n\n# CaptchaCracker\n\n## Introduction\n\nCaptchaCracker is an open source Python library that provides functions to create and apply deep learning models for Captcha Image recognition. You can create a deep learning model that recognizes numbers in the Captcha Image as shown below and outputs a string of numbers, or you can try the model yourself.\n\n## Input\n\nhttps://preview.redd.it/qltiatrqxgr81.png?width=250&amp;format=png&amp;auto=webp&amp;s=8e4d370203ff502b4681c8b35a8fee1cff7fd259\n\n## Output\n\n    023062\n\n## Installation\n\n    pip install CaptchaCracker\n\n## Dependency\n\n    pip install numpy==1.19.5 tensorflow==2.5.0\n\n## Examples\n\n## Train and save the model\n\nBefore executing model training, training data image files in which the actual value of the Captcha image is indicated in the file name should be prepared as shown below.\n\n* [Download Sample Dataset](https://github.com/WooilJeong/CaptchaCracker/raw/main/sample.zip)\n\nhttps://preview.redd.it/n451dp4sxgr81.png?width=1009&amp;format=png&amp;auto=webp&amp;s=2c0173b4d685f73642edbfd780ae3fdf4d921bdf\n\n    import glob\n    from CaptchaCracker import CreateModel\n    \n    train_img_path_list = glob.glob(\"../data/train_numbers_only/*.png\")\n    \n    CM = CreateModel(train_img_path_list)\n    model = CM.train_model(epochs=100)\n    model.save_weights(\"../model/weights.h5\")\n\n## Load a saved model to make predictions\n\n    from CaptchaCracker import ApplyModel\n    \n    weights_path = \"../model/weights.h5\"\n    AM = ApplyModel(weights_path)\n    \n    target_img_path = \"../data/target.png\"\n    pred = AM.predict(target_img_path)\n    print(pred)", "upvote_ratio": 0.56, "id": "t3_tvwizf", "created_utc": 1649059109.0}
{"sub": "Python", "title": "QualityScaler 1.1.0 - Image/video upscaling &amp; enhancement GUI app based on BRSGAN &amp; RealSR_JPEG", "selftext": "Image/video upscaling GUI app based on BRSGAN &amp; RealSR\\_JPEG\n\n[GUI interface](https://preview.redd.it/9rxldsstugr81.png?width=1392&amp;format=png&amp;auto=webp&amp;s=4760f210cb169dc95679b5a5183ed0e142cbbad4)\n\n## Links.\n\nGithub -&gt; [https://github.com/Djdefrag/QualityScaler/releases/tag/1.1.0](https://github.com/Djdefrag/QualityScaler/releases/tag/1.1.0)\n\nItch -&gt; [https://jangystudio.itch.io/qualityscaler](https://jangystudio.itch.io/qualityscaler)\n\n## Credits.\n\n*BSRGAN -* [*https://github.com/cszn/BSRGAN*](https://github.com/cszn/BSRGAN) *|* [*https://arxiv.org/abs/2103.14006*](https://arxiv.org/abs/2103.14006)\n\n*RealSR\\_JPEG -* [*https://github.com/jixiaozhong/RealS*](https://github.com/jixiaozhong/RealS)R | [https://arxiv.org/pdf/2005.01996.pdf](https://arxiv.org/pdf/2005.01996.pdf)\n\n## Installation.\n\nQualityScaler is completely portable; just download, unzip and execute the file .exe\n\n## Supported AI Backend.\n\n* Nvidia Cuda \\[v10.2\\]\n* CPU \\[works without GPU, but is very slow\\]\n\n## Features.\n\n* Easy to use GUI\n* Images and video upscale\n* Drag&amp;drop files \\[image/multiple images/video\\]\n* Different upscale factors:\n   * auto - automatic choose best upscale factor for the GPU used (to avoid running out of VRAM)\n   * x1\u00a0 \u00a0- will mantain same resolution but will reconstruct the image (ideal for bigger images)\u00a0\n   * x2\u00a0 \u00a0- upscale factor 2: 500x500px -&gt; 1000x1000px\n   * x4\u00a0 \u00a0- upscale factor 4: 500x500px -&gt; 2000x2000px\n* Cpu and Gpu \\[cuda\\] backend\n* Compatible images - PNG, JPEG, BMP, WEBP, TIF\u00a0\u00a0\n* Compatible video\u00a0 - MP4, WEBM, GIF, MKV, FLV, AVI, MOV\u00a0\n\n## Next steps.\n\n1. Use both model for the upscale\n2. Include audio for upscaled video\n3. Support for other GPUs (AMD, Intel) with new backend\n\n## Example.\n\nOriginal photo (200x200 px)\n\nhttps://preview.redd.it/3yb1t6nxugr81.png?width=220&amp;format=png&amp;auto=webp&amp;s=b950ff4af4cfcb5e2fe97cece00f9b4c7c3f1698\n\nBSRGAN (800x800 px)\n\nhttps://preview.redd.it/wghr3c7yugr81.png?width=880&amp;format=png&amp;auto=webp&amp;s=67360909111338bfb01e111e1fab8cdd6c3531e7\n\nRealSR\\_JPEG (800x800 px)\n\nhttps://preview.redd.it/blh6vmlyugr81.png?width=880&amp;format=png&amp;auto=webp&amp;s=68af9f79d102b2c906f8c54bb84fdef44b6db7d2", "upvote_ratio": 0.85, "id": "t3_tvwbu7", "created_utc": 1649058226.0}
{"sub": "Python", "title": "Machine Learning Sandbox in Python", "selftext": "Hope you are doing well. I was reaching out to share one of my projects with all of you . The idea was to build a sandbox of ML algorithms that people could use to demonstrate classical ML algorithms and also break down the same with simple from-the-scratch implementation in Python. You can simply fork the repo and start running the algorithms. I hope you find it useful. Do let me know your feedback on the same and if I can make any improvements in this.\n\n&amp;#x200B;\n\nProject Link: [https://github.com/devAmoghS/Machine-Learning-with-Python](https://github.com/devAmoghS/Machine-Learning-with-Python)", "upvote_ratio": 0.91, "id": "t3_tvw388", "created_utc": 1649057193.0}
{"sub": "Python", "title": "Is List Comprehension The Most Effective Way to Solve Any Tasks? | Python", "selftext": "nan", "upvote_ratio": 0.25, "id": "t3_tvut3j", "created_utc": 1649052016.0}
{"sub": "Python", "title": "Choosing Python over other languages", "selftext": "This is an interesting blog that I read. It's written by Brett Cannon who is a core developer of Python. It essentially explain why you shouldn't automatically ignore Python because it's not as fast as some other programs.\n\n&amp;#x200B;\n\n [Selecting a programming language can be a form of premature optimization (snarky.ca)](https://snarky.ca/programming-language-selection-is-a-form-of-premature-optimization/)", "upvote_ratio": 0.69, "id": "t3_tvubzw", "created_utc": 1649050282.0}
{"sub": "Python", "title": "Open-Source python package to find relevant images for a sentence", "selftext": "&amp;#x200B;\n\n[Search results for \\\\\"a happy potato\\\\\"](https://preview.redd.it/2cnopvno0gr81.jpg?width=744&amp;format=pjpg&amp;auto=webp&amp;s=eecc9e9948c386fbd959289257c347fa19c35644)\n\nYou **don't** need to caption the images for the search to work, and it is **not** just limited to objects in the image but an overall understanding built using a neural network trained on images/text found on internet.\n\nThis is what [CLIP-as-service](https://github.com/jina-ai/clip-as-service) enables. It is an open-source library to create embeddings of images and text using CLIP. These embeddings can be used to find the relevant images for any sentence.\n\n**What is CLIP?**\n\nCLIP is a Neural Network trained on variety of images and natural language sentences available on the internet. It enables understanding of concepts in an image as natural language. This can be used for cases such as searching image by text or describing an image in natural language.\n\nThe demo screenshot shown in the post is an example showing relevant images for a particular query sentence.\n\nMore info on Github readme about how to achieve this\n\n* Source code: [https://github.com/jina-ai/clip-as-service](https://github.com/jina-ai/clip-as-service)\n* License: Apache 2.0\n\nWhat would you use it for? And what next features/improvements should I work on?", "upvote_ratio": 1.0, "id": "t3_tvs0dp", "created_utc": 1649042711.0}
{"sub": "Python", "title": "If you're bored at work or school.", "selftext": "Try to open Python Idle and Import antigravity module\n\n`import antigravity`\n\nThen laugh.", "upvote_ratio": 0.87, "id": "t3_tvp8uf", "created_utc": 1649034470.0}
{"sub": "Python", "title": "swagger-gen: the Swagger spec generator you've been waiting for", "selftext": "Hey Python people,\n\nAs a .NET developer by day, I've always been bummed by the lack of real Swagger spec generators available for Flask compared to Swashbuckle for ASP.NET.  I personally don't want to write some markup or clutter the service with a bunch of code to generate the docs, I wanted something that would generate the spec for you, with no setup required, with optional features to expand the detail of the documentation.\n\nSo I built that!  If you like Swagger, you'll like this.  It's a pretty full-featured library, but I'm sure there's room for improvement and welcome any friends that'd like to contribute.\n\n[https://pypi.org/project/swagger-gen/](https://pypi.org/project/swagger-gen/)\n\n[https://github.com/danleonard-nj/swagger-gen](https://github.com/danleonard-nj/swagger-gen)\n\nThanks for checking it out :)", "upvote_ratio": 1.0, "id": "t3_tvp63o", "created_utc": 1649034247.0}
{"sub": "Python", "title": "Is r/Python participating in r/place?", "selftext": "If so, where can one get involved?", "upvote_ratio": 0.25, "id": "t3_tvom5q", "created_utc": 1649032639.0}
{"sub": "Python", "title": "iMandelbrot is a tool made out of PyGame that visualizes the nature of a Mandelbrot fractal!", "selftext": "# Mandelbrot: An Interactive Module\n\n**iMandelbrot** is a **learning aid** purely made out of **PyGame** that visualizes the nature of a **Mandelbrot** fractal!\n\nGitHub Link: [https://github.com/wonmor/iMandelbrot-Plotter](https://github.com/wonmor/iMandelbrot-Plotter)\n\n## Public Release 1.2.0\n\n**DEVELOPED** AND **MAINTAINED** BY **JOHN SEONG** \u2022 **MIT** LICENSE\n\n[Download for **Intel-based Macs**](https://github.com/wonmor/iMandelbrot-Plotter/raw/main/installer/iMandelbrot_Mac.dmg) \u2192 Make sure to turn on the '**Open Using Rosetta**' option on **M1 Macs**!\n\n## Why Our Product Is Sexier Than Others\n\nFor the sake of **optimization**, **iMandelbrot** only generates the coordinates above the x-axis \u2014 basically duplicating to the corresponding coordinates below the horizontal line.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/jr48nb5fper81.png?width=3456&amp;format=png&amp;auto=webp&amp;s=3b116ded17242fd28d1f685145f0d283932eb5a5", "upvote_ratio": 0.67, "id": "t3_tvofnv", "created_utc": 1649032116.0}
{"sub": "Python", "title": "Monday Daily Thread: Project ideas!", "selftext": "Comment any project ideas beginner or advanced in this thread for others to give a try! If you complete one make sure to reply to the comment with how you found it and attach some source code! If you're looking for project ideas, you might be interested in checking out Al Sweigart's, [\"The Big Book of Small Python Projects\"](https://inventwithpython.com/bigbookpython/) which provides a list of projects and the code to make them work.", "upvote_ratio": 0.78, "id": "t3_tvnusp", "created_utc": 1649030411.0}
{"sub": "Python", "title": "Python libraries for hierarchical or multilevel time series analysis", "selftext": "What are the python libraries best suited for analyzing time series with hierachical or multiple levels like geographic levels between neighborhoods within cities and cities within states.", "upvote_ratio": 0.76, "id": "t3_tvmdfk", "created_utc": 1649026196.0}
{"sub": "Python", "title": "Python and Blockchains", "selftext": "Why is Python is not used as much in blockchain technology, Specially in the core of blockchains", "upvote_ratio": 0.22, "id": "t3_tvl7m8", "created_utc": 1649023159.0}
{"sub": "Python", "title": "Two new articles this week: Beginner tutorial on classes and object-oriented programming with exercises and Python careers research", "selftext": "Hi all,\n\nIn the last few days, I had a lot of fun publishing two brand new articles for you:  \n\n\n* [Python Classes Zero to Expert: A Tutorial with Exercises](https://codesolid.com/getting-started-with-python-classes/)  \nFor new programmers just beginning with Python and learning about object orientation, this beginner-focused article covers the basics in some detail, with exercises at the end to reinforce what you've learned.  \n\n* [Best Python Careers: New Research Reveals Top Fields](https://codesolid.com/career-paths-for-python-programmers/)  \nBased on my own experience in the industry as well as job board research, I detail how many jobs there are in different areas of Python programming and what sorts of hoops you might need to jump through to break into the field.\n\nPlease upvote if you like them, and enjoy!", "upvote_ratio": 0.85, "id": "t3_tvkqlt", "created_utc": 1649021930.0}
{"sub": "Python", "title": "we need to make the python logo in r/place", "selftext": "Don't you think? Imo it would be so cool if some of us got together and started to plan it :)", "upvote_ratio": 0.25, "id": "t3_tvjg5x", "created_utc": 1649018716.0}
{"sub": "Python", "title": "Twitter read and analysis", "selftext": "    # Import tweepy to work with the twitter API\n    import tweepy as tw\n    \n    # Import numpy and pandas to work with dataframes\n    import numpy as np\n    import pandas as pd\n    \n    from matplotlib import pyplot as plt\n    \n    consumer_key ='mycode'\n    consumer_secret = 'mycode'\n    \n    access_token ='mycode'\n    access_token_secret = 'mycode'\n    \n    # Authenticate\n    auth = tw.OAuthHandler(consumer_key, consumer_secret)\n    # Set Tokens\n    auth.set_access_token(access_token, access_token_secret)\n    # Instantiate API\n    api = tw.API(auth, wait_on_rate_limit=True)\n    \n    hashtag = \"#slap\"\n    query = tw.Cursor(api.search_tweets, q=hashtag).items(1000)\n    tweets = [{'Tweet':tweet.text, 'Timestamp':tweet.created_at} for tweet in query]\n    print(tweets)\n    \n    df = pd.DataFrame.from_dict(tweets)\n    df.head()\n    \n    willsmith_handle = ['willsmith', 'will smith', 'will', 'smith', 'slap\\'s']\n    \n    def identify_subject(tweet, refs):\n        flag = 0\n     for ref in refs:\n     if tweet.find(ref) != -1:\n                flag = 1\n     return flag\n    \n    df['will smith'] = df['Tweet'].apply(lambda x: identify_subject(x, willsmith_handle))\n    df.head(10)\n    \n    df.to_csv('Twitter_feed.csv', encoding='utf-8')\n\n \n\nA working process based on an online tutorial. The code takes in twitter tweets and exports them to csv file.\n\nI'm going to try and expand to read the sentiment and feedback of different tweets.", "upvote_ratio": 0.6, "id": "t3_tvj90z", "created_utc": 1649018223.0}
{"sub": "Python", "title": "Dice roll Probability", "selftext": "    import random\n    import collections\n    import time\n    \n    ##Outcome\tList of Combinations\tTotal\n    ##2\t1+1\t1\n    ##3\t1+2, 2+1\t2\n    ##4\t1+3, 2+2, 3+1\t3\n    ##5\t1+4, 2+3, 3+2, 4+1\t4\n    ##6\t1+5, 2+4, 3+3, 4+2, 5+1\t5\n    ##7\t1+6, 2+5, 3+4, 4+3, 5+2, 6+1\t6\n    ##8\t2+6, 3+5, 4+4, 5+3, 6+2\t5\n    ##9\t3+6, 4+5, 5+4, 6+3\t4\n    ##10\t4+6, 5+5, 6+4\t3\n    ##11\t5+6, 6+5\t2\n    ##12\t6+6\t1\n    \n    \n    \n    #create list\n    dice_list=[]\n    \n    for i in range(1,7):\n        #dice 1\n        dice1= i\n    \n        for j in range(1,7):\n            #dice 2\n            dice2= j\n            #dice total\n            dice_reads=dice1+dice2\n    \n            ##add to list\n            dice_list.append(dice_reads)\n    \n    ##sort list\n    dice_list.sort()\n    \n    ##dictionary\n    probability = {}\n    \n    ##Place a Bet\n    User_bet = input(\"Enter a bet between 2 to 12: \")\n    \n    \n    ###using collections.Counter for number frequency\n    probability=collections.Counter(dice_list)\n    \n    # printing the probability\n    #print(probability)\n    \n    ##percentage method\n    def math_percentage(User_bet):\n        print(\"The user bets a \"+str(User_bet))\n        for dice in probability:\n            prob=probability[dice]\n            percentage=round(float(((prob)*100)/36))\n            #print(\"The percentage of a \"+str(dice) +\" is \"+str(percentage)+ \" %\")\n            if dice==int(User_bet):\n                print(\"The percentage of a \" + str(dice) + \" is \" + str(percentage) + \" %\")\n    \n                ##Pause\n                time.sleep(10)\n    \n    ##call method\n    math_percentage(User_bet)\n    \n    ##100 dice rolls\n    for i in range(0,1):\n        #dice 1\n        dice1= (random.randint(1,6))\n        #dice 2\n        dice2= (random.randint(1,6))\n        #dice total\n        dice_reads=dice1+dice2\n    \n        print(\"Dice reads: \"+str(dice_reads))", "upvote_ratio": 0.29, "id": "t3_tvinqx", "created_utc": 1649016754.0}
{"sub": "Python", "title": "Who exactly is a python developer? I know this is a broad term which include many paths. But do we have to cover them all to become one? much like, one term to rule them all?", "selftext": "Hii all \n\nI've seen the term \"Python developer\" been around online, and the  roadmap to become one. As we all know, it does include many paths, web dev, analysis, DS, ML and so on. To be called a python dev, do we actually need to cover all potential paths and then just choose one? or we should be able to do anything under this big umbrella term? \n\nMy question is coming from my interest in learning python and DS. That's when I started my journey. I am still in process of learning. But I also want to learn some kills related for backend dev via python. For many reasons, first even though my love goes for DS but there's less opportunities and more competition. (as far as I know). So, probably having extra skills can add value or come in handy for model production. Also, I won't mind working as backend dev, as I'll def use DS in research for later future plans. \n\nWhen I share this with someone, like my interest in DS and web dev. I got the response of these are totally different and I am heading to the wrong direction (well, maybe I am) but I wanted to cover my interest in more of a broader term like \"python dev\" but there's the fact that some of the  paths under this term I don't have much of interest in.\n\nShould I specify myself as python dev with some specific libraries \"Generally speaking\". I do understand when it come to job haunting  I should be more specific like looking for DS, analysis...etc. \n\nBut let's say I want include sth in my linkedin bio. I don't want to throw analysis, DS, ML, web dev all at once. I was considering python dev and then when it's come to job haunting I can pick and choose. \n\nI hope this is not a weird question :)", "upvote_ratio": 0.45, "id": "t3_tvi6ql", "created_utc": 1649015670.0}
{"sub": "Python", "title": "python logo in r/place", "selftext": "guys let\u2019s make the python logo\ndiscuss coordinates and such in the comments", "upvote_ratio": 0.29, "id": "t3_tvhwuf", "created_utc": 1649015005.0}
{"sub": "Python", "title": "I am working on some 3D reconstruction using TOMVIZ, does anybody know any tutorial for segmentation in this software?", "selftext": "nan", "upvote_ratio": 0.33, "id": "t3_tvhi0d", "created_utc": 1649014055.0}
{"sub": "Python", "title": "Introducing File Folder Hider! (made with Tkinter Python)", "selftext": "I've been working on this project for a couple of months now, first it was in the from of a terminal where the user had to give certain commands to get the job done, but now, its a fully-fledged GUI with buttons and labels to guide you. It has its two main features, \"Hide\" and \"Unhide\", which help you hide and unhide any file and/or folder from the File Explorer (more in the readme.txt). Moreover, you can opt in optionally to enter your email address and in case you ever forget the Master Password used to access the program, you can press the \"Forgot Password\" button to have a recovery code sent to your email and after verifying, you will be allowed to change the current Master Password, or if you happen to remember it again, simply go back to the main menu (because you've already verified yourself through the recovery procedure, it is safe to assume the user is the correct one). One thing to keep in mind is that all the credentials (the Master Password and your email address specifically) are encoded to ensure minimum security. At least that's better than none at all. Another important thing to note is that some command prompt screens will appear when running the program and hiding/unhiding the files/folders. That is a side-effect of trying to have the program run on the user level in order to get around permission issues and not running as admin, so sorry for that.\n\nLinks:\n\nFile Folder Hider repository: [File Folder Hider on Github](https://github.com/Mubashir78/File_Folder_Hider_with_dialog_box)\n\nMy other repositories (basically to my Github profile): [My Profile on Github (Mubashir78)](https://github.com/Mubashir78)\n\nPlease feel free to try it out for your daily routine and let me know your guys' feedback in the comments!\n\n[Dialog box for when creating\\/changing Master Password. Email is optional as it says](https://preview.redd.it/if9ff53mqcr81.png?width=502&amp;format=png&amp;auto=webp&amp;s=2b074f0ee02eec7f5e5394fe291a6ebf95432a5e)\n\n[Main Menu Dialog Box \\(after successfully creating\\/entering the Master Password\\)](https://preview.redd.it/63y52tj0scr81.png?width=501&amp;format=png&amp;auto=webp&amp;s=75fb030989360f44e6e922927211b199ce5dfefe)\n\n[Dialog box for after creating the Master Password and entering the email and running the program again. Incase of not adding an email, the \\\\\"Forgot Password\\\\\" is grayed out and an \\\\\"?\\\\\" icon shows up which tells you that you haven't added email in order to use the feature when hovering over it](https://preview.redd.it/rkxmbcp7tcr81.png?width=404&amp;format=png&amp;auto=webp&amp;s=2618f24e11794d151e83000f5ceebe04da14e056)", "upvote_ratio": 0.73, "id": "t3_tvflrx", "created_utc": 1649009385.0}
{"sub": "Python", "title": "I made a video comparing different data storage formats that python users should consider. Feedback welcome!", "selftext": "nan", "upvote_ratio": 0.75, "id": "t3_tver6p", "created_utc": 1649007270.0}
{"sub": "Python", "title": "Easier Regexps", "selftext": "nan", "upvote_ratio": 0.33, "id": "t3_tveh1e", "created_utc": 1649006548.0}
{"sub": "Python", "title": "A Redux inspired store pattern for asyncio applications - SimpleEventMonitor", "selftext": "So, recently I had to deal with the issue of setting up a solution to monitor a few on-prem deployments for health status. The on-prem clusters were configured to send emails with multiple tables with the status of every node etc.\n\nWell, it is a pretty straightforward solution was writing a python script to check the mailbox for new mail and download recent mail from the cluster, parse the mail and for each table entry verify for failed nodes and create an alert ticket of each node. I stumbled upon a [Github Project](https://github.com/wichmannpas/simple-mail-monitoring) that does something like this.\n\nConsidering the job was pretty much done, I was able to quickly use that script to do basic email retrieval and write parsers for my mail and get the job done in a few hours.\n\nHowever,  the code felt very complicated and it was pretty much thrown away, but after giving it a few hours of solace at home, it occurred to me,  this kind of problem could be better tackled with queues or pubsub model and it has been a while I have played around with python.  I wanted to avoid the hassle of setting up Redis/rabbitmq etc, I was rather looking for something like redux for python applications, which I can use to separate out my data producers and consumers.\n\nI had a few things in my mind for this side project :\n\n\\- In memory pubsub\n\n\\- Use asyncio\n\n\\- Be able to schedule events without cron tasks\n\n\\- Unobtrusive pattern to design my python code into producer and consumer model\n\n\\-  Ability to develop pull-based data sync, which can connect to your real time data feed\n\nHere is the basic hello world program which demonstrates how you can create a publisher function and schedule it with run\\_every to execute every 180 seconds after the last execution.\n\n[aioredux hello world](https://preview.redd.it/6vb91s6accr81.png?width=3164&amp;format=png&amp;auto=webp&amp;s=6de8ffdbda5fad9eb22404325887e8873a105c60)\n\nHere is the link to my [fork](https://github.com/aregee/simple-mail-monitoring) of the Simple Mail Monitoring Github project, which utilizes this pattern to run the existing functionality provided by the original source.\n\nI can't share my private repo, but I must say after moving that hackish script to this slightly better hack has made the code so clean and flexible, we have not just accomplished that email report task, but rather we are able to see many more such use cases internally where we can utilize this pattern.\n\nPlease feel free to share your thoughts, feedback, or even pull requests on the project.", "upvote_ratio": 0.75, "id": "t3_tve7uh", "created_utc": 1649005890.0}
{"sub": "Python", "title": "magic-timer: Conveniently get a rough idea of how long things take. (package)", "selftext": "nan", "upvote_ratio": 0.54, "id": "t3_tvcnmr", "created_utc": 1649001856.0}
{"sub": "Python", "title": "Darts in higher dimensions", "selftext": "While browsing YouTube, I came across the following video in which a little puzzle is presented that is solved mathematically. Using maths, an answer was presented that I validated by implementing a simulation. \n\nI thought this was a fun little challenge to program. Maybe others want to give it a go as well?   \nHow would you have approached this?   \n\n\nVideo: [https://www.youtube.com/watch?v=6\\_yU9eJ0NxA](https://www.youtube.com/watch?v=6_yU9eJ0NxA)  \nMy code: [https://gitlab.com/JeroenStiers/scripts/-/blob/main/darts\\_in\\_higher\\_dimensions.py](https://gitlab.com/JeroenStiers/scripts/-/blob/main/darts_in_higher_dimensions.py)", "upvote_ratio": 0.64, "id": "t3_tvbluw", "created_utc": 1648999132.0}
{"sub": "Python", "title": "r/Place", "selftext": "Are people interested in putting the 2 snakes in there?", "upvote_ratio": 0.25, "id": "t3_tvakqu", "created_utc": 1648996309.0}
{"sub": "Python", "title": "Simple and Super Easy-to-use Colabs", "selftext": "[Github](https://github.com/BreezeWhite/interesting-colabs)\n\nTired of figuring out how to use a research demo colab? Feeling lost in the arrangements of all the messy colab blocks and information?\n\nI want to share my personal collections of colabs that is definitely easy to use. This is not just another \u201cawesome list\u201d series of repo, with just links to the original resources. All the colabs within this collections are refactored and deliberately simplified by me. And I can promise you that they are all super easy to use, even your 6-year-old child would know how to use. \n\nThere are at most three code blocks to be executed!! Minimum steps are required to get directly to the expected final results. Example outputs are also provided to have a quick experience. Everything is just simple and clean, no lengthy and confusing intros, nor chaotic arrangements of blocks.\n\nI will continually update the repo, adding more interesting yet hard to get started projects. Feel free to comment and leave feedbacks below. If you know something is really awesome and interesting, but just no way to figure out how to use, I would be very happy adding to the list and wrapping it!", "upvote_ratio": 0.71, "id": "t3_tva26l", "created_utc": 1648994803.0}
{"sub": "Python", "title": "Instagram bot and Data gathering with GUI", "selftext": "Hi,\n\nInstaMachine is a post-oriented Instagram bot which means it is more focused on gathering data and storing it inside the SQLite database. I made this app solely to put to test my web scraping abilities as a beginner programmer and build up some portfolio, therefore I did not use any python's Instagram library such as instapy.\n\n[https://github.com/theRJorj/Insta-Machine](https://github.com/theRJorj/Insta-Machine)\n\nI would appreciate your feedback on any aspects of this project\n\n[InstaMachine](https://preview.redd.it/8gftzkswibr81.png?width=1500&amp;format=png&amp;auto=webp&amp;s=e5c30426a7429a8c9245914df4c03862d8ac566e)", "upvote_ratio": 0.5, "id": "t3_tv9nz9", "created_utc": 1648993681.0}
{"sub": "Python", "title": "Python beginners created these games based on my tutorials! What do you think?", "selftext": "nan", "upvote_ratio": 0.67, "id": "t3_tv9cin", "created_utc": 1648992723.0}
{"sub": "Python", "title": "Python open-source OpenBB Terminal against Bloomberg Terminal", "selftext": "nan", "upvote_ratio": 0.95, "id": "t3_tv7xqp", "created_utc": 1648988114.0}
{"sub": "Python", "title": "How is PyPy Tested?", "selftext": "nan", "upvote_ratio": 0.8, "id": "t3_tv7nyw", "created_utc": 1648987167.0}
{"sub": "Python", "title": "PtPython as default Django shell - my first Django package", "selftext": "Hey there!\n\nI've built a package to use PtPython as default Django shell.\n\nDue to extra functionalities of PtPython, I'm using that rather than Python and maybe many of you using ptpython too  so I tried to make it default shell in Django framework.\n\nYou can give it a try:\n\n`pip install django-ptpython`\n\nGithub repository: [https://github.com/reganto/django-ptpython/](https://github.com/reganto/django-ptpython/)\n\nPyPi: [https://pypi.org/project/django-ptpython/](https://pypi.org/project/django-ptpython/)\n\n&amp;#x200B;\n\nI'd like to get your feedbacks.\n\nThank you.", "upvote_ratio": 1.0, "id": "t3_tv7jej", "created_utc": 1648986698.0}
{"sub": "Python", "title": "Python dictionary implementation", "selftext": "nan", "upvote_ratio": 0.7, "id": "t3_tv4p4i", "created_utc": 1648974980.0}
{"sub": "Python", "title": "Share Python Code Snippets Like A Pro Without Using GitHub", "selftext": "nan", "upvote_ratio": 0.45, "id": "t3_tv33br", "created_utc": 1648968433.0}
{"sub": "Python", "title": "Python Selenium Tutorial #7 - Save &amp; Reuse Cookies", "selftext": "nan", "upvote_ratio": 0.95, "id": "t3_tv2rby", "created_utc": 1648967078.0}
{"sub": "Python", "title": "What Lies Between the Fibonacci Numbers?", "selftext": "Enjoy this brief video, where we examine the Fibonacci numbers by using a Jupyter Notebook. It is this easy, and you can get started today!\n\n[https://www.youtube.com/watch?v=Ge0k4kS9wE8](https://www.youtube.com/watch?v=Ge0k4kS9wE8)\n\nIn this little python video, we have a look at Fibonacci numbers. First, we try a few functions to generate the normal numbers: 0, 1, 1, 2, 3, 5, 8, 13, 21, ... Then we look at what happens between these numbers. If Fx is the xth Fibonacci number, then F7 = 8 and F8 = 13, but then what is F7.5? Join me and find out!\n\nThe notebook used in this video is available at:\n\n[https://colab.research.google.com/drive/1K6jWQSXzhCz9sOy5SyEHA-k4samWhWhr](https://colab.research.google.com/drive/1K6jWQSXzhCz9sOy5SyEHA-k4samWhWhr)", "upvote_ratio": 0.56, "id": "t3_tv1yd2", "created_utc": 1648963949.0}
{"sub": "Python", "title": "Data Visualization and simple Regression using Dash", "selftext": "Hi guys, I created my first application with Dash and am excited to share it.\n\nIt's a simple tool for analyzing already cleaned datasets. You can upload any dataset but generally it caps out at 100MB. Eventually, i'm hoping to allow for bigger datasets.\n\nsite: [https://www.regress.me/](https://www.regress.me/)\n\nsource: [https://github.com/SuljicAmar/Regress.me](https://github.com/SuljicAmar/Regress.me)\n\nI have a ton more planned for this (logistic/ mixed effects etc) but would love to hear any thoughts from others!\n\nhttps://reddit.com/link/tv1tq9/video/nnintgojxcr81/player\n\nEDIT: Added 3D plots, to initial visualizations and to fit\n\n&amp;#x200B;\n\nhttps://reddit.com/link/tv1tq9/video/a17qsfy5hdr81/player", "upvote_ratio": 0.97, "id": "t3_tv1tq9", "created_utc": 1648963469.0}
{"sub": "Python", "title": "Python App", "selftext": "Hello, I did a simple app on Python. This app is making a common shortcut from several. You can test it right here: [https://github.com/naelxd/Nepo](https://github.com/naelxd/Nepo)\n\nI will be very glad to read your opinions and advices. Thanks!\n\nhttps://i.redd.it/m7ytbr2i18r81.gif", "upvote_ratio": 0.14, "id": "t3_tuy9d5", "created_utc": 1648951431.0}
{"sub": "Python", "title": "Sunday Daily Thread: What's everyone working on this week?", "selftext": "Tell /r/python what you're working on this week! You can be bragging, grousing, sharing your passion, or explaining your pain. Talk about your current project or your pet project; whatever you want to share.", "upvote_ratio": 0.92, "id": "t3_tuwz6h", "created_utc": 1648944010.0}
{"sub": "Python", "title": "Naming convention for type aliases", "selftext": "I recently did a remote technical challenge/interview through a company that does assessments for other companys.\n\nI scored very well, but they made a comment that they didn't like my naming convention, and marked me down in one category. The part they didn't like was:\n\n    SimpleTable = Dict[str, Dict[str, str]]\n\nThey're saying `SimpleTable` should be `simple_table`. I cased it the way I did because it's used as a type for hinting.\n\nWhat are people's thoughts here? I can't find anything official.", "upvote_ratio": 0.9, "id": "t3_tuvz80", "created_utc": 1648941052.0}
{"sub": "Python", "title": "A commit from my lead dev: \"Improve readability\".", "selftext": "I don't get it. Help!\n\nhttps://preview.redd.it/emb8jxjrv6r81.png?width=607&amp;format=png&amp;auto=webp&amp;s=871eae5047ac3c13dcba968fa2b49dd707469287", "upvote_ratio": 0.83, "id": "t3_tuuq5l", "created_utc": 1648937544.0}
{"sub": "Python", "title": "Space Science with Python - Asteroids meet Deep Learning #10", "selftext": "Hey everyone,\n\ntoday I'd like to show you how to optimize a Conv1D network using [Keras-Tuner](https://keras.io/keras_tuner/). It enables one to automatically test some pre-defined networks; or it applies Bayesian or Hyperband optimization to find the best model!\n\nIn our case, we'll use it to find a good convolutional neural network for asteroid spectra to distinguish between 4 classes.\n\nLink to video: [https://youtu.be/vhr48KgL-Ys](https://youtu.be/vhr48KgL-Ys)\n\nCorresponding Code: [https://github.com/ThomasAlbin/Astroniz-YT-Tutorials/blob/main/%5BML1%5D-Asteroid-Spectra/10\\_dl\\_hyperparameter\\_search.ipynb](https://github.com/ThomasAlbin/Astroniz-YT-Tutorials/blob/main/%5BML1%5D-Asteroid-Spectra/10_dl_hyperparameter_search.ipynb)\n\nPlease note that this is tutorial #10 in a long series of space science related videos. Feel free to take a look in the corresponding [playlist](https://www.youtube.com/playlist?list=PLNvIBWkEdZ2gagAcgm44cplgSvQ_Cmvbv) or on [my channel](https://www.youtube.com/c/Astroniz).\n\nTutorial #10 concludes the classification task. In the next 3 videos we'll take a look at Autoencoders and how to classify or identify different asteroid spectra in a low-dimensional latent space.\n\nStay tuned!\n\nThomas", "upvote_ratio": 0.71, "id": "t3_tuuc31", "created_utc": 1648936504.0}
{"sub": "Python", "title": "Cloud server", "selftext": "I made an API to managing files remotely, it's protected from directory traversal, shells and more!\n\n[https://github.com/ZSendokame/ViCl](https://github.com/ZSendokame/ViCl)\n\nI wan't to improve it, so if you have any ideas or feedback please leave it on comments", "upvote_ratio": 0.73, "id": "t3_tuszb4", "created_utc": 1648932845.0}
{"sub": "Python", "title": "Beautiful circos plots in Python", "selftext": "Circos is one of the most popular software packages for displaying the inter-relationships between data in a matrix. For example, in the bioinformatics field, circos style plots are often used for visualizing genomic similarities and features. However, the original package is implemented in Perl, and the other derived packages were also developed based on R or d3.js. Thus, there are no practical tools for drawing circos plots in matplotlib. \n\nHere, I made [pyCircos](https://github.com/ponnhide/pyCircos) that allow drawing circos plots with matplotlib. By using this package, not only circos plots but also complex polar plots can be quickly drawn.\n\nI hope the package help users visualize a beautiful polar plot.  \n\n\n[Example plots of pyCircos](https://preview.redd.it/v3g1huqtt5r81.png?width=2952&amp;format=png&amp;auto=webp&amp;s=f8d8bf0f876d9facb27a12974793d70f9570e84b)\n\nAdditionally, example code can be executed on [Google colab.](https://colab.research.google.com/drive/1xmAnv7AHWUTA2HWfjqV1lFWkFMSLJHG0?usp=sharing)", "upvote_ratio": 0.83, "id": "t3_tupzwe", "created_utc": 1648924973.0}
{"sub": "Python", "title": "GitHub - minimaxir/imgbeddings: Python package to generate image embeddings with CLIP without PyTorch/TensorFlow", "selftext": "nan", "upvote_ratio": 0.67, "id": "t3_tuokyp", "created_utc": 1648921291.0}
{"sub": "Python", "title": "Free and reliable language translation API for developers", "selftext": "Hi Pythons,\n\nFor years now, I have been developing and maintaining the [deep-translator](https://github.com/nidhaloff/deep-translator) package.\n\nRecently, I started working on an API based on it and I want to show it to you. The API is based on fastapi and deep-translator. I deployed a \"basic\" version on azure, you can [try it out here](https://deep-translator-api.azurewebsites.net/)\n\nIf you want to check the API code, you can [find it here](https://github.com/nidhaloff/deep-translator-api)\n\nI built the API as an extension to deep-translator to make it easier for other developers/enthusiasts to build apps/websites in their preferred languages and make use of deep-translator through the API. \n\nIn [this issue](https://github.com/nidhaloff/deep-translator/issues/144), I suggested building a translation website or desktop app based on deep-translator. Maybe there are people here, who will find this interesting.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n# TL:DR\n\nI built a free translation API based on deep-translator and fastapi.\n\nAPI repo: [https://github.com/nidhaloff/deep-translator-api](https://github.com/nidhaloff/deep-translator-api)\n\ndeep-translator repo: [https://github.com/nidhaloff/deep-translator](https://github.com/nidhaloff/deep-translator)\n\nSwagger UI: [https://deep-translator-api.azurewebsites.net/](https://deep-translator-api.azurewebsites.net/)", "upvote_ratio": 0.86, "id": "t3_tunvta", "created_utc": 1648919507.0}
{"sub": "Python", "title": "Help building a Python logo in r/place!", "selftext": "we are currently working at 1270 x 888 near the belgium flag\n\n[https://www.reddit.com/r/placepython/comments/ttxp46/rplacepython\\_lounge/](https://www.reddit.com/r/placepython/comments/ttxp46/rplacepython_lounge/)\n\n[https://www.reddit.com/r/placepython/comments/tungzi/starting\\_again\\_at\\_1270\\_x\\_888\\_near\\_the\\_belgium\\_flag/](https://www.reddit.com/r/placepython/comments/tungzi/starting_again_at_1270_x_888_near_the_belgium_flag/)", "upvote_ratio": 0.69, "id": "t3_tuno8s", "created_utc": 1648918996.0}
{"sub": "Python", "title": "Python GUI Programming With Tkinter \u2013 Real Python", "selftext": "nan", "upvote_ratio": 0.7, "id": "t3_tulmi2", "created_utc": 1648913791.0}
{"sub": "Python", "title": "ORM by dataclass with type hints\u2728\u2728\u2728", "selftext": "&amp;#x200B;\n\nhttps://preview.redd.it/vvsaiq7yl4r81.png?width=754&amp;format=png&amp;auto=webp&amp;s=b71bdbfcaabf44d9d52cf1743d1caa26c1efd92f\n\nhttps://preview.redd.it/v6qf97zcl4r81.png?width=1118&amp;format=png&amp;auto=webp&amp;s=281417df407d2cc011e3e02ed61da5791bcc3c3a\n\nsee more at [https://strongbugman.github.io/danio/](https://strongbugman.github.io/danio/), thanks for any suggestion", "upvote_ratio": 0.7, "id": "t3_tukdgh", "created_utc": 1648910416.0}
{"sub": "Python", "title": "we need a python logo in r/place", "selftext": "let's choose a good pixel art logo and draw it somewhere.", "upvote_ratio": 0.5, "id": "t3_tuizod", "created_utc": 1648906468.0}
{"sub": "Python", "title": "py-terraform: Python binding for Terraform", "selftext": "Hi, I wrote a Python binding for Terraform which called [**py-libterraform**](https://github.com/Prodesire/py-libterraform).\n\nTerraform is a great tool for deploying resources. If you need to call the Terraform command in the Python program for deployment, a new process needs to be created to execute the Terraform command on the system. A typical example of this is the [python-terraform](https://github.com/beelit94/python-terraform) library. Doing so has the following problems:\n\n* Requires Terraform commands on the system.\n* The overhead of starting a new process is relatively high.\n\nThis library compiles Terraform as a **dynamic link library** in advance, and then loads it for calling. So there is no need to install Terraform, nor to start a new process.\n\nIn addition, since the Terraform dynamic link library is loaded, this library can further call Terraform's **internal capabilities**, such as parsing Terraform config files.\n\nFeel free to use it and look forward to your feedback!", "upvote_ratio": 0.92, "id": "t3_tuiv20", "created_utc": 1648906072.0}
{"sub": "Python", "title": "An Example of Automated Hacking with Python", "selftext": "nan", "upvote_ratio": 0.94, "id": "t3_tuhe59", "created_utc": 1648901266.0}
{"sub": "Python", "title": "Gradually Migrating Python Code to asyncio", "selftext": "nan", "upvote_ratio": 0.82, "id": "t3_tuhb11", "created_utc": 1648900968.0}
{"sub": "Python", "title": "PyTermGUI now lets you export your terminal contents as an SVG screenshot!", "selftext": "https://preview.redd.it/qg9lyct0s3r81.png?width=1487&amp;format=png&amp;auto=webp&amp;s=7659c8f8280d642bbf36ca4caf512a6377eb1202\n\n**Note**: None of the images in this post are actual, OS-side screenshots, rather images exported by the module.\n\nHey there!\n\nInspired by Rich's upcoming offering, the newest version of my TUI project, [PyTermGUI](https://github.com/bczsalba/pytermgui) supports exporting terminal content as a window-mimicking screenshot! Unlike the aforementioned module, PTG also supports exporting positionally-printed items accurately.\n\nWe also support exporting as a plain, static HTML file which could be very useful for documentation purposes.\n\nhttps://preview.redd.it/1cbazpc6s3r81.png?width=1562&amp;format=png&amp;auto=webp&amp;s=2cbfee8e95bd081bb239a0ab1c21fecc2ec2eea6\n\nhttps://preview.redd.it/4hxei7jot3r81.png?width=1422&amp;format=png&amp;auto=webp&amp;s=e3be4313cebeb8f8f3f081ed6357a97aef1f3041\n\nIf any of this sounds interesting, feel free to check out the [release notes](https://github.com/bczsalba/pytermgui/releases/tag/v4.3.0), or join us at r/PyTermGUI!", "upvote_ratio": 0.86, "id": "t3_tuh5ej", "created_utc": 1648900443.0}
{"sub": "Python", "title": "I made a simple access-code protected webpage. Any suggestions on how I can improve?", "selftext": "So for fun, I am making a webpage that can be only access through a 4 digit access code. Nothing fancy. No Username or password. I hardcoded a 4 digit pin in the flask app and am \"authenticating\" (not sure if I can call it this) in the way below. \n\n&amp;#x200B;\n\nRight now, I am also trying to \"re-auth\" after 100 seconds.  \n\n&amp;#x200B;\n\nPS:  Not sure if this belongs in learnpython. \n\nHow good/bad is my code and the way I am solving the problem? What are the flaws in this? Thanks for the help!\n\n    # This is placed in the utils file that i will be importing in the app.\n        @dataclass\n        class Authanticate:\n            PIN: str = \"1111\"\n        \n            verified: bool = False\n            verified_at = dt.now()\n        \n            def session_valid(self):\n                delta = dt.now() - self.verified_at\n                # print(f\"delta - {delta.seconds} seconds || {delta.days} days\")\n                if(delta.seconds &gt;= 0 and delta.seconds &lt;= 100):\n                    self.verified = False\n                    return True\n        \n                else:\n                    return False\n            \n            def validate_pin(self, entered_pin):\n                if(entered_pin == self.PIN):\n                    self.verified = True\n                    self.verified_at = dt.now()\n                    # print(f\"Verified {self.verified} {self.verified_at}\")\n                    return True\n                else:\n                    self.verified = False\n                    return False\n        \n\nMy actual routing code:\n\n    \n    #Placed at the top\n    # This is a global var. Any better way?\n    auth_obj = utils.Authanticate()\n    \n    @app.route('/handledata', methods=['GET', 'POST'])\n    def handledata():\n        if(request.method == 'POST'):\n            input_1 = request.form['digit-1']\n            input_2 = request.form['digit-2']\n            input_3 = request.form['digit-3']\n            input_4 = request.form['digit-4']\n    \n            entered_pin = f\"{input_1}{input_2}{input_3}{input_4}\"\n    \n            print(f\"entered pin - {auth_obj.validate_pin(entered_pin)}\")\n            \n    \n            return redirect(url_for('showdata'))\n    \n    @app.route('/showdata')\n    def showdata():\n        # print(verified)\n        if(auth_obj.session_valid()):\n            data = {\"Yes\": \"Lord\"}\n        else:\n            data={\"Lol\": \"You wrong\"}\n        return data\n        # return render_template('data.html', show='dir', dir_data = data)", "upvote_ratio": 1.0, "id": "t3_tugf5s", "created_utc": 1648897756.0}
{"sub": "Python", "title": "XRP Blockchain Wrapper", "selftext": "Hi\n\nI made a simple wrapper based on \"xrpl-py\" package which makes it easier to interact with xrp blockchain.\n\nIt's still a work in progress. I hadn't had enough time to write a sphinx doc for it, but the library is very easy to understand\n\nIn other words, it's a more simplified version of the \"xrpl-py\" package.\n\n  \nGithub: [https://github.com/amiwrpremium/xrpy](https://github.com/amiwrpremium/xrpy)  \nPyPi: [https://pypi.org/project/xrpy/](https://pypi.org/project/xrpy/)  \n\n\nFeel free to contribute and use it.  \nAny feedback will be appreciated.  \nIf you bump into any bugs or issues please submit them on Github.\n\n  \nThanks.", "upvote_ratio": 0.29, "id": "t3_tue6z7", "created_utc": 1648888626.0}
{"sub": "Python", "title": "Handle Exception and Close Resource Using Python Context Manager With-Statement", "selftext": "nan", "upvote_ratio": 0.6, "id": "t3_tudfqc", "created_utc": 1648885400.0}
{"sub": "Python", "title": "Best OOP Tutorial", "selftext": "Wanted a primer on Python OOP and came across this course from FreeCodeCamp. The instructor does a great job explaining all the concepts related to OOP. \n\nIf anyone is looking to get more info on OOP, I highly recommend you go and check this course out: https://youtu.be/Ej_02ICOIgs\n\nCheers.", "upvote_ratio": 0.84, "id": "t3_tucz71", "created_utc": 1648883543.0}
{"sub": "Python", "title": "Every 15 Minutes, Random YouTube Comment Gets Featured on the Thumbnail - Dynamic YouTube with Python", "selftext": "I wanted to explore dynamic video making, which are YouTube videos where the title or thumbnail changes during the video's lifetime. You may have seen Tom Scott's changing title video or Mr. Beast's changing thumbnail.\n\nFor this Python project, I made a Python script using the YouTube API hosted on AWS Lambda where every 15 minutes, a random comment from my comment section is put onto the thumbnail. This is a cool way to interact with an audience!\n\nMost of the used code and information can be found on [GitHub](https://github.com/techtribeyt/random_comment_on_thumbnail).\n\nMost importantly, [**here is the video where this is all running live**](https://www.youtube.com/watch?v=RXN1d_UpaAY)! Please leave some comments so that the thumbnail stays active and keep changing!\n\nBasically, the thumbnail below keeps changing every 15 minutes. The top gets updated to the username of the commenter and the body changes to the content of the comment.\n\nhttps://preview.redd.it/ovxjod5he1r81.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;s=7ba2c84f11d57ecaa70114fd3835924ad22297fb", "upvote_ratio": 0.67, "id": "t3_tu9lyd", "created_utc": 1648870971.0}
{"sub": "Python", "title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread", "selftext": "Found a neat resource related to Python over the past week? Looking for a resource to explain a certain topic?\n\nUse this thread to chat about and share Python resources!", "upvote_ratio": 0.75, "id": "t3_tu5ckr", "created_utc": 1648857610.0}
{"sub": "Python", "title": "Pokete: A terminal based Pokemon like game", "selftext": "&amp;#x200B;\n\nhttps://reddit.com/link/tu1zat/video/u3812q8qkzq81/player\n\nI wrote a Pokemon clone, called [Pokete](https://github.com/lxgr-linux/pokete), for the terminal supporting:\n\n* Different Pokete types\n* Effectiveness of those types against each other\n* Attack effects\n* A map\n* NPCs to talk to (partly with complex interaction choices)\n* Trainers to fight against\n* Weather that effects the effectiveness of some attacks\n* Achievements\n* A Dex to see all caught Poketes in\n* Special abilities (like flying)\n* A self written [ASCII game engine](https://github.com/lxgr-linux/scrap_engine) and much more\n* Pipenv", "upvote_ratio": 0.95, "id": "t3_tu1zat", "created_utc": 1648848316.0}
{"sub": "Python", "title": "largestinteriorrectangle - my first PyPI published package", "selftext": "I implemented the in 2019 described [Algorithm for finding the largest inscribed rectangle in polygon](https://journals.ut.ac.ir/article_71280_2a21de484e568a9e396458a5930ca06a.pdf). Today I published the package on [PyPI](https://pypi.org/project/largestinteriorrectangle/). I developed it to solve the [problem of black borders around a stitched image](https://stackoverflow.com/questions/33497928/crop-image-after-stitching).\n\nSince it's my first contribution on PyPI I would love to get feedback from experienced developers!", "upvote_ratio": 0.91, "id": "t3_tu0tfu", "created_utc": 1648845350.0}
{"sub": "Python", "title": "bundling python virtualenv AND interpreter?", "selftext": "I can use pex to bundle a archive of a virtualenv, that I can run with a specific version of a python interpreter on a host.\n\nIs there a project that will let me bundle the virtualenv AND the python interpreter?", "upvote_ratio": 0.79, "id": "t3_ttyskq", "created_utc": 1648840116.0}
{"sub": "Python", "title": "I've made a \"Falling sand\" Sandbox with Python (using pygame)", "selftext": "It's extremely slow and incefficient but it does work :)\n\nIt implements:\n\n \\- Sand &amp; Water Physics \n\n\\- Buoyancy \n\n\\- Heath transfer \n\n\\- Status changes (ice -&gt; water -&gt; vapor)\n\nHere's a demo for how it looks like in action: [https://www.youtube.com/watch?v=7cgfJqnVVTg](https://www.youtube.com/watch?v=7cgfJqnVVTg)\n\nAnd here's a link to the code: [https://github.com/SudoOmbro/OmbroBox](https://github.com/SudoOmbro/OmbroBox)\n\nThanks for reading :)", "upvote_ratio": 0.96, "id": "t3_ttxxxq", "created_utc": 1648837926.0}
{"sub": "Python", "title": "How are python threads different from other languages? I understand GIL limits code exec to one thread but how does \"usual\" threading is like without GIL.", "selftext": "I have used many other languages but I've only touched concurrency in python. I have learnt about threading, asyncio and multiprocessing. \n\nI was reading about GIL and can easily understand \"it limits code exec to one thread\", that's plain definition. Its a mutex and all that.\n\nBut how exactly does it limits and what? So in languages without GIL like Java and C++ can each thread execute code without taking turns as in python? So if there are say 20 threads, all of those are executing code simultaneously?\n\nSo effectively in python threading CAN ONLY HELP I/O BOUND programs but in Java/C++ threading CAN HELP BOTH CPU BOUND AND I/O BOUND PROGRAMS?", "upvote_ratio": 0.83, "id": "t3_ttx3k4", "created_utc": 1648835841.0}
{"sub": "Python", "title": "Are we all joining r/placetux for the Linux community or can we build a python on r/place?", "selftext": "Where else are y'all dedicating your tiles? And who else is using selenium to automate their process?", "upvote_ratio": 0.61, "id": "t3_ttwg06", "created_utc": 1648834226.0}
{"sub": "Python", "title": "A Vigen\u00e8re (de)ciphering program with frequency analysis attack.", "selftext": "The project can be found here: https://github.com/Nathan-Furnal/frequency-analysis\n\nHi everyone, this is a project about ciphers, entirely in Python. The goal is to provide a way to cipher plain text files based on a key and then automatically find the key back to decipher the text.\n\nSome trade-offs were made for simplicity: only the basic 26 letters alphabet was allowed and any spaces or punctuation was removed as well.\n\nIn that sense, it's a good introduction to implement ciphering but not a real-world tool! \n\nMore information about what a cipher is and how they're implemented can be found in the README and in the documentation of the code. \n\nWe can start with the most simple example, a *Caesar* cipher which is a one letter shift. \n\nFor example, the sentence `Hello World!`, shifted by the letter `K` (\"a\" is 0 and \"k\" is 10), will become : \n\n    | H | E | L | L | O |  | W | O | R | L | D |  plain text\n    | K | K | K | K | K |  | K | K | K | K | K |   key \n    | R | O | V | V | Y |  | G | Y | B | V | N |  ciphered text\n\nThis kind of cipher is very susceptible to be cracked because the letters are all shifted by the same key (a unique letter). Since that's the case, one can try to count the frequencies of each letter, plug back in the usual letter frequencies for each letter and it's done. \n\nA Vigen\u00e8re cipher is more elaborate, the key is a word or a sentence and not one letter. Because of this, the same letters can be shifted by a different amount and the resulting ciphered text is not susceptible to the deciphering explained above. \n\nFor example, The sentence `Hello World!`, encrypted with the word `key` (with letters indexed from 0 to 25), becomes: \n\n    | H | E | L | L | O |  | W | O | R | L | D |  plain text\n    | K | E | Y | K | E |  | Y | K | E | Y | K |   key \n    | R | I | J | V | S |  | U | Y | V | J | N |  ciphered text\n\nYou'll find that where the key from this cipher matches the key from the previous one, the ciphered text matches! This is one of the basic building blocks used to attack this ciphering scheme, it's a bit more elaborate but you can find the references and examples in the documentation of the code.\n\nI hope it's clear =)", "upvote_ratio": 0.76, "id": "t3_ttqlac", "created_utc": 1648819457.0}
{"sub": "Python", "title": "Sierpi\u0144ski Triangle With Python Turtle", "selftext": "nan", "upvote_ratio": 0.81, "id": "t3_ttqer3", "created_utc": 1648818933.0}
{"sub": "Python", "title": "I'm creating a tool to enrich your datasets with relevant external data", "selftext": " Hey all,\n\nI love doing market research and all kinds of exploratory analyses, but getting the data is a major pain point, as it is in many places (data dumps, apis, marketplaces, web data) and in all kinds of formats\n\nI'm trying a different approach, where instead of searching for data sources, and then integrating manually, you just upload your dataset. My service has a large index with datasets and api providers, and finds relevant ones for your dataset which you can add easily.\n\n&amp;#x200B;\n\n[Example](https://i.redd.it/7ou7f2egowq81.gif)\n\nDoes this seem useful to you? Would love to hear your thoughts", "upvote_ratio": 0.93, "id": "t3_ttosfb", "created_utc": 1648813824.0}
{"sub": "Python", "title": "jupino: Annotate data in Jupyter notebooks", "selftext": "Often times I need a quick way to annotate data for my experiments and since I use Jupyter notebook quite a lot, overtime I developed a simple way to annotate data using Jupyter widgets and recently published it to Github([https://github.com/jangedoo/jupino](https://github.com/jangedoo/jupino)) and pypi to share it with others.\n\nI hope you find it useful. The README file contains code samples for common data annotation scenarios. I would love to get your feedbacks!\n\nhttps://preview.redd.it/3moyq5fqhwq81.png?width=734&amp;format=png&amp;auto=webp&amp;s=74309c310d96acd056726f354f5de2c24e981878", "upvote_ratio": 0.91, "id": "t3_tto7z2", "created_utc": 1648811835.0}
{"sub": "Python", "title": "Python Data Persistence - DBM Package", "selftext": "nan", "upvote_ratio": 0.67, "id": "t3_ttmu0f", "created_utc": 1648806221.0}
{"sub": "Python", "title": "Old Project Showcase [ThoughtLogger]", "selftext": "## Backstory\n\nI made this application a long time ago. And really didn't think about showcasing this off in this subreddit. It was a big mistake. But now, I think this project might help somebody, so here it is,\n\n# ThoughtLogger.\n\nYour Personal Thought Logging application that you can use anytime without losing any focus. **Made using tkinter in Python.**\n\nYou can write down your thoughts, make a list, use it as a diary, your choice. You get two profiles to write into. This might be useful with people with CBT (Cognitive Behavioral Disorder) \n\nThe text is then saved as a '.log' file.\n\nImage of [Main Window](https://user-images.githubusercontent.com/68242099/137069866-9de85060-c0cf-4907-9e3d-3931f614e35f.png)\n\n## You can also customize stuff\n\nYou can use the '.ini' file to customize the application to your liking. \\[still in beta stages\\]\n\n[ini file](https://user-images.githubusercontent.com/68242099/135040471-4d9fd50b-72d3-4696-baac-9490145b5b62.png)\n\n## You can try it out from this [Github Repo](https://github.com/moiSentineL/ThoughtLogger).\n\nIt was just a beginner project. Didn't know many things.\n\n## Also hoping that you guys would help improve the code.\n\n## Thank you.", "upvote_ratio": 0.8, "id": "t3_ttmrs1", "created_utc": 1648805960.0}
{"sub": "Python", "title": "Removal of supported types in the sample function of the random-library. Why would this be done?", "selftext": "When reading the [documentation of the random module](https://docs.python.org/3/library/random.html) I noded the following:\n\n&gt; Deprecated since version 3.9: In the future, the *population* must be a sequence.  Instances of [set](https://docs.python.org/3/library/stdtypes.html#set) are no longer supported.  The set must first be converted to a [list](https://docs.python.org/3/library/stdtypes.html#list) or [tuple](https://docs.python.org/3/library/stdtypes.html#tuple), preferably in a deterministic order so that the sample is reproducible. \n\nAnd my question is why would you do something like this, rather than further supporting all iterables and then casting them to sequences if needed. Maybe make a keyword only argument that has tuple as default parameter so that a user can pass in a custom function to convert non-sequence iterables and raise an error if and only if the convert function fails? Why would we want reproducible results for the random module anyway? I know it is more for games than security purposes but still doesn't this defeat the purpose.", "upvote_ratio": 0.64, "id": "t3_ttlu6q", "created_utc": 1648801881.0}
{"sub": "Python", "title": "Object Counting by Color. Learn how to count selected color objects using OpenCV and Python.", "selftext": "nan", "upvote_ratio": 0.7, "id": "t3_ttlipp", "created_utc": 1648800408.0}
{"sub": "Python", "title": "Transfer file wirelessly between two computer over the Bluetooth Low Energy protocol Using Python", "selftext": "nan", "upvote_ratio": 0.71, "id": "t3_ttl4ra", "created_utc": 1648798697.0}
{"sub": "Python", "title": "GitHub - brenw0rth/pync: arbitrary TCP and UDP connections and listens (Netcat for Python).", "selftext": "nan", "upvote_ratio": 0.67, "id": "t3_ttkpio", "created_utc": 1648796851.0}
{"sub": "Python", "title": "Why does it seem that python pays more than Java (from what little googling I\u2019ve done)? I would assume that since Java runs faster and more efficient (and since it\u2019s harder to learn, thus, making it a more scarce skill) that Java would be the higher paying language.", "selftext": "nan", "upvote_ratio": 0.33, "id": "t3_ttjbrc", "created_utc": 1648791346.0}
{"sub": "Python", "title": "Automated WiFi Hacking Script In Python - Tutorial", "selftext": "nan", "upvote_ratio": 0.89, "id": "t3_tticqk", "created_utc": 1648787748.0}
{"sub": "Python", "title": "simple and fast reverse port forwarding tool by python", "selftext": "I write a port forwarding tool using pyhton. Please have a try, thanks !\n\n [xitongsys/moonlight: moonlight is a reverse port forwarding tool written by python (github.com)](https://github.com/xitongsys/moonlight)", "upvote_ratio": 0.71, "id": "t3_ttht7k", "created_utc": 1648785809.0}
{"sub": "Python", "title": "my Hacker News interface, displays thumbnails and details about the linked page. written in Python 3.8, with Playwright for scraping, requests for API querying, Wand for thumbnail generation, and pickle for caching link data so I don't poll the Hacker News API too often", "selftext": "I'd like to share with you [my interface for reading the Hacker News feed](https://www.thnr.net/top/1/). It started as a Django app, but several iterations in I realized I was basically just creating static pages (for speed of UX) so I pivoted to using Python for just a backend of querying the Hacker News API to get the link info and generate the page html and the thumbnails and other details. Web serving is just nginx serving the generated static html pages.\n\nI'm happy to answer technical or design/architecture questions. My plan is to make the code public once it's in a less embarrassing state, but I'm happy to share source code as illustrations if anyone is curious about specific points.\n\nOddly, the thumbnails have consumed the majority of my dev time. I use Playwright to get the [og:image](https://duckduckgo.com/?q=og%3Aimage) link (if it's present) and then I use good ole `requests` to download the og:image itself. I use Wand (an ImageMagick binding) to trim away large blocks of solid color and then I pad the image to make it either squarish or a 3:1 width to height aspect ratio. Finally I use Wand to save the file as webp since it's small but still good looking. Currently I also create thumbnails if the linked item (not og:image but the actual link) is to a PDF. I'm using Wand to do the conversion with Inkscape as the backend delegate for rasterizing. But sometimes the quality is kinda meh, so I'd like to experiment with using good old ghostscript for rasterizing PDFs, either through a binding (any suggestions?) or by calling a process from Python to the shell.\n\nFor the curious:\n[Hacker News default interface](https://news.ycombinator.com/), [their public API](https://github.com/HackerNews/API)", "upvote_ratio": 0.75, "id": "t3_tthmk7", "created_utc": 1648785205.0}
{"sub": "Python", "title": "Is taking hand-write note on Python Library courses good? In your opinion?", "selftext": "nan", "upvote_ratio": 0.42, "id": "t3_ttfj0z", "created_utc": 1648778283.0}
{"sub": "Python", "title": "PEP 2241: Back to the Past", "selftext": "nan", "upvote_ratio": 0.84, "id": "t3_ttdl90", "created_utc": 1648772073.0}
{"sub": "Python", "title": "PEP 9001", "selftext": "&amp;#x200B;\n\n[The Best, and Only, Code Autoformatter You'll Ever Need](https://preview.redd.it/t15arr245tq81.png?width=758&amp;format=png&amp;auto=webp&amp;s=b1accb0f300af6253cff6c14a458601abc5d0ebe)\n\n&amp;#x200B;\n\n[PEP 9001](https://peps.pythondiscord.com/pep-9001/)\n\nOur friends over at the [Python Discord](https://discord.gg/python) have been asked to draft and submit a PEP based on their experiences on Discord based on how to make Python development better for all. \n\nAs the Python Discord Server, they are in a unique position to see how  Python programmers grow along side the Python programming language.  With that experience, they've noticed how much developer time and energy is expended on python  formatting and how the guidelines of PEP8 even influence how people learn.\n\nIn an effort to ensure Python continues to be the dominant and best  programming language to ever exist, the Python Discord is submitting [PEP 9001](https://peps.pythondiscord.com/pep-9001/)\u2014the New  Ultimate Final Python Formatting Guide! \n\nThis PEP is the final, ultimate, complete Python Formatting Guide that  also includes proposed changes to the Python's syntax to encourage  better coding practices. We encourage you all to begin porting your code  to this new and final coding style.\n\nTo help with this drastic but very necessary change, they\u2019ve started  drafting a new autoformatter for it, Blurple, so everyone can experience  what their code looks like in it's ultimate form.\n\nThis PEP is still in a draft state, so please suggest and make contributions in the #pep-9001 channel over in the [Python Discord](https://discord.gg/python). Play around with the autoformatter in #blurple-code-formatter and experience what it\u2019s like for your code to be expertly styled.", "upvote_ratio": 0.93, "id": "t3_ttdjl6", "created_utc": 1648771923.0}
{"sub": "Python", "title": "Friday Daily Thread: Free chat Friday! Daily Thread", "selftext": "Use this thread to talk about anything Python related! Questions, news,  projects and any relevant discussion around Python is permitted!", "upvote_ratio": 0.67, "id": "t3_ttdb2f", "created_utc": 1648771207.0}
{"sub": "Python", "title": "5 Powerful Python one-liners \ud83e\udd2f", "selftext": "[https://medium.com/@needablackcoffee/5-powerful-python-one-liners-260dd61c4d09](https://medium.com/@needablackcoffee/5-powerful-python-one-liners-260dd61c4d09)", "upvote_ratio": 0.15, "id": "t3_ttbviw", "created_utc": 1648766763.0}
{"sub": "Python", "title": "Django Boilerplate: new open source project, asking for feedback", "selftext": "I just finished the first iteration on my new open source side project, a Django boilerplate.  \n\n\nWhat do you think about it? I won't explain much here since I'd like the [readme.md](https://readme.md) to be self-explanatory.  \n\n\ninteresting in feedback about:  \n\n\n\\- it is useful?  \n\\- it is properly done?  \n\\- it is something you don't like?  \n\\- there is something \"not django-ish\"?  \n\\- and, of course, any other feedback you may want to leave me! I'd appreciate even the small bit of time you'd dedicate to my project.  \n\n\nhere's the link: [https://github.com/carloocchiena/django\\_boilerplate](https://github.com/carloocchiena/django_boilerplate)", "upvote_ratio": 0.67, "id": "t3_tt8dpi", "created_utc": 1648757067.0}
{"sub": "Python", "title": "I made a random class generator for Elden Ring", "selftext": "`import random`\n\n`classes=['Hero', 'Bandit', 'Astrologer', 'Warrior', 'Prophet' 'Vagabond', 'Confessor', 'Wretch', 'Prisoner', 'Samurai' ]`\n\n`class_selection = random.choice(classes)`\n\n`print(class_selection)`\n\nPlease give me any feedback on how to improve this, it's one of my first projects", "upvote_ratio": 0.58, "id": "t3_tt753s", "created_utc": 1648753658.0}
{"sub": "Python", "title": "Files and Dictionary practice", "selftext": "I had already posted but i did not read the rules all the way so I had to remove it, but I am back. \n\nThis project came on a whim and I thought it would be fun. The code in my opinion is a tad bit messy but readable. Most of the code came from me with some help from Stack-overflow\n\n[Guthub Link](github.com/Gamelift/File-and-Dictionary-Practice/tree/main)", "upvote_ratio": 0.4, "id": "t3_tt53sn", "created_utc": 1648748158.0}
{"sub": "Python", "title": "Create a progression of recommendations with any recommender", "selftext": "Hi! For work I made a [recommendation progression package](https://github.com/askarthur/graph-progression) which we've open sourced! Let me explain.\n\n# Reason\n\nWe are an ArtTech company and have created a few recommendation engines around artists and artworks. For the artwork one in particular, we thought it'd be cool to see a progression of artworks given a \"seed\" work. During that process, we discovered a solution like this doesn't exist when all the recommendations aren't calculated before hand and stored (like a graph database) ... so we built this.\n\n# Summary\n\nThe main idea behind the package is to allow users to create a series of recommendations in a sort of \"progression\" from one another using a *context-based* recommender. So you supply the recommendation engine (or client to it), the starter item (seed), and the progression length. It will then try to create a progression of recommendations from the start (seed -&gt; rec 1 -&gt; rec 2 -&gt; etc.).\n\nSome cool features it has are memoization (it won't call the recommender again if it hits the item again), backtracking (if an item has no valid recommendations), and post-selection filters (if you want to filter out certain recommendations).\n\n## Sample\n\nCheckout out a [sample](https://github.com/askarthur/graph-progression/blob/main/docs/samples/README.md) (for artworks) starting with a VanGogh painting!\n\n&amp;#x200B;\n\nWould love to hear any feedback and/or questions people have! Thanks for reading!", "upvote_ratio": 0.7, "id": "t3_tt4k17", "created_utc": 1648746668.0}
{"sub": "Python", "title": "Interceptor algorithm (no relevance to current world events)", "selftext": "[https://www.youtube.com/watch?v=RoIsWAdIqos](https://www.youtube.com/watch?v=RoIsWAdIqos)\n\nI made a bot that uses a few altered kinematic equations. It's designed to be an aiming algorithm, but it also works as an unjukable bot. I made the control physics work like starships in SWBF just for fun. Change the speed and acceleration values to see some interesting results. Turn off the constant unit vector speeds and watch it intercept extremely well.", "upvote_ratio": 0.77, "id": "t3_tt4i7h", "created_utc": 1648746529.0}
{"sub": "Python", "title": "Asynchronous Web Scraping With Python AIOHTTP", "selftext": "nan", "upvote_ratio": 0.65, "id": "t3_tt2qi2", "created_utc": 1648741724.0}
{"sub": "Python", "title": "Bulk file rename app", "selftext": " \n\n[Tr.Rename](https://preview.redd.it/oubod92w2qq81.jpg?width=150&amp;format=pjpg&amp;auto=webp&amp;s=3a86f3ee1bbace7e43757a38b9dcb0ad1575d74a)\n\n&amp;#x200B;\n\nthis app is my first project on Github. it is a bulk file rename/translator. it is based on python. only tested on windows 10 64bit. the GUI is based on qt.\n\nthe app name is Translate.Rename and it is capable of:\n\n1. translates all the files into a folder and renames. (uses Built-in google translate API).\n2. adds any characters and dates to every file inside the selected folder. (both before or after original filename).\n3. completely renames all files in a folder by user-given characters and adds \\_1,\\_2,...\n4. change the suffixes of given files.\n5. filters affected files by text, date, size, and type.\n\n[https://github.com/theRJorj/Translate.Rename](https://github.com/theRJorj/Translate.Rename)\n\nEXE: [https://sourceforge.net/projects/trans-rename/](https://sourceforge.net/projects/trans-rename/)\n\nwould be glad to have your feedback.", "upvote_ratio": 0.84, "id": "t3_tt007r", "created_utc": 1648734034.0}
{"sub": "Python", "title": "A feature complete W-TinyLFU cache implementation in Python", "selftext": "While learning Python I tried to implement the cache architecture proposed in the paper [A Highly Efficient Cache Admission Policy](https://arxiv.org/pdf/1512.00727.pdf). I have seen that Python only propose LRU cache as a decorator so I thought a more efficient cache was something meaningful and funny to code.  \nNot an expert, tried my best.\n\nBrief description of the cache structure:\n\n&gt;W-TinyLFU uses a small admission LRU that evicts to a large Segmented LRU if accepted by the TinyLfu admission policy. TinyLfu relies on a frequency sketch to probabilistically estimate the historic usage of an entry. The window allows the policy to have a high hit rate when entries exhibit recency bursts which would otherwise be rejected. The size of the window vs main space is adaptively determined using a hill climbing optimization. This configuration enables the cache to estimate the frequency and recency of an entry with low overhead.\n\nHere's the github link: [https://github.com/vanika/TinyPyCache](https://github.com/vanika/TinyPyCache)", "upvote_ratio": 0.63, "id": "t3_tszssv", "created_utc": 1648733396.0}
{"sub": "Python", "title": "I wrote okjson - A fast, simple, and pythonic JSON Schema Validator", "selftext": "I had a requirement to process and validate large payloads of JSON concurrently for a web service, initially I implemented it using [jsonschema](https://github.com/python-jsonschema/jsonschema) and [fastjsonschema](https://github.com/horejsek/python-fastjsonschema) but I found the whole [JSON Schema Specification](https://json-schema.org/) to be confusing at times and on top of that wanted better performance. Albeit there are ways to compile/cache the schema, I wanted to move away from the schema specification so I wrote a validation library inspired by the design of [tiangolo/sqlmodel](https://github.com/tiangolo/sqlmodel) (type hints) to solve this problem easier.\n\n**Here is a simple example:**\n\n```py\nfrom okjson import JSONValidator\n\nschema = { 'name': str, 'age': int }\n\njson_string = '{ \"name\": \"Charly Gordon\", \"age\": 32 }'\n\nassert JSONValidator().is_valid(instance=json_string, schema=schema)\n```\n\nThere is an [example covering all the features](https://github.com/mufeedvh/okjson#every-feature-in-a-single-example) in the README.\n\nIt also has well defined exceptions for each error case when you want to get the reason for the validation failure. (Helpful when you want to show user facing error messages)\n\n**GitHub:** https://github.com/mufeedvh/okjson\n\nThis is my first time publishing a Python library, please share your feedback/suggestions. :)", "upvote_ratio": 0.74, "id": "t3_tsz0i3", "created_utc": 1648730942.0}
{"sub": "Python", "title": "Current situation of library support for M1", "selftext": "I am planing to swap my old MacBook with a M1 Pro MacBook. I would like know if anyone has experience with the current situation of supported library support/ general usability of python on M1. \n\nI need support for most of the common data science libraries. I can sacrifice some performance problems since I am mostly just doing DevOps for a Data Science project. But I need to be able to execute it. \n\nFurthermore, I am doing a lot of Docker stuff, but as far as I know this is mostly supported. \n\nSo, any knowledge or experience is welcomed :)", "upvote_ratio": 0.93, "id": "t3_tsyuig", "created_utc": 1648730378.0}
{"sub": "Python", "title": "Flame graph rendering in the terminal", "selftext": "nan", "upvote_ratio": 0.56, "id": "t3_tsya02", "created_utc": 1648728441.0}
{"sub": "Python", "title": "Concurrent Web Scraping with Selenium Grid and Docker Swarm - updated!", "selftext": "nan", "upvote_ratio": 0.57, "id": "t3_tsy5pb", "created_utc": 1648728034.0}
{"sub": "Python", "title": "Project idea for Python lovers in the Linux world", "selftext": "I'm just throwing this somewhere cuz it'd truly be a blessing if it was made, but I don't get my hopes too high.\n\nI'm very bad at Linux, but I basically spent a week on and off trying to find an easy to use and configure fan control program for CPU and GPU.\n\nThere's plenty that need you to write config files yourself, or that require 15 steps configuration process where it's always failed me somewhere.\n\nSo, having a software with a gui that doesn't require you to jump into the Terminal times and times again would be a blessing to me, and I bet to many other inexperienced Linux users as well.\n\nI find open-source OS so attractive for many reasons, but at the same time I can't do much because I don't have the technical knowledge for it. Life's really hard as a new user who can't deep dive and learn all the technicalities. (the love-hate relationship is very real - &gt;  \\^ 3 \\^ // = --O)\n\nMaybe, if you can relate in some ways and have more knowledge and free time... the door's open for you to contribute.", "upvote_ratio": 0.5, "id": "t3_tsxyc3", "created_utc": 1648727281.0}
{"sub": "Python", "title": "Python in Visual Studio Code \u2013 April 2022 Release", "selftext": "nan", "upvote_ratio": 0.89, "id": "t3_tsxkgp", "created_utc": 1648725776.0}
{"sub": "Python", "title": "^fassert$: Fuzzy assert only data that matters", "selftext": "Hello,\n\nIn almost every project I worked on I had a need to fuzzy match (assert) recursively nested structures such as combinations of lists/dicts/tuples/sets etc... If you need to assert some data deep within the nested structures, especially if the position within the list is not known, the complexity can raise quickly with several nested for loops. I always wrote more-or-less the same test fixture to do this kind of thing but figured I am not the only one having this problem so I decided to made this into a small library with emphasis of having 0 dependencies.\n\nThe whole library is also implemented in a single file (I would like to keep it that way if possible) so if you don't feel like installing something from pypi you can just copy paste the content into your pytest fixtures and start using it straight away.\n\nSource code: [https://github.com/SourceCode-AI/fassert](https://github.com/SourceCode-AI/fassert)", "upvote_ratio": 0.61, "id": "t3_tswigu", "created_utc": 1648721347.0}
{"sub": "Python", "title": "An A Markov-Chain Twitter bot trained on Elon Musk Tweets and Childrens Books", "selftext": "Ever wondered what would happen if Elon Musk started quoting childrens books halfway through his Tweets? Wonder no longer!\n\nGithub: [https://github.com/FlynnOwen/elon-markov-chain-twitter-bot](https://github.com/FlynnOwen/elon-markov-chain-twitter-bot)\n\nTwitter: [https://twitter.com/elonstorybot](https://twitter.com/elonstorybot)", "upvote_ratio": 0.82, "id": "t3_tsvexj", "created_utc": 1648716375.0}
{"sub": "Python", "title": "If an exception is raised but not handled would the process definitely stop/crash? I have seen some programs that keep printing stack trace to cli but don't crash and keep trying.", "selftext": "EDIT: thanks i think i got my answer. \n\n```py\nwhile True:\n  try:\n    something_that_raises_exception()\n  except as e:\n    logger.error(e)\n```", "upvote_ratio": 0.88, "id": "t3_tsux1z", "created_utc": 1648714032.0}
{"sub": "Python", "title": "Print Emoji's using Python.", "selftext": "nan", "upvote_ratio": 0.33, "id": "t3_tsusc7", "created_utc": 1648713463.0}
{"sub": "Python", "title": "Real Time Speech Recognition", "selftext": "nan", "upvote_ratio": 0.83, "id": "t3_tst6rj", "created_utc": 1648706438.0}
{"sub": "Python", "title": "Demo for game made with Pygame", "selftext": "https://preview.redd.it/nxsoj0p5lqq81.png?width=1366&amp;format=png&amp;auto=webp&amp;s=1fe7736ff3cbc42750ad4b38307e0c908968fa20\n\nI had fun making it! If anyone ends up looking at the code, it may be pretty messy because I started it when I was first learning Python. I recently added an A.I. chatbot as an NPC, which I thought was neat. I still have to write more dialogue for him.\n\nFeatures\n\n\\-Play minigames\n\n\\-Talk to A.I. NPC\n\n\\-Explore dream areas\u00a0\n\n\\-Look at A.I. Generated art\n\n[https://borderow.itch.io/liminal-harbor](https://borderow.itch.io/liminal-harbor)\n\n&amp;#x200B;\n\nIf anyone downloads it, please let me know if the executable worked for you. Thanks!", "upvote_ratio": 0.86, "id": "t3_tssu7d", "created_utc": 1648705022.0}
{"sub": "Python", "title": "First Python Application - Blackjack Strategy Trainer - Seeking Feedback", "selftext": "Hi everyone! I just finished my initial beta for a blackjack training simulator. The application helps the user practice basic blackjack strategy. If you check it out and have some feedback for improvement, I would love to hear it! Thanks and enjoy!\n\n&amp;#x200B;\n\n[https://github.com/lrassbach/blackjack-training-simulator/releases](https://github.com/lrassbach/blackjack-training-simulator/releases)", "upvote_ratio": 0.8, "id": "t3_tsrmiu", "created_utc": 1648700446.0}
{"sub": "Python", "title": "I made a utility library for downloading Wikimedia Data Dumps for analysis", "selftext": "I created a small utility library that exposes the directory structure of Wikimedia\u2019s data dump site (and its mirrors).\n\nIt includes a method to easily download and decompress data dumps using Python\u2019s built in libraries - this proved to be an especially difficult problem because the data dumps are sometimes several times larger than the system\u2019s memory. \n\n[Source code here.](https://github.com/jon-edward/wiki_dump)", "upvote_ratio": 0.85, "id": "t3_tsq2c8", "created_utc": 1648695137.0}
{"sub": "Python", "title": "Tuple Reuse Quirks", "selftext": "I was testing tuple reuse today and noticed some strange behavior. If you run all the code below together as a script, 1) a and b have different IDs, as do c and d; but when you enter them in the interpreter line by line: 2) a and b have the same IDs, 3)c and d have the same IDs.\n\nOf the 3 observations, only 2) matches my expectation based on my understanding of tuple reuse, which is that Python moves tuples with fewer than 20 items to a \"free list\" after their reference count reach 0, (to make them available for potential reassignment later)\n\nCould someone explain why 1) and 3) happen? Thank you!\n\n`a = (1,2,3)`\n\n`print(id(a))`\n\n`del a`\n\n`b = (1,2,3)`\n\n`print(id(b))`\n\n&amp;#x200B;\n\n`c = (4,5,6)`\n\n`print(id(c))`\n\n`del c`\n\n`d = (7,8,9)`\n\n`print(id(d))`", "upvote_ratio": 0.7, "id": "t3_tso711", "created_utc": 1648689060.0}
{"sub": "Python", "title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!", "selftext": "Discussion of using Python in a professional environment, getting jobs in Python as well as ask questions about courses to further your python education!\n\n**This thread is not for recruitment, please see** r/PythonJobs **or the thread in the sidebar for that.**", "upvote_ratio": 0.76, "id": "t3_tsmv9p", "created_utc": 1648684810.0}
{"sub": "Python", "title": "Python or R?", "selftext": "I searched some differences but not found definite answer. So, I ask you. Which one is better for business?\n\nMy company has over 3 million customer and tons of data expectedly. I will study in a different department after 3 or 2 years later and it will be about data analysis of costumers. Which one should I start with? Python or R?", "upvote_ratio": 0.4, "id": "t3_tsjvib", "created_utc": 1648676061.0}
{"sub": "Python", "title": "Pollen Robotics' Reachy VR Digital Twin in Python (proof of concept)", "selftext": "Demonstration of a **\"Reachy\"** **digital twin** in **virtual reality**, based on **Python** and **HARFANG**.\n\nThe goal of this P.O.C is to demonstrate that a simple digital twin of an existing robot can be implemented in less than 500 lines of **Python**. This digital twin shows a **VR** experience that could serve to bootstrap a **tele-operation** project :   \n \\- the user can see what the robot sees, using the 3D simulation in **virtual reality**  \n \\- the user can move his hands around, the **arms** of the **robot** will follow  \n \\- it helps to test the **challenge** of a tele-operation process\n\nThe robot in this demonstration is a **Reachy, developped by the French company Pollen Robotics**.  \nThe physics &amp; inverse kinematic solver is handled by PyBullet.  \nThe VR is simulated by HARFANG 3D.\n\n[https://www.youtube.com/watch?v=TBAjNGPpMfc](https://www.youtube.com/watch?v=TBAjNGPpMfc)", "upvote_ratio": 0.85, "id": "t3_tsjsaf", "created_utc": 1648675816.0}
{"sub": "Python", "title": "pymac: Install and manage macOS Python.org installers from the command line", "selftext": "nan", "upvote_ratio": 0.9, "id": "t3_tsjhe7", "created_utc": 1648675001.0}
{"sub": "Python", "title": "A simple Snooker 3D game in Python", "selftext": "Hi there,\n\nI would like to share a tiny **game** project all made in **Python** : a **Snooker simulator.**  \nThe gameplay is rather limited, as the aim of this project is to focus on the simulation aspect : how to produce a realistic snooker physics, with a visual rich enough to be on part with nowadays standards ?  \nTo reach this goal, this project relies on 2 major **features** :  \n \\- a **physics** engine, running at high frequency to handle the friction of the balls  \n \\- a **3D rendering** engine, with 2 quality modes, the highest quality mode including realtime reflections and radiosity (doesn't require a RTX GPU).\n\nI hope you folks will enjoy the project, can **learn** from it and maybe be turn it into something bigger!\n\n[https://github.com/harfang3d/snooker-python-hg2](https://github.com/harfang3d/snooker-python-hg2)", "upvote_ratio": 0.73, "id": "t3_tsixth", "created_utc": 1648673568.0}
{"sub": "Python", "title": "Introducing Slap: A CLI to assist in the Python development process", "selftext": "I would like to share a project with you that I have been working on and actively using for over 1.5 years to streamline my day-to-day tasks developing Python projects. I use Slap tens to hundreds of times a day and would not want to miss it anymore.\n\n**Slap streamlines Python development tasks**\n\n&gt;[Slap](https://niklasrosenstein.github.io/slap/) is a command-line tool to assist in the development of Python projects independent of the [PEP 517](https://peps.python.org/pep-0517/) build backend being used, capable of managing single- and multi-project repositories.\n\nAmong the things that Slap can do, here is a short selection:\n\n* Perform development installs of your project (even for Poetry projects)\n* Manage local &amp; global virtual environments\n* Bump version numbers (not just in `pyproject.toml`)\n* Publish to PyPI\n* Manage structured changelogs in TOML format\n* All while capable of handling mono-repositories that consist of more than one Python project\n\n*--- Check out the*  [*Getting started*](https://niklasrosenstein.github.io/slap/getting-started/) *page in the documentation to get a more in-depth look into what Slap can do for you. ---*\n\n**Slap seems similar to the Poetry CLI, what's different?**\n\nSome people might find this similar to tools like Poetry, and while there is some overlap in functionality, Slap is **not a build backend** and is more targeted towards library development. Most of my projects use Poetry as the build backend but I never even once interact with the Poetry CLI throughout the lifetime of the project.\n\nThe most notable differences to Poetry are\n\n* Slap supports mono-repositories (i.e. multiple related Python projects in the same repository), to the extent that it bumps version numbers of project inter-dependencies and installs your projects in topological order\n* Slap supports development installs independent of the build backend (yes; this means you can install Poetry packages in editable mode even though the Poetry backend right now does not support editable installs)\n* Slap's version bump command (`slap release`) updates the version not just in your `pyproject.toml` but also the `__version__` in your source code  as well as in related projects (see mono-repositories above) and any additional references you can configure via Regex patterns\n* Slap does not automagically create a virtual environment for you when installing your project(s); instead, it errors when you try to install into a non-virtual Python environment and gives you an easy-to-use tool to create and activate virtual environments (and allowing multiple environments per project as well as global environments)\n* Slap uses Pip to install your project(s), unlike Poetry which comes with its own dependency resolver and package installer (which I have been having a lot of issues with in the past).\n* Slap does not have a concept of lock files\n\n**Ideas / Suggestions / Contributions**\n\nSlap is currently very opinionated by the fact alone that I built it as my personal workflow tool, but I welcome suggestions and contributions, and I am hopeful it will be useful to a wider audience than myself.\n\n**Links**\n\n* Documentation: [https://niklasrosenstein.github.io/slap/](https://niklasrosenstein.github.io/slap/)\n* GitHub: [https://github.com/NiklasRosenstein/slap](https://github.com/NiklasRosenstein/slap)\n* PyPI: [https://pypi.org/project/slap-cli/](https://pypi.org/project/slap-cli/)\n\n**Edit**: I've added an in-depth [Getting started](https://niklasrosenstein.github.io/slap/getting-started/) guide to the documentation.", "upvote_ratio": 0.87, "id": "t3_tsgavg", "created_utc": 1648669463.0}
{"sub": "Python", "title": "Python Tutorial - How to create a Screen Recorder using Python?", "selftext": "Hey Everyone! I created a short Python Tutorial explaining how I created a Screen Recorder using Python.  \n[https://www.youtube.com/watch?v=449c5lsGKKw](https://www.youtube.com/watch?v=449c5lsGKKw)\n\nhttps://preview.redd.it/ro2g0wc5hkq81.png?width=2880&amp;format=png&amp;auto=webp&amp;s=649c062e96554d29b4348c07eb076a767b3d929b", "upvote_ratio": 0.73, "id": "t3_tsdftv", "created_utc": 1648666101.0}
{"sub": "Python", "title": "[GUI] DyCall - Run exported functions from dynamic libraries", "selftext": "nan", "upvote_ratio": 0.87, "id": "t3_tsdcn8", "created_utc": 1648665866.0}
{"sub": "Python", "title": "I Created a Simple Pushup Counter Using Mediapipe Python", "selftext": "nan", "upvote_ratio": 0.78, "id": "t3_tsb1mf", "created_utc": 1648659632.0}
{"sub": "Python", "title": "Any good Machine learning udemy courses ?", "selftext": "I recently took python course and I\u2019m learning python I want to get into machine learning and ai is there any good udemy courses for this? I see an A-Z machine learning course but is it good ? If anyone has taken it can you tell me what it exactly teaches you or does it just skip important points thanks", "upvote_ratio": 0.63, "id": "t3_ts9m9n", "created_utc": 1648655819.0}
{"sub": "Python", "title": "Scrape Google Scholar Metrics Results to CSV with Python", "selftext": "Full code:\n\n```python\nimport requests, lxml\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\n\ndef scrape_all_metrics_top_publications():\n\n    params = {\n        \"view_op\": \"top_venues\",  # top publications results\n        \"hl\": \"en\"  # or other lang: pt, sp, de, ru, fr, ja, ko, pl, uk, id\n        }\n\n    # https://docs.python-requests.org/en/master/user/quickstart/#custom-headers\n    # whatismybrowser.com/detect/what-is-my-user-agent\n    headers = {\n        \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.88 Safari/537.36\"\n        }\n\n    html = requests.get(\"https://scholar.google.com/citations\", params=params, headers=headers, timeout=30)\n    soup = BeautifulSoup(html.text, \"lxml\").find(\"table\")\n\n    df = pd.DataFrame(pd.read_html(str(soup))[0])\n    df.drop(df.columns[0], axis=1, inplace=True)\n    df.insert(loc=2,\n              column=\"h5-index link\",\n              value=[f'https://scholar.google.com/{link.a[\"href\"]}' for link in soup.select(\".gsc_mvt_t+ td\")])\n\n    df.to_csv(\"google_scholar_metrics_top_publications.csv\", index=False)\n\n    # save to csv for specific language\n    # df.to_csv(f\"google_scholar_metrics_top_publications_lang_{params['hl']}.csv\", index=False)\n\n\ndef scrape_all_metrics_public_mandates():\n    params = {\n        \"view_op\": \"mandates_leaderboard\",  # public access mandates results\n        \"hl\": \"en\"  # or other lang: pt, sp, de, ru, fr, ja, ko, pl, uk, id\n        }\n\n    # https://docs.python-requests.org/en/master/user/quickstart/#custom-headers\n    # whatismybrowser.com/detect/what-is-my-user-agent\n    headers = {\n        \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.88 Safari/537.36\"\n        }\n\n    html = requests.get(\"https://scholar.google.com/citations\", params=params, headers=headers, timeout=30)\n    soup = BeautifulSoup(html.text, \"lxml\").find(\"table\")\n\n    df = pd.DataFrame(pd.read_html(str(soup))[0])\n    df.drop(df.columns[[0, 2]], axis=1, inplace=True)\n    df.insert(loc=1, column=\"Funder Link\", value=[link.a[\"href\"] for link in soup.select(\"td.gsc_mlt_t\")])\n\n    df.to_csv(\"google_scholar_metrics_public_access_mandates.csv\", index=False)\n\n    # save to csv for specific language\n    # df.to_csv(f\"google_scholar_metrics_public_access_mandates_lang_{params['hl']}.csv\", index=False)\n```\n\nIf you need an in-depth explanation, have a look at [Scrape Google Scholar Metrics Results to CSV with Python](https://serpapi.com/blog/scrape-google-scholar-metrics-results-to-csv-with-python/) blog post at SerpApi.", "upvote_ratio": 0.4, "id": "t3_ts9lzb", "created_utc": 1648655795.0}
{"sub": "Python", "title": "Building a Full Stack Flask HTMx Application", "selftext": "nan", "upvote_ratio": 0.75, "id": "t3_ts8x51", "created_utc": 1648653894.0}
{"sub": "Python", "title": "I created 80 Python and Pandas Challenges with Video Solutions", "selftext": "nan", "upvote_ratio": 0.88, "id": "t3_ts8hpw", "created_utc": 1648652727.0}
{"sub": "Python", "title": "Transcribe Speech to Text with Python for Free", "selftext": "nan", "upvote_ratio": 0.69, "id": "t3_ts6whh", "created_utc": 1648648258.0}
{"sub": "Python", "title": "What to expect from Python 3.11?", "selftext": "nan", "upvote_ratio": 0.95, "id": "t3_ts6idv", "created_utc": 1648647068.0}
{"sub": "Python", "title": "GitLab Webhooks [Python and Google Chat Example]", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_ts66bz", "created_utc": 1648646050.0}
{"sub": "Python", "title": "I made a video about efficient memory use in pandas dataframes!", "selftext": "nan", "upvote_ratio": 0.96, "id": "t3_ts5mmf", "created_utc": 1648644350.0}
{"sub": "Python", "title": "Create a google account in Python", "selftext": " \n\nI would like to be able to create google account with a python script. I've try with Mechanize but nothing conclusive. Do you have any idea to how to do that ?\n\nI tried with the library Mechanize. I get the first form with:\n\n* FirstName\n* LastName\n* UserName\n* Password\n\nI submit it, but impossible to get the second form...", "upvote_ratio": 0.5, "id": "t3_ts5l04", "created_utc": 1648644210.0}
{"sub": "Python", "title": "Python program to find day of any date.(Yes any! from 1000 AD - 9999 AD)", "selftext": "&amp;#x200B;\n\n[Program in action!](https://preview.redd.it/5eorq0mfniq81.png?width=1484&amp;format=png&amp;auto=webp&amp;s=426f40c1b36cd85844c3f1565b8906da58c404f2)", "upvote_ratio": 0.67, "id": "t3_ts5ij2", "created_utc": 1648643992.0}
{"sub": "Python", "title": "ABCMeta library to limit derived classes their methods to follow the same signature as the parent", "selftext": "Python metaclass and abstract method library with restrictions.\n\nThis library provides a restricted way to validate abstract methods. The Python's default abstract method library only validates the methods that exist in the derived classes and nothing else. What this library provides is apart from that validation it provides validations over the method's signature. All you need is to import ABCMeta and abstractmethod from this library.\n\n[https://github.com/mortymacs/abcmeta](https://github.com/mortymacs/abcmeta)", "upvote_ratio": 1.0, "id": "t3_ts4ooj", "created_utc": 1648641121.0}
{"sub": "Python", "title": "Authoring Opportunity: Developing a Python book with Packt Publishing", "selftext": "Hi all, \n\nI am a Product Manager at Packt Publishing.\n\nPackt is a publishing company helping the world put software to work in new ways, through the delivery of effective learning and information services to IT professionals.\u00a0\n\nWe are planning to publish a book on\u00a0Python\u00a0to address the challenges and requirements of the\u00a0Python\u00a0community.\n\nWe are looking for an expert experienced Python developer who could share their valuable insights on the topic and collaborate with us in developing content for a book that we are planning to publish. \n\nIf you feel you might be interested in this opportunity please send me a direct message on or before 7th April 2022. \n\nPS:  Content development experience in terms of writing a blog or anything else is appreciated but not required.", "upvote_ratio": 0.43, "id": "t3_ts4caw", "created_utc": 1648639896.0}
{"sub": "Python", "title": "br4nch 1.2.1 - Data Structure Tree Builder for Python.", "selftext": "***New patch released (1.2.1) that improves alot of features!***\n\n&amp;#x200B;\n\n`br4nch` is created to provide an efficient implementation of tree data structure in ***Python***.\n\n&amp;#x200B;\n\nThe **libary** is built on pure python so you don't need **ANY** other libaries.\n\nThe amount of features are endless such as: **adding**, **deleting**, **renaming**, **moving**, **copying**, **painting**, **sizing**, **change symbols**, **importing folder structure**, **exporting to txt and br4nch file**, etc..\n\n&amp;#x200B;\n\n*PyPi*: [https://pypi.org/project/br4nch/](https://pypi.org/project/br4nch/)\n\n*Documentation*: [https://docs.br4nch.com/](https://docs.br4nch.com/)\n\n*Github*: [https://github.com/TRSTN4/br4nch](https://github.com/TRSTN4/br4nch)", "upvote_ratio": 0.64, "id": "t3_ts4avc", "created_utc": 1648639752.0}
{"sub": "Python", "title": "python dictionary comprehension", "selftext": "nan", "upvote_ratio": 0.67, "id": "t3_ts3lg7", "created_utc": 1648637091.0}
{"sub": "Python", "title": "Do you have experience with stocks prices analysis?", "selftext": "I am about to start a hobby project, mainly for learning and curiosity. I want to analyze stocks data and try to learn some basic concepts about investing and machine learning. The flow should be following:  \n\n\n\\- Use some free API to get historic stocks data.  \n\\- Store data in the database (probably PostgreSQL hosted on Heroku).  \n\\- Train neural network to predict stocks prices (here, I have the biggest knowledge gap).  \n\\- Use some free API to get real-time stocks data.  \n\\- Store real-time data in the database.  \n\\- Compare predicted stocks prices with actual stock prices in BI tools like GoodData, or Looker.  \n\n\nI know it is not rocket science, but as I said, my motivation is to learn something new. Before I start, I would like to ask you if you have experience with such a similar project and if there is something you might recommend to me.", "upvote_ratio": 0.71, "id": "t3_ts2ujz", "created_utc": 1648633956.0}
{"sub": "Python", "title": "How to document python functions?", "selftext": "Link: [https://www.youtube.com/watch?v=fU6qB06rkz0](https://www.youtube.com/watch?v=fU6qB06rkz0)", "upvote_ratio": 0.44, "id": "t3_ts2t1f", "created_utc": 1648633755.0}
{"sub": "Python", "title": "Froyo: A Python GUI utility for downloading works from Archive Of Our Own/AO3 (made in Python with Dear PyGui): a fast, responsive graphical user interface results in a user-friendly tool", "selftext": "&amp;#x200B;\n\n[Froyo: A Python GUI utility for downloading works from Archive Of Our Own\\/AO3 \\(made in Python with Dear PyGui\\): a fast, responsive and user-friendly tool](https://i.redd.it/xyrdfo9zahq81.gif)\n\nFroyo is a small graphical application for downloading works from Archive Of Our Own (AO3). It supports batch downloading of works to supported formats (AZW3, EPUB, HTML, MOBI, PDF). The app is small, fast and functional, a perfect fit for Dear PyGui. Not every app has to be complex. Sometimes a tool just needs to get the job done.", "upvote_ratio": 0.9, "id": "t3_ts1iat", "created_utc": 1648627758.0}
{"sub": "Python", "title": "Programming languages: Python just got a boost from Facebook's Meta", "selftext": "nan", "upvote_ratio": 0.37, "id": "t3_ts1al4", "created_utc": 1648626768.0}
{"sub": "Python", "title": "Understanding __init__ Method in Python", "selftext": "nan", "upvote_ratio": 0.76, "id": "t3_ts0cgt", "created_utc": 1648622580.0}
{"sub": "Python", "title": "How do you define beginner, intermediate, and advanced?", "selftext": "\nThese terms are used frequently, but many seem to often disagree on them. Could be used to describe a persons knowledge, competence, or the qualiry od code/codebase etc.\n\nI would say a beginner is just someone who is learning the actual language, syntax, just learning to code. Maybe touching on the common built-in libraries, learning classes, and basic OOP.\n\nIntermediate is past the syntax and basic concepts and is using libraries. Intermediate is a lot longer phase, I think. Intermediate is learning about programming concepts, practices, patterns, design, libraries, ...  \n\nAdvanced (again, in my perspective) is not a wizard who knows Python internals and corners of the python object model. It's someone who can create useful, well-structured programs from the ground up properly, using the right tools and libraries. Not just doing practice projects, but a larger product properly. Able to pretty much learn any technology needed for a project.\n\n\nWhat do you think? What do you disagree with?", "upvote_ratio": 0.92, "id": "t3_try32l", "created_utc": 1648613843.0}
{"sub": "Python", "title": "ipyvizzu - create animated charts in Jupyter Notebook using Python with this open-source tool", "selftext": "Hey,\n\nWe've built and released a new integration of our open-source charting library Vizzu. We hope this will help data scientists and analysts share their insights easier by utilizing animation for storytelling with data. We'd love to know what you think about it.\n\nMore info, tutorial &amp; examples: [https://github.com/vizzuhq/ipyvizzu](https://github.com/vizzuhq/ipyvizzu)\n\nHere's a short video illustrating how ipyvizzu works.\n\nhttps://reddit.com/link/trxyg5/video/j6rd94q6mcq81/player", "upvote_ratio": 0.56, "id": "t3_trxyg5", "created_utc": 1648613371.0}
{"sub": "Python", "title": "Wednesday Daily Thread: Beginner questions", "selftext": "New to Python and have questions? Use this thread to ask anything about Python, there are no bad questions!\n\nThis thread may be fairly low volume in replies, if you don't receive a response we recommend looking at r/LearnPython or joining the Python Discord server at [https://discord.gg/python](https://discord.gg/python) where you stand a better chance of receiving a response.", "upvote_ratio": 1.0, "id": "t3_trtfcd", "created_utc": 1648598410.0}
{"sub": "Python", "title": "Looking for tips", "selftext": "I made a program to try and simplify square root radicals with python. I would like to get some feedback and tips on how to write some better code.\n\n[https://github.com/Falt-b/simpliify-radicals](https://github.com/Falt-b/simpliify-radicals)", "upvote_ratio": 0.72, "id": "t3_trlofs", "created_utc": 1648587916.0}
{"sub": "Python", "title": "I made a web-based Instagram bot that scrapes people's usernames and follows/unfollow them", "selftext": "&amp;#x200B;\n\nhttps://reddit.com/link/tre7to/video/nf7rtqni9dq81/player", "upvote_ratio": 0.86, "id": "t3_tre7to", "created_utc": 1648579177.0}
{"sub": "Python", "title": "Multi-threaded Omegle bot with proxy support and other neat features, like content spinning and typo generator.", "selftext": "[https://github.com/embium/Omeglebot](https://github.com/embium/Omeglebot)\n\n# Omeglebot\n\nMulti-threaded Omegle bot with proxy support and other neat features, like content spinning and typo generator.\n\nI'm not even sure it's possible to run without proxies.\n\nI could sell this but I rather release it publicly.\n\nThis supported me financially for a few years. Enjoy.", "upvote_ratio": 0.76, "id": "t3_tre8z9", "created_utc": 1648579265.0}
{"sub": "Python", "title": "pync - Netcat for Python", "selftext": "Hi, I've been reading Black Hat Python and decided to try and make a Netcat library for Python developers.\n\nIt's not fully there yet (there's a few things I still need to do), but I thought I'd share it anyway in hopes that someone might find it useful.\n\nAnyway, here's the [GitHub repo](https://github.com/brenw0rth/pync) for the project if you're interested.\n\nAny feedback or criticism is welcome, thank you. \ud83d\ude0a", "upvote_ratio": 0.83, "id": "t3_tre60y", "created_utc": 1648579046.0}
{"sub": "Python", "title": "Tomorrow: a live 45-minutes session on the fundamentals of observability, OpenTelemetry, and distributed tracing in Python", "selftext": "Hi everyone, a quick reminder about the live OpenTelemetry and observability fundamentals session - in 2 days, Wednesday, March 30 at 11 AM PST.\n\nYou will learn how to instrument your apps to capture traces with OpenTelemetry in Python.\n\nThis session is at no cost and vendor-neutral.\n\nYou can expect in this session: 45 minutes of core concepts, how to deploy it yourself hands-on + Q&amp;A.\n\nIf you are interested in observability, OpenTelemetry, and tracing - join!\n\nRegister here [https://www.aspecto.io/get-started-with-opentelemetry/](https://www.aspecto.io/get-started-with-opentelemetry/?utm_source=post&amp;utm_medium=reddit&amp;utm_campaign=r-python-opentelemetry-workshop-pyhon-march-30-2022)", "upvote_ratio": 0.87, "id": "t3_tra9f1", "created_utc": 1648574859.0}
{"sub": "Python", "title": "I built a real-time stock tracker using Python + Fast API, Alpaca, Next.js, and Redis Stack. Check it out!", "selftext": "nan", "upvote_ratio": 0.64, "id": "t3_tra4u1", "created_utc": 1648574515.0}
{"sub": "Python", "title": "I love type annotations, so I made a quick tutorial on how to add types to Flask SQLAlchemy", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_tr9r9n", "created_utc": 1648573518.0}
{"sub": "Python", "title": "IndiePython.com - content about indie projects using Python (specially gamedev)", "selftext": "Hello, I'd like to present a website ([indiepython.com](http://indiepython.com)) I created to publish content about my indie Python projects including apps and games. It is still a work in progress, but it already has a lot of content about projects I'll be releasing, including, for instance, a node editor to be released on June 2022 like you can see below. My apps and their source code are free of charge and will be released to the public domain, both on github and pypi (can be installed via \"pip install\" command).\n\n[Nodezator \\(node editor to be released on June 2022 to the public domain\\)](https://reddit.com/link/tr5l1q/video/qfelxkvcicq81/player)\n\nIt should feature other content like tutorials/articles as well in the future. Thanks for your time.", "upvote_ratio": 0.6, "id": "t3_tr5l1q", "created_utc": 1648569821.0}
{"sub": "Python", "title": "If a person posts Python code to Amazon Web Services can it run all the normal Python libraries? I was thinking to a greater extent about moviepy.editor.", "selftext": "If a person posts Python code to Amazon Web Services can it run all the normal Python libraries?  I was thinking to a greater extent about moviepy.editor.", "upvote_ratio": 0.5, "id": "t3_tr43cm", "created_utc": 1648566871.0}
{"sub": "Python", "title": "Boss told me to learn python.", "selftext": "Got told I need to learn python from my boss, where should I start? Codeacademy? Like how they are teaching you there. But how good is it? Better places to learn?\n\n*Edit* \nMy boss wants me to learn to program for robots and different task for automation of tasks. He told me I should learn the basics and then more about that field. Thinking something like making bots should be a good project? \n\nCurrent knowledge is Visual Basic some years ago.", "upvote_ratio": 0.85, "id": "t3_tr31rs", "created_utc": 1648566271.0}
{"sub": "Python", "title": "Garmin Connect File Manager", "selftext": "Hey everyone,\n\nI put together [this project](https://github.com/lucas-nelson-uiuc/garmin-connect-file-manager/tree/main) to help me store (and, in the future, analyze) activities recorded on my Garmin to my local computer. It builds off the popular [gcexport](https://github.com/kjkjava/garmin-connect-export) repo with additional functionality including summary/geographical data extraction as well as activity-based sorting.\n\nFeel free to look things over if you're looking to critique someone's code today and/or implement this in your Garmin data backup routine.", "upvote_ratio": 0.78, "id": "t3_tr2oml", "created_utc": 1648566073.0}
{"sub": "Python", "title": "Pagination for a User-Friendly Django App \u2013 Real Python", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_tr1m2y", "created_utc": 1648564625.0}
{"sub": "Python", "title": "Loading config from the environment in Quart &amp; Flask", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_tqz50j", "created_utc": 1648558577.0}
{"sub": "Python", "title": "What is a Lambda function and its applications in Python", "selftext": "In this short article, I'm gonna explain Python's lambda functions in detail. A lambda function is a small anonymous function that can take any number of arguments but can only have one expression. Here is its syntax:\n\n**lambda arguments : expression**\n\nThe expression is executed and the result is returned.\n\n## Example:\n\nAdd 10 to argument a, and return the result:\n\n    x = lambda a : a + 10\n    print(x(5))\n\nThe result would be 15. (You could try this in a python IDE such as vs code)\n\n=========================================================================\n\nLambda functions can take any number of arguments:\n\n## Example:\n\nMultiply argument a with argument b and return the result:\n\n    x = lambda a, b : a * b\n    print(x(5, 6))\n\nThe result would be 30.\n\n=========================================================================\n\n**Why Use Lambda Functions?**\n\nThe power of lambda is better shown when you use them as an anonymous function inside another function. Say you have a function definition that takes one argument, and that argument will be multiplied by an unknown number:\n\n    def myfunc(n):\n      return lambda a : a * n\n\nUse that function definition to make a function that always doubles the number you send in:\n\n## Example:\n\n    def myfunc(n):\n      return lambda a : a * n\n    \n    mydoubler = myfunc(2)\n    print(mydoubler(11))\n\nThe result would be 22.\n\n=========================================================================\n\nUse lambda functions when an anonymous function is required for a short period of time.\n\nIf you would like to watch the video version of this tutorial, you could click on the following link:\n\n[https://www.youtube.com/watch?v=DGUl5Uwz3g8](https://www.youtube.com/watch?v=DGUl5Uwz3g8)\n\n**#python** **#pythonlambda** **#lambdafunction**", "upvote_ratio": 0.4, "id": "t3_tqyil5", "created_utc": 1648556436.0}
{"sub": "Python", "title": "My first coding project: Twitter Plays Wordle", "selftext": "After starting my coding journey 3 months ago, I've finally made something that isn't a terminal calculator app :0\n\nSo I made [Twitter Plays Wordle](https://twitter.com/WordleGame_Bot) ([github](https://github.com/afnzmn/twitter-plays-wordle)), a twitter bot that uses replies to its tweet as guesses for a Wordle game.\n\nThe bot first checks if it's a fresh game. If yes, it tweets a blank Wordle grid. If it isn't, then it searches for the replies to its newest tweet, and checks to see if it has a guess in it. This can either be a reply with just a five letter word, or a five letter \\[guess\\] contained in square brackets. It adds this to a replies.csv. Then, it sorts the guesses by the likes, and uses the most liked valid reply as the guess. It then plugs the guess into a Wordle function, which just runs an algorithm for detecting what coloured square goes where, and adding it to a list. The list then gets joined and pushed out with other Wordle things for the Twitter bot to tweet. Finally, it adds the Wordle grid into a text file, and updates a csv on the current row position, the word number, and the win state to be used for the next tweet.\n\nIf you want to play it, here's the most [recent tweet](https://twitter.com/WordleGame_Bot/status/1508769809550434309)\n\nCurrently, I'm just running this on my old laptop since I can't really buy a hosting service, and iirc Heroku wouldn't run for the entire month with its free plan. If someone has an alternative, please let me know! (also if you wanna roast my code, you're welcome to do so)", "upvote_ratio": 0.75, "id": "t3_tqxx94", "created_utc": 1648554369.0}
{"sub": "Python", "title": "The history of performing functions in Django", "selftext": "A question from a beginner.\n\nThe [article](https://russianblogs.com/article/45611627794/) discusses what happens after the command is called *manage.p**y runserver*\n\nIs there any way to see this in the form of a \"log\" or similar functionality, not only after calling this particular command, but also after any actions, for example, in the admin panel, etc.?", "upvote_ratio": 1.0, "id": "t3_tqxkg9", "created_utc": 1648553092.0}
{"sub": "Python", "title": "Sourcepy: source python files straight from the shell", "selftext": "Hey folks, sharing a project I built just for fun, Sourcepy: [https://github.com/dchevell/sourcepy](https://github.com/dchevell/sourcepy)\n\nI was writing some increasingly messy shell functions and thought it would be nice if I could just write Python functions/variables/etc. instead and source them from the shell. So \u2026 that's what this is. It uses a mix of importlib and inspect machinery to \"source\" files (in reality, generating shell stubs that hook back into a bridging interface, and aliasing \\`source\\` so it works just like you\u2019d expect).\n\nAlong the way I ended up deep diving into type annotations and turning plain python functions into dynamic command line apps that can coerce to native values, handle stdin, work with a large variety of types &amp; objects and do a number of other interesting things - all without your code having to know a thing about Sourcepy. You write plain old python functions, add typehints to the parameters for the best results, and in most cases it should \"just work\".\\*\n\nThis is still a work in progress - its use case is largely for folks like me who like excuses to mess around in Python and customise their environment and toolset to an unnecessary degree. \n\nTry it out, share any suggestions (or bugs, or criticism). I've poked around looking for similar projects and the only similar thing I could find required running a daemon which wasn't an approach I liked, so hopefully you'll find some novelty here.\n\n^(\\*not a guarantee)", "upvote_ratio": 0.95, "id": "t3_tqwyrh", "created_utc": 1648550680.0}
{"sub": "Python", "title": "Actual unique fun project ideas: Beginner | Intermediate | Advanced", "selftext": "Beginner: \n- Write a python function that creates 10 folders numbered 1-10 (hint: use context managers) \n- Using PILLOW, generate simple white noise images by randomly toggling individual pixel Colours between black and white in an MxN image (hint: use nested for loops) \n- Write a script that prints \u201cHello World\u201d in red coloured text in the terminal. (hint: look up ANSI escape codes and run this script in the terminal)\n- Create a virtual environment with the built in venv library. Activate this environment. pip install pandas. Confirm the install with pip ls. pip uninstall pandas. (hint: you\u2019ll learn about environments)\n\nIntermediate: \n- Using PILLOW, again generate simple white noise images, but this time by randomly toggling individual pixel Colours between any possible RGB value in an MxN image (hint: use nested for loops) \n- Create a python decorator to run any function you define in an infinite loop (hint: @) \n- Write a script that can simply parse large json files (1GB+) (hint: you\u2019ll learn about memory management and memory allocation errors) \n- Create a bare minimum Flask app to convert temperatures (F to C) - no CSS. (You\u2019ll learn a bit about packages and HTML)\n- Create a Bash Makefile.sh that automates the creation and activation of a virtual environment, and also pip installs pandas. Run this bash file. (hint: assumes Linux)\n\nAdvanced: \n- Using PILLOW and any coloured image as INPUT, write an algorithm that gradually decreases the brightness of an image radially towards the centre pixel \n- Create a bare minimum python based inverted index (like Elasticsearch) (hint: you\u2019ll learn about Classes, NLP, and basic algorithms) \n- Create a Selenium bot to enter random characters into any &lt;input&gt; HTML element on any website (hint: learn about \u201cinspect element\u201d in the browser)", "upvote_ratio": 0.9, "id": "t3_tqvytp", "created_utc": 1648546376.0}
{"sub": "Python", "title": "EXPENSE-TRACKER -&gt; An application to keep your expenses organized", "selftext": "Hey all, few days ago, I have made a post of making an expense tracker - [here](https://www.reddit.com/r/Python/comments/slbhbi/expensetracker_an_application_to_keep_your/).\nAnd you all have helped me improving it. Thanks to all.\n\n# MODIFICATION\nSO, I thought to change the entire project into a package which can be used by installing it through pip.\nFirst of all, I change the code structure to classes and objects.\nthen add one decent functionality - \nCurrency converter\nPreviously, to show the expenses more clearly I use graphs.\nNow, I made another option of changing the currency of your expenses and compare it from the previous. Here, I have used tan API.\n\n# INSTALLATION AND CODE\nYou can see the code as usual on github - [here](https://github.com/Shreejan-35/EXPENSE-TRACKER). Here you can get all information.\n\nYou can install it using pip by the following ways - \n```\npip install expense-tracker\n```\nOR\n```\npip3 install exprense-tracker\n```\n\nThen run,\n```\nexpense-tracker\n```\n\nI was lucky that no one has till now taken **expense-tracker** as a name.\nThat's all to say.\n\nYou can follow me on instagram - [@star_cyber_warrior](https://www.instagram.com/star_cyber_warrior/)", "upvote_ratio": 0.57, "id": "t3_tqvrl9", "created_utc": 1648545457.0}
{"sub": "Python", "title": "How To Hire a Python Web Development Company for Your Project?", "selftext": "nan", "upvote_ratio": 0.14, "id": "t3_tqv3v9", "created_utc": 1648542428.0}
{"sub": "Python", "title": "Top 10 Python Data Science Projects with Source Code", "selftext": "nan", "upvote_ratio": 0.71, "id": "t3_tqu35m", "created_utc": 1648537849.0}
{"sub": "Python", "title": "I'm presenting live in 9 hours at Microsoft Reactor online about Python and Kubernetes. Let me know your questions in advance", "selftext": "nan", "upvote_ratio": 0.86, "id": "t3_tqta40", "created_utc": 1648534367.0}
{"sub": "Python", "title": "My python git binding", "selftext": "hi, i know there are already great python library for git. but i need simple library and working with all git &amp; python 3 version, so i made this binding [https://github.com/guangrei/Gitpybinding](https://github.com/guangrei/Gitpybinding)\n\nglad to hear your thoughts \ud83d\ude4f", "upvote_ratio": 0.79, "id": "t3_tqolbm", "created_utc": 1648517515.0}
{"sub": "Python", "title": "First Full Stack, Mobile Friendly App (Flask)", "selftext": "Yet To Be Named Poll Based Matching App: [http://54.198.186.121:8080/](http://54.198.186.121:8080/)\n\nI was listening to a podcast at a company I want to work for and the host was talking about this idea for an app. I ended up building it to get my foot in the door and build out my project portfolio.\n\nThis is my first production deployment ever. I'd like to get a little foot-traffic and feedback which is what brings me here.\n\nCurrently, signup is required to view anything, however, user authentication is by the book using various flask based modules so all sensitive user data is encrypted.\n\nI think it's a tremendous example of what python is capable of. The whole app is 500ish python lines coupled with a few HTML pages, SQL queries, and a CSS stylesheet.\n\nWith that said, I encourage you to sign up, and either make or take a poll.\n\nAll feedback is welcome in the comments!", "upvote_ratio": 1.0, "id": "t3_tqn48r", "created_utc": 1648512727.0}
{"sub": "Python", "title": "Tuesday Daily Thread: Advanced questions", "selftext": "Have some burning questions on advanced Python topics? Use this thread to ask more advanced questions related to Python.\n\n**If your question is a beginner question we hold a beginner Daily Thread tomorrow (Wednesday) where you can ask any question! We may remove questions here and ask you to resubmit tomorrow.**\n\nThis thread may be fairly low volume in replies, if you don't receive a response we recommend looking at r/LearnPython or joining the Python Discord server at [https://discord.gg/python](https://discord.gg/python) where you stand a better chance of receiving a response.", "upvote_ratio": 0.67, "id": "t3_tqmvv4", "created_utc": 1648512009.0}
{"sub": "Python", "title": "Question: best ways to build Python command line utilities", "selftext": "I have two medium size (less than 1000 lines each) code bases that I would like to make into two command line utilities. Two obvious approaches are:\n\n1. Add a \\_\\_main\\_\\_.py file and a [setup.py](https://setup.py) to a module, and do a python [setup.ps](https://setup.ps) install\n2. Copy all source files into a single Python source file that starts with *#!/usr/bin/env python, and put script in PATH.*\n\nTo complicate matters, I use several different condo environments that I switch between. Is there an accepted best practice for this?", "upvote_ratio": 0.67, "id": "t3_tqgwm3", "created_utc": 1648494868.0}
{"sub": "Python", "title": "ONNX to PyTorch", "selftext": "We have released our Python Open Source tool for conversion ONNX models to PyTorch. Almost all Computer Vision neural networks architectures are supported. Please try it.\n\n[https://github.com/ENOT-AutoDL/onnx2torch](https://github.com/ENOT-AutoDL/onnx2torch?fbclid=IwAR1HpWxialVy-ILdHUYR8CJ_EsUyxbjhrMkZbWPWkQ0DuNuwvYDEpNVuhqc)\n\nI ask the Data Science community to support the project with a star on GitHub.", "upvote_ratio": 0.89, "id": "t3_tqgjsk", "created_utc": 1648493932.0}
{"sub": "Python", "title": "Software Engineering Tools and Best Practices for Data Science", "selftext": "nan", "upvote_ratio": 0.5, "id": "t3_tqfr47", "created_utc": 1648491771.0}
{"sub": "Python", "title": "A zoo for decorators", "selftext": "Hello there! First time posting here :)\n\nLast week I worked on a small python library and wanted to share it! \n\nReason for the library is that I found myself copy-pasting some python decorators over and over across multiple projects.\n\nLibrary is called deczoo (a zoo for decorators) and it's pip-installable.\n\ndocs: [https://fbruzzesi.github.io/deczoo/](https://fbruzzesi.github.io/deczoo/)  \nrepo: [https://github.com/FBruzzesi/deczoo](https://github.com/FBruzzesi/deczoo)", "upvote_ratio": 0.5, "id": "t3_tqefp4", "created_utc": 1648488239.0}
{"sub": "Python", "title": "I made my own Task Deadline Tracker using Python!", "selftext": "Hi all, I'm really excited to share this little project I've finally finished! Meet Tasky, a program made using tkinter that will track the deadlines for the task you feed in and show you the time remaining for your tasks (for example 3d 2h 57m left for some task). I use it regularly to track my own tasks and thus plan accordingly. It's been really helpful to me. You can add, delete and edit tasks in it too.\n\nHope you all like it :)\n\n[Image showing Labelled Parts of Tasky](https://user-images.githubusercontent.com/68178267/160485795-411c9ccb-a133-4eea-a3ac-213c91ae2669.png)  \nMore info and features: [Github Repo](https://github.com/AbhiK002/Tasky/)\n\n&amp;#x200B;\n\n[It comes in 2 themes as well!](https://preview.redd.it/3dcztj22p5q81.png?width=965&amp;format=png&amp;auto=webp&amp;s=618c3f6e291f9e84a3816a675f6d3bb9dd416379)", "upvote_ratio": 0.94, "id": "t3_tqe3tx", "created_utc": 1648487360.0}
{"sub": "Python", "title": "Python project that swipes for me on Tinder", "selftext": "demo link: [https://youtu.be/mRbEcqf1xLw](https://youtu.be/mRbEcqf1xLw)\n\n(the code breakdown link is in the description of the demo)", "upvote_ratio": 0.6, "id": "t3_tqdf63", "created_utc": 1648485556.0}
{"sub": "Python", "title": "In 2 days, running a live 45-minutes session on the fundamentals of observability, OpenTelemetry, and distributed tracing in Python", "selftext": "Hi everyone, there's a live OpenTelemetry and observability fundamentals session - in 2 days, Wednesday, March 30 at 11 AM PST.\n\n**You will learn how to instrument your apps to capture traces with OpenTelemetry in Python.**\n\nThis session is at no cost and vendor-neutral.\n\nYou can expect in this session: 45 minutes of core concepts, how to deploy it yourself hands-on + Q&amp;A.\n\nIf you are interested in observability, OpenTelemetry, and tracing - this is the place to be!\n\nRegister here [https://www.aspecto.io/get-started-with-opentelemetry/](https://www.aspecto.io/get-started-with-opentelemetry/?utm_source=post&amp;utm_medium=reddit&amp;utm_campaign=r-python-opentelemetry-workshop-pyhon-march-30-2022)", "upvote_ratio": 0.66, "id": "t3_tqd44k", "created_utc": 1648484744.0}
{"sub": "Python", "title": "Solverecaptchas - An async Python library to automate solving ReCAPTCHA v2 using Playwright.", "selftext": "[https://github.com/embium/solverecaptchas](https://github.com/embium/solverecaptchas)\n\nSolves audio and image types!\n\n## Compatibility\n\nLinux, macOS, and Windows!\n\n## Installation\n\n$ pip install solverecaptchas\n\n## Usage\n\n    import asyncio\n    import sys\n    \n    from solverecaptchas.solver import Solver\n    \n    if len(sys.argv) == 4:\n         pageurl, sitekey, proxy = sys.argv[1:]\n    else:\n         print('Invalid number of arguments (pageurl, sitekey, proxy)')\n         sys.exit(0)\n    \n    if proxy.lower() == \"none\":\n         proxy = None\n    \n    client = Solver(pageurl, sitekey, proxy=proxy)\n    result = asyncio.run(client.start())\n    if result:\n         print(result)", "upvote_ratio": 1.0, "id": "t3_tqcsdg", "created_utc": 1648483853.0}
{"sub": "Python", "title": "I made a Twitter bot that hates you!", "selftext": "I've messed around in Python on and off for a couple of years now. Most of what I've made has been exceptionally dumb (poorly written text adventures, a thing that used turtle to draw different colored spirals, a Garfield comic generator) and left unfinished. Until now!\n\n[Insult Ivan](https://twitter.com/InsultIvan) is a twitter bot I've created using Tweepy, and marks the first one of my exceptionally dumb projects to actually be done enough to function. All it does is pull words from four large lists (one with phrases to start the insult, and then three words to make up the insult itself), and then tweets this out once a day. It saves insults to a text file, so it won't repeat itself. It also responds to mentions with an insult that it will then only ever use for that user, reusing it in response to subsequent mentions from the same user.\n\nAnyway, the code is an absolute mess (I'm kind of assuming? I don't really know.), and I'm sure there is so much that I could have done better. But it functions and I love it and I wanted to share this beautifully idiotic thing I made. \n\nCode isn't available anywhere if that's okay (I can maybe mess around and see if I can make that happen if anyone really wants it). I'm happy to answer any questions though!", "upvote_ratio": 0.67, "id": "t3_tqc8pe", "created_utc": 1648482379.0}
{"sub": "Python", "title": "PYTHON CERTIFICATIONS LIST - BEST OF 2022", "selftext": "[Python Certification](https://www.sprintzeal.com/blog/python-certifications) is an established general-purpose high-level programming language. It was established by Guido van Rossum in 1991, further developments were done by Python Software Foundation. The primary position in python is given to code readability. Because by using its syntax programmers will be able to create concepts with fewer lines of code.", "upvote_ratio": 0.17, "id": "t3_tqbfx0", "created_utc": 1648480156.0}
{"sub": "Python", "title": "A roadmap of the free CS curriculum on Qvault (JS, Python, Go)", "selftext": "nan", "upvote_ratio": 0.72, "id": "t3_tqaff4", "created_utc": 1648477395.0}
{"sub": "Python", "title": "Feedback Requested - First Script that Calls API and Posts to MS Teams", "selftext": "Looking for feedback regarding my first script. Overall it calls a API, cleans the data a bit and then makes a post to MS Teams.\n\nhttps://imgur.com/a/5MkAsGu\n\nAnything I'm doing terrible? Something I could do better? More efficient?", "upvote_ratio": 0.5, "id": "t3_tq9d4k", "created_utc": 1648474241.0}
{"sub": "Python", "title": "The counter-intuitive rise of Python in scientific computing", "selftext": "nan", "upvote_ratio": 0.93, "id": "t3_tq8jzp", "created_utc": 1648471730.0}
{"sub": "Python", "title": "X-Cash crypto now has a Python wrapper Library", "selftext": "nan", "upvote_ratio": 0.31, "id": "t3_tq8csg", "created_utc": 1648471059.0}
{"sub": "Python", "title": "PYTHON CERTIFICATIONS LIST - BEST OF 2022", "selftext": "Benefits of Python Certification\n\nFollowing are the benefits one would get through python certification,\n\n**Easy to learn**\n\nThe key benefit of python certification is the programming language of python is easy to learn. **Python certification** can be used as a channel to a career, where learning other programming languages and frameworks will start to seem interesting.\n\nIf one is a complete beginner and wants to learn about programming, python will help to start better. Python is a simple and easy-to-understand programming language. And once the course is completed and you get the python certification, you will be confident enough to explore more about the programming world.\n\n**Expertise in Python**\n\nWhen it comes to recruiting python programmers, recruiters always expect good skills and what is the unique skill that makes the candidate stand out from the rest of the crowd. **Python certification** will help in such situations, if you are one of the candidates applying for a position and you have a certificate you will have better chances to get selected.\n\nHence, having [python certification](https://www.sprintzeal.com/blog/python-certifications) will prove that you know better and will help you gain the recruiter's interest.", "upvote_ratio": 1.0, "id": "t3_tq7crh", "created_utc": 1648467751.0}
{"sub": "Python", "title": "py.quit it", "selftext": "nan", "upvote_ratio": 0.29, "id": "t3_tq6wo3", "created_utc": 1648465973.0}
{"sub": "Python", "title": "Scrape all Naver Video Results using pagination in Python", "selftext": "Using [`parsel`](https://parsel.readthedocs.io/):\n\n```python\nimport requests, json \nfrom parsel import Selector \n \nparams = { \n    \"start\": 0,            # page number \n    \"display\": \"48\",       # videos to display. Hard limit. \n    \"query\": \"minecraft\",  # search query \n    \"where\": \"video\",      # Naver videos search engine  \n    \"sort\": \"rel\",         # sorted as you would see in the browser \n    \"video_more\": \"1\"      # required to receive a JSON data \n} \n \nheaders = { \n    \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\", \n} \n \nvideo_results = [] \n \nhtml = requests.get(\"https://s.search.naver.com/p/video/search.naver\", params=params, headers=headers, timeout=30) \njson_data = json.loads(html.text.replace(\"( {\", \"{\").replace(\"]})\", \"]}\")) \nhtml_data = json_data[\"aData\"] \n \nwhile params[\"start\"] &lt;= int(json_data[\"maxCount\"]): \n    for result in html_data: \n        selector = Selector(result) \n \n        for video in selector.css(\".video_bx\"): \n            title = video.css(\".text\").xpath(\"normalize-space()\").get().strip() \n            link = video.css(\".info_title::attr(href)\").get() \n            thumbnail = video.css(\".thumb_area img::attr(src)\").get() \n            channel = video.css(\".channel::text\").get() \n            origin = video.css(\".origin::text\").get() \n            video_duration = video.css(\".time::text\").get() \n            views = video.css(\".desc_group .desc:nth-child(1)::text\").get() \n            date_published = video.css(\".desc_group .desc:nth-child(2)::text\").get() \n \n            video_results.append({ \n                \"title\": title, \n                \"link\": link, \n                \"thumbnail\": thumbnail, \n                \"channel\": channel, \n                \"origin\": origin, \n                \"video_duration\": video_duration, \n                \"views\": views, \n                \"date_published\": date_published \n            }) \n \n    params[\"start\"] += 48 \n    html = requests.get(\"https://s.search.naver.com/p/video/search.naver\", params=params, headers=headers, timeout=30) \n    html_data = json.loads(html.text.replace(\"( {\", \"{\").replace(\"]})\", \"]}\"))[\"aData\"] \n \nprint(json.dumps(video_results, indent=2, ensure_ascii=False)) \nOutput:\n\n[ \n  { \n    \"title\": \"Minecraft : \ud83c\udff0 How to build a Survival Castle Tower house\", \n    \"link\": \"https://www.youtube.com/watch?v=iU-xjhgU2vQ\", \n    \"thumbnail\": \"https://search.pstatic.net/common/?src=https%3A%2F%2Fi.ytimg.com%2Fvi%2FiU-xjhgU2vQ%2Fmqdefault.jpg&amp;type=ac612_350\", \n    \"channel\": \"\uc18c\ud53c Sopypie\", \n    \"origin\": \"Youtube\", \n    \"video_duration\": \"25:27\", \n    \"views\": \"126\", \n    \"date_published\": \"1\uc77c \uc804\" \n  }, \n  { \n    \"title\": \"\uc870\uae08 \ud63c\ub780\uc2a4\ub7ec\uc6b8 \uc218 \uc788\ub294 \ub9c8\uc778\ud06c\ub798\ud504\ud2b8 [ Minecraft ASMR Tower ]\", \n    \"link\": \"https://www.youtube.com/watch?v=y8x8oDAek_w\", \n    \"thumbnail\": \"https://search.pstatic.net/common/?src=https%3A%2F%2Fi.ytimg.com%2Fvi%2Fy8x8oDAek_w%2Fmqdefault.jpg&amp;type=ac612_350\", \n    \"channel\": \"\uc138\ube48 XEBIN\", \n    \"origin\": \"Youtube\", \n    \"video_duration\": \"00:58\", \n    \"views\": \"1,262\", \n    \"date_published\": \"2021.11.13.\" \n  } \n] \n```\n\nUsing [`playwright`](https://playwright.dev/python/) browser automation:\n\n```python\nfrom playwright.sync_api import sync_playwright \nimport json \n \nwith sync_playwright() as p: \n    browser = p.chromium.launch(headless=False) \n    page = browser.new_page() \n    page.goto(\"https://search.naver.com/search.naver?where=video&amp;query=minecraft\") \n \n    video_results = [] \n \n    not_reached_end = True \n    while not_reached_end: \n        page.evaluate(\"\"\"let scrollingElement = (document.scrollingElement || document.body); \n                                 scrollingElement.scrollTop = scrollingElement scrollHeight;\"\"\") \n         \n        if page.locator(\"#video_max_display\").is_visible(): \n            not_reached_end = False \n \n    for index, video in enumerate(page.query_selector_all(\".video_bx\"), start=1): \n        title = video.query_selector(\".text\").inner_text() \n        link = video.query_selector(\".info_title\").get_attribute(\"href\") \n        thumbnail = video.query_selector(\".thumb_area img\").get_attribute(\"src\") \n        channel = None if video.query_selector(\".channel\") is None else video.query_selector(\".channel\").inner_text() \n        origin = video.query_selector(\".origin\").inner_text() \n        video_duration = video.query_selector(\".time\").inner_text() \n        views = video.query_selector(\".desc_group .desc:nth-child(1)\").inner_text() \n        date_published = None if video.query_selector(\".desc_group .desc:nth-child(2)\") is None else \\ \n            video.query_selector(\".desc_group .desc:nth-child(2)\").inner_text() \n \n        video_results.append({ \n            \"position\": index, \n            \"title\": title, \n            \"link\": link, \n            \"thumbnail\": thumbnail, \n            \"channel\": channel, \n            \"origin\": origin, \n            \"video_duration\": video_duration, \n            \"views\": views, \n            \"date_published\": date_published \n        }) \n \n    print(json.dumps(video_results, indent=2, ensure_ascii=False)) \n \n    browser.close() \n```\n\nOutput:\n\n```json\n[ \n  { \n    \"position\": 1, \n    \"title\": \"Minecraft : \ud83c\udff0 How to build a Survival Castle Tower house\", \n    \"link\": \"https://www.youtube.com/watch?v=iU-xjhgU2vQ\", \n    \"thumbnail\": \"https://search.pstatic.net/common/?src=https%3A%2F%2Fi.ytimg.com%2Fvi%2FiU-xjhgU2vQ%2Fmqdefault.jpg&amp;type=ac612_350\", \n    \"channel\": \"\uc18c\ud53c Sopypie\", \n    \"origin\": \"Youtube\", \n    \"video_duration\": \"25:27\", \n    \"views\": \"\uc7ac\uc0dd\uc218126\", \n    \"date_published\": \"20\uc2dc\uac04 \uc804\" \n  }, \n  { \n    \"position\": 1008, \n    \"title\": \"Titanic [Minecraft] V3 | \ud0c0\uc774\ud0c0\ub2c9 [\ub9c8\uc778\ud06c\ub798\ud504\ud2b8] V3\", \n    \"link\": \"https://www.youtube.com/watch?v=K39joThAoC0\", \n    \"thumbnail\": \"https://search.pstatic.net/common/?src=https%3A%2F%2Fi.ytimg.com%2Fvi%2FK39joThAoC0%2Fmqdefault.jpg&amp;type=ac612_350\", \n    \"channel\": \"\ub098\uc774\uc544Naia\", \n    \"origin\": \"Youtube\", \n    \"video_duration\": \"02:40\", \n    \"views\": \"\uc7ac\uc0dd\uc21822\", \n    \"date_published\": \"2021.11.11.\" \n  } \n] \n```\n\nIf you need a more in-depth explanation, you can visit the blog post at SerpApi: https://serpapi.com/blog/scrape-all-naver-video-results-using-pagination-in-python/#full_code", "upvote_ratio": 0.4, "id": "t3_tq6vg2", "created_utc": 1648465829.0}
{"sub": "Python", "title": "Open-source tool to make awesome-looking docs", "selftext": "[https://mkdocs-magicspace.alnoda.org/](https://mkdocs-magicspace.alnoda.org/)\n\nThis is a free tool I made. It will help to make beautiful docs for any Python project, and host them on GitHub pages.\n\nBased on MkDocs, I extended it with lots of plugins and extensions. And created tutorials.\n\nI hope you find it useful.", "upvote_ratio": 0.84, "id": "t3_tq6sgn", "created_utc": 1648465502.0}
{"sub": "Python", "title": "GitHub - enthought/traits: Observable typed attributes for Python classes", "selftext": "nan", "upvote_ratio": 0.88, "id": "t3_tq4p5h", "created_utc": 1648456356.0}
{"sub": "Python", "title": "Top python libraries/ frameworks that you suggest every one", "selftext": "Hit your answer I wonder is there any hidden treasure.", "upvote_ratio": 0.8, "id": "t3_tq483b", "created_utc": 1648454163.0}
{"sub": "Python", "title": "Top 5 Python Time Series Libraries", "selftext": "nan", "upvote_ratio": 0.75, "id": "t3_tq3cjf", "created_utc": 1648450342.0}
{"sub": "Python", "title": "Master Python Comprehensions", "selftext": "Hi folks,\n\nWrote something down about Python Comprehensions, focusing on simplicity and readability.\n\nYou can have a look here [https://towardsdatascience.com/master-python-comprehensions-4ef1c66b452d](https://towardsdatascience.com/master-python-comprehensions-4ef1c66b452d)\n\nHappy to discuss :)\n\nThanks!", "upvote_ratio": 0.75, "id": "t3_tq2ain", "created_utc": 1648445882.0}
{"sub": "Python", "title": "a lightweight DNS traffic flow monitor", "selftext": "Hi, I wrote a lightweight DNS traffic query / response monitor by using Scapy. It will print each transaction per line. Here's the source code link: [https://github.com/meow-watermelon/dns-flow](https://github.com/meow-watermelon/dns-flow) I attached a screenshot to show the output example. Any suggestions are welcome. Thanks!\n\nhttps://preview.redd.it/8ky0mi6z02q81.png?width=2560&amp;format=png&amp;auto=webp&amp;s=051e9591b484ff58d26eafd43b4ed3188faa7ba1", "upvote_ratio": 0.67, "id": "t3_tq1g8u", "created_utc": 1648442658.0}
{"sub": "Python", "title": "What type of language is Python?", "selftext": "I use python in all my classes and got into a discussion with my students about what type of language python is.\n\nInitially, I thought python is a completely \"non-typed\" language because we are able to suppress types while declaring variables and parameters:\n\n*def func(a, b): ...*\n\nBut then I found out that it's possible to type things up explicitly:\n\n*def func(a: int, b: int) -&gt; int: ...*\n\nI ended up categorizing python as an \"implicitly typed\" language, and the interpreter must fill the types I don't explicitly add to the code. However, one of my students pointed out that, as python admits dynamic typing, it is a \"dynamically typed language\", which is the official classification (?).\n\nHowever, I have failed to wrap my mind around the fact that I can explicitly set the types, but the interpreter won't bat an eye in case I recast my variables:\n\n*def func(a: int, b: int) -&gt; int:*\n\n*...c: int = a + b*\n\n*...return c*\n\n*d: int = func(1, 2)*\n\n*print(d)*\n\n*d = \"dynamically typed?\"*\n\n*print(d)*\n\nIs \"weakly-typed dynamic language\" a possible way to classify python?", "upvote_ratio": 0.66, "id": "t3_tpzl61", "created_utc": 1648435995.0}
{"sub": "Python", "title": "List comprehension vs multiplying a list differences?", "selftext": "Is there any difference between the following code?:\n\n    arr = [[]] * 10\n    arr = [[] for x in range(10)]\n\nI used the first method to initiate a list on a hackerank problem and I kept getting wrong answer. When I googled the answer and saw the person use the second method, so I tried it and got the right answer?? `[[]] * 10 == [[] for x in range(10)]` returns True. Why would the way I initiate this list cause a different result?\n\nIf anyone wants to try it themselves.. it's a real head scratcher for me.\n\n[https://programs.programmingoneonone.com/2021/05/hackerrank-dynamic-array-solution.html](https://programs.programmingoneonone.com/2021/05/hackerrank-dynamic-array-solution.html)  \n\n\n    def dynamicArray(n, queries):\n        # Write your code here\n        arr = [[] for x in range(n)]\n        #Now try using the code below to initiate arr. This fails it for me.\n        #arr = [[]] * n\n        lastAnswer = 0\n        answers = []\n        for x in queries:\n            idx = (x[1]^lastAnswer) % n\n            if x[0] == 1:\n                arr[idx].append(x[2])\n            else:\n                lastAnswer = arr[idx][x[2] % len(arr[idx])]\n                answers.append(lastAnswer)\n        return answers", "upvote_ratio": 0.56, "id": "t3_tpz8kn", "created_utc": 1648434777.0}
{"sub": "Python", "title": "Monday Daily Thread: Project ideas!", "selftext": "Comment any project ideas beginner or advanced in this thread for others to give a try! If you complete one make sure to reply to the comment with how you found it and attach some source code! If you're looking for project ideas, you might be interested in checking out Al Sweigart's, [\"The Big Book of Small Python Projects\"](https://inventwithpython.com/bigbookpython/) which provides a list of projects and the code to make them work.", "upvote_ratio": 1.0, "id": "t3_tpwknx", "created_utc": 1648425608.0}
{"sub": "Python", "title": "3D Render engine, written in 100% Python, No external libraries. *EPILEPSY WARNING*", "selftext": "EDIT: I have fixed the flickering issue! As long as you use the latest version there should be no flickering at all!\n\nVideo of it running: [https://youtu.be/7J2Pn8me7m8](https://youtu.be/7J2Pn8me7m8)\n\nGithub link: [https://github.com/E-Parker/Terminal-3D-Render/releases](https://github.com/E-Parker/Terminal-3D-Render/releases)\n\nI've been working on this for a little while, it's a simple 3D render engine that uses only the built-in python libraries.\n\nNOTE: If you suffer from any conditions that cause sensitivity to flashing lights do not use PREVIOUS versions of this program. The latest version is safe and free of flickering.Because the print command is very slow sometimes the screen will refresh before python is done drawing the frame, this causes the image to flicker occasionally. The effect worsens the faster your monitor's refresh rate is.\n\nThe features of this version are:- BMP decoding\n\n\\- OBJ decoding\n\n\\- Perspective Texture mapping\n\n\\- Depth buffer\n\n\\- Simple directional lighting\n\n\\- 231 colours!\n\nI don't think there is much to be learned from this other than how not to write a render engine, I spent a lot of time working out how to do things like decoding .bmp files when there are already solutions that are way faster and much less of a pain to work with. This was mostly an exercise to see how far I could go without any tools.\n\nAlso, do keep in mind this is the 4th project I've made with python, I'm not super experienced so don't expect the code to be very good-looking.", "upvote_ratio": 0.93, "id": "t3_tpv7hs", "created_utc": 1648421114.0}
{"sub": "Python", "title": "How did your coding style in python change after you learned another language?", "selftext": "Different languages make you write different code, but im curious if the way you wrote python changed due to coding in another language!", "upvote_ratio": 0.96, "id": "t3_tpu9p2", "created_utc": 1648418231.0}
{"sub": "Python", "title": "Build signed .aab with Kivy, Buildozer for Google Play Store (Windows &amp; ...", "selftext": "nan", "upvote_ratio": 0.67, "id": "t3_tppdqx", "created_utc": 1648404134.0}
{"sub": "Python", "title": "Made a Flask boilerplate using a services oriented structure", "selftext": "nan", "upvote_ratio": 0.89, "id": "t3_tpp9y4", "created_utc": 1648403841.0}
{"sub": "Python", "title": "Apilytics open-source middleware available for Flask! Install in 5 minutes and visualize all important metrics from your Flask backend", "selftext": "nan", "upvote_ratio": 0.66, "id": "t3_tpntm0", "created_utc": 1648399539.0}
{"sub": "Python", "title": "Actionable Notification with Callback in Python", "selftext": "nan", "upvote_ratio": 0.81, "id": "t3_tpmfni", "created_utc": 1648395499.0}
{"sub": "Python", "title": "Django 4 0 with Amazon RDS &amp; Elastic Beanstalk Full Course", "selftext": "nan", "upvote_ratio": 0.74, "id": "t3_tpky84", "created_utc": 1648390913.0}
{"sub": "Python", "title": "Improved version of Al Sweigart's swordfish program", "selftext": "Yesterday, I worked on improving Al Sweigart's *swordfish* program to limit to three the attempts a user can input their name and password. I'm proud of my work, although I ran into several bugs which left me scratching my head for a while. A big Thank You to Al for writing such a good introduction to Python!\n\nhttps://preview.redd.it/lldmgux5nxp81.png?width=1494&amp;format=png&amp;auto=webp&amp;s=1a881647b5ffd69fc9b2c921db059c636ca15d2b", "upvote_ratio": 0.66, "id": "t3_tpkr49", "created_utc": 1648390282.0}
{"sub": "Python", "title": "Learn Blockchain Concepts Practically With Python", "selftext": "nan", "upvote_ratio": 0.37, "id": "t3_tpjodl", "created_utc": 1648386792.0}
{"sub": "Python", "title": "PYVAULT - PYVAULT is a python program with which you can secure your precious passwords very easily.", "selftext": "Hi guys. So, I have made a python script which can store your passwords in a database and you can encrypt it with a secret key which is generated by the script only. Actually, with this , we can store our passwords pretty securely until our key is exposed.\n\nSo, I have used -\n\ni. sqlite3 (Used for database)\n\nii. cryptograhy (To work with encryption and decryption using the key)\n\nThese are the libraries.\n\nI used fernet to generate the key and to encrypt and decrypt the passwords and storing it in the database.\n\nThe script provides a list of options -\n\ni. Create a database\n\nii. Work with previous database\n\niii. Generate key\n\niv. Load key\n\nv. Retrive passwords\n\nYou can see the code here - [https://github.com/Shreejan-35/PYVAULT/](https://github.com/Shreejan-35/PYVAULT/)\n\nAll the contribution are welcome.\n\nI have done  the project by learning from a video about password manager from a youtube channel - neuralnine. But, I worked with database and tried to take it to some advance steps.\n\n&amp;#x200B;\n\nEdit 1:\n\nGuys I have uploaded this as a package on [pypi.org](https://pypi.org) as vaulter-py.\n\nYou can install it by typing -\n\n **pip install vaulter-py** \n\n&amp;#x200B;\n\nThanks", "upvote_ratio": 0.33, "id": "t3_tpic7y", "created_utc": 1648381816.0}
{"sub": "Python", "title": "python-nbxmpp 2.0.6 out!", "selftext": "nan", "upvote_ratio": 0.6, "id": "t3_tpi6fh", "created_utc": 1648381174.0}
{"sub": "Python", "title": "Backtesting A Put Option With An Example Of A Sell Put", "selftext": "Here's a simple post for people looking to try to backtest a put option strategy, or even a sell put strategy. I did this up because I was trying to look at a stock with a favorable put premium, and wanted to see based on a historical basis what are the chances of profit if I kept selling puts on these underlying at that premium\n\n[https://medium.com/@derivativestester/backtesting-a-vanilla-put-option-or-warrant-on-python-eec3ebaea5b](https://medium.com/@derivativestester/backtesting-a-vanilla-put-option-or-warrant-on-python-eec3ebaea5b)\n\nHope that helps.\n\nThanks.", "upvote_ratio": 0.63, "id": "t3_tpggqk", "created_utc": 1648373495.0}
{"sub": "Python", "title": "I made a Python program that AUTOMATICALLY edits YouTube Videos!", "selftext": "https://youtu.be/WYiKb5Ggtjc", "upvote_ratio": 0.9, "id": "t3_tpgeiw", "created_utc": 1648373209.0}
{"sub": "Python", "title": "I've created a extension for the Tornado framework that makes working with the framework easier and code development faster.", "selftext": "The extension is located in the [PyPi](https://pypi.org/project/usernado/) and you can easily install and use it. But the documentation and doc strings are not complete. This can be a good starting point if you want to [participate in open source](https://github.com/reganto/Usernado/issues).\n\nInstall it via pip: `pip install usernado`\n\nCheck it out here: [https://github.com/reganto/usernado](https://github.com/reganto/Usernado)\n\nThank you for your attention.", "upvote_ratio": 0.74, "id": "t3_tpfzq3", "created_utc": 1648371196.0}
{"sub": "Python", "title": "Using Python and Django, I've built to a GPT-3 powered web application to help myself write better", "selftext": "I am good in writing in my native language but when it comes to my second language, sometimes I struggle. So, for this reason, using Python and Django, I've built to a GPT-3 powered web application to help myself write better. \n\nI've been using it for a while and it is working so good! \n\n[https://www.youtube.com/watch?v=mfzW-LDajog](https://www.youtube.com/watch?v=mfzW-LDajog)\n\nLooking forward to your feedback.\n\nBest,", "upvote_ratio": 0.55, "id": "t3_tpb1qd", "created_utc": 1648349987.0}
{"sub": "Python", "title": "I made my first gui program!", "selftext": "I finally made a gui using tkinter. Its a super simple program that takes in an image and then makes it into text art using the braille unicode characters. You can specify the final dimensions of the characters. Also you can control how sensitive it is. Seemed like a fun thing to try!\n\nNot many ive told have been interested or understood so I thought I should try here.  \n[https://imgur.com/gallery/kMth4Cn](https://imgur.com/gallery/kMth4Cn)  \n\n\nFor those curious Ill explain my process  \nI use tkinter to make a menu with all the settings and when you click generate it runs my image algorithm  \n\n\nThe algorithm is pretty simple and uses pillow  \nUsing pillow I get an image and convert it to black and white  \n\n\nThen I use two for loops to go through the grid of data. It starts and the 0,0 pixel. It then grabs a 2 wide and 3 tall chunk of values. If the values are above the contrast value they are then set to a 1, else its a 0.  \n\n\nThen I reorganize the list of values of a chunk into a binary number. This is because the unicode counts up in binary. So once i arrange all the numbers to their binary number. I just convert that number to the unicode character. Then i save that chunk to a list. I do this for ever chunk on that layer. It then moves down and repeats until we have a matrix of braille characters.  \n\n\nFinally it just prints those arrays to a txt file  \n\n\nThe image kinda depends on how the text is displayed. On windows its inconsistent based on the zoom, certain levels of zoom cause distortions.  \n\n\nSorry if this didn't make sense I tried my best to explain for those interested. Thank you for reading!", "upvote_ratio": 0.86, "id": "t3_tpak0x", "created_utc": 1648348186.0}
{"sub": "Python", "title": "Tools for designing hardware in Python", "selftext": "Any hardware designers here who use Python for designing hardware? There are a bunch of libraries that all seem promising [MyHDL](https://www.myhdl.org/), [PyRTL](https://github.com/UCSBarchlab/PyRTL), [PyVerilog](https://github.com/PyHDI/Pyverilog), [PyLog](https://github.com/hst10/pylog), [PyMTL3](https://github.com/pymtl/pymtl3), ... All seem to work roughly the same. Write code in Python and transpile it to VHDL/Verilog. Which of these are popular and well-maintained? MyHDL looks good but it's last release was 0.10 in 2018 and for hardware design you don't want to rely on 0.x software. Anything like Chisel for Python.", "upvote_ratio": 0.83, "id": "t3_tpa4eo", "created_utc": 1648346641.0}
{"sub": "Python", "title": "Sunday Daily Thread: What's everyone working on this week?", "selftext": "Tell /r/python what you're working on this week! You can be bragging, grousing, sharing your passion, or explaining your pain. Talk about your current project or your pet project; whatever you want to share.", "upvote_ratio": 1.0, "id": "t3_tp7ye1", "created_utc": 1648339210.0}
{"sub": "Python", "title": "Live video to ascii", "selftext": "Hi!\n\n&amp;#x200B;\n\nFirst time posting, this is a small passion project which I coded in my free time.\n\n[https://github.com/DSERIOUSGUY/whoami.git](https://github.com/DSERIOUSGUY/whoami.git)\n\nIt basically converts Image/live video to ascii art (can save as Image or text) using openCV.\n\nCurrently it does not save video and only supports black/white and white/black coloring(my initial goal was to make it look a bit retro - if that makes any sense).\n\nI would love to hear your suggestions on how it could be improved on and what features could be added!\n\nEdit: Attached are samples of all modes (all are captured at different moments)\n\n&amp;#x200B;\n\n[Video](https://preview.redd.it/19cpror02tp81.png?width=602&amp;format=png&amp;auto=webp&amp;s=dd17430640f30d8c886c6ff79b42551a95de7841)\n\nSince I don't really care about latency for an image, higher resolution is used as compared to video.\n\n[Image](https://preview.redd.it/f5ufsr6g2tp81.png?width=1920&amp;format=png&amp;auto=webp&amp;s=c8a52f4e27abdba583f2001d535e16bc19b6c1f5)\n\n&amp;#x200B;\n\nA higher resolution image spreads out too much to be viewed as a picture, so had lower the resolution before writing the file\n\n[Text](https://preview.redd.it/9vb45izl2tp81.png?width=1920&amp;format=png&amp;auto=webp&amp;s=ae36fb691f1ca9734b48cc3bdcc7ea36ae5f7f28)", "upvote_ratio": 0.8, "id": "t3_tp684n", "created_utc": 1648333884.0}
{"sub": "Python", "title": "toolgui: A modular event-driven GUI system for quickly building tools with Python and pyimgui.", "selftext": "nan", "upvote_ratio": 0.91, "id": "t3_tp50ru", "created_utc": 1648331504.0}
{"sub": "Python", "title": "Arcade version 2.6.13 has been released", "selftext": "Arcade, a 2D graphics library, has released version 2.6.13.\n\n[https://api.arcade.academy/en/latest/development/release\\_notes.html](https://api.arcade.academy/en/latest/development/release_notes.html)\n\n&amp;#x200B;\n\n* New Features \n   * Arcade can now run in headless mode on linux servers **opening more possibilities for users in for example the data science** community ([\\#1107](https://github.com/pythonarcade/arcade/issues/1107)). See [Headless Arcade](https://api.arcade.academy/en/latest/advanced/headless.html#headless) for more information.\n* Bugfixes\n   * The random text glitching issue especially affecting users with iGPUs is finally resolved in pyglet. For that reason we have upgraded to the pyglet 2.0a2 release.\n   * Fixed an issue causing [arcade.draw\\_circle\\_filled()](https://api.arcade.academy/en/latest/api/drawing_primitives.html#arcade.draw_circle_filled) and [arcade.draw\\_circle\\_outline()](https://api.arcade.academy/en/latest/api/drawing_primitives.html#arcade.draw_circle_outline) to always render with 3 segments on some iGPUs.\n   * Fixed an issue causing interactive widgets to unnecessarily re-draw when hovering or pressing them. This could cause performance issues.\n   * SectionManager\u2019s on\\_show\\_view  \n was never called when showing a view\n* Various Improvements\n   * [arcade.load\\_font()](https://api.arcade.academy/en/latest/api/text_pyglet.html#arcade.load_font) now supports resource handles\n   * [PhysicsEngineSimple](https://api.arcade.academy/en/latest/api/physics_engines.html#arcade.PhysicsEngineSimple) can now take an iterable of wall spritelists\n   * Sprite creation is now \\~6-8% faster.\n   * Removed warning about missing shapely on startup\n   * Window titles are now optional. If no window title is specified the title will be the absolute path to the python file it was created in. This was changed because of the new headless mode.\n   * Removed arcade.quick\\_run  \n. This function had no useful purpose.\n   * Added clear method to UIManager ([\\#1116](https://github.com/pythonarcade/arcade/pull/1116))\n   * Updated from Pillow 9.0.0 to 9.0.1\n* Tilemap\n   * Rectangle objects which are empty(have no width or height) will now be automatically converted into single points.\n   * The Tile ID of a sprite can be access with sprite.properties\\[\"tile\\_id\"\\]  \n. This refers to the local ID of the tile within the Tileset. This value can be used to get the tile info for a given Sprite created from loading a tilemap.\n* Docs\n   * Added python version support info to install instructions ([\\#1122](https://github.com/pythonarcade/arcade/pull/1122))\n   * Fixed typo in [append\\_texture()](https://api.arcade.academy/en/latest/api/sprites.html#arcade.Sprite.append_texture) docstring([\\#1126](https://github.com/pythonarcade/arcade/pull/1126))\n   * Improved the raycasting tutorial ([\\#1124](https://github.com/pythonarcade/arcade/issues/1124))\n   * Replace mentions of 3.6 on Linux install page ([\\#1129](https://github.com/pythonarcade/arcade/pull/1129))\n   * Fix broken links in the homepage ([\\#1139](https://github.com/pythonarcade/arcade/pull/1130))\n   * Lots of other improvements to docstrings throughout the code base\n   * General documentation improvements\n* OpenGL\n   * [arcade.gl.Geometry](https://api.arcade.academy/en/latest/gl/geometry.html#arcade.gl.Geometry) now supports transforming to multiple buffers.\n   * Added and improved examples in experimental/examples\n   * Major improvements to API docs\n\nSpecial thanks to [Mohammad Ibrahim](https://github.com/Ibrahim2750mi), [pushfoo](https://github.com/pushfoo), [Alejandro Casanovas](https://github.com/janscas), [Maic Siemering](https://github.com/eruvanos), [Cleptomania](https://github.com/Cleptomania), [pvcraven](https://github.com/pvcraven) and [einarf](https://github.com/einarf) for their contributions to this release. Also, thanks to everyone on the Pyglet team! We depend heavily on Pyglet\u2019s continued development.", "upvote_ratio": 0.83, "id": "t3_tp4o3d", "created_utc": 1648330422.0}
{"sub": "Python", "title": "Why is Python so popular?", "selftext": "I am sorry if this is provocative to anyone, but personally I can't stand Python. I hate the indentations, Python is slow, no private keyword in OOP, dynamically typed ... \n\nObviously I can see that it has its reasons to be popular in certain areas like data science. But why in the world in the backend for production level code? Please actually explain to me why you think Python is great from a software engineering perspective. I am coming from C# and simply cannot see any advantages in Python.", "upvote_ratio": 0.26, "id": "t3_tp3od4", "created_utc": 1648327393.0}
{"sub": "Python", "title": "Space Science with Python - AI 1-9: A Convolutional Approach", "selftext": "Hey everyone,\n\nthe asteroid reflectance spectra project reaches almost its  end. There are 5 more videos to come and then I will focus on another topic (Near-Earth Objects).\n\nAnyway, spectra are still amazing and I show in today's session how to conduct an ML multi-class  classification experiment using keras and Conv1D layers.\n\nYou may thing: \"well... creating a Deep Learning architecture is like magic\". You are right. A few blueprints help one to get an idea how an architecture may look like. But improving e.g., the number of filters, layers, activations functions etc. is something that MUST be automatized. To tackle this, the next session will show \"Keras Tuner\" to automatize exactly this!  \nHope you still enjoy the combination of Space - Python - ML and I am looking forward to chat with you either here or on [Twitter](https://twitter.com/MrAstroThomas).\n\nGitHub: [https://github.com/ThomasAlbin/Astroniz-YT-Tutorials/blob/main/%5BML1%5D-Asteroid-Spectra/9\\_dl\\_convnet\\_multiclass.ipynb](https://github.com/ThomasAlbin/Astroniz-YT-Tutorials/blob/main/%5BML1%5D-Asteroid-Spectra/9_dl_convnet_multiclass.ipynb)\n\nYouTube: [https://www.youtube.com/watch?v=648XF1o0B3c](https://www.youtube.com/watch?v=648XF1o0B3c)\n\nCheers,  \nThomas", "upvote_ratio": 0.71, "id": "t3_tp0vo1", "created_utc": 1648322880.0}
{"sub": "Python", "title": "Show r/python: Sailor - a tiny PaaS to install on your servers/VPS that uses git push to deploy micro-apps, micro-services, sites with SSL, on your own servers or VPS", "selftext": "Hey Pythonistas, here's a tool that I would like to share with y'all that allows you deploy multiple apps on servers/VPS. Let me know what you think and some feedback or what you would like to see in it.\n\nIntroducing, Sailor. [https://github.com/mardix/sailor](https://github.com/mardix/sailor)\n\n**Sailor** is a tiny **PaaS** to install on your servers/VPS (DigitalOcean, Hetzner, Linode).\n\nIt uses git push to deploy micro-apps, micro-services and sites.\n\nIt natively supports Python, Nodejs, Static sites, and any other languages that can use the command line.\n\nSites deployed with **Sailor** automatically have SSL assigned with LetsEncrypt.\n\n**Sailor** can run long-running background workers and cron jobs.\n\nIt allows you to deploy multiple sites/apps using a single repository.\n\nIt gives you the option of having testing/staging/production environment deployed from the same codebase.\n\n**Sailor** lets you see some stats about your apps, along with scaling them.\n\n**Sailor** makes deploying apps a smooth sailing.\n\n*Ship it like a* ***Sailor!***\n\n[https://github.com/mardix/sailor](https://github.com/mardix/sailor)", "upvote_ratio": 0.57, "id": "t3_tozhrt", "created_utc": 1648318743.0}
{"sub": "Python", "title": "Interview with a Postdoc, Junior Python Developer in 2022", "selftext": "nan", "upvote_ratio": 0.93, "id": "t3_toy9gu", "created_utc": 1648315157.0}
{"sub": "Python", "title": "What is the most pythonic way to limit while loop iterations?", "selftext": "Hi all\n\nWhenever I write a while loop, I always want to limit the number of iterations it can run to avoid infinite loops while developing the program,  what would the most pythonic way of doing that?\n\ntheres obviously the basic \n\n    n = 0\n    while condition and n &lt; 100:\n        n += 1\n\nbut the `n+=1` can easily get lost or commented out when you are working on the code  \n\n\nIve also tried \n\n    def count_generator():\n        n = 0\n        while True:\n            yield n\n            n += 1\n    \n    counter = count_generator()\n    \n    while condition and next(counter) &lt; 100:\n\nor \n\n    class Counter:\n        def __init__(self):\n            self.count = 0\n    \n        def inc(self):\n            self.count += 1\n            return self.count\n    \n    counter = Counter()\n    \n    while condition and counter.inc() &lt; 100:\n\nboth of these feel like a lot of boiler plate for what should be a simple task, is there anything better?", "upvote_ratio": 0.79, "id": "t3_towbf7", "created_utc": 1648311909.0}
{"sub": "Python", "title": "Image Processing With the Python Pillow Library \u2013 Real Python", "selftext": "nan", "upvote_ratio": 0.84, "id": "t3_tov052", "created_utc": 1648310781.0}
{"sub": "Python", "title": "oCrypt0r - AES encryption library", "selftext": "Hello everyone! \n\n\nI have decided to for the first time, try my hand at making an encryption package/library. It uses salted AES encryption.\n\nAs of right now, I'm not sure what else to add to it or do as I have everything pretty much covered in terms of what could be encrypted. From just normal messages/strings to files and directories/folders.\n\nI hope you all like it or at least get some use out of it! \n\n\n\nHere is my github repo and PyPi link:\n- [GitHub - oCrypt0r](https://github.com/therealOri/oCrypt0r)\n- [PyPi - oCrypt0r](https://pypi.org/project/oCrypt0r/)", "upvote_ratio": 0.56, "id": "t3_tosyou", "created_utc": 1648308863.0}
{"sub": "Python", "title": "PIPELY - new lightweight pipeline library", "selftext": "hey there, just released a new version of `pipely` \\- a lightweight pipeline library that can trigger any sequence of classes in any order.\n\n# Simple Example:\n\ncreate `collect.yaml` \n\n    steps:\n        a1_print:\n            exec: src/file1.py:firstA\n        a2_print:\n            exec: src/file1.py:secondA\n        final_print:\n            exec: src/file2.py:printDone\n            depends_on:\n            - a1_print\n            - a2_print\n\n`depends_on` parameter sets the following order for pipely:\n\n1. firstly it parallelly executes `firstA` and `secondA` classes (`a1_print` and `a2_print` steps) from file `src/file1.py` \n2. and then executes  `printDone` class (step `final_print`) from file `src/file2.py`\n\nIt also allows **value transformations** between classes and other cool things. \n\nCheck out [https://pypi.org/project/pipely/](https://pypi.org/project/pipely/) for more info, and let me know your feedback! \n\nThis is just a pet project I had during my free lunch time :)", "upvote_ratio": 0.64, "id": "t3_tosvcq", "created_utc": 1648308584.0}
{"sub": "Python", "title": "Simple Key Event Sending to an Application Wrapper I made", "selftext": "I made  a Python library to send inputs to an executable, useful for scenarios such as having a neural network send decisions to a game. Essentially a wrapper around the win32api.   \nYou can send events to a window in just 2 lines:  \n\n\n    hwnd  =  pyinput.get_handle(window_name) # Gets the handle\n    pyinput.press_key(hwnd, pyinputkeycodes.VK_RETURN) # Sends Return key\n\nI got annoyed at doing boilerplate over and over for this so I made a quick library!  \n[https://github.com/GaryFrazier/PyInput](https://github.com/GaryFrazier/PyInput)", "upvote_ratio": 0.75, "id": "t3_toryjt", "created_utc": 1648305770.0}
{"sub": "Python", "title": "Encrypto - A simple way of complicating/encrypting data", "selftext": "This is a repost I made to fit community guidelines.\n\n*Meet Encrypto, a revolutionary technology that can protect your data from the government **(Joke Intended)** and prevent the Feds from understanding your text.*\n\nThe code itself does a simple task. When asked to encrypt data, it will generate two random keys. One till be used for the Caesar Cipher offset and the other defines how many times the symmetric algorithm is run. This will give out a gibberish looking string, which is then converted into a BrainF\\*\\*k *like* looking code. Wrong keys during decryption will give you the erroneous data.\n\nNote that since each character is encoded with 8-characters, and that the Base64 encryption happens multiple times, file size may skyrocket. \n\nI visualized this concept a long time ago, but spent some months completely understanding Python and the logic required (Encryption types, etc.). This code was designed to mimic the Enigma machine with its separate codes for different encryption types.\n\nHere's the link to the code:\nhttps://github.com/Programmer-X31/Encrypto.git\n\nPlease give me your comments and give your ideas for what I should base the *third key* on.", "upvote_ratio": 0.67, "id": "t3_tonad8", "created_utc": 1648296993.0}
{"sub": "Python", "title": "Creating mp4 clips with soccer data and Python", "selftext": "Hi Pythonistas! I\u2019ve updated my package to allow users to creat mp4 clips of soccer data. Here is the short tutorial I wrote up on how to use it: https://todofootballclub.com/?p=1056. \nI\u2019ve also run into an issue of wanting to use latitude and longitude data in the future but realized there is a learning curve with trans Mercator projections and other things to do it. I\u2019m curious if anyone has ever started a project and realized \u2018oh wow, there\u2019s more to this than I thought\u2019?", "upvote_ratio": 0.9, "id": "t3_tomsiv", "created_utc": 1648295093.0}
{"sub": "Python", "title": "NiceScaler update 1.3.0", "selftext": "&amp;#x200B;\n\n[GUI](https://preview.redd.it/gogk3hgavop81.png?width=2790&amp;format=png&amp;auto=webp&amp;s=31396204dac0bb003f7ff7920a4c4555bf15109c)\n\n[example upscale](https://preview.redd.it/q9nwpoz8vop81.jpg?width=2486&amp;format=pjpg&amp;auto=webp&amp;s=955eb9817ddc84a6d9392b00cf67d11b35577253)\n\nItch -&gt; [https://jangystudio.itch.io/nicescaler](https://jangystudio.itch.io/nicescaler)\n\nGithub -&gt; [https://github.com/Djdefrag/NiceScaler/releases/tag/1.3.0](https://github.com/Djdefrag/NiceScaler/releases/tag/1.3.0)\n\n&amp;#x200B;\n\nupdate 1.3.0 (26.03.2022)\n\nOpenCL backend / UX improvement / Speed improvement\n\nNew feature\n\n* GPU OpenCL backend (to use GPU horsepower to upscale)\n* support for more images and video file types\n\nImprovements\n\n* updated Python (3.7.9 -&gt; 3.9.10)\n* bugfix in AI model creation (speed boost \\~10% with all models)\n* deleted EDSR model (lighter .exe 110Mb -&gt; 70Mb)\n* general code cleanup\n\nUX\n\n* deleted EDSR model button\n* new colors for left bar\n* new button to select OpenCL backend\n* removed AI models info under buttons\n* removed NiceScaler icon\n* added Github button\n* drag&amp;drop space cleaning", "upvote_ratio": 0.93, "id": "t3_tok71u", "created_utc": 1648283523.0}
{"sub": "Python", "title": "The Oregon Trail", "selftext": "How many have ever played The Oregon Trail? I remember playing it in my later days of High School. Not the video game but the teletype version. I just ported over the original version from Fortran to Python.\n\nIf you want to read out the different parts of the port or even get the source here are some links:\n\n[https://keithmfoster.com/the-oregon-trail/](https://keithmfoster.com/the-oregon-trail/)\n\n[Github Repository](https://github.com/KeithMFoster/the-oregon-trail)\n\nHappy Trails.", "upvote_ratio": 0.93, "id": "t3_toa6c6", "created_utc": 1648256653.0}
{"sub": "Python", "title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread", "selftext": "Found a neat resource related to Python over the past week? Looking for a resource to explain a certain topic?\n\nUse this thread to chat about and share Python resources!", "upvote_ratio": 0.75, "id": "t3_to8lbi", "created_utc": 1648252810.0}
{"sub": "Python", "title": "What's the dumbest, trivial, shoddily-written project you've made that somehow impressed a non-programmer?", "selftext": "I wrote a script that grabs an Overwatch screenshot, tries to recognize whatever hero portraits are present on the tab screen, then copies some text making hero suggestions into your clipboard. I'm 90% certain that the one person in the hiring process that sounded impressed when I mentioned it didn't actually look at it, because I got hired to try automating website tests with Selenium.\n\n(Note: I've long since taken this script down as it's in kind of a grey area with Blizzard's TOS)", "upvote_ratio": 0.93, "id": "t3_to8jhz", "created_utc": 1648252736.0}
{"sub": "Python", "title": "AI Project Ideas", "selftext": "Hi everyone,  \nI need to make an AI project with full-fledged programming ... it has to be based on a social issue nd focused mainly on the data science domain. Does anyone have any suggestions for an idea that can be implemented using AI? I'm open to anything as it's a group project and we're mainly putting forth any ideas right now at the start and will start ruling out to a final within a few days", "upvote_ratio": 0.85, "id": "t3_to33to", "created_utc": 1648246367.0}
{"sub": "Python", "title": "pointers.py - segmentation faults in python", "selftext": "nan", "upvote_ratio": 0.69, "id": "t3_tnz5uw", "created_utc": 1648242515.0}
{"sub": "Python", "title": "Kids Learn to Code in Python with the Raspberry Pi Foundation", "selftext": "nan", "upvote_ratio": 0.88, "id": "t3_tnw8zx", "created_utc": 1648234236.0}
{"sub": "Python", "title": "I've built a wrapper that makes building Telegram Bots a lot easier", "selftext": "It's called **MATE** (*Easy Telegram Application Maker*, but reversed lol) and it automatically handles the boring under the hood stuff, letting you focus on the logic of your app.\n\nIt comes with (fairly) complete **documentation** &amp; a **PyPI** release.\n\ncheck it out here: [https://github.com/SudoOmbro/MATE](https://github.com/SudoOmbro/MATE)\n\nTell me what you think! \n\nThank you for your time :\\^)", "upvote_ratio": 0.87, "id": "t3_tnsarp", "created_utc": 1648229199.0}
{"sub": "Python", "title": "I Made a Hangman Game with Music, Light Artwork, and Sound Effects (GitHub Repo Below)", "selftext": "As my first python project, I decided to create a hangman game with music, light artwork, and sound effects. \n\nAfter completing PY4E on Coursera, I decided to create a program that combined all elements from the course. In all, it uses urllib for web scraping, sqlite3 to create and manage the leaderboard and wordbank, colorama for color, and playsound for sound. \n\nThere are also \"Classic\" and \"Hardcore\" modes. Classic operates as hangman traditionally works, and hardcore mode gives the user 8 seconds for input before subtracting a life and adding a part to the hangman. \n\nI started python in January, and it feels great to have actually created something!\n\nHere is the Github link: [https://github.com/DaSaltyPancake/Hangman.git](https://github.com/DaSaltyPancake/Hangman.git)", "upvote_ratio": 0.89, "id": "t3_tns9go", "created_utc": 1648229099.0}
{"sub": "Python", "title": "A free and flexible translation library in python", "selftext": "Hi y all,\n\n*I'm not making this post only to promote the project. I also want to invite contributors and get feedback.*\n\nFeel free to check the GitHub [repo here](https://github.com/nidhaloff/deep-translator)\n\ndeep-translator is a flexible and **free** translation library that supports multiple translators. \n\nI tried to make the API as consistent as possible so that it can be updated easily in the future. We fixed some bugs and added new features in the new releases.\n\nSince I don't have much time, I'm looking for maintainers &amp; contributors, who want to join the project. Feel free to contact me if you are interested. You don't have to be a python professional, I want to help people make their first contribution and join the open-source world.", "upvote_ratio": 0.81, "id": "t3_tnp85j", "created_utc": 1648224421.0}
{"sub": "Python", "title": "GitHub - Rog3rSm1th/Frelatage: The Python Fuzzer that the world deserves", "selftext": "nan", "upvote_ratio": 0.43, "id": "t3_tnovr2", "created_utc": 1648223456.0}
{"sub": "Python", "title": "Python Selenium Tutorial #6 - Bypass Detection using plugins, settings &amp; proxies", "selftext": "nan", "upvote_ratio": 0.8, "id": "t3_tnmxy8", "created_utc": 1648217926.0}
{"sub": "Python", "title": "Python - LinkedIn Skill Assessments Quizzes with Answers | MeshWorld", "selftext": "nan", "upvote_ratio": 0.3, "id": "t3_tnljc5", "created_utc": 1648213623.0}
{"sub": "Python", "title": "Python Pub/Sub", "selftext": "Let's see how far this rabbit hole goes. It's late evening. I started reading up on networking. It's 2am. Let's see how Redis implements pub/sub under the hood. Basically, I ended up writing my own pub/sub implementation in Python:\n\n[https://github.com/Salaah01/py-pub-sub/blob/master/server/server.py](https://github.com/Salaah01/py-pub-sub/blob/master/server/server.py)\n\n&amp;#x200B;\n\nhttps://i.redd.it/u867y09txip81.gif", "upvote_ratio": 0.6, "id": "t3_tnkxf3", "created_utc": 1648211635.0}
{"sub": "Python", "title": "Value objects with Python", "selftext": "Hello r/Python\n\nThis is my first post here.\n\nI created a blog post about value objects with Python.\n\n[https://blog.szymonmiks.pl/p/value-objects-with-python/](https://blog.szymonmiks.pl/p/value-objects-with-python/)\n\n&amp;#x200B;\n\nCode examples are available on my GitHub [https://github.com/szymon6927/szymonmiks.pl/tree/master/blog/content/post/06-value-objects-with-python/value-object-examples](https://github.com/szymon6927/szymonmiks.pl/tree/master/blog/content/post/06-value-objects-with-python/value-object-examples).\n\n I hope you will enjoy it. I would love to hear your opinion", "upvote_ratio": 0.6, "id": "t3_tnk6p3", "created_utc": 1648209000.0}
{"sub": "Python", "title": "Ethereum Price Email Alerts With Python", "selftext": "nan", "upvote_ratio": 0.13, "id": "t3_tnjf86", "created_utc": 1648206202.0}
{"sub": "Python", "title": "Reactivex like operators that can be used directly on async iterables", "selftext": "I'm a fan of the [operators](https://rxpy.readthedocs.io/en/latest/reference_operators.html) available in ReactiveX. I'm not a fan of observables, and all the other cruft that comes with using reactivex.  I would much rather be writing async generators, and using them in async for loops.\n\nSo I've started writing a library that implements the operators typically found in Reactivex libraries, and I'm posting it here for some early feedback.  \nSource: [https://github.com/garyvdm/aiterx](https://github.com/garyvdm/aiterx)  \nExample:\n\n    &gt;&gt;&gt; from asyncio import run, sleep\n    &gt;&gt;&gt; from aiterx import debounce\n    &gt;&gt;&gt; \n    &gt;&gt;&gt; async def source():\n    ...     yield 1\n    ...     await sleep(0.2)\n    ...     yield 2\n    ...     await sleep(0.1)\n    ...     yield 3\n    ...     await sleep(0.1)\n    ...     yield 4\n    ...     await sleep(0.2)\n    ...     yield 5\n    ... \n    &gt;&gt;&gt; async def test_debounce():\n    ...     async for item in debounce(source(), 0.15):\n    ...         print(item)\n    ... \n    &gt;&gt;&gt; run(test_debounce())\n    1\n    4\n    5\n\nHave I missed some existing library that does what I need?  I did looked at these before I started:\n\n* [rxpy](https://rxpy.readthedocs.io/en/latest/)\n* [aioreactive](https://github.com/dbrattli/aioreactive)\n* [async-rx](https://geronimo-iia.github.io/async-rx/)\n\nAny feedback on the work I have done so far?", "upvote_ratio": 0.78, "id": "t3_tnifu4", "created_utc": 1648202004.0}
{"sub": "Python", "title": "Scrape Google Top Carousel Results in Python", "selftext": "Full code:\n\n```python\nimport requests, lxml, re, json\nfrom parsel import Selector\n\n# https://docs.python-requests.org/en/master/user/quickstart/#custom-headers\nheaders = {\n  \"User-agent\":\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.83 Safari/537.36\"\n  }\n\nparams = {\n  \"q\": \"dune actors\",  # search query\n  \"gl\": \"us\",          # country to search from\n  }\n\n\ndef parsel_get_top_carousel():\n  html = requests.get('https://www.google.com/search', headers=headers, params=params)\n  selector = Selector(text=html.text)\n\n  carousel_name = selector.css(\".yKMVIe::text\").get()\n  all_script_tags = selector.css(\"script::text\").getall()\n\n  data = {f\"{carousel_name}\": []}\n\n  decoded_thumbnails = []\n\n  for _id in selector.css(\"img.d7ENZc::attr(id)\").getall():\n    # https://regex101.com/r/YGtoJn/1\n    thumbnails = re.findall(r\"var\\s?s=\\'([^']+)\\'\\;var\\s?ii\\=\\['{_id}'\\];\".format(_id=_id), str(all_script_tags))\n    thumbnail = [\n      bytes(bytes(img, \"ascii\").decode(\"unicode-escape\"), \"ascii\").decode(\"unicode-escape\") for img in thumbnails\n      ]\n    decoded_thumbnails.append(\"\".join(thumbnail))\n\n  for result, image in zip(selector.css('.QjXCXd.X8kvh'), decoded_thumbnails):\n\n    title = result.css(\".JjtOHd::text\").get()\n    link = f\"https://www.google.com{result.css('.QjXCXd div a::attr(href)').get()}\"\n    extensions = result.css(\".ellip.AqEFvb::text\").getall()\n\n    if title and link and extensions is not None:\n      data[carousel_name].append({\n        \"title\": title,\n        \"link\": link,\n        \"extensions\": extensions,\n        \"thumbnail\": image\n        })\n\n  print(json.dumps(data, indent=2, ensure_ascii=False))\n```\n\nBlog post with more explanation: https://serpapi.com/blog/scrape-google-carousel-results-with-python/", "upvote_ratio": 1.0, "id": "t3_tngota", "created_utc": 1648193728.0}
{"sub": "Python", "title": "Build a Word Guessing Game in Python [video + source code]", "selftext": "Here is the link for the video : \n\n[https://youtu.be/M1t4RJ5XRHE](https://youtu.be/M1t4RJ5XRHE)\n\nHere is the link for the source code : \n\n [Word-Guessing-Game/wordgame.py at main \u00b7 The-Nerdy-Dev/Word-Guessing-Game (github.com)](https://github.com/The-Nerdy-Dev/Word-Guessing-Game/blob/main/wordgame.py)", "upvote_ratio": 0.72, "id": "t3_tngfjr", "created_utc": 1648192532.0}
{"sub": "Python", "title": "Build your own Feature Rich J.A.R.V.I.S in Python - Python Project", "selftext": "nan", "upvote_ratio": 0.67, "id": "t3_tngd7q", "created_utc": 1648192249.0}
{"sub": "Python", "title": "Meta donates $300,000 to the Python Software Foundation", "selftext": "nan", "upvote_ratio": 0.91, "id": "t3_tnfh9l", "created_utc": 1648188306.0}
{"sub": "Python", "title": "Friday Daily Thread: Free chat Friday! Daily Thread", "selftext": "Use this thread to talk about anything Python related! Questions, news,  projects and any relevant discussion around Python is permitted!", "upvote_ratio": 1.0, "id": "t3_tn96gz", "created_utc": 1648166410.0}
{"sub": "Python", "title": "Python for AWS Lambda Functions: A Beginner's Guide and Tutorial", "selftext": "[https://codesolid.com/python-and-aws-lambda-functions](https://codesolid.com/python-and-aws-lambda-functions/)\n\nI just finished this -- it's very much focused on Python developers who may not have written an AWS Lambda function before and are wondering what all the fuss is about.  Enjoy!", "upvote_ratio": 0.87, "id": "t3_tn7qfv", "created_utc": 1648162106.0}
{"sub": "Python", "title": "A simple python3 script to keep 2 folders synced", "selftext": "Hi r/Python, I'm here today to showcase a project that I've developed to experiment with python, github, ci/cd best practices and pytest.I'm not really a beginner, I've been coding in python for a while now (I'm a junior GCP developer). This post is just a request for feedback and/or suggestion on how to improve from more experienced python developers. Here is the repo : [https://github.com/davideolgiati/raidify](https://github.com/davideolgiati/raidify), feel free to open an issue on bug and suggested enhancements.", "upvote_ratio": 0.79, "id": "t3_tn733z", "created_utc": 1648160259.0}
{"sub": "Python", "title": "Guido van Rossum on Twitter: It's happening! BPO is migrating to GitHub tomorrow.", "selftext": "nan", "upvote_ratio": 0.95, "id": "t3_tn4lyj", "created_utc": 1648155805.0}
{"sub": "Python", "title": "Is Spock-Config the only tool that integrates object-oriented config files and command-line interfaces?", "selftext": "[Spock-Config](https://github.com/fidelity/spock) allows one to create OO configuration files. That's how I roll. I currently use [PYdantic settings](https://pydantic-docs.helpmanual.io/usage/settings/) and it's great. But it does not offer command-line re-configuration of what you have in the OO config file. \n\nSure you could manually do all the mappings yourself. But that's why I like spock.\n\nWhat I dont like about Spock is that [there is already another PyPI package with that same top-level-namespace](https://github.com/fidelity/spock/issues/235) ... why does PyPI even allow that? What would I do if I wanted to use both in the same project???\n\n### So somebody rock my world\n\nTell me about an alternative. [Plumbum](https://plumbum.readthedocs.io/en/latest/cli.html) kinda-sorta fits the bill.... but it really is just OO CLI development.", "upvote_ratio": 0.75, "id": "t3_tmz4yi", "created_utc": 1648150974.0}
{"sub": "Python", "title": "Performance: SQLAlchemy vs Django vs EdgeDB", "selftext": "nan", "upvote_ratio": 0.64, "id": "t3_tmqev6", "created_utc": 1648143624.0}
{"sub": "Python", "title": "IDE-style autocomplete that integrates with Python tools (pip, pyenv, etc)", "selftext": "&amp;#x200B;\n\nhttps://reddit.com/link/tmleui/video/r3kh3f0pycp81/player", "upvote_ratio": 0.82, "id": "t3_tmleui", "created_utc": 1648139283.0}
{"sub": "Python", "title": "Github - Multiplatform (arm7, arm64, amd64) Docker Image for Celery with precompiled gevent", "selftext": "nan", "upvote_ratio": 0.75, "id": "t3_tmcp8z", "created_utc": 1648131220.0}
{"sub": "Python", "title": "Animating a Sprite Sheet in Python w/ PyGame", "selftext": "nan", "upvote_ratio": 0.8, "id": "t3_tmbg9f", "created_utc": 1648129996.0}
{"sub": "Python", "title": "How to create a digital clock with python easily", "selftext": " This tutorial is about creating a project using python language. I\u2019m going to show you how to create a digital clock with Python. Before reading the article, if you like to know more about Python, you could check out these two important articles:\n\n* [Why should I learn Python?](https://progskillss.com/why-should-i-learn-python)\n* [How to learn Python?](https://progskillss.com/how-to-learn-python-programming)\n\nThis project is suitable for beginners. To write the code, you need to know how to use modules in Python. Two necessary modules that I use in this project are Tkinter and time. Tkinter is a GUI library that helps you develop a graphical user interface for the digital clock, and using the time module; you could get the current time, date, timezone, etc. Let\u2019s dive deep into these modules.\n\nIf you want to read more about this project, you could click on the following link:\n\n[https://progskillss.com/how-to-create-a-digital-clock-with-python](https://progskillss.com/how-to-create-a-digital-clock-with-python)", "upvote_ratio": 0.57, "id": "t3_tmabdm", "created_utc": 1648128741.0}
{"sub": "Python", "title": "OpenTelemetry and Python: A Complete Instrumentation Guide", "selftext": "A blog post exploring how to instrument a Python application to emit tracing data (metric and log data interfaces are not stable quite yet). It examines:\n\n* How auto-instrumentation of the same codebase works.\n* The differences with manual instrumentation.\n* How to mix manual instrumentation with auto-instrumentation.\n* How to add information about exceptions.\n\nHere's the link: [https://www.timescale.com/blog/opentelemetry-and-python-a-complete-instrumentation-guide/](https://www.timescale.com/blog/opentelemetry-and-python-a-complete-instrumentation-guide/)", "upvote_ratio": 0.92, "id": "t3_tm8jte", "created_utc": 1648127261.0}
{"sub": "Python", "title": "Web2py Framework in Python", "selftext": "nan", "upvote_ratio": 0.6, "id": "t3_tm3sxx", "created_utc": 1648121398.0}
{"sub": "Python", "title": "Tree Traversal Algorithms in Python-InsideAIML", "selftext": "nan", "upvote_ratio": 0.67, "id": "t3_tm3jvl", "created_utc": 1648120520.0}
{"sub": "Python", "title": "Scientific computation using NumPy library", "selftext": "nan", "upvote_ratio": 0.43, "id": "t3_tm30b4", "created_utc": 1648118458.0}
{"sub": "Python", "title": "Learn Python yield interactively from your browser!", "selftext": "nan", "upvote_ratio": 0.67, "id": "t3_tm2yyc", "created_utc": 1648118307.0}
{"sub": "Python", "title": "RaViewer: parsing and displaying binary data straight from camera (made with Dear PyGui)", "selftext": "&amp;#x200B;\n\n[RaViewer screenshot](https://preview.redd.it/6e3n26fe7bp81.png?width=1220&amp;format=png&amp;auto=webp&amp;s=9163087922bf30b0038e173e9290d6b56fa9f586)\n\n[RaViewer demo](https://i.redd.it/s4m2bmsa7bp81.gif)\n\n[RaViewer](https://github.com/antmicro/raviewer) is an open-source utility dedicated to parsing and displaying binary data acquired straight from camera. After opening a binary image, you can specify the color format, the image size and append or remove n bytes from the beginning of the image series. The binary image will be processed and shown based on these values. You can control which color channels are displayed and zoom in and out. For detailed information, you can view the hexadecimal pixel values in table format. The resulting image can be exported entirely or just a selected part to more complex formats (JPEG, PNG) or raw data. The source code is available in the project's [GitHub repository](https://github.com/antmicro/raviewer).\n\nYou can read more about RaViewer in this [article](https://antmicro.com/blog/2021/11/raviewer-open-source-tool-for-debugging-video-pipelines/) by [antmicro](https://antmicro.com/).\n\nMade with Python and the GUI was created with [Dear PyGui](https://github.com/hoffstadt/DearPyGui/wiki/Dear-PyGui-Showcase).", "upvote_ratio": 0.87, "id": "t3_tm2wb6", "created_utc": 1648118019.0}
{"sub": "Python", "title": "httpx worked fine for me... any reason to consider urllib3?", "selftext": "I've found [httpx](https://www.python-httpx.org/) to be very approachable for my API consumption tasks and less wordy than [urllib3](https://urllib3.readthedocs.io/).\n\nDo you have a preference for API consumption (perhaps requests) and why?", "upvote_ratio": 0.67, "id": "t3_tm1grs", "created_utc": 1648111891.0}
{"sub": "Python", "title": "I created a Python Script to find out Broken links by scanning entire WordPress Website", "selftext": "I created a Python script to check links in all the WordPress posts to find out broken ones. I tried creating this after learning multi-threading based, I had this idea for so long because other broken link checking software such as SiteBulb and ScreamingFrog take very long time (\\`24 hours) to crawl a one of my websites (having around 28000 posts). So, instead of crawling, I used WordPress API and this way it is much faster.\n\nI will modify it later to do the same thing for non-WordPress website, by analyzing the sitemap.\n\nIt goes through all the posts one by one from WordPress API, extract all the URLs from &lt;a&gt; tag and then checks their status by making a HEAD request.\n\n404 and other status codes are recorded. If the HEAD request fails, then name of the exception class is recorded.\n\nFinally, save the report in CSV file.\n\n[wpbroken script in action](https://preview.redd.it/30t209zd0ap81.png?width=1366&amp;format=png&amp;auto=webp&amp;s=1ad89fc845ee0e77ee20197b0a887301b46f9748)\n\nGitHub Gist:  [wpbroken.py](https://gist.github.com/ilovefreesw/fa763e1f84cd9f7101dc4816e293beda)", "upvote_ratio": 0.92, "id": "t3_tlzj7n", "created_utc": 1648103526.0}
{"sub": "Python", "title": "5 Python Libraries for Automating OSINT Operations", "selftext": "nan", "upvote_ratio": 0.91, "id": "t3_tlz7q5", "created_utc": 1648102201.0}
{"sub": "Python", "title": "Tutorial on GPU-based ray-casting with Python", "selftext": " \n\nIf you are interested in doing 'shadows' for a 2D game, here's a tutorial:\n\n[https://api.arcade.academy/en/development/tutorials/raycasting/index.html](https://api.arcade.academy/en/development/tutorials/raycasting/index.html)\n\nhttps://preview.redd.it/fd1dyzmoi8p81.png?width=1200&amp;format=png&amp;auto=webp&amp;s=92fd25296d83102fd08e5d8f21b59d5d07c2e80f", "upvote_ratio": 0.94, "id": "t3_tlsb8q", "created_utc": 1648085480.0}
{"sub": "Python", "title": "Including packages in project", "selftext": "I've been working on project for a raspberry pi, which I've been writing on my PC in Pycharm, and SSHing over to my pi.  What's the best way to move the whole project over, including the dependencies? Been just manually adding them on the pi and realized there has to be a better way.\nThanks!", "upvote_ratio": 0.67, "id": "t3_tlp1vn", "created_utc": 1648082311.0}
{"sub": "Python", "title": "Thursday Daily Thread: Python Careers, Courses, and Furthering Education!", "selftext": "Discussion of using Python in a professional environment, getting jobs in Python as well as ask questions about courses to further your python education!\n\n**This thread is not for recruitment, please see** r/PythonJobs **or the thread in the sidebar for that.**", "upvote_ratio": 0.67, "id": "t3_tlmt0a", "created_utc": 1648080009.0}
{"sub": "Python", "title": "Coin toss probability", "selftext": "    import random\n    from collections import Counter\n    import itertools\n    \n    ##method for coin toss\n    def cointoss():\n        rand_i = random.randint(0, 1)\n        outcomes = [\"Heads\", \"Tails\"]\n        #outcomes = [0,1]\n        return outcomes[rand_i]\n    \n    #create list\n    list=[]\n    \n    ##Number of coin tosses\n    n=3\n    \n    ##calls method and adds to the list\n    for i in range(0, n):\n        t1 = cointoss()\n        list.append(t1)\n    \n    print(list)\n    Counter(list)\n    print(Counter(list))\n    \n    ##compare previous coin tosses with the last\n    for index, i in enumerate(list):\n        for j in list[index+1:]:\n            print(\"list item i \"+str(i))\n            print(\"list item j \" + str(j))\n    \n            if i==j:\n                print(\"Same\")\n            else:\n                print(\"Not the same\")\n\n&amp;#x200B;\n\nI'm trying to edit this code so that it outputs the number of times a certain side occurs in a row.\n\n&amp;#x200B;\n\nSo for example, a person makes ten coin toss, how many times does heads occur three times or two times in a row.\n\n&amp;#x200B;\n\nAny suggestions on how to edit it?", "upvote_ratio": 0.56, "id": "t3_tlkryn", "created_utc": 1648077791.0}
{"sub": "Python", "title": "Can we get hired just to do EDA\u2019s in Python ? Exploratory Data Analysis using libraries-", "selftext": "I\u2019ve been relatively new to Datascience, spent couple of years with sql, r, python, power BI &amp; tableau.. I\u2019ve specifically loved getting involved with EDA\u2019s - what do you guys think ?", "upvote_ratio": 0.33, "id": "t3_tli7e4", "created_utc": 1648073230.0}
{"sub": "Python", "title": "Where will Python be in the Web3 space?", "selftext": "Hi guys, I noticed that there are two prominent libraries for web3 applications: [web3.py](https://web3.py) and web3.js . After doing some searching online, I have found there is some preference for web3.js over [web3.py](https://web3.py). Does this mean that Python might be obsolete in Web3?", "upvote_ratio": 0.32, "id": "t3_tlhee1", "created_utc": 1648072417.0}
{"sub": "Python", "title": "Pants 2.10: Multiple Python lockfile support, PyOxidizer, Thrift codegen, and better linter parallelization", "selftext": "nan", "upvote_ratio": 0.78, "id": "t3_tlgwka", "created_utc": 1648071777.0}
{"sub": "Python", "title": "Programming community for freetime team projects", "selftext": "The problem:\n\nI started learning programming 3 year ago, partly in university, partly at home, and built a few larger projects since then. I have a lot of new ideas for projects, which are fun, but could also be profitable if done right. The problem is the scale of these imagined projects is too large to handle them all by myself. Hiring developers doesn't feel like a solution for me. I would love to build projects together with other \"advanced\" programmers and exchange ideas, learn from each other, inspire each other. But how do I find people who are interested aswell?\n\nThe Solution:\n\nA more private, trustbased community of developers, who have time and energy to contribute unpaid work to programming projects. People could share and discuss ideas, group together in teams and start projects. Also if there is an intention to turn projects profitable teams could determine at the beginning how profits will be shared. I imagine this community as a discord server right now, but could be something else of cause.\n\n&amp;#x200B;\n\n\\-Does anyone know if a community like this exists somewhere?\n\n\\-If no are there people here interested in starting a discord server to fill this purpose?\n\n&amp;#x200B;\n\nThanks for reading and greetings from Hamburg, Maxim", "upvote_ratio": 0.67, "id": "t3_tlg6u2", "created_utc": 1648070961.0}
{"sub": "Python", "title": "Custom BurpSuite extensions in Python: recreating Cloud2Butt", "selftext": "nan", "upvote_ratio": 0.67, "id": "t3_tlg281", "created_utc": 1648070801.0}
{"sub": "Python", "title": "Text Similarity w/ Levenshtein Distance in Python", "selftext": "nan", "upvote_ratio": 0.67, "id": "t3_tldsug", "created_utc": 1648068506.0}
{"sub": "Python", "title": "Mail merge in python", "selftext": "I have a excel/CSV data where in a single customer has multiple orders in different lines. Can I generate a list of text files in which each customer's multiple orders are present. Much like a invoice copy. Any clues on how to do this is highly appreciated ...\n\nPS ... I am learning python", "upvote_ratio": 0.67, "id": "t3_tl9dbz", "created_utc": 1648063812.0}
{"sub": "Python", "title": "i created a game controller using Arduino touch sensor and python", "selftext": "nan", "upvote_ratio": 0.45, "id": "t3_tl3raq", "created_utc": 1648057722.0}
{"sub": "Python", "title": "Creating a Python CLI with Go(lang)-comparable startup times", "selftext": "Hi Folks.\n\nI recently put some effort into creating a command line interface (CLI) made with Python.\n\nBackground: I started a new project called Gefyra, a tool for local application development directly with Kubernetes. Check it out the website [https://gefyra.dev](https://gefyra.dev) or have a glance at the code [https://github.com/gefyrahq/gefyra/tree/main/client](https://github.com/gefyrahq/gefyra/tree/main/client)\n\nI'd like to have an executable with (almost) the startup performance of `kubectl` (the executable to control a Kubernetes cluster). That means, I need fast startup times (which is crucial for a CLI) and ideally just one file (which is statically-linked) for easy distribution. In addition, I\u2019d like to provide executables for Windows, MacOS and Linux. For those requirements people would usually go for Go (needless to say it's awesome), however I started out with a prototype written in Python and it evolved over time. So I tried to find a way to make this work with Python.\n\nI went the following way:\n\n1. PyInstaller: [https://pyinstaller.readthedocs.io/en/stable/](https://pyinstaller.readthedocs.io/en/stable/)\n2. Nuitka: [https://nuitka.net/](https://nuitka.net/)\n3. PyOxidizer: [https://pyoxidizer.readthedocs.io/en/stable/](https://pyoxidizer.readthedocs.io/en/stable/)\n\n**PyInstaller**\n\nPyInstaller was quite easy to set up. However, the resulting executable was complained about by Virustotal (see: [https://www.virustotal.com/gui/home/upload](https://www.virustotal.com/gui/home/upload)) because of PyInstaller's bootloader. Somehow the code signature was also found in viruses (lol). To workaround this I compiled a bootloader myself which at least removed the virus issues. \n\nOn MacOS I faced startup times of more than 10 s with internet connection and about 3 s without internet connection. Interestingly, the former docker-compose command was also created from PyInstaller and Mac users complained about the startup performance, too: [https://github.com/docker/compose/issues/6956](https://github.com/docker/compose/issues/6956) :)\n\nI didn\u2019t find much to improve. The concept of PyInstaller will potentially always be a problem for fast startup times (which IMHO makes it unsuitable for CLI applications).\n\n**Nuitka**\n\nWith Nuitka, I generated very large binaries of about 150 Mb. The startup performance was already much better than PyInstaller for Mac and Linux. However, I was not completely satisfied and very long compile times bothered me a little bit (about 10 min).\n\n**PyOxidizer**\n\nI ended up using PyOxidizer. This well-crafted toolkit compiles Python to Rust code and also includes all dependencies into one handy binary executable. With no special optimizations I saw startup times of about 700 ms. That is almost acceptable, though I wanted to go a little further.\n\nI started to examine the output of `python -X importtime -m gefyra 2&gt; import.log` just to check the imports. There is an awesome tool to analyze the Python imports: tuna (see: [https://github.com/nschloe/tuna](https://github.com/nschloe/tuna)). `tuna` allows analyzing the import times from the log. Run it like so `tuna import.log`. It opens a browser window and visualizes the import times. With that I was able to manually move all imports to the functions in which they are needed (and bring in some other optimizations). This greatly violates PEP 8 ([https://peps.python.org/pep-0008/#imports](https://peps.python.org/pep-0008/#imports)) but leads to very fast startup times.\n\nThese are the startup values I finally reached with `gefyra` under average modern Ubuntu:\n\n    &gt; python -m timeit \"__import__('os').system(gefyra)\"  \n    10 loops, best of 5: 33.5 msec per loop  \n\nPretty neat, isn\u2019t it?   \nIn comparison the `kubectl` executable:\n\n    &gt; python -m timeit \"__import__('os').system('kubectl')\"  \n    10 loops, best of 5: 24.9 msec per loop  \n\nIn addition, I created GitHub actions to run the PyOxidizer builds once a new version is released (see: [https://github.com/gefyrahq/gefyra/blob/main/.github/workflows/dist-build-linux.yaml](https://github.com/gefyrahq/gefyra/blob/main/.github/workflows/dist-build-linux.yaml)). Only Windows is missing at the moment.\n\nAlthough, PyInstaller and Nuitka did not deliver the best startup times, I would not say it's bad software. They probably shine at other aspects.\n\nI hope these insights can be useful for someone else, too.", "upvote_ratio": 0.84, "id": "t3_tl3jwz", "created_utc": 1648057171.0}
{"sub": "Python", "title": "\u201cLike it or not, it\u2019s going to be primarily up to Python devs to crush the business side\u2019s dreams AI can magically make their company better.\u201d The CTO of an AI company explains why AI can be a waste of time and resources at most companies primarily because it\u2019s retrofitted onto existing products.", "selftext": "nan", "upvote_ratio": 0.91, "id": "t3_tkyuz8", "created_utc": 1648052349.0}
{"sub": "Python", "title": "Minos Demo - Stocks Index Wallet with Microservices", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_tkv9ax", "created_utc": 1648045935.0}
{"sub": "Python", "title": "Python Timer Functions: Three Ways to Monitor Your Code \u2013 Real Python", "selftext": "nan", "upvote_ratio": 0.8, "id": "t3_tkv08a", "created_utc": 1648045223.0}
{"sub": "Python", "title": "The top 5 advanced Python highly rated free courses On Udemy with real-world projects.", "selftext": "Hello,\n\n&amp;#x200B;\n\n[Top 5 Python free courses](https://preview.redd.it/9p3qq8gacbp81.png?width=1024&amp;format=png&amp;auto=webp&amp;s=b225e18de619f201c4aa711d229a72b5a7704aa2)\n\n**The top 5 Python highly rated free courses On Udemy with real-world projects.**\n\n[Course1: Applied Deep Learning Build a Chatbot Theory And Application.](https://www.udemy.com/course/applied-deep-learning-build-a-chatbot-theory-application/)\n\n[Course2: Master Data Analysis with Python Intro to Pandas.](https://www.udemy.com/course/master-data-analysis-with-python-intro-to-pandas/)\n\n[Course3: Machine Learning Crash Course for Beginners.](https://www.udemy.com/course/easy-machine-learning/)\n\n[Course4: The Art of Doing Video Game Basics with Python and Pygame](https://www.udemy.com/course/the-art-of-doing-video-game-basics-with-python-and-pygame/).\n\n[Course5: Master Data Analysis with Python \u2013 Selecting Subsets of Data.](https://www.udemy.com/course/master-data-analysis-with-python-selecting-subsets-of-data/)\n\nThe Courses List:\n\n[https://netslovers.com/2022/03/17/advanced-python-free-courses-udemy/?feed\\_id=277&amp;\\_unique\\_id=623390a11ddad](https://netslovers.com/2022/03/17/advanced-python-free-courses-udemy/?feed_id=277&amp;_unique_id=623390a11ddad)\n\nI hope you found this post helpful.", "upvote_ratio": 0.87, "id": "t3_tkuqsi", "created_utc": 1648044502.0}
{"sub": "Python", "title": "(I think) I stress tested matplotlib for real-time graping.", "selftext": "&amp;#x200B;\n\n[Matplotlib Stress Test](https://i.redd.it/x3xvfvmeq4p81.gif)\n\nIt can run at 60 FPS if you push it to its limits, which is more than I need.\n\nThere are occasional hiccups, but I think it's fine.\n\nI am developing a simple GUI for an open-source DAQ module called [PlainDAQ](https://www.crowdsupply.com/kuncu-teknoloji/plaindaq). Before I start, I decided to stress test it to see if it can handle fast changing waveforms.\n\nI think this is pretty enough for my application\n\nHere is the [code](https://github.com/AlperenAkkuncu/PlainDAQ/blob/main/Development/GUI/sinewave_stress_test.py)\n\n**What more can I do to make it smoother? One guy in eevblog suggested me to look into garbage collection. What are your opinions?**", "upvote_ratio": 0.76, "id": "t3_tkt5v8", "created_utc": 1648039696.0}
{"sub": "Python", "title": "Python Generators", "selftext": "nan", "upvote_ratio": 0.71, "id": "t3_tksmex", "created_utc": 1648037852.0}
{"sub": "Python", "title": "Bloxs: display your data in an attractive way in your notebook", "selftext": "Hi, I would like to share with you a small python package that I've created to display data in a notebook in an attractive way.  The package is called `bloxs` and is available on GitHub https://github.com/mljar/bloxs \n\nThe package can display as a block following data:\n\n- number with the title\n\n- progress bar \n\n- chart (can be a line, stepped line, bar)\n\nWhat is more, there can be several blocks displayed in one row.\n\n\nThe implementation is very simple, there is only one class, called `B`. It displays a single block or row of blocks. Each object of the `B` class has the `_repr_html_()` method that returns the HTML with a block.\n\nA very basic example:\n\n    from bloxs import B\n    B(1234, \"Bloxs in a notebook!)\n\n\nThe package works with Jupyter Notebook, Google Colab, Deepnote, and Kaggle.\n\nI hope you find it useful and it will help you to create beautiful dashboards, reports, and apps directly from the notebook.", "upvote_ratio": 0.76, "id": "t3_tkrlom", "created_utc": 1648034233.0}
{"sub": "Python", "title": "How To Automate Your Statistical Data Analysis", "selftext": "nan", "upvote_ratio": 0.79, "id": "t3_tkpe69", "created_utc": 1648024528.0}
{"sub": "Python", "title": "The Right Way to Compare Floats in Python", "selftext": "nan", "upvote_ratio": 0.65, "id": "t3_tkp7wz", "created_utc": 1648023709.0}
{"sub": "Python", "title": "Spotify LED Matrix", "selftext": "I created an LED matrix that takes your currently playing spotify song and displays it to an led panel.   It uses a raspi 0w to create one thread to get data from the spotify api, and another to update the board. The code is rough around the edges, but I hope y'all enjoy it!  \n\nDemo: \n\nhttps://reddit.com/link/tkp0xt/video/56ihrzuzb3p81/player\n\nRepo: [https://github.com/Evan-Nishi/spotify-panel-client](https://github.com/Evan-Nishi/spotify-panel-client) (stars would be appreciated :D)", "upvote_ratio": 0.87, "id": "t3_tkp0xt", "created_utc": 1648022796.0}
{"sub": "Python", "title": "Yet another simple wordle searcher", "selftext": "It is a script that will help narrow down the list of possible solutions to a wordle puzzle. In the end, you still have to use your grey matter, and, depend on luck. It scrapes data from Lou Hevly's website (which provides a regex search functionality). And then it applies some simple filtering algorithms on the results.\n\nScript &amp; docs: [https://gist.github.com/deostroll/6014bd0cf3cc4b0a22894d0981cacddc#file-wordle\\_search-py](https://gist.github.com/deostroll/6014bd0cf3cc4b0a22894d0981cacddc#file-wordle_search-py)\n\n[2 minute video](https://www.youtube.com/watch?v=Js2DNbNynw4), where I am thinking a lot, but eventually defer to the script for help...But in the video I had used selenium/msedge. Later decided to use python requests and beautifulsoup...that is whats shared above...", "upvote_ratio": 0.6, "id": "t3_tkoi7q", "created_utc": 1648020431.0}
{"sub": "Python", "title": "Wednesday Daily Thread: Beginner questions", "selftext": "New to Python and have questions? Use this thread to ask anything about Python, there are no bad questions!\n\nThis thread may be fairly low volume in replies, if you don't receive a response we recommend looking at r/LearnPython or joining the Python Discord server at [https://discord.gg/python](https://discord.gg/python) where you stand a better chance of receiving a response.", "upvote_ratio": 0.67, "id": "t3_tkgwne", "created_utc": 1647993608.0}
{"sub": "Python", "title": "Flask web blog", "selftext": "Some days ago i made a Blog based-on Flask, it lets you add a custom password, username, upload small comments with HTML and it has a basic user authentication system!\n\nI called it \"comments\" (because of it's feature, it let's you to post small text with custom HTML) [https://github.com/ZSendokame/comments](https://github.com/ZSendokame/comments)\n\n&amp;#x200B;\n\nI wan't ideas and/or feedback about how to improve it!\n\nBye", "upvote_ratio": 0.72, "id": "t3_tkgecq", "created_utc": 1647992128.0}
{"sub": "Python", "title": "I Created a Visualization Package for Soccer", "selftext": "I\u2019ve put my package on PyPi. The world of sports analytics, at least for soccer, is restricted. I wanted to learn how to analyze a game but found a large lack of data. Even worse, a lack of applications. I decided to creat my own Python package and help educate the public through my website. I\u2019ve made a YouTube tutorial to help others get started as well.\n\nhttps://youtu.be/tZlrULiN26E\n\nThe PyPi package is todofcpy", "upvote_ratio": 0.8, "id": "t3_tkgccb", "created_utc": 1647991972.0}
{"sub": "Python", "title": "Meta deepens its investment in the Python ecosystem", "selftext": "nan", "upvote_ratio": 0.95, "id": "t3_tkedpx", "created_utc": 1647986506.0}
{"sub": "Python", "title": "I made bubble slort. I dont think it looks clean but it works. Comments both in Polish and English", "selftext": "Here is image and link\n\n[https://github.com/bibi100101/Bubble-Sort](https://github.com/bibi100101/Bubble-Sort)\n\nhttps://preview.redd.it/t1oni4fc60p81.png?width=1920&amp;format=png&amp;auto=webp&amp;s=76703d2816371f63a113f7d32130aa0cfaae37de\n\nShowcase\n\nTranslation:\n\n\\-Lista Wcze\u015bniej = list before\n\n\\-Lista Pozniej = list after\n\nhttps://preview.redd.it/cnqeybzq60p81.png?width=1359&amp;format=png&amp;auto=webp&amp;s=e7f108267104423cdcc723689deb5d4b2c4953e9", "upvote_ratio": 0.56, "id": "t3_tkdp7o", "created_utc": 1647984659.0}
{"sub": "Python", "title": "Declarative command line parser library [Heated Arguments]", "selftext": "I was trying out a bunch of different command line parser libraries recently and wanted to see if I could take a different approach.  I have a proof of concept for a command line parser that let's you define parameters and sub-commands in a declarative way.\n\nI have a repository with a couple demo scripts to show what the interface looks like and I was hoping to get some feedback if it looks interesting enough that I should keep developing it. Thanks!\n\nhttps://github.com/mjcaley/heated", "upvote_ratio": 0.63, "id": "t3_tkcfmu", "created_utc": 1647981274.0}
{"sub": "Python", "title": "Today i released a small package called geoiter. Used for web scraping", "selftext": "# geoiter\n\niterates the planet.\n\n**edit**\n\nA simple tool to iterate coordinates within given boundaries.  \nThe usage is mostly for querying/searching by location.   \nGeoiter provides many locations within a boundary, like a country.   \nLet's say Germany has in sum 5000 houses to sell. Then most platforms will only allow you to visit the first 200 houses.   \nNow to get the others, you need to dissect the big boundary area into smaller ones.  \nAnd this is where geoiter provides you with coordinates\n\nyou can find the source on\n\n[https://github.com/cloasdata/geoiter](https://github.com/cloasdata/geoiter)\n\n**/ edit**\n\n&amp;#x200B;\n\nhttps://preview.redd.it/vlzva9z6rzo81.png?width=665&amp;format=png&amp;auto=webp&amp;s=72c8ebf635d7ae8ec3bfcbf1f74217fd53065daf\n\n**geoiter** can be used for web scraping to utilize geo/location queries:\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nhttps://preview.redd.it/kdypecx8rzo81.png?width=1590&amp;format=png&amp;auto=webp&amp;s=930cb8c7837bb1b1d9a80718b12538c713f229b0\n\nIn many cases the web page restrict the result items to a fixed number. With geoiter you can now dissect this one query to a many location queries to relax the result density under the restriction limit.\n\ngeoiter has only one additional dependency called [haversine](https://pypi.org/project/haversine/).\n\n## install\n\n    pip install geoiter\n\n## usage\n\n    import pickle\n    \n    from geoiter.util.ressource_example import germany\n    from geoiter import GeoIter\n    \n    # get you boundary for example\n    with open(germany, \"rb\") as file:\n        germany = pickle.load(file)\n    \n    # prepare\n    gi = GeoIter(\n        boundary=germany,\n        radius=100,\n        comp_rate=20\n        )\n    \n    if __name__ == \"__main__\":\n        # plot them as example\n        for coordinate in gi:\n            print(coordinate)\n\n## speed\n\none may consider that geo data have mb of coordinates. Which may make the this iteration very slow, because it needs to look up coordinates in the boundary often. To accelerate the **geoiter** provides a very simple compressor and uses bisect instead of list iteration. However, it still can be slow.\n\n## extensions\n\nThere two extensions which give additional help\n\n    pip install geoiter[\"gpx]\n\nprovides you with an gpx exporter.\n\n    pip install geoiter[\"plot\"]\n\nprovides a plotting function to visualize the grid.\n\n## data\n\nget boundaries from osm or others sources like\n\n* [https://www.geoboundaries.org/](https://www.geoboundaries.org/)\n* [https://osm-boundaries.com/](https://osm-boundaries.com/)\n* ...", "upvote_ratio": 0.94, "id": "t3_tkbqms", "created_utc": 1647979432.0}
{"sub": "Python", "title": "Directories in Python", "selftext": " \n\nDirectories in Python\n\nhttps://youtu.be/Qip7MToDr18\n\n\\#python #python3 #directories #os #PythonProgramming #tutorial #PythonForResearchers", "upvote_ratio": 0.11, "id": "t3_tkb40y", "created_utc": 1647977755.0}
{"sub": "Python", "title": "Top 5 Benefits of Python Web Development That You Need to Know", "selftext": "nan", "upvote_ratio": 0.25, "id": "t3_tk9o0p", "created_utc": 1647973886.0}
{"sub": "Python", "title": "I made a tutorial on How to convert Images to ASCII using pygame", "selftext": "Hi everyone! I have been looking at various image filtering libraries, and some of them have image to ASCII filters. So I thought, why not make it using pygame?\n\nTutorial can be found [here](https://www.youtube.com/watch?v=oEacnqQgE4A)\n\nSource code can be found [here](https://github.com/tank-king/Tutorials/tree/main/Python%20Pygame/image_to_ASCII)\n\n&amp;#x200B;\n\n[Image to ASCII](https://i.redd.it/7zvoo287azo81.gif)", "upvote_ratio": 0.89, "id": "t3_tk9jqo", "created_utc": 1647973579.0}
{"sub": "Python", "title": "Scrape Google Scholar Profiles from a certain University in Python", "selftext": "nan", "upvote_ratio": 0.87, "id": "t3_tk8itk", "created_utc": 1647970895.0}
{"sub": "Python", "title": "Text editors are stupid, change my mind", "selftext": "WHAT is the point of text editors for Python? Sure, they make you code faster but why not choose the standard IDLE? I mean, PyCharm never helped me. It always made a simple project too complicated to save. SO WHY CHOOSE TEXT EDITORS?", "upvote_ratio": 0.11, "id": "t3_tk7umy", "created_utc": 1647969075.0}
{"sub": "Python", "title": "Running a live 45-minutes session on the fundamentals of observability, OpenTelemetry, and distributed tracing in Python", "selftext": "Hi everyone, there's a live OpenTelemetry and observability fundamentals session - Wednesday, March 30 at 11 AM PST.\n\n**You will learn how to instrument your apps to capture traces with OpenTelemetry in Python.**\n\nThis session is at no cost and vendor-neutral.\n\nYou can expect in this session: 45 minutes of core concepts, how to deploy it yourself hands-on + Q&amp;A.\n\nIf you are interested in observability, OpenTelemetry, and tracing - this is the place to be!\n\nRegister here [https://www.aspecto.io/get-started-with-opentelemetry/](https://www.aspecto.io/get-started-with-opentelemetry/?utm_source=post&amp;utm_medium=reddit&amp;utm_campaign=r-python-opentelemetry-workshop-pyhon-march-30-2022)", "upvote_ratio": 0.86, "id": "t3_tk71j6", "created_utc": 1647966912.0}
{"sub": "Python", "title": "We developed a Python tool to generate a map of your embedded or edge distributed system", "selftext": "Hi guys, we recently developed a Python tool that allows you to generate and visualize a map of your embedded or edge distributed system. With the open-source project r/Luos, we are trying to code and make accessible CI/CD for these systems. \n\nIs this feature useful for your needs?\n\n&amp;#x200B;\n\n[Routing Table Luos](https://preview.redd.it/r66ib7hniyo81.png?width=570&amp;format=png&amp;auto=webp&amp;s=d0d8c32b52d368c8d92a2e62e3dca49c77aced25)", "upvote_ratio": 0.75, "id": "t3_tk69zo", "created_utc": 1647964904.0}
{"sub": "Python", "title": "I've released a cache backend that uses dynamodb which is compatible with Django's cache framework.", "selftext": "Hello Pythonistas!\n\nI've released a cache backend that uses dynamodb which is compatible with Django's cache framework!\n\nplease enjoy :)  \n\nhttps://github.com/xncbf/django-dynamodb-cache", "upvote_ratio": 0.82, "id": "t3_tk54q8", "created_utc": 1647961771.0}
{"sub": "Python", "title": "Unraveling the Mystery Behind Background Filters in Video Calling Apps", "selftext": "Ever wondered how video calling apps apply background filters during meetings and replace the background in the video with a background of your choice. This article explains this concept in detail and guides in step by step to implement one in Python.\n\n&amp;#x200B;\n\n[https://medium.com/geekculture/unraveling-the-mystery-behind-background-filters-in-video-calling-apps-6802507f88a0](https://medium.com/geekculture/unraveling-the-mystery-behind-background-filters-in-video-calling-apps-6802507f88a0)", "upvote_ratio": 0.5, "id": "t3_tk40cc", "created_utc": 1647958616.0}
{"sub": "Python", "title": "GitHub - gretelai/gretel-python-client: The Gretel Python Client allows you to interact with the Gretel REST API.", "selftext": "nan", "upvote_ratio": 0.5, "id": "t3_tk3uqx", "created_utc": 1647958170.0}
{"sub": "Python", "title": "Python Project Workflow", "selftext": "nan", "upvote_ratio": 0.63, "id": "t3_tk1o33", "created_utc": 1647951494.0}
{"sub": "Python", "title": "I've been a bit confused on which data class library to use. I wrote this article as part of my own investigation into protobuf, pydantic, etc..", "selftext": "nan", "upvote_ratio": 0.75, "id": "t3_tk0i4d", "created_utc": 1647947327.0}
{"sub": "Python", "title": "Launching Open Source python library, VevestaX: track your Machine Learning experiments and features in an excel using 5 lines of code", "selftext": " \n\nHi everyone! We have launched  an awesome open source python project that we have developed.\n\nWe have created a Python module named **VevestaX:** Easiest library in to track Machine learning experiments and features in an excel file with 5 lines of code !\n\nYou can check out the source code at our **GitHub page**: [https://github.com/Vevesta/VevestaX](https://github.com/Vevesta/VevestaX)\n\nPlease find a sample output file [https://docs.google.com/spreadsheets/d/1iOL3jiiQ834\\_vep5E4fPpxj7QDGVxOBJ/edit](https://docs.google.com/spreadsheets/d/1iOL3jiiQ834_vep5E4fPpxj7QDGVxOBJ/edit)\n\n**Please register** [https://forms.gle/samkZ1gDR53xDvPg7](https://forms.gle/samkZ1gDR53xDvPg7) **as beta testers**. Eager to hear from you.\n\nWe are also reachable on [vevestaX@vevesta.com](mailto:vevestaX@vevesta.com).\n\n**Please star our repository if you want to see it grow !**", "upvote_ratio": 0.57, "id": "t3_tjz9fx", "created_utc": 1647942213.0}
{"sub": "Python", "title": "Gufo Err: Python error handling framework", "selftext": "[Gufo Err](https://pypi.org/project/gufo-err/) is the flexible Python error handling framework. We'd used the same approach in the [NOC](https://getnoc.com/) for a long time. Now we reworked it as an independent component.\n\nBesides the extended tracebacks and *Sentry* integration it offers\na middleware-based approach for error handling, reporting, analysis, and mitigation. Fail-fast behavior allows detecting unrecoverable errors and quick termination of the application.\n\nError handling done right is a good foundation for all ranges of python applications, from simple automation scripts to high-load services.", "upvote_ratio": 0.6, "id": "t3_tjykut", "created_utc": 1647939172.0}
{"sub": "Python", "title": "Asynchronous Web Scraping With Python GRequests", "selftext": "nan", "upvote_ratio": 0.6, "id": "t3_tjy5ed", "created_utc": 1647937217.0}
{"sub": "Python", "title": "python syntax", "selftext": "does anyone feel like changing languages because of how anyone can understand python syntax?\n\ni feel like its cooler writing code that no one can ynderstand.. maybe its just me", "upvote_ratio": 0.22, "id": "t3_tjxth4", "created_utc": 1647935730.0}
{"sub": "Python", "title": "PEP 675 titled \"Arbitrary Literal String Type\" just got accepted.", "selftext": "nan", "upvote_ratio": 0.92, "id": "t3_tjx689", "created_utc": 1647932830.0}
{"sub": "Python", "title": "How to be a Successful Python developer: Tips to help your hiring prospects", "selftext": "nan", "upvote_ratio": 0.1, "id": "t3_tjwwnn", "created_utc": 1647931691.0}
{"sub": "Python", "title": "Just learned implementing decorators", "selftext": "Hi, I just learned how to make my own decorators and want to know some examples to use them. Some are using a `timer` decorator or a `log` decorator. Would like to get more examples to start thinking about the possibilities. Thanks in advance!\n\nEDIT:\nThank you everyone \ud83d\ude4f\ud83d\ude4f\ud83d\ude4f", "upvote_ratio": 0.75, "id": "t3_tjv4c3", "created_utc": 1647924427.0}
{"sub": "Python", "title": "The code for `import this` is amazing", "selftext": "Maybe this has been shared here before, but I recently took a peek at the code for the `this` module (the famous `import this` Easter egg that displays Tim Peters' \"Zen of Python\"). I think it might be the greatest \"do as I say not as I do\" code example that I've ever seen.\n\n    s = \"\"\"Gur Mra bs Clguba, ol Gvz Crgref\n    Ornhgvshy vf orggre guna htyl.\n    Rkcyvpvg vf orggre guna vzcyvpvg.\n    Fvzcyr vf orggre guna pbzcyrk.\n    Pbzcyrk vf orggre guna pbzcyvpngrq.\n    Syng vf orggre guna arfgrq.\n    Fcnefr vf orggre guna qrafr.\n    Ernqnovyvgl pbhagf.\n    Fcrpvny pnfrf nera'g fcrpvny rabhtu gb oernx gur ehyrf.\n    Nygubhtu cenpgvpnyvgl orngf chevgl.\n    Reebef fubhyq arire cnff fvyragyl.\n    Hayrff rkcyvpvgyl fvyraprq.\n    Va gur snpr bs nzovthvgl, ershfr gur grzcgngvba gb thrff.\n    Gurer fubhyq or bar-- naq cersrenoyl bayl bar --boivbhf jnl gb qb vg.\n    Nygubhtu gung jnl znl abg or boivbhf ng svefg hayrff lbh'er Qhgpu.\n    Abj vf orggre guna arire.\n    Nygubhtu arire vf bsgra orggre guna *evtug* abj.\n    Vs gur vzcyrzragngvba vf uneq gb rkcynva, vg'f n onq vqrn.\n    Vs gur vzcyrzragngvba vf rnfl gb rkcynva, vg znl or n tbbq vqrn.\n    Anzrfcnprf ner bar ubaxvat terng vqrn -- yrg'f qb zber bs gubfr!\"\"\"\n    \n    d = {}\n    for c in (65, 97):\n        for i in range(26):\n            d[chr(i+c)] = chr((i+13) % 26 + c)\n    \n    print(\"\".join([d.get(c, c) for c in s]))\n\nSeriously! You can see it [here](https://github.com/python/cpython/blob/main/Lib/this.py).\n\nBeautiful is better than ugly unless you're Tim Peters and can make something that is both beautiful AND ugly!", "upvote_ratio": 0.67, "id": "t3_tjskz4", "created_utc": 1647915671.0}
{"sub": "Python", "title": "would it be more human readable if we had a \"the\" and \"where\" keywords that turned this: [item for item in items &lt; 0] into: [the item in items where item &lt; 0]", "selftext": "The reason I ask is because I was wondering if list comprehension always starts with item for item in items?\n\nif so, it's so inefficient and clumsy to always write thing for thing in things if... rather than just the thing in things where...\n\nPlease be kind if this is the stupidest idea ever. It was just a thought.", "upvote_ratio": 0.29, "id": "t3_tjr17p", "created_utc": 1647910929.0}
{"sub": "Python", "title": "Useful Tools and Programs list for Python", "selftext": "Useful Tools and Programs list for Python including learning resources, development tools , and frameworks. [https://github.com/mikeroyal/Python-Guide](https://github.com/mikeroyal/Python-Guide)", "upvote_ratio": 1.0, "id": "t3_tjqlra", "created_utc": 1647909621.0}
{"sub": "Python", "title": "Tuesday Daily Thread: Advanced questions", "selftext": "Have some burning questions on advanced Python topics? Use this thread to ask more advanced questions related to Python.\n\n**If your question is a beginner question we hold a beginner Daily Thread tomorrow (Wednesday) where you can ask any question! We may remove questions here and ask you to resubmit tomorrow.**\n\nThis thread may be fairly low volume in replies, if you don't receive a response we recommend looking at r/LearnPython or joining the Python Discord server at [https://discord.gg/python](https://discord.gg/python) where you stand a better chance of receiving a response.", "upvote_ratio": 1.0, "id": "t3_tjpsnz", "created_utc": 1647907211.0}
{"sub": "Python", "title": "LPT: Pandas DataFrames have a \"to_clipboard\" method", "selftext": "For those that don't know, Pandas has very useful to_clipboard and read_clipboard methods that make it easy to drop a DataFrame into an Excel sheet or to move it across python sessions without having to read and write CSV files. This is really useful for me and I hope it will help you too!", "upvote_ratio": 0.97, "id": "t3_tjodin", "created_utc": 1647903155.0}
{"sub": "Python", "title": "My TUI now automatically downgrades RGB colors to the richest palette available in your terminal!", "selftext": "&amp;#x200B;\n\nhttps://preview.redd.it/56i77k4h6to81.png?width=1150&amp;format=png&amp;auto=webp&amp;s=8b1cbeb3f7d719c0c47d5c9c11dc08cfa806e138\n\n[PyTermGUI](https://github.com/bczsalba/pytermgui), my terminal user interface library has now gained the ability to determine the highest-grade color that can be displayed in the terminal emulator it is running in, so that it can convert anything you would normally not be able to see into a color supported. This calculation is done with human perception of colors and brightness factored in, so it looks surprisingly accurate, even with only 16 colors.\n\nIt also gained extensive [https://no-color.org](https://no-color.org) support, turning all the colors that would normally be displayed into an xterm-256 greyscale based on their luminance &amp; brightness, once again using human-based formulae.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/4m0otk4h6to81.png?width=1150&amp;format=png&amp;auto=webp&amp;s=a912b1f662ea7b3653805ffacaa4ab04974cd4b3\n\nIf this interests you, check out the [release notes](https://github.com/bczsalba/pytermgui/releases/tag/v4.2.0), see our [subreddit](https://reddit.com/r/pytermgui) or simply install the module with [PIP](https://pypi.org/projects/pytermgui)!", "upvote_ratio": 0.79, "id": "t3_tjnd4p", "created_utc": 1647900398.0}
{"sub": "Python", "title": "I'm making my own 3D engine ! (I'm trying my best to improve the projection system but it's still garbage for now)", "selftext": "Source code: https://github.com/uItimatech/Python-3D-engine\n\nDemo: https://youtu.be/W-oaCeMkQkw", "upvote_ratio": 0.99, "id": "t3_tjlv0x", "created_utc": 1647896295.0}
{"sub": "Python", "title": "What is the difference between PyInstaller and PyOxydizer?", "selftext": "nan", "upvote_ratio": 0.14, "id": "t3_tjla5f", "created_utc": 1647894759.0}
{"sub": "Python", "title": "We created a tool to enrich your datasets", "selftext": "Hi everyone!\n\nI\u2019m Nathan, working on [subsets.io](https://subsets.io/), a platform where you can upload datasets, and get matched with relevant external data which you can add with one click.\n\nI've created a prototype to access our platform via sdk:\n\n&amp;#x200B;\n\nhttps://i.redd.it/wp7qu2xy8so81.gif\n\nOur goal is to make it easier to pull in relevant external data. No more dealing with APIs and their rate limits, pagination, etc.\n\n Do you think this is be useful? Would love to hear your thoughts", "upvote_ratio": 0.71, "id": "t3_tjjdcf", "created_utc": 1647889658.0}
{"sub": "Python", "title": "Programmable HTTP CLI Tool", "selftext": "Instead of rewriting the same requests or sending a few data one by one while testing the api, I wrote a programmable http tool that can make many requests at once.\n\n[https://github.com/SinanKanidagli/httpy](https://github.com/SinanKanidagli/httpy)\n\nSome key features:\n\n* Expressive and intuitive syntax\n* Formatted and colorized terminal output\n* Programmable requests\n   * Multiple requests one line\n   * Value incremented each time\n   * Random number per request\n   * Read each value from the lines in the file\n   * Value per each request as a list of multiple values\n* Built-in JSON support\n* Arbitrary request data\n* Custom headers", "upvote_ratio": 1.0, "id": "t3_tjhae1", "created_utc": 1647884233.0}
{"sub": "Python", "title": "I wrote a short article about the advantages and disadvantages of python with an example of how to use Python for working with data.", "selftext": "[https://dev.to/patrikbraborec/why-you-should-use-python-for-your-next-project-1lin](https://dev.to/patrikbraborec/why-you-should-use-python-for-your-next-project-1lin)", "upvote_ratio": 0.78, "id": "t3_tjcr91", "created_utc": 1647871996.0}
{"sub": "Python", "title": "Python Interpreter API? but a little bit better (Coding 101)", "selftext": " Coding 101 is a  API that you can use in making an application similar to leetcode, codewars and other platforms that helps you practice technical interviews. Example calls are getting an easy question this API GET call will return the following\n\n  \n\n{\"Id\": 23\n\n\"Description\":\n\n\"Create a Function and Name it Add and it will take 2 arguments and will return the sum of these 2 arguments\"\n\nFunctionName\":\"Add(a,b)\"\n\n\"Level\":1 }\n\n&amp;#x200B;\n\n[API GET QUESTION CALL](https://preview.redd.it/xahvguwrgro81.png?width=940&amp;format=png&amp;auto=webp&amp;s=6c016a3e5d9e867b71187439125bca510f5e7987)\n\n&amp;#x200B;\n\n  \n\nAfter receiving your first Api request call you can now call the PostCode api call which is a POST request you will send a json object that will contain your answer to the question and the Question ID.\n\nSee example below\n\n&amp;#x200B;\n\n  \n\n{Code: 'def Add(a , b) : return len(a)', Id: 23}\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n[Tests return](https://preview.redd.it/s0laomztgro81.png?width=940&amp;format=png&amp;auto=webp&amp;s=f1bd860da8e91927516271ff82ef35349c6c8341)\n\n&amp;#x200B;\n\n  \n\nIt will return the output of your answer respectively above picture is an example if your code gets accepted on all 3 testcases. If you get it wrong see below picture for the incorrect output\n\n&amp;#x200B;\n\n[Incorrect Output](https://preview.redd.it/fnl46dbwgro81.png?width=940&amp;format=png&amp;auto=webp&amp;s=4e141aec346b6c1bbf4c291dbb04863c8ad1ccbf)\n\n&amp;#x200B;\n\n  \n\n  \n\nAnd That\u2019s It! Now you can create your own Website like leetcode , codewars if you have any creative ideas go ahead :D api link \n\n[https://rapidapi.com/pacejhayict--7\\_la6-gv3/api/coding101](https://rapidapi.com/pacejhayict--7_la6-gv3/api/coding101)", "upvote_ratio": 1.0, "id": "t3_tjffhc", "created_utc": 1647879245.0}
{"sub": "Python", "title": "Tetris in Pygame", "selftext": "Just my casual project\n[Demo](https://youtu.be/DjAszPLisic)|[Source Code(GitHub(Fixed))](https://github.com/Jatan-Bhatt-21/Tetris)\n\n:P", "upvote_ratio": 0.63, "id": "t3_tjeoz6", "created_utc": 1647877303.0}
{"sub": "Python", "title": "Video: Build a Speech Recognition System on a Raspberry Pi", "selftext": "nan", "upvote_ratio": 0.63, "id": "t3_tjenh3", "created_utc": 1647877197.0}
{"sub": "Python", "title": "I Use Python for Soccer Analytics", "selftext": "Hello Pythonauts!\n\nI use python for Data Analytics; specifically for soccer. I am a Software Developer but this is a passion project for me. On my website, [TodoFootballClub](https://todofootballclub.com/), I write about soccer and the burgeoning use of data in the game. All of my studies are done in python as it is my favorite language to use.\n\nI've done a case study on fatigue in soccer in this article: [https://todofootballclub.com/?p=620](https://todofootballclub.com/?p=620)\n\nI provide the full code in the article as well. I'd love to hear what you think!\n\nhttps://preview.redd.it/qn7xlb186ro81.png?width=984&amp;format=png&amp;auto=webp&amp;s=86156733e61b2baa55d1be441caf47a44c31b614\n\n&amp;#x200B;\n\nhttps://preview.redd.it/ls08atns5ro81.png?width=982&amp;format=png&amp;auto=webp&amp;s=cfeb2be3ad82bcff75b23b35fb8572030366f7c3", "upvote_ratio": 0.72, "id": "t3_tje08f", "created_utc": 1647875451.0}
{"sub": "Python", "title": "When do you use generators?", "selftext": "I know what they are, but I never write code and come to a point where I go \"ah yeah, perfect place to use a generator\"\nHow do you guys recognize that right case or what's the common pattern where you use it and why?", "upvote_ratio": 0.95, "id": "t3_tjd9l1", "created_utc": 1647873449.0}
{"sub": "Python", "title": "Async web server on ESP32 using Microdot | Micropython tutorial", "selftext": "Comprehensive written (and video!!) guide on how to use Microdot for your Micropython projects.\n\nIncludes how to deal with more advanced async coding, i.e. running web server while making pretty neopixel animations.\n\n[https://bhave.sh/micropython-microdot](https://bhave.sh/micropython-microdot)\n\nIf you have questions or comments, please let me know below!", "upvote_ratio": 0.6, "id": "t3_tjd7b8", "created_utc": 1647873272.0}
{"sub": "Python", "title": "I created a sorting algorithm, it is a schrobogosort. Basically it bogosorts and if it isn't sorted there is a 50% chance to shut down your PC every time it fails to sort", "selftext": "[https://replit.com/@KieranMcevoy/SchrodingerSort?v=1](https://replit.com/@KieranMcevoy/SchrodingerSort?v=1)\n\nWARNING: THIS WILL SHUTDOWN YOUR PC IF RAN OFF AN IDE NOT REPL", "upvote_ratio": 0.65, "id": "t3_tjcn6j", "created_utc": 1647871678.0}
{"sub": "Python", "title": "Aproches to planning a project", "selftext": "Tldr:\nAre there any tools/tips/tricks you guys use when starting out/ mapping a new project? Or rewriting an existing one?\n\nI mainly use python to analyze data from experiments i run in as part of my phd.\nSo for the past few years it's been very sufficient to simply create scripts per job/project i work on.\nBut I'm getting to a point where I'm trying to rework things so it's easier to understand - and for that i want to kind of map out what different steps i do overall.\n\nI guess the most basic way to represent this would be to say i have:\n input data -&gt; parsed and combined to a csv -&gt; different calculation -&gt; different plots\nIt's obviously more complex but that is where i get lost - i can't think of a way to represent all the things i do. And it got me wandering since i guess this is a widely spread issue - how to layout a big project.\nSo again - any resources/tools/tips/tricks would be appreciated", "upvote_ratio": 1.0, "id": "t3_tjcfcy", "created_utc": 1647871105.0}
{"sub": "Python", "title": "All-in-One Python book bundle by Packt", "selftext": "nan", "upvote_ratio": 0.89, "id": "t3_tjbbj6", "created_utc": 1647867698.0}
{"sub": "Python", "title": "I have my own database that I wanna check plagiarism against, how difficult is it making a plagiarism checker?", "selftext": "nan", "upvote_ratio": 0.4, "id": "t3_tjb86q", "created_utc": 1647867437.0}
{"sub": "Python", "title": "Daily dose of Python", "selftext": "I've been creating small posts about slightly more advanced Python topics lately. They are all available in [https://jerry-git.github.io/daily-dose-of-python/](https://jerry-git.github.io/daily-dose-of-python/).\n\nThe episodes so far:\n\n1. [Union vs TypeVar](https://jerry-git.github.io/daily-dose-of-python/doses/1/)\n2. [contextmanager from contextlib](https://jerry-git.github.io/daily-dose-of-python/doses/2/)\n3. [Modern Python web stack](https://jerry-git.github.io/daily-dose-of-python/doses/3/)\n4. [Exhaustiveness checking with mypy](https://jerry-git.github.io/daily-dose-of-python/doses/4/)\n5. [PEP 673 Self Type](https://jerry-git.github.io/daily-dose-of-python/doses/5/)\n6. [Final qualifier](https://jerry-git.github.io/daily-dose-of-python/doses/6/)\n7. [apischema](https://jerry-git.github.io/daily-dose-of-python/doses/7/)\n8. [Static duck typing via Protocol](https://jerry-git.github.io/daily-dose-of-python/doses/8/)\n9. [dirty-equals](https://jerry-git.github.io/daily-dose-of-python/doses/9/)\n\nAnd countless more to come \ud83d\ude09\n\nThe site itself is mkdocs based static website which is hosted as GitHub pages. Here's the [GitHub repo](https://github.com/jerry-git/daily-dose-of-python).", "upvote_ratio": 0.97, "id": "t3_tj9tz1", "created_utc": 1647862900.0}
{"sub": "Python", "title": "How to Deploy a TensorFlow Model as a RESTful API Service", "selftext": "nan", "upvote_ratio": 0.83, "id": "t3_tj8uzz", "created_utc": 1647859275.0}
{"sub": "Python", "title": "oneFace: Generating interfaces(CLI, Qt GUI, Dash web app) from a Python function.", "selftext": "oneFace can generate CLI, Qt GUI and Dash web app at the same time from a Python function. Just mark the function parameters with type and range, for example:\n\n    from oneface import one, Arg\n    \n    @one\n    def bmi(name: Arg(str),\n            height: Arg(float, [100, 250]) = 160,\n            weight: Arg(float, [0, 300]) = 50.0):\n        BMI = weight / (height / 100) ** 2\n        print(f\"Hi {name}. Your BMI is: {BMI}\")\n        return BMI\n    \n    \n    # run cli\n    bmi.cli()\n    # or run qt_gui\n    bmi.qt_gui()\n    # or run dash web app\n    bmi.dash_app()\n\nThese code will generate the following interfaces:\n\n[CLI](https://preview.redd.it/nrgkxp68cpo81.png?width=536&amp;format=png&amp;auto=webp&amp;s=58c872f37c513fc5d2dd120e77065795479455a8)\n\n&amp;#x200B;\n\n[Qt GUI](https://preview.redd.it/c5xrs7lccpo81.png?width=212&amp;format=png&amp;auto=webp&amp;s=5fce4feaa5bec5298e35df574a50b3b72904da64)\n\n&amp;#x200B;\n\n[Dash web app](https://preview.redd.it/nxylugpdcpo81.png?width=610&amp;format=png&amp;auto=webp&amp;s=8df8d72bc95c151caba500c532257c33dfcd0d4e)\n\n## Other features\n\n* Automatically check the type and range of input parameters and pretty print them.\n* Easy extension of parameter types and GUI widgets.\n\n## Application and Limitations\n\nProvides interfaces to programs in a very simple way. For example converting functions directly into a web apps for people who don't know how to use the command line. The limitation is that it is only suitable for creating simple interfaces.\n\n## Links\n\n* Repo: [https://github.com/Nanguage/oneFace](https://github.com/Nanguage/oneFace)\n* Documentation: [https://oneface.readthedocs.io/en/latest/](https://oneface.readthedocs.io/en/latest/)\n\n## Similar Projects\n\n* [Fire](https://github.com/google/python-fire): Generate CLI from Python objects.\n* [Gooey](https://github.com/chriskiehl/Gooey): Turn (almost) any Python 3 Console Program into a GUI application with one line.", "upvote_ratio": 0.9, "id": "t3_tj7d1w", "created_utc": 1647853109.0}
{"sub": "Python", "title": "Performance testing FastAPI ML APIs with Locust | Rubik's Code", "selftext": "nan", "upvote_ratio": 0.76, "id": "t3_tj6v1x", "created_utc": 1647850944.0}
{"sub": "Python", "title": "Why venv?", "selftext": "I'm new to Python and haven't worked with virtual environments before. I've seen a lot of folks utilising venv and was confused. I searched the web, but I couldn't comprehend much of it. I have a question that I'd want every one of you to answer.\n\n1. Why venv?", "upvote_ratio": 0.83, "id": "t3_tj32vd", "created_utc": 1647835530.0}
{"sub": "Python", "title": "Codecat", "selftext": " CodeCat is an open-source tool to help you find/track user input sinks and security bugs using static code analysis. These points follow regex rules.   [https://github.com/CoolerVoid/codecat](https://github.com/CoolerVoid/codecat)", "upvote_ratio": 0.76, "id": "t3_tj2g24", "created_utc": 1647833363.0}
{"sub": "Python", "title": "LinkedIn Researchers Open-Source \u2018FastTreeSHAP\u2019: A Python Package That Enables An Efficient Interpretation of Tree-Based Machine Learning Models", "selftext": "Researchers from LinkedIn open-source the FastTreeSHAP package which is a Python module based on the paper \u2018[Fast TreeSHAP: Accelerating SHAP Value Computation for Trees](https://arxiv.org/abs/2109.09847).\u2019 Implementing the widely-used TreeSHAP algorithm in the SHAP package allows for the efficient interpretation of tree-based machine learning models by estimating sample-level feature significance values. Its package includes two new algorithms: FastTreeSHAP v1 and FastTreeSHAP v2, both of which improve TreeSHAP\u2019s computational efficiency by taking a different approach.\u00a0\n\nThe empirical benchmarking tests show that FastTreeSHAP v1 is 1.5x faster than TreeSHAP while keeping memory costs the same, and FastTreeSHAP v2 is 2.5x faster while using slightly more memory. The FastTreeSHAP package fully supports parallel multi-core computing to speed up its computation.\n\n[**Continue Reading The Full Summary Article**](https://www.marktechpost.com/2022/03/20/linkedin-researchers-open-source-fasttreeshap-a-python-package-that-enables-an-efficient-interpretation-of-tree-based-machine-learning-models/)\n\nPaper: https://arxiv.org/pdf/2109.09847.pdf\n\nGithub: https://github.com/linkedin/fasttreeshap", "upvote_ratio": 0.76, "id": "t3_tj1koz", "created_utc": 1647830519.0}
{"sub": "Python", "title": "I created a self-hosted security camera system", "selftext": "I don't like the idea of having to stream my video camera feeds to the cloud, so I created a privacy-focused, self-hosted security camera system using python!\n\n[https://github.com/scottbarnesg/smart-sec-cam](https://github.com/scottbarnesg/smart-sec-cam)\n\n Some key features:\n\n* Multi-camera support w/ minimal configuration. Supports USB cameras and the Raspberry Pi camera module.\n* Motion detection that automatically saves videos and lets you view them in the web app.\n* Encrypted in transit, both from the cameras to the server and the server to your browser.\n* Self-hosted and FOSS", "upvote_ratio": 0.96, "id": "t3_tizhpw", "created_utc": 1647823860.0}
{"sub": "Python", "title": "Monday Daily Thread: Project ideas!", "selftext": "Comment any project ideas beginner or advanced in this thread for others to give a try! If you complete one make sure to reply to the comment with how you found it and attach some source code! If you're looking for project ideas, you might be interested in checking out Al Sweigart's, [\"The Big Book of Small Python Projects\"](https://inventwithpython.com/bigbookpython/) which provides a list of projects and the code to make them work.", "upvote_ratio": 0.71, "id": "t3_tiyhw1", "created_utc": 1647820810.0}
{"sub": "Python", "title": "Live Python Course from computer science professor on Cyber Insecurity Youtube Channel", "selftext": "If anyone wants to learn Python from a professor who's been teaching computer science for over a decade, jump on the Cyber Insecurity Youtube channel on Tuesday nights at 8PM ET. [https://www.youtube.com/channel/UCL4JGzitDkX5TOwzs9A02Kg](https://www.youtube.com/channel/UCL4JGzitDkX5TOwzs9A02Kg)", "upvote_ratio": 0.83, "id": "t3_tixven", "created_utc": 1647818906.0}
{"sub": "Python", "title": "The LAST 20 Python Packages you will ever need (Machine Learning, Data S...", "selftext": "nan", "upvote_ratio": 0.14, "id": "t3_tith9h", "created_utc": 1647806495.0}
{"sub": "Python", "title": "EZTV-AutoDownloader / Transmission Manager", "selftext": " Hello! (I don't know if this is for this sub.)  \nI made Auto Downloader from EZTV and Transmission-Remote Manager written in Python.  \n\n\nSome features:\n\n* Automatic download from EZTV\n* Check Episode and Season to see if ep is newer\n* Remove torrent from Transmission if Completed\n* Email notifications on add and complete.\n\nIf you're interested, go to the git repo for more information\n\nhttps://github.com/xhico/EZTV-AutoDownloader  \nThanks!", "upvote_ratio": 0.83, "id": "t3_tiqz2t", "created_utc": 1647799587.0}
{"sub": "Python", "title": "MD now avaible on Termux", "selftext": "A while ago I made a text based tool to download music in mp3 with tags and artworks. Now it's available  on Termux. Cheers!  \n\n\n[https://github.com/eliamazzon/MusicDownloader](https://github.com/eliamazzon/MusicDownloader)", "upvote_ratio": 0.83, "id": "t3_tiqdvp", "created_utc": 1647798001.0}
{"sub": "Python", "title": "Looking for an official documentation regarding security best practices and how to write secure code", "selftext": "I am looking for resources on how to write secure code with Python, I have been in python.org but there is only a Security reporting section. Before considering Google top results I would like to check if there is an official documentation", "upvote_ratio": 0.94, "id": "t3_tiq3ts", "created_utc": 1647797237.0}
{"sub": "Python", "title": "tkthread - Easy multithreading with tkinter", "selftext": "nan", "upvote_ratio": 0.7, "id": "t3_tip8va", "created_utc": 1647794947.0}
{"sub": "Python", "title": "I just started python 10 days ago an I am super happy. I just created my first program and would love some feedback", "selftext": "I am a complete programming beginner. I started a course on Udemy 10 days ago and this is my first program. I know it is probably not the most efficient way to do it which is why I would like your feedback.\n\nhere is the Github link: [https://github.com/RVP97/TicTacToe/blob/main/Project1](https://github.com/RVP97/TicTacToe/blob/main/Project1)\n\n`f_top= '     |       |    '`\n\n`f_mid= '  1  |   2   |  3 '`\n\n`f_low= '_____|_______|____'`\n\n`s_top= '     |       |    '`\n\n`s_mid= '  4  |   5   |  6 '`\n\n`s_low= '_____|_______|____'`\n\n`t_top= '     |       |    '`\n\n`t_mid= '  7  |   8   |  9 '`\n\n`t_low= '     |       |    '`\n\n&amp;#x200B;\n\n`f_top_1= '     |       |    '`\n\n`f_mid_1= '     |       |    '`\n\n`f_low_1= '_____|_______|____'`\n\n`s_top_1= '     |       |    '`\n\n`s_mid_1= '     |       |    '`\n\n`s_low_1= '_____|_______|____'`\n\n`t_top_1= '     |       |    '`\n\n`t_mid_1= '     |       |    '`\n\n`t_low_1= '     |       |    '`\n\n&amp;#x200B;\n\n`def print_options():`\n\n`print(f_top)`\n\n`print(f_mid)`\n\n`print(f_low)`\n\n`print(s_top)`\n\n`print(s_mid)`\n\n`print(s_low)`\n\n`print(t_top)`\n\n`print(t_mid)`\n\n`print(t_low)`\n\n\n\n`def print_game():`\n\n`print(f_top_1)`\n\n`print(f_mid_1)`\n\n`print(f_low_1)`\n\n`print(s_top_1)`\n\n`print(s_mid_1)`\n\n`print(s_low_1)`\n\n`print(t_top_1)`\n\n`print(t_mid_1)`\n\n`print(t_low_1)`\n\n\n\n`def print_every():`\n\n`print('OPTIONS')`    \n\n`print_options()`\n\n`print('')`\n\n`print('GAME')`\n\n`print_game()`\n\n\n\n`print('Hello players. We are going to play a round of Tic Tac Toe!')`\n\n&amp;#x200B;\n\n`def player_one_name():`\n\n`player_one=input(\"What is player one's name?: \")`\n\n\n\n`return player_one`\n\n&amp;#x200B;\n\n`def player_two_name():`\n\n`player_two=input(\"What is player two's name?: \")`\n\n\n\n`return player_two`\n\n&amp;#x200B;\n\n`player_one=player_one_name()`\n\n&amp;#x200B;\n\n`player_two=player_two_name()`\n\n&amp;#x200B;\n\n`print(f\"It is time for {player_one} to play against {player_two}\")`\n\n&amp;#x200B;\n\n`import random`\n\n&amp;#x200B;\n\n`list = [player_one,player_two]`\n\n`rand = random.choice(list)`\n\n`print(f\"It is {rand}'s turn\")`\n\n`if rand == player_one:`\n\n`other=player_two`\n\n`else:`\n\n`other=player_one`\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n`from IPython.display import clear_output`\n\n`print_options()`\n\n`option_list= ['1','2','3','4','5','6','7','8','9']`\n\n`condition=True`\n\n`while condition:`\n\n\n\n`while True:`\n\n`if (f_mid_1[2]=='O' and f_mid_1[9]=='O' and f_mid_1[16]=='O') or (s_mid_1[2]=='O' and s_mid_1[9]=='O' and s_mid_1[16]=='O') or (t_mid_1[2]=='O' and t_mid_1[9]=='O' and t_mid_1[16]=='O') or (f_mid_1[2]=='O' and s_mid_1[2]=='O' and t_mid_1[2]=='O') or(f_mid_1[9]=='O' and s_mid_1[9]=='O' and t_mid_1[9]=='O') or (f_mid_1[16]=='O' and s_mid_1[16]=='O' and t_mid_1[16]=='O') or (f_mid_1[2]=='O' and s_mid_1[9]=='O' and t_mid_1[16]=='O') or (f_mid_1[16]=='O' and s_mid_1[9]=='O' and t_mid_1[2]=='O'):`\n\n`print(f'{other} wins')`\n\n`condition=False`\n\n`break`\n\n`if (f_mid_1[2]=='X' and f_mid_1[9]=='X' and f_mid_1[16]=='X') or (s_mid_1[2]=='X' and s_mid_1[9]=='X' and s_mid_1[16]=='X') or (t_mid_1[2]=='X' and t_mid_1[9]=='X' and t_mid_1[16]=='X') or (f_mid_1[2]=='X' and s_mid_1[2]=='X' and t_mid_1[2]=='X') or(f_mid_1[9]=='X' and s_mid_1[9]=='X' and t_mid_1[9]=='X') or (f_mid_1[16]=='X' and s_mid_1[16]=='X' and t_mid_1[16]=='X') or (f_mid_1[2]=='X' and s_mid_1[9]=='X' and t_mid_1[16]=='X') or (f_mid_1[16]=='X' and s_mid_1[9]=='X' and t_mid_1[2]=='X'):`\n\n`print(f'{rand} wins')`\n\n`condition=False`\n\n`break`\n\n`if len(option_list)==0:`\n\n`print(\"It's a tie\")`\n\n`condition=False`\n\n`break`\n\n\n\n\n\n&amp;#x200B;\n\n`player_choice=input(f'{rand}, pick a number between 1 and 9: ')`\n\n`if player_choice in option_list:`\n\n`player_choice = int(player_choice)`\n\n`if player_choice == 1:`\n\n`clear_output()`\n\n`f_mid_1 = f_mid_1[:2] + 'X' + f_mid_1[2+1:]`\n\n`option_list.remove('1')`\n\n`print_every()`\n\n`break`\n\n`if player_choice == 2:`\n\n`clear_output()`\n\n`f_mid_1 = f_mid_1[:9] + 'X' + f_mid_1[9+1:]`\n\n`option_list.remove('2')`\n\n`print_every()`\n\n`break`\n\n`if player_choice == 3:`\n\n`clear_output()`\n\n`f_mid_1 = f_mid_1[:16] + 'X' + f_mid_1[16+1:]`\n\n`option_list.remove('3')`\n\n`print_every()`\n\n`break`\n\n&amp;#x200B;\n\n`if player_choice == 4:`\n\n`clear_output()`\n\n`s_mid_1 = s_mid_1[:2] + 'X' + s_mid_1[2+1:]`\n\n`option_list.remove('4')`\n\n`print_every()`\n\n`break`\n\n&amp;#x200B;\n\n`if player_choice == 5:`\n\n`clear_output()`\n\n`s_mid_1 = s_mid_1[:9] + 'X' + s_mid_1[9+1:]`\n\n`option_list.remove('5')`\n\n`print_every()`\n\n`break`\n\n&amp;#x200B;\n\n`if player_choice == 6:`\n\n`clear_output()`\n\n`s_mid_1 = s_mid_1[:16] + 'X' + s_mid_1[16+1:]`\n\n`option_list.remove('6')`\n\n`print_every()`\n\n`break`\n\n&amp;#x200B;\n\n`if player_choice == 7:`\n\n`clear_output()`\n\n`t_mid_1 = t_mid_1[:2] + 'X' + t_mid_1[2+1:]`\n\n`option_list.remove('7')`\n\n`print_every()`\n\n`break`\n\n&amp;#x200B;\n\n`if player_choice == 8:`\n\n`clear_output()`\n\n`t_mid_1 = t_mid_1[:9] + 'X' + t_mid_1[9+1:]`\n\n`option_list.remove('8')`\n\n`print_every()`\n\n`break`\n\n&amp;#x200B;\n\n`if player_choice == 9:`\n\n`clear_output()`\n\n`t_mid_1 = t_mid_1[:16] + 'X' + t_mid_1[16+1:]`\n\n`option_list.remove('9')`\n\n`print_every()`\n\n`break`\n\n\n\n`else:`\n\n`clear_output()`\n\n`print('Pick a valid number or a position that has not been picked before')`\n\n`print_every()`\n\n`continue`\n\n\n\n`while True:`\n\n`if (f_mid_1[2]=='O' and f_mid_1[9]=='O' and f_mid_1[16]=='O') or (s_mid_1[2]=='O' and s_mid_1[9]=='O' and s_mid_1[16]=='O') or (t_mid_1[2]=='O' and t_mid_1[9]=='O' and t_mid_1[16]=='O') or (f_mid_1[2]=='O' and s_mid_1[2]=='O' and t_mid_1[2]=='O') or(f_mid_1[9]=='O' and s_mid_1[9]=='O' and t_mid_1[9]=='O') or (f_mid_1[16]=='O' and s_mid_1[16]=='O' and t_mid_1[16]=='O') or (f_mid_1[2]=='O' and s_mid_1[9]=='O' and t_mid_1[16]=='O') or (f_mid_1[16]=='O' and s_mid_1[9]=='O' and t_mid_1[2]=='O'):`\n\n`condition=False`\n\n`break`\n\n`if (f_mid_1[2]=='X' and f_mid_1[9]=='X' and f_mid_1[16]=='X') or (s_mid_1[2]=='X' and s_mid_1[9]=='X' and s_mid_1[16]=='X') or (t_mid_1[2]=='X' and t_mid_1[9]=='X' and t_mid_1[16]=='X') or (f_mid_1[2]=='X' and s_mid_1[2]=='X' and t_mid_1[2]=='X') or(f_mid_1[9]=='X' and s_mid_1[9]=='X' and t_mid_1[9]=='X') or (f_mid_1[16]=='X' and s_mid_1[16]=='X' and t_mid_1[16]=='X') or (f_mid_1[2]=='X' and s_mid_1[9]=='X' and t_mid_1[16]=='X') or (f_mid_1[16]=='X' and s_mid_1[9]=='X' and t_mid_1[2]=='X'):`\n\n`print(f'{rand} wins')`\n\n`condition=False`\n\n`break`\n\n`if len(option_list)==0:`\n\n`print(\"It's a tie\")`\n\n`condition=False`\n\n`break`\n\n\n\n&amp;#x200B;\n\n`player_choice=input(f'{other}, pick a number between 1 and 9: ')`\n\n`if player_choice in option_list:`\n\n`player_choice = int(player_choice)`\n\n`if player_choice == 1:`\n\n`clear_output()`\n\n`f_mid_1 = f_mid_1[:2] + 'O' + f_mid_1[2+1:]`\n\n`option_list.remove('1')`\n\n`print_every()`\n\n`break`\n\n`if player_choice == 2:`\n\n`clear_output()`\n\n`f_mid_1 = f_mid_1[:9] + 'O' + f_mid_1[9+1:]`\n\n`option_list.remove('2')`\n\n`print_every()`\n\n`break`\n\n`if player_choice == 3:`\n\n`clear_output()`\n\n`f_mid_1 = f_mid_1[:16] + 'O' + f_mid_1[16+1:]`\n\n`option_list.remove('3')`\n\n`print_every()`\n\n`break`\n\n&amp;#x200B;\n\n`if player_choice == 4:`\n\n`clear_output()`\n\n`s_mid_1 = s_mid_1[:2] + 'O' + s_mid_1[2+1:]`\n\n`option_list.remove('4')`\n\n`print_every()`\n\n`break`\n\n&amp;#x200B;\n\n`if player_choice == 5:`\n\n`clear_output()`\n\n`s_mid_1 = s_mid_1[:9] + 'O' + s_mid_1[9+1:]`\n\n`option_list.remove('5')`\n\n`print_every()`\n\n`break`\n\n&amp;#x200B;\n\n`if player_choice == 6:`\n\n`clear_output()`\n\n`s_mid_1 = s_mid_1[:16] + 'O' + s_mid_1[16+1:]`\n\n`option_list.remove('6')`\n\n`print_every()`\n\n`break`\n\n&amp;#x200B;\n\n`if player_choice == 7:`\n\n`clear_output()`\n\n`t_mid_1 = t_mid_1[:2] + 'O' + t_mid_1[2+1:]`\n\n`option_list.remove('7')`\n\n`print_every()`\n\n`break`\n\n&amp;#x200B;\n\n`if player_choice == 8:`\n\n`clear_output()`\n\n`t_mid_1 = t_mid_1[:9] + 'O' + t_mid_1[9+1:]`\n\n`option_list.remove('8')`\n\n`print_every()`\n\n`break`\n\n&amp;#x200B;\n\n`if player_choice == 9:`\n\n`clear_output()`\n\n`t_mid_1 = t_mid_1[:16] + 'O' + t_mid_1[16+1:]`\n\n`option_list.remove('9')`\n\n`print_every()`\n\n`break`\n\n\n\n`else:`\n\n`clear_output()`\n\n`print('Pick a valid number or a position that has not been picked before')`\n\n`print_every()`\n\n`continue`", "upvote_ratio": 0.43, "id": "t3_tiox8d", "created_utc": 1647794055.0}
{"sub": "Python", "title": "I made a video tutorial on how to work with image data with python. Hope its a helpful introduction for anyone interested in learning about image processing.", "selftext": "nan", "upvote_ratio": 0.89, "id": "t3_tinykr", "created_utc": 1647791398.0}
{"sub": "Python", "title": "Sending Emails With Python", "selftext": "nan", "upvote_ratio": 0.5, "id": "t3_tin22d", "created_utc": 1647788786.0}
{"sub": "Python", "title": "pip install medium-api", "selftext": "Hi Guys, I recently published a python wrapper around [Medium API](https://rapidapi.com/nishujain199719-vgIfuFHZxVZ/api/medium2) which helps you to extract/fetch data from [medium.com](https://medium.com). \n\nTo install it, just run:\n\n```\npip install medium-api\n```\n\n**What can you extract with it?**\n\n- Medium User information and user-written articles\n- Medium Articles information and their textual content\n- Medium Publications information\n- Medium\u2019s Top Writers\n- Medium\u2019s Topfeeds (Trending, Latest, All time best, best of year/month/week)\n- Medium\u2019s Latest Posts (distributed articles)\n\n**How to use it?**\n\n```python\nfrom medium_api import Medium\n\nmedium = Medium('YOUR_RAPIDAPI_KEY')\nuser = medium.user(username='nishu-jain')\n\nuser.fetch_articles()\n\nfor article in user.articles:\n    print(article.title)\n```\n\n**A few references:**\n\n- Github repository: https://github.com/weeping-angel/medium-api\n- Website: https://mediumapi.com\n- Swagger documentation: https://docs.mediumapi.com\n- ReadTheDocs: https://medium-api.readthedocs.io/en/latest/\n- PyPI: https://pypi.org/project/medium-api/\n\nHoping someone will find it useful :)", "upvote_ratio": 0.82, "id": "t3_timph1", "created_utc": 1647787819.0}
{"sub": "Python", "title": "Python Collection Classes - Queues, NamedTuples, DefaultDict, the Professor and Mary Ann", "selftext": "[https://codesolid.com/useful-collection-classes-in-python-you-may-not-know/](https://codesolid.com/useful-collection-classes-in-python-you-may-not-know/)\n\nEnjoy!", "upvote_ratio": 1.0, "id": "t3_tilukh", "created_utc": 1647785280.0}
{"sub": "Python", "title": "Creating Real-Time Charts with FastAPI", "selftext": "https://github.com/roniemartinez/real-time-charts-with-fastapi\n\nI've made sample application for anyone interested in writing real-time charts with FastAPI + Server-Sent Events. You can check my Github repo above.", "upvote_ratio": 0.83, "id": "t3_tilg4d", "created_utc": 1647784101.0}
{"sub": "Python", "title": "Create A REST API In Python Flask", "selftext": "nan", "upvote_ratio": 0.4, "id": "t3_tikn0t", "created_utc": 1647781534.0}
{"sub": "Python", "title": "Step by Step Procedure to Deploy A Flask App On Heroku", "selftext": "nan", "upvote_ratio": 0.17, "id": "t3_tikmj3", "created_utc": 1647781487.0}
{"sub": "Python", "title": "Python Flask, ElasticSearch and docker environment", "selftext": "Hi, the second article devoted to the theme: \u201cHow to work with ElasticSearch using Python and Flask framework\u201d is already ready for reading. It is about preparing microservice environment using docker and docker compose. All details are here: \"[Python Flask, ElasticSearch and Docker environment](https://sergiiblog.com/python-flask-elasticsearch-and-docker-environment/)\". Have a pleasant reading.", "upvote_ratio": 0.76, "id": "t3_tiid1f", "created_utc": 1647772934.0}
{"sub": "Python", "title": "Gufo Loader", "selftext": "[Gufo Loader](https://github.com/gufolabs/gufo_loader) is the flexible Python plugin infrastructure framework. We'd used this approach in the [NOC](https://getnoc.com/) \\- an open-source network management system and finally decided to release this component as the independent package. Very useful tool to build extendable and reliable python applications and services.", "upvote_ratio": 0.75, "id": "t3_tihlqu", "created_utc": 1647769687.0}
{"sub": "Python", "title": "How to make the most of Pydantic", "selftext": "nan", "upvote_ratio": 0.84, "id": "t3_tih4e7", "created_utc": 1647767586.0}
{"sub": "Python", "title": "Sudoku Solver Using Python | AI projects", "selftext": "Github: [https://github.com/bhimrazy/Artificial-Intelligence-Projects](https://github.com/bhimrazy/Artificial-Intelligence-Projects)\n\nhttps://preview.redd.it/xswtp52b3ho81.png?width=1097&amp;format=png&amp;auto=webp&amp;s=8ce31043a6238eb74bddc97ff6dc6b4072f82f38", "upvote_ratio": 0.86, "id": "t3_tidx7v", "created_utc": 1647753441.0}
{"sub": "Python", "title": "I Wanted to Make a Game from Scratch (visuals and all) Using Python so I did.", "selftext": "&amp;#x200B;\n\n[https:\\/\\/github.com\\/M0pps\\/Dinosaur-Game.git](https://reddit.com/link/tid126/video/pvim4rectgo81/player)", "upvote_ratio": 0.97, "id": "t3_tid126", "created_utc": 1647750072.0}
{"sub": "Python", "title": "Creating a DCGAN with PyTorch", "selftext": "I wrote a short tutorial on creating a deep convolutional GAN to generate new wild animals images from the wildlife animal faces dataset using PyTorch:\n\n[https://taying-cheng.medium.com/create-new-animals-using-dcgan-with-pytorch-2ce47810ebd4](https://taying-cheng.medium.com/create-new-animals-using-dcgan-with-pytorch-2ce47810ebd4)", "upvote_ratio": 0.67, "id": "t3_ti9u5w", "created_utc": 1647738961.0}
{"sub": "Python", "title": "Modularizing a Chalice Application", "selftext": "If you're looking to develop Chalice lambda functions and needing to realistically modularize your application and files, here's a pretty good article (I'm not the author) on how to go about that. Chalice is used for AWS lambda function development and deployment and has routing decorators similar to Flask and other APIs framework.\n\n[https://medium.com/tensoriot/modularizing-a-chalice-application-for-teams-f716f496b94b](https://medium.com/tensoriot/modularizing-a-chalice-application-for-teams-f716f496b94b)", "upvote_ratio": 0.5, "id": "t3_ti9ryn", "created_utc": 1647738759.0}
{"sub": "Python", "title": "Space Science with Python - AI 1-8: A Dense Neural Network", "selftext": "Hey Python coders,\n\ntoday I would like to show you a new tutorial  video within my Space Science with Python series. I am still working on a  Machine Learning project, and this time we will use TensorFlow / Keras  to create a multi-class classifier for our asteroids reflectance  spectra.\n\nSince our spectra data are already perfectly cleaned and  processed, even a simple Dense layer architecture provides fair  results; you'll see! And for starters, Dense-layer-based networks are  easy to \"digest\".\n\nThe next video will consider convolution  networks and afterwards you will see some nice Autoencoder architecture  and unsupervised classification methods. Hopefully you'll like it!\n\nAnyway.  A word of \"warning\". When I create my tutorials I prepare my code, test  it, format it, test it again etc. Also this time. But: I had some  functions in the Google Colab cache. My \"new\" functions were not called  correctly and I figured it out in the last minutes! Anyway, I left it in  the video for others to learn from my mistake I made there. Please  note: reset your runtime when you start something new and be aware of  issues when using Notebooks in a productive environment.\n\nGitHub: [https://github.com/ThomasAlbin/Astroniz-YT-Tutorials/blob/main/%5BML1%5D-Asteroid-Spectra/8\\_dl\\_dense\\_multiclass.ipynb](https://github.com/ThomasAlbin/Astroniz-YT-Tutorials/blob/main/%5BML1%5D-Asteroid-Spectra/8_dl_dense_multiclass.ipynb)\n\nYouTube: [https://youtu.be/O0rQswBZJ7o](https://youtu.be/O0rQswBZJ7o)\n\nCheers,  \nThomas", "upvote_ratio": 0.81, "id": "t3_ti90ps", "created_utc": 1647736289.0}
{"sub": "Python", "title": "Sunday Daily Thread: What's everyone working on this week?", "selftext": "Tell /r/python what you're working on this week! You can be bragging, grousing, sharing your passion, or explaining your pain. Talk about your current project or your pet project; whatever you want to share.", "upvote_ratio": 1.0, "id": "t3_ti8esn", "created_utc": 1647734409.0}
{"sub": "Python", "title": "I teach python for middle and high schoolers \u2013 I made this little trophy for a competition of who could make the best image using Turtle/Python", "selftext": "I will also make some keychains with the best images 3d-printed on them \u2013 I hope the students like them!\n\n&amp;#x200B;\n\nhttps://i.redd.it/7cyldg3odfo81.gif", "upvote_ratio": 0.97, "id": "t3_ti7uyh", "created_utc": 1647732704.0}
{"sub": "Python", "title": "Automate Key board and mouse", "selftext": "see a lot of people probably using this for gaming but can be used for your job. I use this type of stuff to auto mate stuff I have to type over and over then click a bunch of bottons. Hope it helps\n\n&amp;#x200B;\n\n[https://www.youtube.com/watch?v=shi5Ba\\_y4HA&amp;t=264s](https://www.youtube.com/watch?v=shi5Ba_y4HA&amp;t=264s)\n\n&amp;#x200B;\n\nHas anyone used Python to automate other stuff. I want to expand on automating on Python. Or know good modules for stuff like that?", "upvote_ratio": 0.92, "id": "t3_ti5hm4", "created_utc": 1647725723.0}
{"sub": "Python", "title": "New Python package for stock market data: YFrake", "selftext": " I've released a new open-source Python package named [YFrake (Github Repository)](https://github.com/aspenforest/yfrake), which returns stock market data from Yahoo Finance and can also serve it to other applications. It has built-in swagger documentation and can run in async and threaded modes. More features are planned for future releases. Package documentation is available at [yfrake.readthedocs.io](http://yfrake.readthedocs.io/) .", "upvote_ratio": 0.71, "id": "t3_ti5geq", "created_utc": 1647725628.0}
{"sub": "Python", "title": "I broke Math. I generated a quantum random number in Python. Split the decimal part of it into groups of 5 digits. Found every single group in the decimal expansion of PI (just upto a million digits). Somebody tell me I'm wrong or give me a Nobel prize. I won't sleep tonight.", "selftext": "    import quantumrandom as qr\n    \n    with open(\"piDigits.txt\", 'r') as f:\n        pidecimals = f.read()\n    f.close()\n    \n    maxrange = int(input(\"Enter Maxmimum of Range \"))\n    count = 0\n    \n    for i in range (0,maxrange):\n        myx = qr.randint()\n        print(f\"Quantum Number {myx}\")\n        decimalstring = str(myx)\n        substring1 = decimalstring[2:7]\n        substring2 = decimalstring[7:12]\n        substring3 = decimalstring[12:17]\n        if substring1 in pidecimals:\n            count += 1\n            print(f\"Quantum digit sequence {substring1} found in Pi\")\n        if substring2 in pidecimals:\n            count += 1\n            print(f\"Quantum digit sequence {substring2} found in Pi\")\n        if substring3 in pidecimals:\n            count += 1\n            print(f\"Quantum digit sequence {substring3} found in Pi\")\n    print(f\"Quantum digit sequences found in Pi {count} times out of {3*maxrange}\")\n\nSample Output: (Ran the code many times to look for an exception. None found. Yet.)\n\n`Enter Maxmimum of Range 20`\n\n`Quantum Number 4.434424353398947`\n\n`Quantum digit sequence 43442 found in Pi`\n\n`Quantum digit sequence 43533 found in Pi`\n\n`Quantum digit sequence 98947 found in Pi`\n\n`Quantum Number 9.409781033035783`\n\n`Quantum digit sequence 40978 found in Pi`\n\n`Quantum digit sequence 10330 found in Pi`\n\n`Quantum digit sequence 35783 found in Pi`\n\n`Quantum Number 9.769588769359885`\n\n`Quantum digit sequence 76958 found in Pi`\n\n`Quantum digit sequence 87693 found in Pi`\n\n`Quantum digit sequence 59885 found in Pi`\n\n`Quantum Number 7.9896238651102465`\n\n`Quantum digit sequence 98962 found in Pi`\n\n`Quantum digit sequence 38651 found in Pi`\n\n`Quantum digit sequence 10246 found in Pi`\n\n`Quantum Number 1.8486305027847716`\n\n`Quantum digit sequence 84863 found in Pi`\n\n`Quantum digit sequence 05027 found in Pi`\n\n`Quantum digit sequence 84771 found in Pi`\n\n`Quantum Number 5.2133974212253`\n\n`Quantum digit sequence 21339 found in Pi`\n\n`Quantum digit sequence 74212 found in Pi`\n\n`Quantum digit sequence 253 found in Pi`\n\n`Quantum Number 3.3063248645761805`\n\n`Quantum digit sequence 30632 found in Pi`\n\n`Quantum digit sequence 48645 found in Pi`\n\n`Quantum digit sequence 76180 found in Pi`\n\n`Quantum Number 6.982528419928283`\n\n`Quantum digit sequence 98252 found in Pi`\n\n`Quantum digit sequence 84199 found in Pi`\n\n`Quantum digit sequence 28283 found in Pi`\n\n`Quantum Number 4.838788433661402`\n\n`Quantum digit sequence 83878 found in Pi`\n\n`Quantum digit sequence 84336 found in Pi`\n\n`Quantum digit sequence 61402 found in Pi`\n\n`Quantum Number 0.5404745555809872`\n\n`Quantum digit sequence 54047 found in Pi`\n\n`Quantum digit sequence 45555 found in Pi`\n\n`Quantum digit sequence 80987 found in Pi`\n\n`Quantum Number 5.2051575494010835`\n\n`Quantum digit sequence 20515 found in Pi`\n\n`Quantum digit sequence 75494 found in Pi`\n\n`Quantum digit sequence 01083 found in Pi`\n\n`Quantum Number 2.928511482413977`\n\n`Quantum digit sequence 92851 found in Pi`\n\n`Quantum digit sequence 14824 found in Pi`\n\n`Quantum digit sequence 13977 found in Pi`\n\n`Quantum Number 8.577248798352025`\n\n`Quantum digit sequence 57724 found in Pi`\n\n`Quantum digit sequence 87983 found in Pi`\n\n`Quantum digit sequence 52025 found in Pi`\n\n`Quantum Number 0.699015793087663`\n\n`Quantum digit sequence 69901 found in Pi`\n\n`Quantum digit sequence 57930 found in Pi`\n\n`Quantum digit sequence 87663 found in Pi`\n\n`Quantum Number 6.524299992370489`\n\n`Quantum digit sequence 52429 found in Pi`\n\n`Quantum digit sequence 99923 found in Pi`\n\n`Quantum digit sequence 70489 found in Pi`\n\n`Quantum Number 5.860379949645227`\n\n`Quantum digit sequence 86037 found in Pi`\n\n`Quantum digit sequence 99496 found in Pi`\n\n`Quantum digit sequence 45227 found in Pi`\n\n`Quantum Number 6.33020523384451`\n\n`Quantum digit sequence 33020 found in Pi`\n\n`Quantum digit sequence 52338 found in Pi`\n\n`Quantum digit sequence 4451 found in Pi`\n\n`Quantum Number 4.562294956893263`\n\n`Quantum digit sequence 56229 found in Pi`\n\n`Quantum digit sequence 49568 found in Pi`\n\n`Quantum digit sequence 93263 found in Pi`\n\n`Quantum Number 2.0715648126955064`\n\n`Quantum digit sequence 07156 found in Pi`\n\n`Quantum digit sequence 48126 found in Pi`\n\n`Quantum digit sequence 95506 found in Pi`\n\n`Quantum Number 8.903639276722362`\n\n`Quantum digit sequence 90363 found in Pi`\n\n`Quantum digit sequence 92767 found in Pi`\n\n`Quantum digit sequence 22362 found in Pi`\n\n`Quantum digit sequences found in Pi 60 times out of 60`", "upvote_ratio": 0.2, "id": "t3_ti584e", "created_utc": 1647724969.0}
{"sub": "Python", "title": "PyMacApp: Build, Package, and Code-Sign Python Projects on MacOS in just 10-lines of Python Code!", "selftext": "Hi All:\n\nI recently posted about a project I was working on called PyMacApp. I just released a ton of updates that brings full support for building, packaging, and code-signing in as little as just 10 lines of code (2 import statements, 4 for the app, and 4 for the package; it supports function chaining, so you could even accomplish this in less than 10 lines)!\n\nGet started with ```pip3 install pymacapp```\nGH: https://github.com/The-Nicholas-R-Barrow-Company-LLC/PyMacApp\n\nA more-detailed starter is available below. \n\n```\n# build.py\nfrom pymacapp import App, Package\nfrom pymacapp.helpers import get_first_application_hash, get_first_installer_hash\n\n# Apple Account Information\n# You can get rid of the input(...) functions and instead enter the strings directly so you do not have to enter them each time.\nAPPLE_DEVELOPER_ACCOUNT_EMAIL = input(\"Apple Developer ID Email (str): \")\nAPPLE_DEVELOPER_ACCOUNT_APP_SPECIFIC_PASSWORD = input(\"Apple Developer ID App-Specific Password (str): \")\n\napp = App(\"My New App\", \"com.identifier\")\napp.setup(\"./app/main.py\", overwrite=True)\napp.build()\napp.sign(get_first_application_hash())\n\npackage = Package(app, \"0.0.1\", \"com.identifier.pkg\")\npackage.build(get_first_installer_hash())\npackage.sign(get_first_installer_hash())\npackage.notorize(APPLE_DEVELOPER_ACCOUNT_EMAIL, APPLE_DEVELOPER_ACCOUNT_APP_SPECIFIC_PASSWORD).wait()\n```", "upvote_ratio": 0.66, "id": "t3_ti4ztb", "created_utc": 1647724308.0}
{"sub": "Python", "title": "Small Line Counter Script", "selftext": "I made a command line tool that lets you specify a file/directory path on the command line and will count the lines of code in a file or all files within a directory or directories recursively. I plan to add more features. I am a systems dev by trade and have worked with python in the past but not very much and that was a couple years ago, so I didn't know if I should flag this as beginner or intermediate but let me know if I should change it. Please feel free to contribute. If you do read the code (its short) I would love feedback.\n\n[https://www.github.com/carterdugan/LineCounter](https://www.github.com/carterdugan/LineCounter)", "upvote_ratio": 0.81, "id": "t3_ti4zcz", "created_utc": 1647724271.0}
{"sub": "Python", "title": "Is pygame still worth it in 2022??", "selftext": "nan", "upvote_ratio": 0.85, "id": "t3_thzhqp", "created_utc": 1647709018.0}
{"sub": "Python", "title": "What do you think is the most valuable Python package?", "selftext": "In the spirit of March Madness, we created a tournament to choose the MVP Python package. This is just meant to be fun, but it might jumpstart an interesting discussion about your choices and the kind of work you're doing. \n\n[https://deephaven.io/community/experiments/python-bracket/](https://deephaven.io/community/experiments/python-bracket/)", "upvote_ratio": 0.15, "id": "t3_thw2h9", "created_utc": 1647699262.0}
{"sub": "Python", "title": "Build a Hash Table in Python With TDD \u2013 Real Python", "selftext": "nan", "upvote_ratio": 0.75, "id": "t3_thvrq5", "created_utc": 1647698366.0}
{"sub": "Python", "title": "How to Shortlist the Best Python Development Company ?", "selftext": "nan", "upvote_ratio": 0.33, "id": "t3_thvj46", "created_utc": 1647697596.0}
{"sub": "Python", "title": "DeepForSpeed: A self driving car in Need For Speed Most Wanted built with python + pytorch", "selftext": "[video here](https://youtu.be/t0iqfM36mRc)\n\n[code here](https://github.com/edilgin/DeepForSpeed)\n\nSo i built a self driving car with python in need for speed most wanted(2005). I was really impressed when i saw nvidia build their own self driving car with just a single algorithm(cnn) so i decided to try it myself. Basically i record training data while i'm playing the game (i played around 2 hours i think) my key presses associated with every frame are recorded. Later i process this training data and train the algorithm (which is almost the same as the nvidia's). Latest step is just running the algo. Important hings i've used are: numpy, opencv, matplotlib and pytorch. \n\nPlease take a look at the code i tried to document everything and i would appreciate any pull requests and advice in general :)", "upvote_ratio": 0.97, "id": "t3_thsp8c", "created_utc": 1647686990.0}
{"sub": "Python", "title": "Python for beginners from Harvard CS50x. Starting April 1st. Free course, awesome teacher, explains how computer thinks in terms of programming. 11 weeks 3-9 hours per week! Sign up now, highly recommended.", "selftext": "nan", "upvote_ratio": 0.94, "id": "t3_ths5wg", "created_utc": 1647684560.0}
{"sub": "Python", "title": "Red Mail: All you need from an email sender", "selftext": "Hi all,\n\nI have made a couple of posts about Red Mail in the past but I recently received some comments feeling this being underrated compared to how useful it really is. So, I hope you don't mind an update,  I also just released a new version.\n\nSo what is Red Mail? It's an email library that aims for simplicity without compromising features. You will find a bunch of email senders from Pypi but there is nothing quite like this.\n\nSo what can it do?\n\n* [Supports sending HTML, text, attachments](https://red-mail.readthedocs.io/en/latest/tutorials/sending.html)\n* [Send to regular receivers, cc (carbon copy) or bcc (blind carbon copy)](https://red-mail.readthedocs.io/en/latest/tutorials/sending.html)\n* [Attachments from various types: paths, bytes, Pandas dataframes, etc.](https://red-mail.readthedocs.io/en/latest/tutorials/attachments.html)\n* [Emails with embedded images in HTML](https://red-mail.readthedocs.io/en/latest/tutorials/body_content.html#embedding-content)\n* [Embedded images from various types: Matplotlib plots or PIL images](https://red-mail.readthedocs.io/en/latest/tutorials/body_content.html#embedding-content)\n* [Emails with embedded (prettified) tables from Pandas dataframes](https://red-mail.readthedocs.io/en/latest/tutorials/body_content.html#embedded-tables)\n* [Templated emails using Jinja](https://red-mail.readthedocs.io/en/latest/tutorials/jinja_support.html)\n* [Gmail and Outlook pre-configured](https://red-mail.readthedocs.io/en/latest/tutorials/config.html)\n* [Logging handlers!](https://red-mail.readthedocs.io/en/latest/extensions/logging.html)\n* [Flask integration](https://flask-redmail.readthedocs.io/en/stable/index.html)\n\nA minimal example for Gmail users:\n\n    from redmail import gmail\n    \n    gmail.username = \"example@gmail.com\"\n    gmail.password = \"&lt;MY PASSWORD&gt;\"\n    \n    gmail.send(\n        sender=\"example@gmail.com\",\n        receivers=[\"you@example.com\"],\n        subject=\"An example email\",\n        html=\"\"\"\n            &lt;h1&gt;Hi,&lt;/h1&gt;\n            &lt;p&gt;nice to meet you.&lt;/p&gt;\n        \"\"\",\n    )\n\nMore advanced features:\n\n    from redmail import EmailSender\n    \n    email = EmailSender(host=\"smtp.myhost.com\", port=0)\n    \n    email.send(\n        sender=\"me@example.com\",\n        receivers=[\"you@example.com\"],\n        subject=\"An example email\",\n        html=\"\"\"\n            &lt;h1&gt;Hi {{ friend }},&lt;/h1&gt; \n            &lt;p&gt;look at this image:&lt;/p&gt;\n            {{ nice_image }}\n        \"\"\",\n        body_params={\"friend\": \"Jack\"},\n        body_images={\"nice_image\": 'path/to/image.png'},\n        attachments={\"file.csv\": pd.DataFrame({\"col\": [1, 2, 3]})}\n    )\n\nSo it's pretty clean and does everything you wished from an email sender. There are alternatives for email sending but nothing quite like this. It is also well tested and documented.\n\nResources:\n\n* Source code: [https://github.com/Miksus/red-mail](https://github.com/Miksus/red-mail)\n* Documentation: [https://red-mail.readthedocs.io/en/latest/](https://red-mail.readthedocs.io/en/latest/)\n* Releases: [https://pypi.org/project/redmail/](https://pypi.org/project/redmail/)\n\nIf you need to integrate it to a Flask application:\n\n* Source code: [https://github.com/Miksus/flask-redmail](https://github.com/Miksus/flask-redmail)\n* Documentation: [https://flask-redmail.readthedocs.io/en/latest/](https://flask-redmail.readthedocs.io/en/latest/)\n* Releases: [https://pypi.org/project/Flask-Redmail/](https://pypi.org/project/Flask-Redmail/)\n\nSo what has changed? Now the email structures are more structured and more likely gets rendered across email providers, fixed a bug related to embedded emails and aliases and improved documentation.\n\nIf you found it useful, leave it a star on Github. That's the way to get visibility and it lets me know I'm building useful things in my free time. Thanks again for all the support!", "upvote_ratio": 0.89, "id": "t3_thrdr9", "created_utc": 1647681015.0}
{"sub": "Python", "title": "I created a super simple customizable desktop clock with python", "selftext": "Have you ever wanted to program a simple clock for your desktop? This [Simple Desktop Clock](https://github.com/underpig1/simplest-desktop-clock) uses python's tkinter to create a desktop clock for cleaner layouts. The best part is, you can customize how it looks and where it appears on your screen really easily, and it will change colors based on your wallpaper colors. I created this program a few weeks back because I was annoyed with the tiny clock in the bottom right of my screen and wanted something like the big clock on the lock screen. I decided to share it when I heard others were encountering a similar problem. The clock functions just as a part of your wallpaper--as in, you can't drag it or click on it, and it by default stays behind your windows. Let me know if you enjoy it or find it useful!\n\nhttps://preview.redd.it/0thaq2fkzao81.png?width=1674&amp;format=png&amp;auto=webp&amp;s=f00ab6723f9de61dc72fe1fc4adfb36fd814301d", "upvote_ratio": 0.88, "id": "t3_thqxd6", "created_utc": 1647678891.0}
{"sub": "Python", "title": "3 Things You Might Not Know About Numbers in Python", "selftext": "nan", "upvote_ratio": 0.5, "id": "t3_thqu6i", "created_utc": 1647678482.0}
{"sub": "Python", "title": "The Zen Of Python, One-Liners and Being Pythonic", "selftext": "nan", "upvote_ratio": 0.8, "id": "t3_thqfid", "created_utc": 1647676626.0}
{"sub": "Python", "title": "What extension is useful when doing Python in VS code?", "selftext": "Hi! I'm new to using VS code. Before this I was using Spyder and found that VS code to be more complicated (for me). Please do recommend me necessary extension :D", "upvote_ratio": 0.71, "id": "t3_thpx8i", "created_utc": 1647674284.0}
{"sub": "Python", "title": "Solution to Ramanujan equations", "selftext": "[https://todaymylearn.blogspot.com/2022/03/solution-to-ramanujan-equations.html](https://todaymylearn.blogspot.com/2022/03/solution-to-ramanujan-equations.html)\n\nI made a small program in python to solve Ramanujan equations.", "upvote_ratio": 0.5, "id": "t3_thplm6", "created_utc": 1647672828.0}
{"sub": "Python", "title": "PEP 686: Make UTF-8 mode default", "selftext": "nan", "upvote_ratio": 0.97, "id": "t3_thnk4l", "created_utc": 1647664164.0}
{"sub": "Python", "title": "Complete Guide to PyGame Setup in 14 mins!", "selftext": "nan", "upvote_ratio": 0.75, "id": "t3_thmkk7", "created_utc": 1647660554.0}
{"sub": "Python", "title": "What Python GitHub repos are good examples of best practices?", "selftext": "I've been writing more packages and recently enjoyed looking through Perfect's GitHub. It felt like a good example of code structuring, they implemented their cli well. I even started to look at how they were branching and what they were putting in commit messages. \n\nI very frequently work on islands without the oversight of more senior engineers and wanted to know if there were other GitHub repos you guys recommend looking at to learn how to learn to write good software.", "upvote_ratio": 0.85, "id": "t3_thmbi1", "created_utc": 1647659670.0}
{"sub": "Python", "title": "Euporie: a terminal app for working with Jupyter notebooks", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_thlcky", "created_utc": 1647656373.0}
{"sub": "Python", "title": "A Happy Success Story and Python!", "selftext": "Hi everyone!\n\n&amp;#x200B;\n\nI just wanted to share some exciting events, as well as a good story for those maybe demotivated learning Python.\n\n&amp;#x200B;\n\nLike many of you, I started learning Python (around 3-4 months ago), and quickly felt overwhelmed by just the huge mass of tutorials, libraries, and posts. However, I pushed through it, and have made some real progress!\n\n&amp;#x200B;\n\nToday I launched v1 of my AI/startup business [https://finned.tech](https://finned.tech) \\- selling a (Python) solution for personalized marketing on digital billboards, a solution now patent-pending. In addition to this, I built a licensing solution from the ground up, again using Python, and finally a Python webserver (using FastAPI) to handle auth, license checks, and payments.\n\n&amp;#x200B;\n\nWhat I learned from all of this (besides that it's hard to start a business), is that if you keep persevering, watching those tutorials, and building those things you think no one will ever use, that, before you know it, you'll be creating things you never thought possible before!\n\n&amp;#x200B;\n\nHopefully this post has inspired some of you to put a bit more effort into learning and building some projects, as I know many posts here did with me.\n\n&amp;#x200B;\n\n(Ok, now I'm done with my story, go build some cool things and learn more!)", "upvote_ratio": 0.85, "id": "t3_thlata", "created_utc": 1647656210.0}
{"sub": "Python", "title": "Elden Ring Open Source API", "selftext": "Good night tarnished guys and gals. Do we have some developers among us? I made this open-source API that contains all kinds of data scraped from Elden Ring that can be used for all sorts of student projects. So if you're new to programming/web development, loves Elden Ring, and want to build a cool app, feel free to use and abuse this API.\n\nThis is an Open Source project, so feel free to contribute. All the data and media can be found in the GitHub repository. Also, since the game came out not long ago, there are still some missing data and gaps here and there. If you find any issues, feel free to open an issue or open a PR that I will gladly merge into the codebase.\n\nThe API is available in both REST and GraphQL formats. Get started at [https://eldenring.fanapis.com/](https://eldenring.fanapis.com/)\n\n&amp;#x200B;\n\nSource code available at: [https://github.com/deliton/eldenring-api](https://github.com/deliton/eldenring-api)", "upvote_ratio": 0.93, "id": "t3_thl39l", "created_utc": 1647655499.0}
{"sub": "Python", "title": "Saturday Daily Thread: Resource Request and Sharing! Daily Thread", "selftext": "Found a neat resource related to Python over the past week? Looking for a resource to explain a certain topic?\n\nUse this thread to chat about and share Python resources!", "upvote_ratio": 1.0, "id": "t3_thiqqa", "created_utc": 1647648010.0}
{"sub": "Python", "title": "*UPDATED* Random numbers generator list with Mean, Median, Mode and frequency of each number", "selftext": "    import random\n    import numpy\n    from statistics import mode\n    import csv\n    import collections\n    \n    \n    #create list\n    list=[]\n    \n    ##random numbers\n    for i in range (100):\n        number= (random.randint(0,10))\n        #add to list\n        list.append(number)\n    \n    print(\"Random numbers: \"+(str(list)))\n    list.sort()\n    print(\"Numbers sorted: \"+(str(list)))\n    \n    #Mean, median and mode of list\n    mean1=numpy.mean(list)\n    median1=numpy.median(list)\n    mode1=mode(list)\n    \n    print(\"The mean of the numbers is \"+(str(mean1)))\n    print(\"The median of the numbers is \"+(str(median1)))\n    print(\"The mode of the numbers is \"+(str(mode1)))\n    \n    ##frequency method\n    \n    ##dictionary\n    frequency = {}\n    \n    \n    ###using collections.Counter for number frequency\n    frequency=collections.Counter(list)\n    \n    # printing the frequency\n    print(frequency)\n    \n    \n    ##Export dictionary to CSV file\n    with open('numbers.csv', 'w') as f:\n        for key in frequency.keys():\n            f.write(\"%s,%s\\n\"%(key,frequency[key]))\n    \n    \n    #df.to_csv(\"numbers.csv\", header=[\"Number\", \"Frequency\"], index=False)", "upvote_ratio": 0.6, "id": "t3_thgvms", "created_utc": 1647642439.0}
{"sub": "Python", "title": "Random numbers generator list with Mean, Median, Mode and frequency of each number", "selftext": "    import random\n    import numpy\n    from statistics import mode\n    import csv\n    \n    #create list\n    list=[]\n    \n    ##random numbers\n    for i in range (100):\n        number= (random.randint(0,10))\n        #add to list\n        list.append(number)\n    \n    print(\"Random numbers: \"+(str(list)))\n    list.sort()\n    print(\"Numbers sorted: \"+(str(list)))\n    \n    #Mean, median and mode of list\n    mean1=numpy.mean(list)\n    median1=numpy.median(list)\n    mode1=mode(list)\n    \n    print(\"The mean of the numbers is \"+(str(mean1)))\n    print(\"The median of the numbers is \"+(str(median1)))\n    print(\"The mode of the numbers is \"+(str(mode1)))\n    \n    ##frequency method\n    \n    ##dictionary\n    frequency = {}\n    \n    # iterating over the list\n    for item in list:\n       # checking the element in dictionary\n       if item in frequency:\n          # incrementing the counr\n          frequency[item] += 1\n       else:\n          # initializing the count\n          frequency[item] = 1\n    \n    # printing the frequency\n    print(frequency)\n    \n    ##Export dictionary to CSV file\n    with open('numbers.csv', 'w') as f:\n        for key in frequency.keys():\n            f.write(\"%s,%s\\n\"%(key,frequency[key]))", "upvote_ratio": 0.33, "id": "t3_thdm72", "created_utc": 1647633270.0}
{"sub": "Python", "title": "I've heard that \"if a class is just a constructor and one method, then it should be a function\". What is your opinion on this and what are counter examples?", "selftext": "I watched a seminar a few years ago about someone talking about the misuses of OOP in python and talked extensively about the case where people write a class, implement the `__init__()` method and one extra method and that's it.\n\nHe provided examples and showed functional code that simplifies the solution to a few lines and he said:\n\n&gt; \"If a class is just a constructor and one method, then it shouldn't be a class at all. It should be a function.\"\n\nI can't find the video anymore, but I was just wondering what reddit thinks about this statement and if you have any counter examples or in what cases it could be useful.\n\nI'm also interested in the cases where if it is true, how would that functional implementation look like when I need some sort of a state to persist.\n\n\nEDIT: Awesome discussion in the comments. Thanks for everyone's input!", "upvote_ratio": 0.97, "id": "t3_th6ztt", "created_utc": 1647623696.0}
{"sub": "Python", "title": "Resume Generation With Python", "selftext": "nan", "upvote_ratio": 0.33, "id": "t3_th6lvy", "created_utc": 1647623183.0}
{"sub": "LanguageTechnology", "title": "HuSpaCy: Industrial-strength Hungarian NLP", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_udox1z", "created_utc": 1651128559.0}
{"sub": "LanguageTechnology", "title": "Hey all! I'm making a pytorch transformer from scratch and I'm wondering if you have any tips.", "selftext": "Here's a post I have open on the pytorch forums: https://discuss.pytorch.org/t/help-simple-generate-predict-inference-function-for-attention-transformer/150251\n\nI can't seem to figure out what's going on in my forward method and I'm not sure how well it's learning. I damn sure don't know how to generate text. If anyone can help me with this problem, I'd appreciate it.", "upvote_ratio": 1.0, "id": "t3_udgf7q", "created_utc": 1651100509.0}
{"sub": "LanguageTechnology", "title": "Any annotation tool for argument mining?", "selftext": "Hi, I've been testing several annotation tools for my research at university. Each one has its particular advantages but none of the ones I've tried seem to be created for annotating argument relations. Inception could probably be the most flexible for my needs, yet it requires too many technical requirements for installing and using it as an admin. And most importantly now, it is usable but not optimal for argument mining, which would require being able to see the relations between sometimes across far-away-in-the-text arguments in an organised and intuitive way - both in-text and extracted separately from the original text. Arrows that go horizontally through several lines in a text, although they can at least show a relation across paragraphs, tend to accumulate and are very difficult to follow.\n\nI've recently seen Tiara, which has better visualisations for AM,  but you can't see annotations from different annotators and calculate inter-rater agreement, which is basic unless you're just annotating for yourself...\n\nDoes anyone know of an annotation app that allows for manually annotating argument units and visualising relations in different texts by more than one annotator?", "upvote_ratio": 0.84, "id": "t3_udeuwy", "created_utc": 1651096155.0}
{"sub": "LanguageTechnology", "title": "I just released the first six modules of my free NLP course.", "selftext": "If you're new to NLP, these first six modules will get you up and running with the basics.\n\n\nWe'll cover:\n\n* what makes NLP challenging and what we'll learn in the course.\n* various ways to preprocess our text depending on our goals including tokenization, lemmatization, part-of-speech-tagging, and more.\n* how to turn our text into numbers with basic bag-of-words approaches and performing document similarity and search.\n* how to use spaCy and scikit-learn to do all of the above.  \n\n\nNo registration needed but you can sign up for updates. Check it out at [nlpdemystified.org](https://nlpdemystified.org) and let me know what you think!", "upvote_ratio": 0.96, "id": "t3_ud7hrz", "created_utc": 1651076631.0}
{"sub": "LanguageTechnology", "title": "BERTScore: Evaluating Text Generation with BERT (Summary)", "selftext": "BERTScore is an automatic evaluation metric for text generation\ud83d\udd25\n\nBERTScore is found to correlate better with human judgments and provides stronger model selection performance than existing metrics like ROUGE, BLEU and it's variants \ud83d\udd25\ud83d\udd25\n\nWatch paper summary at\u00a0https://lnkd.in/dPE3g3_p\nPaper: https://arxiv.org/abs/1904.09675", "upvote_ratio": 1.0, "id": "t3_ud5pln", "created_utc": 1651071938.0}
{"sub": "LanguageTechnology", "title": "How to find synonyms in text.", "selftext": "Hi all, I am working on a problem where, input is a piece of text (generally text summary of 4-5 paragraphs), and out is the synonym pairs/phrases used in this text. If anyone has worked on this problem before do help me with any lead.\n\nThis provided data could be of any domain so wordnet kind of approach is not possible. For example, if let say it\u2019s a bio-medical data then synonym terms would be \u201cdoctor of cancer\u201d and \u201concologist\u201d. \n\nAny possible lead is most welcome.\n\nCurrently I am experimenting by extracting all possible NNP pairs and comparing there embedding for the similarities. But this could be very exhaustive if document is large. And getting good embeddings is also a challenge. \n\nIf you guys have any better approach in mind, please do share.", "upvote_ratio": 1.0, "id": "t3_ud2kja", "created_utc": 1651063040.0}
{"sub": "LanguageTechnology", "title": "Is there an evaluation metric for how much a sentence \"makes sense\"", "selftext": "So I'm doing a project involving fine-tuning a language model on a dataset of text and generating text that resembles text in the dataset. I was wondering if there was a metric that could evaluate how much that generated text \"makes sense\" as in how much does it resemble a real sentence. Thanks in advance.", "upvote_ratio": 0.96, "id": "t3_ucucoi", "created_utc": 1651030555.0}
{"sub": "LanguageTechnology", "title": "Review my basic Sentiment Analysis web app", "selftext": "I'm doing a Computer Science conversion course and going to do my thesis on Sentiment Analysis. For this, I did a very simple web app to analyse public tweets which I will want to use as a tool for my thesis. I would appreciate feedback on my application. Are there any features you can think of that would be interesting to include?\n\nHere is the link to it:\n\n[https://share.streamlit.io/alexanderverheecke/sentimentanalysisapp/main/main.py](https://share.streamlit.io/alexanderverheecke/sentimentanalysisapp/main/main.py)", "upvote_ratio": 0.84, "id": "t3_ucdbl1", "created_utc": 1650981868.0}
{"sub": "LanguageTechnology", "title": "Text Encoder output giving zero values", "selftext": "I have been working on an Out of Domain detection model which takes in two strings, one domain string and one out of domain string, and a list of strings that can be taken as an anchor. Based on this I have trained an encoder model that converts these strings into vectors of 100 dimensions. Each input sentence is converted to tokens and then they are converted to word vectors using glove vectors. Post that a CNN model is trained on the vectors. Batch norm is done before pooling and then they are passed through a linear layer to get the final vectors. Below is the code for the model.\n\n    class Encoder(nn.Module):\n        def __init__(self):\n            super(Encoder, self).__init__()\n            input_size = 40\n            dim = 100\n            self.batch_size = BATCH_SIZE\n            kernel = 2\n            self.conv_x_in = nn.Conv1d(dim, 300, kernel_size=kernel)\n            self.batch_norm_in = nn.BatchNorm1d(300)\n            self.pool_in = nn.AvgPool1d(MAXLEN - kernel + 1)\n            self.linear_in = nn.Linear(300, 100)\n            \n            self.conv_S_in = nn.Conv1d(dim, 300, kernel_size=kernel)\n            self.batch_norm_S_in = nn.BatchNorm1d(300)\n            self.pool_S_in = nn.AvgPool1d(MAXLEN - kernel + 1)\n            self.linear_S_in = nn.Linear(300, 100)\n            \n        def encoded_x_i_in(self, x_i_in):\n            x_i_in = x_i_in.transpose(1, 2) # make batch x seq x dim =&gt; batch x dim x seq\n            x = F.relu(self.conv_x_in(x_i_in))\n            x = self.batch_norm_in(x)\n            x = self.pool_in(x)\n            x = x.squeeze(2)\n            x = F.relu(self.linear_in(x))\n            return x\n    \n        def encode_S_in(self, x_i_in):\n            x_i_in = x_i_in.transpose(1, 2) # make batch x seq x dim =&gt; batch x dim x seq\n            x = F.relu(self.conv_S_in(x_i_in))\n            x = self.batch_norm_S_in(x)\n            x = self.pool_S_in(x)\n            x = x.squeeze(2)\n            x = F.relu(self.linear_S_in(x))\n            return x\n    \n        def forward(self, x_i_in, x_j_out, S_in):\n            in_emb = self.encoded_x_i_in(x_i_in)\n            out_emb = self.encoded_x_i_in(x_j_out)\n            overall_emb = [[self.encode_S_in(y) for y in x] for x in S_in]\n            overall_emb = [[torch.sum(y, 0) for y in x] for x in overall_emb]\n            overall_emb = [torch.stack(x) for x in overall_emb]\n            overall_emb = torch.stack(overall_emb)\n            return in_emb, out_emb, overall_emb\n\nAfter training the outputs are coming as mostly zeros, which makes the resulting vectors almost unusable. Not sure what is the issue here. I can try changing the base vectors from glove to something else but I think the problem stems from something else.\n\nAn example of the encoder output for the \\`\\``in_emb`\\` variable.\n\n    tensor([[0.0000, 0.0413, 0.0174, 0.0000, 0.1767, 0.2171, 0.0037, 0.0000, 0.0000,\n             0.0000, 0.0000, 0.0799, 0.0000, 0.0000, 0.3925, 0.0695, 0.2034, 0.0000,\n             0.0422, 0.0000, 0.0000, 0.1015, 0.2971, 0.1513, 0.1093, 0.0368, 0.0000,\n             0.0000, 0.0000, 0.0000, 0.0767, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n             0.0000, 0.0411, 0.0000, 0.0000, 0.1053, 0.0000, 0.0000, 0.0000, 0.0694,\n             0.0000, 0.0000, 0.1459, 0.0000, 0.1779, 0.0464, 0.0333, 0.0000, 0.1581,\n             0.0000, 0.0032, 0.0000, 0.0113, 0.0206, 0.0168, 0.0000, 0.0000, 0.0709,\n             0.0173, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1660, 0.0000,\n             0.0000, 0.0000, 0.0000, 0.0000, 0.1795, 0.2580, 0.0000, 0.0000, 0.2024,\n             0.0416, 0.0000, 0.5906, 0.0000, 0.0350, 0.2130, 0.0000, 0.0000, 0.2970,\n             0.0000, 0.0000, 0.0559, 0.0863, 0.0313, 0.0000, 0.2553, 0.0000, 0.0858,\n             0.0279]])", "upvote_ratio": 0.67, "id": "t3_ubi4qz", "created_utc": 1650883199.0}
{"sub": "LanguageTechnology", "title": "What made you interested in nlp? What's your motivation for picking it up?", "selftext": "I used to suck at writing English when I was a kid, my teacher used to bully me, but once was 15 IDK how but my spelling got better ( I have ADHD if that matters). I still suck to some extent but now I have keyboard predictions to my rescue and also text to speech has been a life saver to me when my senses overload cant read so I listen to the text instead.", "upvote_ratio": 1.0, "id": "t3_ubhx15", "created_utc": 1650882355.0}
{"sub": "LanguageTechnology", "title": "Anybody working on auto speech recognition", "selftext": "nan", "upvote_ratio": 0.41, "id": "t3_uaovi2", "created_utc": 1650783365.0}
{"sub": "LanguageTechnology", "title": "sparse sinkhorn transformer", "selftext": "nan", "upvote_ratio": 0.86, "id": "t3_ua28f7", "created_utc": 1650708691.0}
{"sub": "LanguageTechnology", "title": "Language Data Researcher @Amazon", "selftext": "Hello!! I had my first call with HR today for the language data researcher role at Amazon and the recruiter mentioned there would be technical questions asked in my follow up interview. I haven\u2019t had any luck finding information online regarding this role, does anyone have advice on how I should prepare for the technical questions or what kinds they may be asking me? Also, if anyone has some salary insight or experience with this role I would appreciate hearing about it, thank you :)", "upvote_ratio": 0.95, "id": "t3_u9tl3x", "created_utc": 1650675819.0}
{"sub": "LanguageTechnology", "title": "Hashtag inference - Looking for guidance", "selftext": "I have a a dataset of tweets and their corresponding hashtags, I would like to train a model/create an ML pipeline, that given a tweet's text it will provide a list of hashtags that are relevant to that piece text.\n\nHonestly, not sure how to start with this task, or even how to break it down to several ML operations (if needed).\n\nI understand the basics of deep learning and NLP, but I would appreciate your thoughts on how you would approach to solve this problem.\n\nRecommendations for papers, sample projects, etc are also welcome.", "upvote_ratio": 0.76, "id": "t3_u9oxgf", "created_utc": 1650662063.0}
{"sub": "LanguageTechnology", "title": "Has anybody tried using spacy with Cython?", "selftext": "Asking this so that I can speed up my Python Code using Cython (namely using stating typing of variables like strings and lists)", "upvote_ratio": 0.91, "id": "t3_u99qj1", "created_utc": 1650616133.0}
{"sub": "LanguageTechnology", "title": "Which huggingface model is the best for sentence as input and a word from that sentence as the output?", "selftext": "I am trying to build a pun detector and I would appreciate it if you could help me understand what would the best huggingface model to fine-tune be for this type of task:\n\nExample input 1:\n\n`If there's one person you don't want to interrupt in the middle of a sentence it's a judge.`\n\nExample output 1:\n\n`sentence`\n\nExample input 2:\n\n`A good baker will rise to the occasion, it's the yeast he can do.`\n\nExample output 2:\n\n`yeast`", "upvote_ratio": 0.94, "id": "t3_u91wzh", "created_utc": 1650587925.0}
{"sub": "LanguageTechnology", "title": "mGPT model release - multilingual generative transformer for 61 languages", "selftext": "Hi everyone. Today we released the mGPT model: multilingual generative pre-trained transformer\n\nThe checkpoints are available on Huggingface [model page](https://huggingface.co/sberbank-ai/mGPT)\n\nThe example usage is at the Github repo [https://github.com/ai-forever/mgpt](https://github.com/ai-forever/mgpt)  \n\n* The model has 1.3 billion parameters\n* The context length is 512 tokens. \n\nThe model can generate sequences after the input prompt, can be used for fine-tuning or for zero- and few-shot learning:\n\n    from transformers import GPT2LMHeadModel, GPT2Tokenizer\n    model_name = \"sberbank-ai/mGPT\"\n    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n    model = GPT2LMHeadModel.from_pretrained(model_name)\n    model.cuda()\n    model.eval()\n    \n    texts = [\n        \"My favourite holiday is \",\n        \"\u053b\u0574 \u057d\u056b\u0580\u0565\u056c\u056b \u057f\u0578\u0576\u0576 \u0567 \",\n        \"\u041c\u043e\u0454 \u0443\u043b\u044e\u0431\u043b\u0435\u043d\u0435 \u0441\u0432\u044f\u0442\u043e \",\n        \"mi fiesta favorita es \",\n        \"\u092e\u0947\u0930\u0940 \u092a\u0938\u0902\u0926\u0940\u0926\u093e \u091b\u0941\u091f\u094d\u091f\u0940 \u0939\u0948\",\n        \"\u6211\u6700\u559c\u6b22\u7684\u8282\u65e5\u662f\",\n        \"Min favorithelg \u00e4r \"\n    ]\n    transformers.set_seed(1337)\n    for text in texts:\n        input_ids = tokenizer.encode(text, return_tensors=\"pt\").cuda()\n        out = model.generate(\n            input_ids, \n            min_length=100,\n            max_length=100,\n            eos_token_id=5, \n            pad_token=1,\n            do_sample=True,\n            top_k=0,\n            top_p=0.9,\n            no_repeat_ngram_size=4)\n        generated_text = list(map(tokenizer.decode, out))[0]\n    \n    ```\n    My favourite holiday is \ufffdThanksgiving\ufffd so, I wanted to share the recipe I made from a recipe I found on the fool, Flockish Street Bakery. The banana bread is delicious and a good way to treat those stained teeth. Everyone loves a chocolate treat, so I thought I would share it with you, hopefully others will like it too. This bread is SO good!! \n    ---\n    \u053b\u0574 \u057d\u056b\u0580\u0565\u056c\u056b \u057f\u0578\u0576\u0576 \u0567 \u0577\u0561\u057f \u056c\u0561\u057e \u0565\u0572\u0565\u055e\u056c. \u0554\u056b\u0579 \u0578\u0582 \u057a\u0561\u056f\u0561\u057d \u0570\u0561\u0572\u0569\u0561\u0576\u0561\u056f \u0570\u0561\u0580\u057d\u057f\u0561\u0581\u0580\u056b\u0576\n    ---\n    \u041c\u043e\u0454 \u0443\u043b\u044e\u0431\u043b\u0435\u043d\u0435 \u0441\u0432\u044f\u0442\u043e \u0454 \u0420\u0456\u0437\u0434\u0432\u043e\n    ---\n    mi fiesta favorita es @marhuval__ La gente queremos fique muy feliz, estoy pensando en celebrarlo el 2 de abril \n    ---\n    \u092e\u0947\u0930\u0940 \u092a\u0938\u0902\u0926\u0940\u0926\u093e \u091b\u0941\u091f\u094d\u091f\u0940 \u0939\u0948 \u0938\u0940\u0927\u0940 \u0930\u093e\u0924, \u0907\u0902\u091f\u0930\u0928\u0947\u091f \u0938\u0947 \u091c\u0941\u0921\u093c\u0947 \u092c\u0939\u0941\u0924 \u0938\u093e\u0930\u0947 \u0935\u093f\u0915\u0932\u094d\u092a \u0939\u0948\u0902 \u0914\u0930 \u092f\u0926\u093f \u0906\u092a \u0935\u093e\u092a\u0938 \u0938\u0940\u0927\u0947 \u0915\u093f\u0938\u0940 \u0918\u0930 \u092c\u0938\u094b\u0902 \u092e\u0947\u0902 \u0918\u0941\u0938\u0924\u0947 \u0939\u0948\u0902, \u0924\u094b \u0906\u092a\u0915\u094b \u0938\u094d\u0935\u093e\u0917\u0924 \u0939\u0948 \u092c\u0948\u0920\u0915\u0947\u0902 \u0939\n    ---\n    \u6211\u6700\u559c\u6b22\u7684\u8282\u65e5\u662f-\u201c\u4fdd\u536b\u56fd\u201d\u65e5\uff01\u201d \u6fb3\u95e8\u8bba\u575b\n    \u6fb3\u95e8\u8bba\u58c7&lt;&lt; \u4e0a\u4e00\u7bc7\uff1a\u70b9\u77f3\u6210\u91d1\uff01\u6b66\u78ca\n    \u4e0b\u4e00\u7bc7\uff1a\u4f60\u8fd8\u5728\u7231\u5f97\u6d51\u8eab\u53d1\u6296\u5417\uff1f\u4f46\u5a74\u513f\u5728\u5988\u5988\u8eab\u4e0a~~\n    ---\n    Min favorithelg \u00e4r ute, og din blog er m\u00f8dested for s\u00e5 mange som muligt af dem i \u00f8jeblikket.\n    ```\n\nFull language list:  *Afrikaans, Arabic, Armenian, Azeri, Bashkir, Basque, Belarusian, Bengali, Bulgarian, Burmese, Buryat, Chinese, Chuvash, Danish, Dutch, English, Finnish, French, Georgian, German, Greek, Hebrew, Hindi, Hungarian, Indonesian, Italian, Japanese, Kalmyk, Kazakh, Korean, Kyrgyz, Latvian, Lithuanian, Malay, Malayalam, Marathi, Moldovan, Mongolian, Ossetian, Persian, Polish, Portuguese, Romanian, Russian, Spanish, Swahili,  Swedish, Tadjik, Tamil, Tatar, Telugu, Thai, Turkish, Turkmen, Tuvan, Ukrainian, Urdu, Uzbek, Vietnamese, Yakut and Yoruba.*", "upvote_ratio": 0.76, "id": "t3_u8zixf", "created_utc": 1650580710.0}
{"sub": "LanguageTechnology", "title": "Where did Intent / Slot schema even come from?", "selftext": "hi all, i'm writing some internal documentation at my company about labelset design best practices for NLU models. i'd like this documentation to be approachable by new hires / those maybe broadly familiar with classification tasks, but not NLU. it occurred to me that I don't even know where the idea for intent / slot schemata originated. it feels like trying to find a citation for the invention of the wheel. \n\n&amp;#x200B;\n\nany pointers here? what's the earliest paper we can find that references intent / slot  ontologies or classification?", "upvote_ratio": 0.76, "id": "t3_u8xytg", "created_utc": 1650576193.0}
{"sub": "LanguageTechnology", "title": "Leetcode for NLP positions?", "selftext": "Location: Germany, Baden-W\u00fcrttemberg\n\nI recently finished my dual-major MA in English and Computational Linguistics in Croatia, got my Goethe B2 certificate, and built a GitHub portfolio with some NLP/ML projects. I need to tweak my CV a bit, and I plan to start applying for positions in Germany within a week or two.\n\nI was wondering if I should focus my prep time for the interviews on Leetcode style problems or actually relevant coding exercises with Spacy/NLTK/TensorFlow etc.\n\nWhat are your experiences? Any additional tips are welcome, of course, especially if someone wants to provide feedback on my portfolio, and/or my CV when I'm done.\n\nHere's the GitHub link: [https://github.com/SkarletXx](https://github.com/SkarletXx)\n\nNote: if relevant, I already live in Germany.", "upvote_ratio": 0.8, "id": "t3_u8t1t2", "created_utc": 1650562607.0}
{"sub": "LanguageTechnology", "title": "Building Dense Passage Retrievers(DPR)", "selftext": "Hi, I made a video explaining the ideas behind building a **Dense Passage Retriever(DPR)**. Whenever we talk about retrievers, we mostly refer to the DPR formulation which appeared [in this paper](https://arxiv.org/pdf/2004.04906.pdf). A lot of publicly available implementations also use this formulation. \n\nIn a previous video, we discussed how to use the DPR End-to-End QA system which uses DPR with a QA model. **In this video, we solely focus on retrievers and the ideas behind building them.** The implementation is quite similar to retrievers pre-trained with [Inverse Close Task](https://www.youtube.com/watch?v=JUJD6So-2Lo). \n\nThis video is part 8 of a 9 video series on Open-domain question answering using Dense retrievers. Thanks for the support and I will appreciate any feedback.\n\n[https://www.youtube.com/watch?v=w61p0HLo7gc](https://www.youtube.com/watch?v=w61p0HLo7gc)", "upvote_ratio": 1.0, "id": "t3_u8nudu", "created_utc": 1650548215.0}
{"sub": "LanguageTechnology", "title": "Amazon releases 51-language dataset for language understanding", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_u8n2qn", "created_utc": 1650545944.0}
{"sub": "LanguageTechnology", "title": "Cross Validation using Simple Transformers for NER", "selftext": " \n\nWhile implementing the code below for an IOB dataset with columns of word, labels and sentence\\_id for NER:- (retrieved from [https://www.philschmid.de/k-fold-as-cross-validation-with-a-bert-text-classification-example](https://www.philschmid.de/k-fold-as-cross-validation-with-a-bert-text-classification-example))\n\n    14     # prepare cross validation \n    15     n=5 \n    16     kf = KFold(n_splits=n, random_state=42, shuffle=True) \n    17  \n    18     results = [] \n    19  \n    20     for train_index, val_index in kf.split(train_data): \n    21             # splitting Dataframe (dataset not included) \n    22         train_df = train_data.iloc[train_index] \n    23         val_df = train_data.iloc[val_index] \n    24         # Defining Model \n    25         model = ClassificationModel('bert', 'bert-base-uncased') \n    26             # train the model \n    27         model.train_model(train_df) \n    28             # validate the model \n    29         result, model_outputs, wrong_predictions = model.eval_model(val_df, f1=f1_score) 30         print(result['f1']) \n    31             # append model score 32         results.append(result['f1']) \n\nI received this error message:- \"ValueError: You appear to be using a legacy multi-label data representation. Sequence of sequences are no longer supported; use a binary array or sparse matrix instead - the MultiLabelBinarizer transformer can convert to this format.\"\n\nAny help is very much appreciated!", "upvote_ratio": 1.0, "id": "t3_u8jdku", "created_utc": 1650532473.0}
{"sub": "LanguageTechnology", "title": "Why does it say Import \"transformers\" could not be resolvedPylance\" when I try to import this specific package", "selftext": "nan", "upvote_ratio": 0.73, "id": "t3_u8islf", "created_utc": 1650529940.0}
{"sub": "LanguageTechnology", "title": "NLP techniques related to distinguishing scenes in a story", "selftext": "Say I have a story or novel with multiple scenes. Are there any NLP work/techniques that allow me to know when the author switches from one scene to another? Searching things like \"context\", \"scene\" in google does not yield good results.\n\nAssume that I do not have a list of characters' names to begin with and there may be nameless NPCs.", "upvote_ratio": 0.81, "id": "t3_u8ajqf", "created_utc": 1650500354.0}
{"sub": "LanguageTechnology", "title": "Using subwords for POS predictions", "selftext": "Hi everyone,\nI was given a problem to build a classifier to predict POS using embedding layer that encode every word to the sum if its encoding, 3 character prefix and 3 character suffix.\nmy question is what happens when i take a word like \u201cis\u201d which has no prefix/suffix of size 3?\nIgnore the subwords?\nAs special tokens?", "upvote_ratio": 0.8, "id": "t3_u8410w", "created_utc": 1650481929.0}
{"sub": "LanguageTechnology", "title": "txtai 4.4 released - explainable semantic search, build your own query language", "selftext": "nan", "upvote_ratio": 0.93, "id": "t3_u80160", "created_utc": 1650471065.0}
{"sub": "LanguageTechnology", "title": "Most Suitable library for large data NLP sentiment analysis in Python?", "selftext": "Hi All,\n\nI'm trying to do a sentiment analysis of some stocks in Farsi.  I am using googletrans library to translate to English and then running a sentiment analysis using NLTK/BERT. \n\nBERT seems to be more accurate but it doesn't seem suitable for big datasets.  I have 16gb ram but my PC still cannot process the data.  I also admit I'm a rookie with it (and also pytorch/tensorflow).\n\nI've also tried NLTK but it doesn't seem as accurate.\n\nI have watched many YouTube tutorials, but I'm still not sure what to do - I could do with some personalised advice.\n\n**The project**: Extract Data from Telegram/Twitter -&gt; Clean in Pandas -&gt; Analyse sentiment using NLTK/BERT\n\nThanks!", "upvote_ratio": 1.0, "id": "t3_u7wc5a", "created_utc": 1650460728.0}
{"sub": "LanguageTechnology", "title": "what are some practical applications of language technology one would like to see in use?", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_u7uhno", "created_utc": 1650454751.0}
{"sub": "LanguageTechnology", "title": "Unable to get any TF-IDF values when analyzing Twitter data using R", "selftext": "I am analyzing (using Tidytext) about 13,000 tweets that I downloaded using the AcademicTwitteR package. As far as I can tell, the problem is that when transforming the data to the Tidytext format, it for some reason assumes I only have 2 documents. As I understand it, each Tweet should be its own document. Can someone tell me what is wrong?\n\n     tweet_words_total &lt;- tweets_total %&gt;% \n      select(text, id) %&gt;% \n      unnest_tokens(word, text)\n    \n    my_stop_words &lt;- stop_words %&gt;% select(-lexicon) %&gt;% \n      bind_rows(data.frame(word = c(\"https\", \"t.co\", \"rt\", \"amp\",\"der\",\"20\")))\n    tweet_clean_total&lt;-tweet_words_total%&gt;%\n      filter(!(word %in% stopwords(source = \"snowball\")))%&gt;%\n        anti_join(my_stop_words)\n    \n    \n    total_corpus&lt;-Corpus(VectorSource(tweet_clean_total))\n    \n    total_tdm&lt;-TermDocumentMatrix(total_corpus,\n                             control = list(weighting = weightTfIdf,\n                                            removePunctuation = T,\n                                            removeNumbers = T,\n                                            stemming = T,\n                                            stripWhitespace = T))\n\n\\&gt; total\\_tdm\n\n&lt;&lt;TermDocumentMatrix (terms: 16860, documents: 2)&gt;&gt;\n\nNon-/sparse entries: 16860/16860\n\nSparsity           : 50%\n\nMaximal term length: 36\n\nWeighting          : term frequency - inverse document frequency (normalized) (tf-idf)", "upvote_ratio": 1.0, "id": "t3_u7tvzb", "created_utc": 1650452552.0}
{"sub": "LanguageTechnology", "title": "Text Entailment approach for Zero-shot Text Classification (Research Paper Walkthrough)", "selftext": "nan", "upvote_ratio": 0.81, "id": "t3_u7ty0s", "created_utc": 1650452743.0}
{"sub": "LanguageTechnology", "title": "Hey people im aspiring to publish a conference paper in nlp specifically in nlg but finding difficult to frame problem statement anybody would like to guide and collab !? Interested ones kindly DM for working together and kindly advice how to frame problem statement", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_u7pi0w", "created_utc": 1650433715.0}
{"sub": "LanguageTechnology", "title": "NLP on Bugzilla issue tracking system", "selftext": "Hello guys,\n\nI haven't had any experience in artificial intelligence yet, but I think that Natural Language Processing could be helpful here.\n\nWe have a issue tracking system which is based on bugzilla. We have about 300,000 entries in this system, where the entries are structured like this:\n\n&amp;#x200B;\n\n\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\n\n\\-- Entered from Customer\\_XY on 8/4/2021 11:12:39 AM\n\nServer:\n\nDatabase:\n\nVersion:\n\nPriority: \n\nOwner:\n\n&amp;#x200B;\n\nObserved Behaviour: Application is blocking since 20 min, Log file entries are as follows\n\n...\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\-- From COMPANY/AZR on 9/4/2021 11:00:29 AM\n\nLogfile shows that stored procedure xy has been the main cause of the blockings, created new index xy.\n\nPerformance should be fine again.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n=&gt; Maybe again answer from Customer \n\n...\n\n\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nSo this issue system also serves us as a knowledge base where many issues have been solved before.\n\nIt would now be interesting to have a tool or plug-in that could be integrated into our issue tracking system, which could already give possible solution suggestions based on the customer's entry.\n\nThis can be used as help for new \n\n&amp;#x200B;\n\n&amp;#x200B;\n\nI would like to learn more about AI technologies. My main concern now would be this:\n\n\\- What technologies could be helpful here?\n\n\\- The 300,000 entries could serve as training data: All entries written by a \"CUSTOMER\" are problems, while everything starting with COMPANY/xy contains the solutions.\n\n&amp;#x200B;\n\nI would like to hear what technologies could be used here.\n\nThis could help new staff to answer customer problems by having the NPL already suggest what possible solutions to problems could be.\n\n&amp;#x200B;\n\nThanks a lot", "upvote_ratio": 1.0, "id": "t3_u7oqgi", "created_utc": 1650430695.0}
{"sub": "LanguageTechnology", "title": "Any upcoming biomedical NLP conferences?", "selftext": "I recently had a paper rejected from NAACL 2022. I have revised it according to the reviewers recommendations (which weren't all that helpful anyway), and am looking for another suitable conference. The paper's broad topic is knowledge discovery using biomedical text corpora. \n\nSo far I have found the International Conference on Biomedical Natural Language Processing 2023 - I can't find much info about this conference, and also their website seems to have a lot of non-NLP submissions under \"selected papers.\" Seems kind of sketch tbh.\n\nIt seems like I missed the paper submission deadline for BioNLP 2022 at ACL. \n\nDo you guys know of any upcoming biomedical domain-specific NLP conferences that might be coming up and that are still open for submission? \n\nAny recommendations would be appreciated. Thanks.", "upvote_ratio": 0.9, "id": "t3_u7jfmi", "created_utc": 1650413496.0}
{"sub": "LanguageTechnology", "title": "Comprehensive Certification for Conversational Design - from UX and copywriting to NLP and A\u00cd", "selftext": "Hello, is there any chance someone could give me some advice for a conversational design certification? I would like a comprehensive course that would cover UX, copywriting and AI. \n\nI already work with it but the company I work for fails to help me grow my tool box and I am immigrating to Canada, so a NA certificate would help, since I have lost an opportunity for the lack of formal education on the field. I\u2019m a differentiated professional with strong sales and customer service background and I do have formal education on data science but only have worked with NLP and honestly, thanks to YouTube, DataCamp and so on. \n\nI\u2019m looking into [conversational design institute](https://www.conversationdesigninstitute.com/courses) and since I\u2019m not making dollars yet, the investment must be very well calculated! \n\nThanks in advance", "upvote_ratio": 1.0, "id": "t3_u7g30z", "created_utc": 1650403903.0}
{"sub": "LanguageTechnology", "title": "Decoder Pre-trained and Encoder Fine-Tuned?", "selftext": "Hello I am currently writing my masters thesis and diving into the world of PLMs and NLP models.\n\nMy goal is to design an architecture of some kind of PLM model, that asks questions to a human and uses the answer to gain knowledge. Basically it should learn from dialogues with humans.\n\nI thought about using an pretrained Transformer model and subsequently use the question-answer pairs as labeld data for fine-tuning. That way a generic model should adapt more and more to the knowledge wihtin a certain domain.\n\nAt the moment I am considering whether I should go for an encoder or decoder model. I think they are not too much different but the encoder is better in nlu and the decoder ist better at nlg and few-shot learning, right?\n\nBecause I think these models arent so much different I got the following idea: Is it possible to use a unidirectional pre trained decoder model for having the QA dialogue with humans, while fine-tune the model on MLM (bidirecitonal). It is because I think a decoder model would be better in the communication but in the fine-tuning part (to adapt new information in the model, without forgetting the pretrained knowledge and not overfitting new knowledge) MLM seems like the better solution.\n\nDo you have any thoughts on this? \n\nThank you1", "upvote_ratio": 1.0, "id": "t3_u7fmb8", "created_utc": 1650402641.0}
{"sub": "LanguageTechnology", "title": "I made a multi class toxic text detection model, and in order to test it out in a practical scenario I am looking for an open source social media app template or even a comment section depicting scenario component including backends, Any code stack is fine (tho I would love a MERN stack one), Thanks", "selftext": "nan", "upvote_ratio": 0.5, "id": "t3_u7ccqz", "created_utc": 1650393949.0}
{"sub": "LanguageTechnology", "title": "Abstractive summarization TLDR Slackbot", "selftext": "We\u2019ve recently launched a new Slackbot that leverages abstractive summarization to TLDR webpages, text, and more. Currently working on adding support for summarizing YouTube videos.\n\nSo you share whatever you need summarizating with the Slackbot and you get TLDRs back.\n\n[Check it out](https://www.seenopsi.com)! Currently looking for more feedback and ideas. We also have a [beta testing option](https://www.seenopsi.com/beta) if you want to give it a go.", "upvote_ratio": 1.0, "id": "t3_u7bskb", "created_utc": 1650392491.0}
{"sub": "LanguageTechnology", "title": "AI thesaurus", "selftext": "Has anyone tried making a modern thesaurus trained on a massive corpus of famous writers which is just a neural machine translation engine with the source and target language the same language (a reflexive NMT engine) in order to serve as a modern thesaurus, not only word for word but creative, original, impressive rhetorical phrases and even entire suggested sentence rewordings?\n\nI think it\u2019s a good idea and has potential.\n\nThanks very much", "upvote_ratio": 0.81, "id": "t3_u7au2j", "created_utc": 1650390048.0}
{"sub": "LanguageTechnology", "title": "Can machine learning NLP be applied to ancient text/languages?", "selftext": "I'm new to machine learning/NLP. Is it possible to apply machine learning to predict constructs/classes/categories in an ancient Hebrew or Sumerian corpus for example?  what is the best Python module or platform to use for this? I took a look at SpaCy and watched a video of Prodigy used on English text, can those be used?\n\nDoes it have to be in a dataset format? This is ancient hebrew dataset...word tokens can have up to 6 parts. The morphology breaks it all down:\n\n    Ref,Eng,Heb6,Heb5,Heb4,Heb3,Heb2,Heb1,Highlight,Morphology,Strongs\n    Gen.1.1-01,in the head,,,,,\u05e8\u05b5\u05d0\u05e9\u05c1\u05d9\u05ea,\u05d1\u05bc\u05b0,p,HR/Ncfsa,H9003=\u05d1=in/H7225=\u05e8\u05b5\u05d0\u05e9\u05c1\u05b4\u05d9\u05ea=first_\u00a71_beginning\n    Gen.1.1-02,he has cut-out,,,,,,\u05d1\u05bc\u05b8\u05e8\u05b8\u05d0,,HVqp3ms,H1254a=\u05d1\u05bc\u05b8\u05e8\u05b8\u05d0=to create\n    Gen.1.1-03,elohim,,,,,,\u05d0\u05b1\u05dc\u05b9\u05d4\u05d9\u05dd,,HNcmpa,H0430=\u05d0\u05b1\u05dc\u05b9\u05d4\u05b4\u05d9\u05dd=God_\u00a7God@Gen.1.1\n    Gen.1.1-04,\u05d0\u05b5\u05ea-,,,,,,\u05d0\u05b5\u05ea,b,HTo,H0853=\u05d0\u05b5\u05ea=obj.\n    Gen.1.1-05,the Heavenly-pair,,,,,\u05e9\u05c1\u05bc\u05b8\u05de\u05b7\u05d9\u05b4\u05dd,\u05d4\u05b8,,HTd/Ncmpa,H9009=\u05d4=the/H8064=\u05e9\u05c1\u05b8\u05de\u05b7\u05d9\u05b4\u05dd=heaven\n    Gen.1.1-06,\u05d0\u05b5\u05ea-and,,,,,\u05d0\u05b5\u05ea,\u05d5\u05b0,b,HC/To,H9002=\u05d5=and/H0853=\u05d0\u05b5\u05ea=obj.\n    Gen.1.1-07,,,,,\u05c3,\u05d0\u05b8\u05e8\u05b6\u05e5,\u05d4\u05b8,,HTd/Ncbsa,H9009=\u05d4=the/H0776=\u05d0\u05b6\u05ab\u05e8\u05b6\u05e5=land_\u00a73_planet/H9016=\u05c3=verseEnd", "upvote_ratio": 1.0, "id": "t3_u76fyw", "created_utc": 1650378445.0}
{"sub": "LanguageTechnology", "title": "SparseServer.UI : A UI to test performance of Sparse Transformers", "selftext": "You can now load multiple transformers (each model has a unique sparsification recipe) on top of the DeepSparse server behind Streamlit, and it's open-source. This was battle tested on a 16GB of RAM with only 4 core CPU virtual machine. These compute requirements are enough to load up to 19 sparse BERT models in memory and compare their performance on question answering (P.S. they are really fast on just CPUs).\n\n\ud83d\udcbbcode: [**https://github.com/neuralmagic/deepsparse/tree/main/examples/sparseserver-ui**](https://github.com/neuralmagic/deepsparse/tree/main/examples/sparseserver-ui)", "upvote_ratio": 1.0, "id": "t3_u74rel", "created_utc": 1650373685.0}
{"sub": "LanguageTechnology", "title": "Using spaCy to find words describing the environment", "selftext": "Hey guys, I am working on a program that produces illustrations from pages in story books and I need a way to find words that describe the environment (e.g. mountains, river, ceiling, floor, table, road etc.). Is there a way to do this in spaCy? also any other suggestions of words I can find and use to generate illustrations and how I could find them with spaCy would be appreciated.", "upvote_ratio": 1.0, "id": "t3_u737fp", "created_utc": 1650368791.0}
{"sub": "LanguageTechnology", "title": "NeuroX toolkit for interpreting Deep NLP models", "selftext": "We have developed a toolkit to facilitate the interpretation of transformer models of NLP\nhttps://neurox.qcri.org/\nThe tool is well documented and it provides a number of basic functionalities that takes away the pain of initial work e.g. activation extraction, combing subwords to words, visualizing neurons, and running common interpretation methods. \n\nI would love to hear your feedback.", "upvote_ratio": 0.85, "id": "t3_u71fkb", "created_utc": 1650362152.0}
{"sub": "LanguageTechnology", "title": "Why is natural language so hard to process?", "selftext": "There are theories that the ambiguity from polysemy and homonymy (many to many matches between terms and meanings/concepts) is a result of optimizing language for communication of ideas between humans.\n\nSee for example:  \n[https://medium.com/ontologik/why-ambiguity-is-necessary-and-why-natural-language-is-not-learnable-79f0e719ac78](https://medium.com/ontologik/why-ambiguity-is-necessary-and-why-natural-language-is-not-learnable-79f0e719ac78)\n\nBut frankly I am not at all convinced.  For example, while this explains why one word can have many meanings, how does it explain why a single meaning can be expressed in multiple ways?  And then there is all of the seemingly unnecessary complexity in natural language grammars.\n\nFrom my experience, it seems that the real reason is that there are at least two fundamental roles of natural languages. One role is to convey meaning, but another equally important role is to manipulate the thinking of the listener into some state desired by the speaker. The second role appears to have resulted in aspects of natural languages that actually obscure communication of ideas through ambiguity and complexity. This can be useful in poetry or motivational speeches, but when the aim is to transfer knowledge as accurately as possible, e.g. in a classroom or an academic conference, the second role gets in the way. \n\nIs anyone here familiar with such a theory and any work that has been done to prove/disprove it?", "upvote_ratio": 0.9, "id": "t3_u6y3az", "created_utc": 1650348070.0}
{"sub": "LanguageTechnology", "title": "Game Theory and NLP", "selftext": "Anyone know of any work using game theory paradigms in NLP? I've been finding some fun ideas with alignment but nothing beyond that. Would appreciate if anyone can suggest some rabbit holes.", "upvote_ratio": 1.0, "id": "t3_u6wzug", "created_utc": 1650343964.0}
{"sub": "LanguageTechnology", "title": "How to implement the theory of copy movement in Prolog?", "selftext": "Using this input:\n\n    parse(Parse, [what,did,thomas,eat], [])\n\nI want to produce this output:\n\n    sbarq(\n    whnp(\n        wp(what)\n        ),\n    sq(\n        vbd(did),\n        np(nnp(thomas)),\n        vp(\n            vb(eat),\n            whnp(\n                wp(what)\n                )\n          ) % this is not in the input, I need to copy the entire whnp here\n    )\n\nwith this code:\n\n    parse(Tree) --&gt; sbarq(Tree).\n    \n    % rules\n    sbarq(sbarq(WHNP, SQ)) --&gt; whnp(WHNP), sq(SQ).\n    whnp(whnp(WP)) --&gt; wp(WP).\n    sq(sq(VBD, NP, VP)) --&gt; vbd(VBD), np(NP), vp(VP).\n    np(np(NNP)) --&gt; nnp(NNP).\n    vp(vp(VB)) --&gt; vb(VB).\n    \n    % lexicon\n    wp(wp(what)) --&gt; [what].\n    vbd(vbd(did)) --&gt; [did].\n    nnp(nnp(thomas)) --&gt; [thomas].\n    vb(vb(eat)) --&gt; [eat].\n\nhow can I change my code to copy the whnp into the vp?", "upvote_ratio": 1.0, "id": "t3_u6whtp", "created_utc": 1650342185.0}
{"sub": "LanguageTechnology", "title": "At what level of text aggregation should I conduct topic modeling?", "selftext": "I have a dataset of \\~50k documents that are each several hundred sentences long, as well as various metadata about the documents. I have conducted several correlated topic models of these data in R using the stm package.\n\nThe problem is that the topics are hard to interpret because even high proportion documents only contain a small amount of text actually devoted to a particular topic. It's also not clear which parts of the document are about one topic vs. another (especially when the topics are related).\n\nFor this reason, I'm considering breaking up documents and modeling them at a smaller level of aggregation (sentences; document-folds), then getting document-topic proportions by summing these units within-document. \n\n&amp;#x200B;\n\nIs this a reasonable strategy? Is there any work on topic modeling at different levels of text aggregation?", "upvote_ratio": 1.0, "id": "t3_u6v814", "created_utc": 1650338028.0}
{"sub": "LanguageTechnology", "title": "Can anyone help me build a model wrapper around a toxic comments classification model?", "selftext": "nan", "upvote_ratio": 0.5, "id": "t3_u6tcyy", "created_utc": 1650332344.0}
{"sub": "LanguageTechnology", "title": "Reddit IRL - a corpus of relatable Reddit humour", "selftext": "Hello all,\n\nWe have compiled a dataset of /r/meirl and /r/me_irl posts and comments for your viewing pleasure.\n\nYou can use it to search for reposts, analyze topics, or even try understand Redditors' sense of humour.\n\nYou can find the data [here](https://socialgrep.com/datasets/the-reddit-irl-dataset?utm_source=reddit&amp;utm_medium=link&amp;utm_campaign=theredditirldataset). (Or [here](https://huggingface.co/datasets/SocialGrep/the-reddit-irl-dataset), if you prefer using Huggingface Datasets). Hope you enjoy.", "upvote_ratio": 1.0, "id": "t3_u6nv4k", "created_utc": 1650316821.0}
{"sub": "LanguageTechnology", "title": "UKRUWAR22: A Collection of Ukraine-Russia War Related Tweets Dataset", "selftext": "It contains 55186 unique tweets in 57 different languages loosely coupled  to the Ukraine-Russia war. The details about the date and time of the tweet, language used, attachments URL, number of likes, retweets and replies, and hashtags used for each tweet are present in the dataset.  The data can be used for sentiment analysis and other NLP-related works.\n\n[https://ieee-dataport.org/documents/ukruwar22-collection-ukraine-russia-war-related-tweets](https://ieee-dataport.org/documents/ukruwar22-collection-ukraine-russia-war-related-tweets)", "upvote_ratio": 0.92, "id": "t3_u6k8vs", "created_utc": 1650307307.0}
{"sub": "LanguageTechnology", "title": "Best models for paraphrase generation?", "selftext": "I'm currently working with the fine-tuned `PEGASUS` model provided at `tuner007/pegasus_paraphrase`, but was wondering if there are any good paraphrase models that allow longer than `max_input_len = 60`, as the articles I have are more akin to paragraphs, sometimes more than a few sentences long. I'm not sure exactly where to go digging around for paraphrase generation methods; most of what I search seems to redirect me to text summarization models, which I always thought of as different (intentionally trying to shrink the sequence length, versus simply saying the same think with different words). Thanks for any help you can provide!", "upvote_ratio": 0.83, "id": "t3_u6k08v", "created_utc": 1650306677.0}
{"sub": "LanguageTechnology", "title": "Word-Meaning Dictionary Dataset", "selftext": "Hey all!\n\nSo I intend to make an application that, very naively speaking, outputs synonyms to a given word regardless of context (like if word1 is \"bank\", the model should output both \"money\" and \"river\", and the order does not matter). For this, I intend to use a Doc2Vec type of classifier, where the meanings of each word can serve as a document, and then similar words can easily be returned using a cosine similarity function. I chose this over a classic Word2Vec as this will be able to predict uncommon words (which blimey the English language has a lot of) which would otherwise be processed as &lt;UNK&gt; tokens. To this end, I am searching for a suitable dataset. Any ideas?", "upvote_ratio": 1.0, "id": "t3_u6hv49", "created_utc": 1650301055.0}
{"sub": "LanguageTechnology", "title": "Where do I begin with gpt-2-simple?", "selftext": "I want to make a chatbot that acts like a pre-existing character from a game or an anime, and was considering using gpt-2, as I have recently found out about it. Problem is, I don't really understand what I'm reading, and when I try to do things like install packages, I just get tons of errors that I don't understand. Where should I start if my end goal is the character based chat bot?", "upvote_ratio": 1.0, "id": "t3_u6gx9e", "created_utc": 1650298537.0}
{"sub": "LanguageTechnology", "title": "Translate natural language queries to vector search SQL", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_u69tf9", "created_utc": 1650277289.0}
{"sub": "LanguageTechnology", "title": "POS tagger Question: Should I keep embedding weights 0 if it is excluded from the Word2Vec training model? or should I set min_count to 1 for my training model?", "selftext": "I'm training a POS tagger (for multiple languages that may not have word-vector dataset) and intended to include a embedding layer with pre-trained weights produced by a self-trained model via Word2Vec using a training set.\n\nI assume that the rows of the array for embedding weights need to resembles the number of unique words in the vectorised token dictionary (i.e., I have 15000 unique words + padding term in the 'term to index' dictionary --&gt; 15001 rows for the embedding weights array).\n\nWhen training a Word2Vec model, min\\_count is at 5 and many words are excluded. Thus, if I were to apply all available vectors into the embedding weights array, many rows will be left with arrays of 0s because they were not trained in the Word2Vec model.\n\nIs this fine to be implemented in the embedding layer? Or should I set min\\_count = 1? Or should I not implement this at all?", "upvote_ratio": 0.81, "id": "t3_u6978p", "created_utc": 1650274812.0}
{"sub": "LanguageTechnology", "title": "Google AI\u2019s Latest Research on Language Model Proposes Two Different Sequence-To-Sequence Approaches Toward Zero-Shot Transfer For Dialogue Modeling", "selftext": "Conversational agents are nothing but computer systems intended to converse with humans. These agents have a wide range of applications, including booking flights, finding restaurants, playing music, and telling jokes. It can analyze and reply in ordinary natural languages. However, adding this functionality might be cumbersome as each new assignment necessitates new data collection and retraining of the conversational agent\u2019s models.\n\nMost task-oriented dialogue (TOD) models are trained on a particular task-specific ontology, which poses a significant problem. Ontology is just a catalog of conceivable user intentions or the activities that the conversational agent must complete and the available attribute fields to retrieve from any given interaction. A tight ontology can limit the model\u2019s ability to generalize to new tasks or contexts. Consider the case where an agent already knows how to buy train tickets; adding the ability to order airline tickets would necessitate new data training. In an ideal world, the agent would be able to apply existing information from one ontology to new ones.\n\n[Continue reading our Bite on this research](https://www.marktechpost.com/2022/04/17/google-ais-latest-research-on-language-model-proposes-two-different-sequence-to-sequence-approaches-toward-zero-shot-transfer-for-dialogue-modeling/)\n\nPaper 1: https://arxiv.org/pdf/2201.08904.pdf\n\nPaper 2: [https://arxiv.org/pdf/2204.04327.pdf](https://arxiv.org/pdf/2204.04327.pdf)\n\nGoogle Blog: https://ai.googleblog.com/2022/04/simple-and-effective-zero-shot-task.html", "upvote_ratio": 0.75, "id": "t3_u634n2", "created_utc": 1650250816.0}
{"sub": "LanguageTechnology", "title": "what is the easiest way to deploy a nlp model?", "selftext": " \n\nI created a model and i have saved .pb file and the variables, from here on, how i deploy my model in a server? lets say i want to send inputs from a front end and get predicted data outputs from the model, how do I establish this?\n\nThank you", "upvote_ratio": 0.93, "id": "t3_u5iisu", "created_utc": 1650183127.0}
{"sub": "LanguageTechnology", "title": "Questions about ACL Rolling Review", "selftext": "A few questions about ACL ARR:\n\n\\- If you request to reassign a reviewer, would the editor aim for reassigning all three reviewers or he would go for reassigning only that particular reviewer? Assume you have given a valid reason for reassignment and the editor is convinced.\n\n\\- If you request to reassign a reviewer, can the new reviewer see the previous reviews/scores before submitting his own review? Or he would access to the previous revision after submitting his own review.\n\nI already know (have heard) that in many cases reviewers are not available, and it becomes inevitable to get an entirely new set of reviews. I already know this. But my questions are about the case that the reviewer availability is not an issue. Juts trying to find out how things are managed", "upvote_ratio": 0.6, "id": "t3_u59y9d", "created_utc": 1650151400.0}
{"sub": "LanguageTechnology", "title": "Evaluation metric for entity linking?", "selftext": "I am writing a system to perform entity linking and coreference resolution. I know how I can score the coreference chains, using the Conll score, but how should I score the entity linking performance?", "upvote_ratio": 0.72, "id": "t3_u54844", "created_utc": 1650134434.0}
{"sub": "LanguageTechnology", "title": "Tips for posting code review", "selftext": "Hello everyone,\n\nI have been working on a project for my masters thesis that looks to classify nuclear power plant outage reports based on severity. \n\nIt's taken me quite some time to figure out and learn bert.\n\nI do not have much support in NLP work and hearing feedback from the community is what I have mostly relied on up to this point.\n\nWith this said, what is the best way to post code and to have it looked at to make sure I am doing things correctly, as expected? \n\nI do not intend to just throw code up and expect feedback. I will be sure to clean things up, document and comment as clearly and concise as possible. I just need tips figuring out where I can appropriately  the code and how to show the input data in a way that streamlines feedback.\n\nThanks for the help everyone!", "upvote_ratio": 0.76, "id": "t3_u53gfw", "created_utc": 1650132277.0}
{"sub": "LanguageTechnology", "title": "Funds to spend on continued education / training", "selftext": "My employer is sponsoring ~$1k of support for any continued training in text analysis / NLP / Python programming that I choose, to be used on any combination of short-courses, books, online subscription services, etc. This money will disappear if unspent. \n\nBeyond the free options available, has anyone found success or heard positive things about other specific paid resources? All suggestions are welcome, and perhaps this may benefit others in a similar situation as myself (bonus: resources that teach/use PyTorch over TF are a plus). \n\nMany thanks in advance.", "upvote_ratio": 0.86, "id": "t3_u52j9p", "created_utc": 1650129682.0}
{"sub": "LanguageTechnology", "title": "Text Segmentation Using Neural Network and Sentence Transformer", "selftext": "Hi everyone, I am a CS student and a beginner in Nlp. I am trying to do text segmentation but with video transcripts. Basically, the model will divide the text into segments based on topics. I am encoding all the sentences in a video with sentenceBert. So let's say, v1 = \\[s1,s2, ...sn\\] where si represents a sentence.  I am using SentenceTransformer to directly get sentence embedding from the \"sentence\\_transformers\" library, and feeding these sentence embeddings to a transformer model and then a feedforward layer to predict a binary output ( 0 if the sentence doesn't start a new segment, 1 if it is starting a new segment). The input looks like this (batch, sentences, embedding\\_dim). If I am having 4 videos in a batch and the maximum length of the video in this batch is 200 sentences, then it is (4, 200, 384).   \n\nI have been following the paper \"Transformer over Pre-trained Transformer for Neural Text Segmentation with Enhanced Topic Coherence\", and they did something like that, but with my very little understanding, I am trying to recreate it and use it with sentence\\_transformer.\n\nMy question is should I need to add positional encoding to the output of SentenceTransformer and then pass it to my Transformer model ? And also If this all seems correct. \n\n I am not sure because we use positional encodings when the inputs are words, but in my case, I am feeding the whole sentence at one timestep. Please any answer would be really helpful.", "upvote_ratio": 0.81, "id": "t3_u50nnv", "created_utc": 1650124331.0}
{"sub": "LanguageTechnology", "title": "BERT Toxic text classification problem, detects cvs but not raw texts", "selftext": " \n\nexpected shape=(None, 128), found shape=(None, 3)\n\nWhy do i get this error when i pass in a single string to predict?\n\nI am trying out a hate speech detection model and it predicts whole datasets but not sentences, it throws this error,\n\nany help would be much appreciated! Thank you\n\nNotebook link: [https://www.kaggle.com/code/abrh119/test-toxic-comments-bert](https://www.kaggle.com/code/abrh119/test-toxic-comments-bert)", "upvote_ratio": 0.76, "id": "t3_u4xqdy", "created_utc": 1650115420.0}
{"sub": "LanguageTechnology", "title": "Why does this notebook uses way too ram?", "selftext": " Is there a way to run this notebook without exceeding the memory usage? [https://colab.research.google.com/drive/14Ea4lIzsn5EFvPpYKtWStXEByT9qmbkj?usp=sharing](https://colab.research.google.com/drive/14Ea4lIzsn5EFvPpYKtWStXEByT9qmbkj?usp=sharing) any help would be much appreciated! thank you so much", "upvote_ratio": 0.5, "id": "t3_u4uorb", "created_utc": 1650103657.0}
{"sub": "LanguageTechnology", "title": "Simplest modifications to Spacy segmentation", "selftext": "The segmentation engine is pretty good but it doesn\u2019t segment on menu headers and random lines of text that aren\u2019t complete sentences.\n\nI usually segment and then split on newlines afterwards, it works pretty well.\n\nHowever, what are some simple options I can use to customize the segmentation?\n\nI know I can load different language models but as far as I know that\u2019s just about speed rather than a fundamentally different segmentation. Or are there models for different text types for example?\n\nIs there any other small thing I can change, in a config file for example, to try out different ways of using their segmentation engine?\n\nThe sentences are accessible in \u201cdoc.sents\u201d so I\u2019m pretty sure I would need to configure the NLP object before instantiating it on the string; in other words, there\u2019s nothing to be done after you\u2019ve created the NLP object.\n\nThanks very much", "upvote_ratio": 0.67, "id": "t3_u4u9wc", "created_utc": 1650101836.0}
{"sub": "LanguageTechnology", "title": "How to normalize irrelavent numbers in the dataset?", "selftext": "Hi I am using wikidump as the dataset for building a tokenizer(BPE) using [tokenizer from huggingface](https://huggingface.co/docs/tokenizers/python/latest/index.html), and I want to improve the dataset by reduce the noise in my dataset.\n\nLet say I am building for English, and I have the following data:\n\n    The total area of the United Kingdom is 93,628 square miles (242,500\u00a0km2), with an estimated population in 2020 of over 67 million.\n\nHow should I deal with those numbers? Removing them doesn't make sense to me, but keeping them doesn't look right to me as well. Should I increase the occurrence needed for a merge to avoid these noise?", "upvote_ratio": 0.76, "id": "t3_u4i255", "created_utc": 1650057466.0}
{"sub": "LanguageTechnology", "title": "Salesforce AI Researchers Introduce Converse: A Task-Oriented Dialogue System That Simplifies Chatbot Building And Handles Complex Tasks", "selftext": "One of the long-term ambitions of Artificial Intelligence (AI) has been to create a machine capable of having a meaningful conversation with a person and assisting them in performing tasks. For decades, many academics have dreamed of creating a strong chatbot that can assist humans with very difficult work through natural and fluent chats.\n\nRecent developments in this domain have made voice assistants and chatbots popular in our everyday lives. Task-oriented conversation systems, for example, can assist you in booking a table at a restaurant, purchasing airline tickets, cross-checking vaccination appointment status, or tracking the status of your transaction.\u00a0\n\nHowever, frequently chatbots get trapped in an infinite loop or misunderstand what\u2019s being said. This makes people want to connect to a human agent in case of emergencies or urgency, which negates the point of having the chatbot in the first place. Furthermore, building a robust chatbot that can handle several complicated tasks is challenging for many chatbot developers.\u00a0\n\nThis article sheds light on a breakthrough achieved by Salesforce AI Research Engineers that helps developers quickly construct smart bots that assist users in performing tasks. The team introduces Converse, a flexible and modular task-oriented conversation system. Using an interactive configuration tool and a little code, bot creators can easily develop jobs in Converse.\n\n[Continue Reading](https://www.marktechpost.com/2022/04/15/salesforce-ai-researchers-introduce-converse-a-task-oriented-dialogue-system-that-simplifies-chatbot-building-and-handles-complex-tasks/)\n\nPaper: https://arxiv.org/pdf/2203.12187.pdf\n\nGithub: https://github.com/salesforce/Converse", "upvote_ratio": 1.0, "id": "t3_u4ejv1", "created_utc": 1650047490.0}
{"sub": "LanguageTechnology", "title": "Dictionary for country names or references?", "selftext": " I am doing some text analysis and trying to find out how often countries are named in a series of texts, the problem is names like USA, US United States, the States, etc, are not linked in any dictionary that i can find.\n\nI spent half a day looking but didn't find anything. Maybe you know?", "upvote_ratio": 0.86, "id": "t3_u4dbfa", "created_utc": 1650044100.0}
{"sub": "LanguageTechnology", "title": "Zero-shot and Few-shot Text Classification Methods", "selftext": "Few-shot and Zero-shot learning aims for the ML models to make prediction under the scenario of Small number or ZERO examples of the relevant class for these models to learn from. \ud83d\udd25\ud83d\udd25\n\nThis tech report from Cloudera Fast-forward labs talks about various techniques for doing Zero-shot and Few-shot text classification.\n\nInterested? Then watch the full report walkthrough: https://youtu.be/2250AqR1RF8\n\n\nTech Report: https://few-shot-text-classification.fastforwardlabs.com/", "upvote_ratio": 0.74, "id": "t3_u45lml", "created_utc": 1650020810.0}
{"sub": "LanguageTechnology", "title": "Really struggling with fine-tuning a wav2vec2 model: confused on difference between 30 vs 256 vocab sizes..", "selftext": "I am trying to fine-tune a wav2vec2.0 model on only a little Assyrian speech data (\\~40 minutes). Somebody on this subreddit pointed me to an [Arabic ASR model here](https://huggingface.co/asafaya/hubert-large-arabic-ft) which I would like to fine-tune. I'm struggling with this process however. I have approximately 5 pages of text which I can train a tokenizer on. Should I go with a vocabulary  size of 30-ish (1 token per phoneme) or of 256?\n\nHere's my thought process: the [examples I'm going off of](https://github.com/huggingface/transformers/blob/main/examples/research_projects/wav2vec2/FINE_TUNE_XLSR_WAV2VEC2.md) show how to fine-tune a pretrained [facebook/wav2vec2-large-xlsr-53](https://huggingface.co/facebook/wav2vec2-large-xlsr-53) checkpoint. This example assumes you train the tokenizer to have 1 token per character. The issue with that is the model I actually want to fine-tune, [this one right here](https://huggingface.co/asafaya/hubert-large-arabic-ft), as an LM-net with 256 output nodes for 256 tokens. Does this mean for my fine-tuning to work, I should use a 256 tokenizer? I was told this is not a good idea since I only have 5 pages of text so the tokenizer will be really bad...\n\nAlso, for some reason the Arabic ASR [has a random sequence of layers](https://imgur.com/a/r1e19kl) sitting between the wav2vec2 model and the LM net. I'm so confused, does anybody know what this is doing in here!? Besides this block, the Arabic ASR model and the Facebook wav2vec2 model from the examples have exactly exactly the same architecture. Does this extra block have to do with the fact the Arabic ASR uses a 256 tokenizer instead of a \\~30 tokenizer? And more importantly, should I freeze this extra block when fine-tuning or no?\n\nThanks I know this was a lot.", "upvote_ratio": 1.0, "id": "t3_u3nynt", "created_utc": 1649960799.0}
{"sub": "LanguageTechnology", "title": "Spotify's Natural Language Search Explained", "selftext": "nan", "upvote_ratio": 0.98, "id": "t3_u3iela", "created_utc": 1649945362.0}
{"sub": "LanguageTechnology", "title": "purpose of using re.escape()", "selftext": "nan", "upvote_ratio": 0.4, "id": "t3_u3e8gu", "created_utc": 1649931839.0}
{"sub": "LanguageTechnology", "title": "If Anybody got NLP startup ideas do share", "selftext": "nan", "upvote_ratio": 0.25, "id": "t3_u39tyk", "created_utc": 1649913278.0}
{"sub": "LanguageTechnology", "title": "Any idea to improve the performance on multi-class classification problem?", "selftext": "Hi there!\n\nI am recently working on a multi-class text classification problem with BERT. My training set contains 12800 sentences, which are uniformly distributed in 5 classes. The dev set contains 1600 samples. And I am using a pre-trained model \"bert-base-chinese\" from HuggingFace.\n\nThe problem is, my dev acc raised from 20% but stuck at 52%. Even if I tried different parameters, including lr, batch\\_size, maxLength of tokens and dropout rates, the performance didn't get better.\n\nCould someone help with this problem, and give some instructions to improve the performance? Or should I turn to another model?", "upvote_ratio": 1.0, "id": "t3_u2yixx", "created_utc": 1649878901.0}
{"sub": "LanguageTechnology", "title": "Supercharged UI for MLflow", "selftext": "Hi guys, we've built a plugin that seamlessly reads MLflow logs and provides a beautiful UI to compare multiple runs with just a few clicks. You can:\n\nfilter runs with a super versatile fully pythonic search group and aggregate your metrics / images We are trying make it work seamlessly with MLflow and complement its other awesome features \ud83c\udf89\n\nHere is more info about it https://bit.ly/3xvyYbn Would love your feedback!!", "upvote_ratio": 1.0, "id": "t3_u2tu0c", "created_utc": 1649866156.0}
{"sub": "LanguageTechnology", "title": "How to improve your video editing software with language technology AI API?", "selftext": "nan", "upvote_ratio": 0.67, "id": "t3_u2lkg2", "created_utc": 1649838411.0}
{"sub": "LanguageTechnology", "title": "Translation/localization market research report out for 2022", "selftext": "The 2022 market report on translation, localization, and language services is out. Credit Nimdzi Insights. [http://mh.nimdzi.com/l10n\\_research\\_2022](http://mh.nimdzi.com/l10n_research_2022)", "upvote_ratio": 1.0, "id": "t3_u2k9g3", "created_utc": 1649832594.0}
{"sub": "LanguageTechnology", "title": "Fine-Tuning LayoutLM v2 For Invoice Recognition", "selftext": " With the advent of deep learning models, automated data extraction is becoming more accessible. [In this article](https://towardsdatascience.com/fine-tuning-layoutlm-v2-for-invoice-recognition-91bf2546b19e), we demonstrate step-by-step how to fine-tune layoutLM V2 on invoices starting from data annotation to model training and inference.\n\nEnjoy the read and if you have any questions, leave them below.", "upvote_ratio": 0.95, "id": "t3_u28bcc", "created_utc": 1649794508.0}
{"sub": "LanguageTechnology", "title": "[D] Expert Advice is needed on designing a feedback Loop for a (Textual Classification + NER) task in Production.", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_u1v85f", "created_utc": 1649756830.0}
{"sub": "LanguageTechnology", "title": "Subword Segmentation Algorithm", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_u1u300", "created_utc": 1649751865.0}
{"sub": "LanguageTechnology", "title": "Looking for Professors/Experts in the field of NLP/ML with some works on the Arabic Language", "selftext": "Hi there,\n\nI am currently doing a Ph.D. in Computational Linguistics at Korea Advanced Institute of Science and Technology (KAIST), South Korea. I have reached the final stage of my Ph.D. and currently preparing to do my thesis proposal. \n\nThe research within my thesis takes an attempt at modeling the Arabic dialects and brings them up to date with recent NLP breakthroughs such as the transformer-based language models. My advisor asked me to find a member with some background in the Arabic NLP field to be part of my thesis committee. I have contacted some Professors that I could find online but received no answers as of yet. So, I would really appreciate it if any of you could put me in touch with a professor or an expert in this field. \n\nThe proposal (scheduled to be in the next two weeks) and defense (2 months after the proposal) are going to be online and will only take about an hour each to complete. \n\nThank you for your help and have a great day.", "upvote_ratio": 1.0, "id": "t3_u1tlmg", "created_utc": 1649749746.0}
{"sub": "LanguageTechnology", "title": "Word Mover's distance between 2 documents", "selftext": "I'm trying to compute the word mover's distance (a variant of earthmover distance) between two documents. I want to represent any transformer-based model to generate document embeddings. Is there any resource/code-snippet I should look into?", "upvote_ratio": 0.5, "id": "t3_u1spht", "created_utc": 1649745930.0}
{"sub": "LanguageTechnology", "title": "Entity extractor", "selftext": "I am trying to build an entity extractor that can extract the shop number from the full shop name (e.g. McDonalds F1287, result should be 1287). I tried making a custom spacy ner but it does not work at all (ent.text and ent.label are blank). How can I go about doing this? Does this need DNNs? LSTM, CNN? Or transformer models?", "upvote_ratio": 0.5, "id": "t3_u1rino", "created_utc": 1649742539.0}
{"sub": "LanguageTechnology", "title": "Curriculum to learn NLP", "selftext": "I'm planning on self-studying natural language processing. I'm wondering if someone can create an outline of a curriculum that I can follow. Something like prerequisites before studying NLP, like what math and linguistics concepts I should know. I am already confident in programming and can learn Python easily.", "upvote_ratio": 1.0, "id": "t3_u1pja3", "created_utc": 1649733947.0}
{"sub": "LanguageTechnology", "title": "Master Program in LT in Gothenburg or Potsdam", "selftext": "Hello!\n\nI'm looking for a piece of advice where to do my Master's. I'm choosing between Cognitive Systems at Potsdam and LT at Gothenburg. I have a background in linguistics as my major and data science as my minor so I have enough programming background. I'm interested in research facilities and math courses. What should I choose?\n\nThank you for your answers", "upvote_ratio": 1.0, "id": "t3_u1lzho", "created_utc": 1649723240.0}
{"sub": "LanguageTechnology", "title": "Should I go BERT?", "selftext": "Should I go BERT if I am just looking for tokenization, POS tagging? I know it is the state of the art way of doing NLP, but given I just need to execute those relatively simple tasks, should I go BERT or something easier?", "upvote_ratio": 1.0, "id": "t3_u1jyfw", "created_utc": 1649717319.0}
{"sub": "LanguageTechnology", "title": "ClinicalBERT model for Medical NLP Tasks", "selftext": "Hi,\n\nI am using the Clinical BERT model to de id the i2b2 dataset. However, this is the first time I am dealing with all this data science and code, and do not know how to go about doing this. Does anyone here have experience in using these Domain-Specific BERT models? If so, any help would be greatly appreciated!", "upvote_ratio": 1.0, "id": "t3_u1h9vo", "created_utc": 1649710301.0}
{"sub": "LanguageTechnology", "title": "Question about text classification", "selftext": " \n\nGood afternoon,\n\nI want to do a comparative study that adds new in this area\n\n(classification of documents according to their subject),\n\nI chose these families to make the comparison:\n\n{GNN : taxteGcn , SGC }\n\n{trans : bert , roberta}\n\n{GNN With trans : bertGCN, robertaGcn,BertMp-GCn}\n\nbut the problem if I do this comparison at this time\n\nI will redo a job that exists, according to the articles\n\nthe majority of comparisons compare Accuracy.\n\n(for me what I am interested in is performance and time too)\n\nMy question: how can we avoid redoing a comparison that exists\n\nin order to add something new between the 3 families that I have chosen\n\nalways in the field of classification of documents according to their subjects\n\nor what do you think to do in this case?\n\nWarm Regards.", "upvote_ratio": 1.0, "id": "t3_u185f1", "created_utc": 1649684802.0}
{"sub": "LanguageTechnology", "title": "How do you deploy ML models in production (Kubernetes) for scalability?", "selftext": "I\u2019m currently looking into scalable ways to deploy machine learning models (esp. Huggingface models like T5) in a CPU-only production (on-premise) environment.\n\nThere are many different tutorials and guides available online, however almost all of them only scratch the surface and basically describe how you load a model and wrap it as it is in a Flask / Fast API application, dockerize it and deploy it into a Kubernetes cluster to quickly get it accessible through an API.\n\nIn my experience this works as long as you have a very low number of requests. As soon as the number of (simultaneous) requests increases, this approach runs into multiple problems like pods running Out-Of-Memory due to each requests increasing the memory requirements by the pod by a certain amount (e.g. mt5 requires \\~3GB memory when idling. This grows by X00MB for each request that is forwarded through the model. If this exceeds the available memory for the pod, the pod crashes with OOM) etc.\n\nAlso, due to the large size of ML containers, one of the main advantages of Kubernetes - horizontal scaling - is also tricky to achieve due to the lag between initialising the new pod and it being ready to use. Even if this would work well, you still run into the OOM errors described above.\n\nOf course, one can always try to optimise models by distilling or pruning their models, deploy them using ONNX or apply queuing mechanisms to handle incoming requests sequentially, however this only treats the symptoms and does not solve the underlying challenge.\n\nWhat are commonly used ways to overcome this shortcomings? Are there best practices?", "upvote_ratio": 0.92, "id": "t3_u17o3f", "created_utc": 1649683383.0}
{"sub": "LanguageTechnology", "title": "How to extract keyword from multilingual texts", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_u17dyw", "created_utc": 1649682529.0}
{"sub": "LanguageTechnology", "title": "Frameworks for bringing together NLP and enterprise document management systems?", "selftext": "From my perspective, applying NLP tools to documents on a larger scale requires frameworks that bring together document management systems and context-sensitive NLP functions that can be customized based on user-defined business rules.\n\nIn other words: A (i) document management system plus a (ii) rule-engine that serves as an interface to dynamically include customer-specific NLP functions into the automated processing pipeline for documents.\n\nThere are some proprietary closed-sourced systems on the market. But I wonder what open source frameworks / open standards do exist to define such customer-specific rules and NLP tasks.\n\nI am not aware of any active community digging into these questions. Would be happy to get some references here.", "upvote_ratio": 1.0, "id": "t3_u15nuu", "created_utc": 1649676972.0}
{"sub": "LanguageTechnology", "title": "Where can I learn about style transfer in text?", "selftext": "I'm interested in improving my writing by making a model to give suggestions on writing style. I've heard that there is work in style transfer for text. Where can I learn this, from the beginner's perspective to implementation? Thanks", "upvote_ratio": 1.0, "id": "t3_u0mqg9", "created_utc": 1649612598.0}
{"sub": "LanguageTechnology", "title": "How to change BERT pre-training tasks on HuggingFace?", "selftext": "Hello all!\n\nI am fairly new to HuggingFace, and have only used the most famous functionalities as of now and am now looking into how to make my own BERT model. I found a few blogs which detail the procedure to train a model from scratch and some to further pre-train/finetune on a custom dataset. What I am unable to find is \"How to pre-train a BERT model (say RoBERTa) on different pre-training tasks (say MLM+RTD) using HuggingFace?\" Is it even possible? I would appreciate all and any help regarding my query.\n\nThanks!", "upvote_ratio": 0.92, "id": "t3_u0ei3v", "created_utc": 1649584691.0}
{"sub": "LanguageTechnology", "title": "Check Out This DeepMind\u2019s New Language Model, Chinchilla (70B Parameters), Which Significantly Outperforms Gopher (280B) and GPT-3 (175B) on a Large Range of Downstream Evaluation Tasks", "selftext": "Extreme-scale language models have recently exhibited incredible performance on natural language processing challenges. This is due to their ever-increasing size, exceeding 500 billion parameters. However, while these models have grown in popularity in recent years, the amount of data utilized to train them has not increased. The current generation of huge language models is clearly undertrained. Three prediction approaches for optimally choosing both model size and training length have been proposed by a DeepMind research team.\n\nThree approaches have been mentioned to estimate the optimal parameter:\n\n* Change the size of the models and the number of training tokens.\n* IsoFLOP profiles\n* Using a parametric loss function to fit a model\n\nThe ultimate pretraining loss is calculated as the number of model parameters and training tokens. They minimize the loss function under the restriction of the FLOPs function, which is equal to the computational budget because the computational budget is a probabilistic function of the number of observed training tokens and model parameters.\n\n[Continue Reading This Research Summary](https://www.marktechpost.com/2022/04/09/check-out-this-deepminds-new-language-model-chinchilla-70b-parameters-which-significantly-outperforms-gopher-280b-and-gpt-3-175b-on-a-large-range-of-downstream-evaluation-tasks/)\n\nPaper: https://arxiv.org/pdf/2203.15556.pdf", "upvote_ratio": 0.96, "id": "t3_tzzor1", "created_utc": 1649530414.0}
{"sub": "LanguageTechnology", "title": "OpenAI Introduces DALL-E 2: A New AI System That Can Create And Edit Realistic Images And Art From A Description In Natural Language", "selftext": "New research by the OpenAI team has released a new version of DALL-E, its text-to-image generation tool. [DALL-E 2](https://openai.com/dall-e-2/) is a higher-resolution and lower-latency variant of the original system, generating images based on user-written descriptions. It also has additional features, such as altering an existing image.\n\nIn January of 2021, the first DALL-E, a portmanteau of the artist \u201cSalvador Dal\u201d and the robot \u201cWALL-E,\u201d emerged, limited to AI\u2019s capacity to visualize concepts. The researchers aimed to address the difficulties with technical safeguards and a new content policy, lower its computational load and advance the model\u2019s basic capabilities.\n\nInpainting, one of the new DALL-E 2 features, applies DALL-E\u2019s text-to-image capabilities at a finer level. Users can begin by selecting a section of an existing photograph and instructing the model to alter it. For example, users can cover a painting on a living room wall with a new picture or put a vase of flowers on a coffee table. The model can also fill (or remove) objects while considering factors such as shadow directions in a room. Variations is another function that works as an image search tool for photographs that don\u2019t exist. Users can start with a single image and then make various modifications based on it.\n\n[Continue Reading](https://www.marktechpost.com/2022/04/09/openai-introduces-dall-e-2-a-new-ai-system-that-can-create-and-edit-realistic-images-and-art-from-a-description-in-natural-language/)\n\nPaper: https://cdn.openai.com/papers/dall-e-2.pdf\n\n&amp;#x200B;\n\nhttps://reddit.com/link/tzunwv/video/rywbl253nis81/player", "upvote_ratio": 0.94, "id": "t3_tzunwv", "created_utc": 1649515566.0}
{"sub": "LanguageTechnology", "title": "looking for a generative models company training in Germany", "selftext": "Hi everyone. I'm looking for a company or organization that could provide a few days long training on basic nlp in python and training generative text models with huggingface. Onsite in office in Germany. Any recommendations?", "upvote_ratio": 0.88, "id": "t3_tz5eim", "created_utc": 1649429840.0}
{"sub": "LanguageTechnology", "title": "Dense Passage Retriever(DPR) Open-QA System", "selftext": "Hi, I made a video explaining **Dense Passage Retriever(DPR)** paper. We specifically explain the End to End QA system suggested in the latter part of the paper which discusses how to build an Open-QA system using dense retrievers. \n\nDPR was one of the first papers that discussed building dense retrievers using QA pairs only and didn't require a big pretraining computational setup like ORQA or REALM. It is currently used in a lot of places as a dense retriever. You can find [Hugginface](https://huggingface.co/docs/transformers/model_doc/dpr) and [Haystack](https://github.com/deepset-ai/haystack/blob/master/tutorials/Tutorial9_DPR_training.py) implementations also. \n\nThis video is part of a series on Open-QA using dense retrievers. We have made 2 videos on DPR. In the latter, we discuss how to build a dense retriever from scratch. Thanks for the support and it would be great if you could give any feedback.\n\n[https://www.youtube.com/watch?v=rvcyyJNjPU0](https://www.youtube.com/watch?v=rvcyyJNjPU0)", "upvote_ratio": 1.0, "id": "t3_tz2z5u", "created_utc": 1649422723.0}
{"sub": "LanguageTechnology", "title": "Evaluating gpt2", "selftext": "Hi everyone,\n\nI'm a beginner and I'm using [https://github.com/shawwn/gpt-2](https://github.com/shawwn/gpt-2) to fine tune gpt-2. I want to know how to measure the perplexity of my fine tuned model. Is there any tutorial that can help me implementing this task ? that would save my life .", "upvote_ratio": 1.0, "id": "t3_tyzqw2", "created_utc": 1649410600.0}
{"sub": "LanguageTechnology", "title": "Dependency parsing for Inter-chunk dependency annotated treebank", "selftext": "I have a treebank in Hindi which has sentences divided into chunks (NP,PP, VGF etc.) and the dependency relation of this chunk with one other chunk in the sentence, through their respective chunkheads.  How can I train a dependency parser model to learn and predict these inter-chunk dependency relations.   \nMy main issue is which format should i convert this data(SSF format) into? to train a dependency parser. Most Dparsers use the CONLL format but how can i use that for an inter-chunk learning task. \n\nA sentence of the data format i have from the treebank.\n\n&amp;#x200B;\n\n 1    ((            NP                     &lt;fs name='NP1' drel='k1:VGF'&gt;\n\n 1.1  rAma   NNP\n\n)) \n\n2      ((           NP                      &lt;fs name='NP2' drel='k2:VGF'&gt;\n\n2.1  PZala    NN \n\n))\n\n 3    ((           VGF                    &lt;fs name='VGF'&gt;\n\n3.1 KAwA    VM \n\n3.2 hE         VAUX\n\n)) \n\n The above SSF shows the relations between chunk heads. The fourth field of the SSF contains the dependencies. The head is identified by a unique id using an atrribute value pair. This can be seen above at node no. 3, where \u2018name\u2019 is given an id \u2018VGF\u2019. The dependents are then related to the head using \u2018drel\u2019 attribute. The value for drels is \u201cdependency relation:head id\u201d . The fourth field also has other morpholigical data for the word but i have edited those out here.", "upvote_ratio": 1.0, "id": "t3_tyxhza", "created_utc": 1649400563.0}
{"sub": "LanguageTechnology", "title": "Tensorflow-Transformers 2.0 ( for NLP, CV, Audio )", "selftext": "Hi  Everyone,\n\nI am delighted to announce, I am releasing Tensorflow-Transformers version 2.0 soon. It comes with lot of speedup and designed specifically for Tensorflow users, who are interested in using SOTA Transformer models in NLP, CV, AUDIO etc.\n\nText-Generation is 90 times faster comparing to Huggingface TF implementation and other equivalent TF implementation in most cases. Designed with complete serialisation in mind, it offers a lot of other features.\n\nWe have custom tokenizers using tf-text, end to end serialization pipeline and many more.It supports GPU, TPU, mixed precision, TFlite by default.\n\nCurrently supporting 12 major models Bert, Albert, Roberta, CLIP, VIT, T5, GPT2, mT5, BART and SentenceTransformers etc.\n\nHave a look and share your feedback, open issues . Thanks.\n\nCode : [GitHub - legacyai/tf-transformers: State of the art faster Natural Language Processing in Tensorflow 2.0 . 1](https://github.com/legacyai/tf-transformers)Website : [https://legacyai.github.io/tf-transformers 1](https://legacyai.github.io/tf-transformers)\n\n&amp;#x200B;\n\n\\- \\[Read and Write TFRecords using tft\\](https://github.com/legacyai/tf-transformers/blob/main/tutorials/1\\_read\\_write\\_tfrecords.ipynb)  \n\\- \\[Text Classification using Albert\\](https://github.com/legacyai/tf-transformers/blob/main/tutorials/2\\_text\\_classification\\_imdb\\_albert.ipynb)  \n\\- \\[Dynamic MLM (on the fly pre-processing using tf-text) in TPU\\](https://github.com/legacyai/tf-transformers/blob/main/tutorials/3\\_masked\\_lm\\_tpu.ipynb)  \n\\- \\[Image Classification ViT multi GPU mirrored\\](https://github.com/legacyai/tf-transformers/blob/main/tutorials/4\\_image\\_classification\\_vit\\_multi\\_gpu.ipynb)  \n\\- \\[Sentence Embedding train from scratch using Quoara on Roberta + Zeroshot STS-B\\](https://github.com/legacyai/tf-transformers/blob/main/tutorials/5\\_sentence\\_embedding\\_roberta\\_quora\\_zeroshot.ipynb)  \n\\- \\[Prompt Engineering using CLIP\\](https://github.com/legacyai/tf-transformers/blob/main/tutorials/6\\_prompt\\_engineering\\_clip.ipynb)  \n\\- \\[Question Answering as Generation - Squad v1 using GPT2\\](https://github.com/legacyai/tf-transformers/blob/main/tutorials/7\\_gpt2\\_question\\_answering\\_squad.ipynb)  \n\\- \\[Code to Code translation (CodexGLUE - Java to C#) using T5\\](https://github.com/legacyai/tf-transformers/blob/main/tutorials/8\\_code\\_code\\_java\\_to\\_csharp\\_t5.ipynb)  \n", "upvote_ratio": 0.95, "id": "t3_tyvdst", "created_utc": 1649392239.0}
{"sub": "LanguageTechnology", "title": "Google AI Introduces a Common Voice-Based Speech-to-Speech Translation Corpus (CVSS) That Can Be Directly Used For Training Direct S2ST Models Without Any Extra Processing", "selftext": "Speech-to-speech translation is the automatic translation of speech from one language to speech in another (S2ST). S2ST models have been widely accepted for bridging communication gaps between persons who speak different languages.\u00a0\n\nS2ST systems are traditionally developed with a text-centric cascade comprising automatic speech recognition (ASR), text-to-text machine translation (MT), and text-to-speech (TTS) synthesis subsystems. Recent studies have introduced S2ST, which does not rely on intermediary text representation. However, there are currently very few publicly available corpora directly relevant for such research.\n\nA new Google study has released [CVSS](https://arxiv.org/pdf/2201.03713.pdf), a Common Voice-based Speech-to-Speech translation corpus. From Arabic to Slovenian, CVSS provides sentence-level parallel speech-to-speech translation pairs into English from 21 languages. The Common Voice project used 1153 hours of crowdsourced human volunteer recordings to create the source talks in these 21 languages.\n\n[Continue Reading](https://www.marktechpost.com/2022/04/07/google-ai-introduces-a-common-voice-based-speech-to-speech-translation-corpus-cvss-that-can-be-directly-used-for-training-direct-s2st-models-without-any-extra-processing/)\n\nPaper: https://arxiv.org/pdf/2201.03713.pdf\n\nGithub: https://github.com/google-research-datasets/cvss", "upvote_ratio": 1.0, "id": "t3_tyg7n5", "created_utc": 1649347182.0}
{"sub": "LanguageTechnology", "title": "Gaining Psychological insights from text analysis", "selftext": "I'm trying to look for some research papers that used text analysis (mostly from social media) to gain social psychological insights, similiarly to what Cambridge Analytica did.\n\nIn particular i would like to understand the algorithms used to mine this sort of data from the text and what psychological insights can be gained.\n\nFor example, i would like to measure the coefficient of polarization for a certain group, how would i perform this ? What algorithms should i look for ? Can i do this with just python or should i use something else ?\n\nI have some knowledge with python and i would like to try perform this type of research but i'm not finding much informations about what to actually look for inside the text and how to analyze it.", "upvote_ratio": 0.5, "id": "t3_tydka7", "created_utc": 1649339500.0}
{"sub": "LanguageTechnology", "title": "How to identify covariate shift with text data?", "selftext": "Let say, I am building a model (NER, or let say classification into sports categories) on a stream of data (short summaries) which are all related to sports. Now let say this stream data has been changed over the period and now it's a mixture of sports and finance. \nSo how do we identify whether distribution of our new data has been changed? Or in a very naive terms, when do we decide to retrain our model?\n\n\nLet say our model is being deployed over the production environment, so we can not wait until someone report us that model is not performing well. I am exploring some active approach to solve this issue. Maybe like periodic checking, or everyday checking.. but still how to check, what to check??\n\nI consider these two ways to do this. But not sure, whether these are correct or not.\n\n1. Using word distribution of training corpus and new data. After that perform chi-square test.\n\n2. Using length wise distribution of training corpus and new data. After that chi-square test. \n\nLet me know, if someone has worked in this field before and help me understand this.", "upvote_ratio": 1.0, "id": "t3_ty7qho", "created_utc": 1649316888.0}
{"sub": "LanguageTechnology", "title": "\ud83d\udc49 Meet GPT-NeoX-20B, A 20-Billion Parameter Natural Language Processing AI Model Open-Sourced by EleutherAI", "selftext": "In the latest AI research breakthrough, researchers from EleutherAI open-sourced GPT-NeoX-20B, a 20-billion parameter natural language processing AI model similar to GPT-3. The model was trained on nearly 825GB of publicly available text data and performed comparably to GPT-3 models of similar size. It\u2019s the world\u2019s largest dense autoregressive model with publicly accessible weights. GPT-NeoX-20B obtained an accuracy similar to a linear interpolation between OpenAI\u2019s Curie and DaVinci models when tested on various typical NLP benchmark tasks and its one-shot performance on the MATH test dataset outperformed GPT-3 175B. GPT-NeoX-20B, according to EleutherAI, is the world\u2019s largest open-source pre-trained autoregressive language model.\n\nOpenAI announced the GPT-3 model with 175B parameters in 2020 but did not provide the trained model files. Instead, OpenAI offered an API that allows developers to use web service calls to integrate the model into their programs. Megatron-11B, Pangu-13B, Meta\u2019s Fairseq 13B, and EleutherAI\u2019s early models, GPT-Neo and GPT-J-6b are among the larger models that have been open-sourced since then.\n\n[Continue Reading](https://www.marktechpost.com/2022/04/06/meet-gpt-neox-20b-a-20-billion-parameter-natural-language-processing-ai-model-open-sourced-by-eleutherai/)\n\nPaper: http://eaidata.bmk.sh/data/GPT\\_NeoX\\_20B.pdf\n\nGithub: https://github.com/EleutherAI/gpt-neox", "upvote_ratio": 0.95, "id": "t3_ty4j65", "created_utc": 1649304328.0}
{"sub": "LanguageTechnology", "title": "IBM Watson NLU API inconsistencies", "selftext": "I'm testing the Watson NLU engine and it sure looks like it's not up to the task, even on the following simple case. Anyone have suggestions on an engine that can provide a better sentiment analysis?\n\n**Test 1** (gives a sentiment value I expect)\n\n  \"text\": \"WidgetX has been recently created, and it is terrible.\"\n\nOutput:\n\n      \"keywords\": [\n        {\n          \"text\": \"WidgetX\",\n          \"sentiment\": {\n            \"score\": -0.964476,\n            \"label\": \"negative\"\n          },\n          \"relevance\": 0.5,\n          \"count\": 1\n        }\n      ],\n      \"entities\": [\n        {\n          \"type\": \"Organization\",\n          \"text\": \"WidgetX\",\n          \"sentiment\": {\n            \"score\": -0.964476,\n            \"label\": \"negative\"\n          },\n    \n\n**Test 2** (gives a very wrong sentiment)\n\n  \"text\": \"WidgetX has been recently created. It is terrible.\"\n\nOutput:\n\n      \"keywords\": [\n        {\n          \"text\": \"WidgetX\",\n          \"sentiment\": {\n            \"score\": 0,\n            \"label\": \"neutral\"\n          },\n          \"relevance\": 0.5,\n          \"count\": 1\n        }\n      ],\n      \"entities\": [\n        {\n          \"type\": \"Organization\",\n          \"text\": \"WidgetX\",\n          \"sentiment\": {\n            \"score\": 0,\n            \"label\": \"neutral\"\n          },", "upvote_ratio": 1.0, "id": "t3_ty43sa", "created_utc": 1649302891.0}
{"sub": "LanguageTechnology", "title": "Amazon Researchers Developed a Universal Model Integration Framework That Allows To Customize Production Voice Models in a Quick and Scalable Way", "selftext": "Alexa and other voice assistants frequently use a range of speech synthesizers, which varies in terms of expressivity, personality, language, and speaking style. The machine learning models that underpin these applications can have vastly diverse architectures. Integrating them into a single voice service is time-consuming and difficult.\n\nA new Amazon research presents a universal model integration framework that enables quick, scalable customizable production voice models.\n\n[Continue Reading](https://www.marktechpost.com/2022/04/06/amazon-researchers-developed-a-universal-model-integration-framework-that-allows-to-customize-production-voice-models-in-a-quick-and-scalable-way/)", "upvote_ratio": 1.0, "id": "t3_txv2qi", "created_utc": 1649275957.0}
{"sub": "LanguageTechnology", "title": "Who, What &amp; How", "selftext": "Given a sentence, what's the state of the art method to extract the \"who\" \"what\" and \"how\"?   \nWhether the technique is extractive or abstractive, based on transformers or POS tagging, please drop few words below!", "upvote_ratio": 0.5, "id": "t3_txt0h0", "created_utc": 1649270370.0}
{"sub": "LanguageTechnology", "title": "Auto generated text based on many conditions", "selftext": "Hi,\n\nI am new to language processing. I have to do some research on a specific use cases:\n\nThe input values are small product descriptions e.g. and a category e.g. \"beverage\":\n\n* COCA COLA LIGHT 20x0,33L GLAS\n\nOutput:\n\n* Coca Cola Light 0,33L\n\nBased on the product category there are many conditions as how the output has to look like (e.g. the input contains the word \"vegan\" it has to be the first word in the output etc.). What are some keywords for me to start researching? I came across models like \" GPT-3\", but from what I understood these are more for natural long text creations and not for outputs based on many conditions?\n\n&amp;#x200B;\n\nSo what I am looking for is to get some keywords to guide me in the right direction\n\nthanks guys", "upvote_ratio": 1.0, "id": "t3_txs7ul", "created_utc": 1649268196.0}
{"sub": "LanguageTechnology", "title": "Methods for measuring inter-word similarity or relatedness", "selftext": "What are common (traditional) methods to estimate the degree of semantic similarity or relatedness between words?\n\nI am looking for a reasonable baseline to compare \"cosine similarity on word embeddings\" with.", "upvote_ratio": 1.0, "id": "t3_txpuw3", "created_utc": 1649261793.0}
{"sub": "LanguageTechnology", "title": "Stanford Researchers Propose \u2018Time Control (TC)\u2019: A Language Model that Uses Stochastic Processes to Enhance the Efficiency and Coherence of Long Text Generation", "selftext": "Writing a few lines is an easy chore for most individuals, but even seasoned authors frequently run into difficulties when trying to construct their second chapter. A similar problem plagues today\u2019s large-scaled pretrained language models, such as GPT-2, which excel at short text production but degrade into incoherence when used for lengthier texts. The incapacity of such models to plan or reflect long-range dynamics might be blamed for the failure to evolve texts from beginning to conclusion correctly.\n\nTo address these challenges, a Stanford University research team introduced Time Control (TC), a language model that implicitly plans using a latent stochastic process and seeks to generate sentences that follow this secret plan. Human assessors scored the outputs 28.6 percent higher than baseline approaches, indicating that the unique strategy enhances performance on long text production.\n\n[Continue Reading](https://www.marktechpost.com/2022/04/06/stanford-researchers-propose-time-control-tc-a-language-model-that-uses-stochastic-processes-to-enhance-the-efficiency-and-coherence-of-long-text-generation/)\n\nPaper: https://arxiv.org/pdf/2203.11370.pdf\n\nGithub: https://github.com/rosewang2008/language\\_modeling\\_via\\_stochastic\\_processes", "upvote_ratio": 1.0, "id": "t3_txnuow", "created_utc": 1649256470.0}
{"sub": "LanguageTechnology", "title": "Word representations with positive weights", "selftext": "The dense vectors usually contain negative and positive values for features. Is there a representation scheme apart from TFIDF and CountVectoriser to positively weight the features for text classification?", "upvote_ratio": 0.67, "id": "t3_txmygu", "created_utc": 1649253920.0}
{"sub": "LanguageTechnology", "title": "An intuitive explanation to Singular Value Decomposition. #MathsForMachineLearning", "selftext": "nan", "upvote_ratio": 0.29, "id": "t3_txil0z", "created_utc": 1649239041.0}
{"sub": "LanguageTechnology", "title": "Researchers From Allen Institute for AI Introduce \u2018MERLOT Reserve\u2019: A Novel Multimodal Video Question Answering Model", "selftext": "We humans navigate the environment using all of our senses. Allen Institute researchers propose MERLOT Reserve, a model that learns to represent videos over time and across several modalities, including audio, subtitles, and video frames. It was trained using a new learning objective and more than 20 million YouTube videos.\n\nMERLOT Reserve is a unique, cutting-edge methodology for solving video-related inquiries. MERLOT Reserve can dependably choose the correct answer from a selection of multiple-choice answers when given a video and a question. This forecast is made by MERLOT Reserve jointly reasoning over the visual frames of the video, the video subtitles, and the audio in the movie.\n\n[Continue reading this cool research update from AI2](https://www.marktechpost.com/2022/04/05/researchers-from-allen-institute-for-ai-introduce-merlot-reserve-a-novel-multimodal-video-question-answering-model/)\n\nPaper: https://arxiv.org/pdf/2201.02639.pdf\n\nDemo: https://merlot-reserve.apps.allenai.org/\n\nProject: https://rowanzellers.com/merlotreserve/\n\nGithub: https://github.com/rowanz/merlot\\_reserve", "upvote_ratio": 1.0, "id": "t3_tx2g8n", "created_utc": 1649185685.0}
{"sub": "LanguageTechnology", "title": "Classifying sections of a document", "selftext": "Hi! I work with clinical documents and am trying to develop a method of section classification. Rules-based methods perform okay, but I'm curious about deep learning methods as well. The only neural approach I've seen is [http://www.oeft.de/su/pdf/specom2018.pdf](http://www.oeft.de/su/pdf/specom2018.pdf), and it only does binary classification of boundaries. \n\n&amp;#x200B;\n\nI was thinking of using a longformer and using token classification modeled after NER. But these would be really long \"entities\". Anybody have any experience/recommendations for this type of problem?", "upvote_ratio": 1.0, "id": "t3_twx1ik", "created_utc": 1649171083.0}
{"sub": "LanguageTechnology", "title": "Query Generation", "selftext": "I'm trying to create a search system that pulls in relevant documents from a database that may be needed to continue writing the next portion of what someone is writing.\n\ni.e. the writing so far: \"I've been living here for 15 years. My home address\"\n\nIn this case, the query should return a document containing my home address to fill this in.\n\nHow could this be done?", "upvote_ratio": 1.0, "id": "t3_twopvs", "created_utc": 1649141134.0}
{"sub": "LanguageTechnology", "title": "Google AI\u2019s Latest 540-Billion Parameter Model (Pathways Language Model Called PaLM) Unlocks New Tasks Proportional To Scale", "selftext": "In recent years, large neural networks trained for language recognition and creation have shown remarkable outcomes in various tasks. GPT-3 demonstrated that large language models (LLMs) could be utilized for few-shot learning and achieve outstanding results without significant task-specific data or model parameter modification. Recent LLMs, including GLaM, LaMDA, Gopher, and Megatron-Turing NLG, have scaled model size, used sparsely activated modules, and trained on larger datasets from more diverse sources to attain state-of-the-art few-shot performance on numerous tasks.\n\nIn a recent [research paper](https://storage.googleapis.com/pathways-language-model/PaLM-paper.pdf), Google researchers introduced [Pathways Language Model (PaLM)](https://storage.googleapis.com/pathways-language-model/PaLM-paper.pdf). PaLM is a 540-billion parameter, dense decoder-only Transformer model learned with the Pathways system that allowed efficient training of a single model across several TPU v4 Pods. PaLM was tested on hundreds of language understanding and generation tasks, and it was discovered that it achieved state-of-the-art few-shot performance across the board, in many cases by a large margin.\n\nRead this summary in a little more [detail Here](https://www.marktechpost.com/2022/04/04/google-ais-latest-540-billion-parameter-model-pathways-language-model-called-palm-unlocks-new-tasks-proportional-to-scale/)\n\nPaper: [https://storage.googleapis.com/pathways-language-model/PaLM-paper.pdf](https://storage.googleapis.com/pathways-language-model/PaLM-paper.pdf)\n\nGoogle blog: https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html", "upvote_ratio": 1.0, "id": "t3_two1h3", "created_utc": 1649138304.0}
{"sub": "LanguageTechnology", "title": "Microsoft Researchers Introduce \u2018Jigsaw\u2019: An AI Tool To Augment Large Language Models (GPT-3, Codex, etc.) By Deploying Post-Processing Techniques That Understand The Programs\u2019 Syntax And Semantics", "selftext": "GPT-3, Codex, and other sizable pre-trained language models can be adjusted to create code from natural language descriptions of programmer intent. Every developer in the world might benefit from these automated models, which have the potential to increase productivity. However, because the models may fail to understand program semantics, the quality of the generated code cannot be guaranteed.\n\nMicrosoft researchers introduce Jigsaw, a new tool that can help these big language models perform better. Jigsaw is a Python Pandas API code generator that accepts multi-modal inputs. Jigsaw uses post-processing techniques to decipher the syntax and semantics of programs and then uses user feedback to improve future performance.\n\n[**Continue Reading**](https://www.marktechpost.com/2022/04/04/microsoft-researchers-introduce-jigsaw-an-ai-tool-to-augment-large-language-models-gpt-3-codex-etc-by-deploying-post-processing-techniques-that-understand-the-programs-syntax-and-se/)\n\nPaper: https://arxiv.org/pdf/2112.02969.pdf\n\nDataset: https://github.com/microsoft/JigsawDataset", "upvote_ratio": 0.96, "id": "t3_tw9158", "created_utc": 1649096448.0}
{"sub": "LanguageTechnology", "title": "Scalable framework lets multiple text-to-speech models coexist", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_tw8nd6", "created_utc": 1649095504.0}
{"sub": "LanguageTechnology", "title": "Run Sparse ONNX models on the DeepSparse Engine", "selftext": "Hey all,  want to share a new revamped README for the open-source DeepSparse Engine repo.\n\nThe code base allows anyone to run sparse ONNX models on top of popular frameworks (e.g. Transformers) for ultra fast speed-ups in inference performance. A few of the highlights include the DeepSparse Server (built on top of FastAPI) to deploy your models and the DeepSparse Benchmark to see how fast your models run.\n\nLet me know what you think and if want to see anything else appear on the repo! \ud83e\uddbe\n\n[https://github.com/neuralmagic/deepsparse](https://github.com/neuralmagic/deepsparse)\n\n&amp;#x200B;", "upvote_ratio": 1.0, "id": "t3_tw6hx0", "created_utc": 1649090300.0}
{"sub": "LanguageTechnology", "title": "Edinburgh NLP taught masters vs cambridge research masters", "selftext": "I was wondering if anyone here has done the Edinburgh masters and has any thoughts on it vs a research masters. Would the taught course be more likely to give me stronger practical skills compared to a more self-directed research programme focusing on a narrower part of NLP? I'm coming from a linguistics background + mostly self-taught on the computational side of things, so going through a more rigorous, organised course is kind of appealing. I think the end goal is going into industry after the masters, not entirely sure if I might want to end up doing a phd - if that affects where I should go. thanks in advance :)", "upvote_ratio": 1.0, "id": "t3_tw5n84", "created_utc": 1649088185.0}
{"sub": "LanguageTechnology", "title": "Natural Language Query (NLQ) in BI: A brief history and comparison", "selftext": "nan", "upvote_ratio": 0.25, "id": "t3_tw2eb6", "created_utc": 1649079941.0}
{"sub": "LanguageTechnology", "title": "Questions regarding Empath module", "selftext": "Hi,\n\nI'm using Empath module ([link](https://www.researchgate.net/publication/301872654_Empath_Understanding_Topic_Signals_in_Large-Scale_Text)) to analyze topics of texts. Is there any way I can check which specific word/ term is categorized into which general big topic (e.g 'violence', 'negative\\_emotion',...) in the Module?\n\nThanks for your help!", "upvote_ratio": 0.81, "id": "t3_tvymcd", "created_utc": 1649067741.0}
{"sub": "LanguageTechnology", "title": "Preparing a dataset for seq2seq transforners", "selftext": "Hello there,\n\nI am sorry for the stupid question. Are there any tutorials on making datasets for BART fine-tuning? I mean, suppose I have a parallel Hungarian-Polish corpus. I split it into one simple text file in Polish,and  the second in Hungarian. How does one transform these into a dataset that is suitable for fine-tuning pre-trained BART model? Are there any notebooks, probably?", "upvote_ratio": 1.0, "id": "t3_tvlbov", "created_utc": 1649023433.0}
{"sub": "LanguageTechnology", "title": "Determine text quality", "selftext": "I very often work with unlabeled and unstructured reviews data. You wouldn\u2019t believe what \u201cadults\u201d will write when they assume the text wouldn\u2019t ever see the light of day. \n\nAnyways, I want to calculate a quality metric in order to identify quality and poor text reviews. My first thought went to perplexity and coherence but if someone has any better ideas feel free to share.", "upvote_ratio": 1.0, "id": "t3_tvtihw", "created_utc": 1649047503.0}
{"sub": "LanguageTechnology", "title": "What are the unsupervised/deep learning methods used for aspect extraction without the use of any seed or candidate words?", "selftext": "There are several methods for AE, but each uses some seeds words which are already pre-defined. Are seed words always necessary for unsupervised methods? What are the properties an aspect has in a sentence for aspect extraction? What are the problems or challenges with the current research in the area of aspect extraction (AE)?\n\nI tried the spacy and text blob library, which are useful for only short texts and have limitations. The research papers I read so far used annotated dataset for the experiments and uses a pre-defined list of aspects and find their synonyms or similar words to other aspects. So, I want to know about any experiments done without the use of these candidate words.\n\nI am trying to start my research in the aspect term extraction (ATE) area. Any resources that can be used as a guideline will be very helpful.\n\n[https://stackoverflow.com/questions/71640984/what-are-the-unsupervised-deep-learning-methods-used-for-aspect-extraction-witho](https://stackoverflow.com/questions/71640984/what-are-the-unsupervised-deep-learning-methods-used-for-aspect-extraction-witho)", "upvote_ratio": 1.0, "id": "t3_tvo3fd", "created_utc": 1649031089.0}
{"sub": "LanguageTechnology", "title": "Twitter Releases \u2018Qurious\u2019 For Next-Generation Data Insights Using Natural Language Queries", "selftext": "Twitter processes over 400 billion events in real-time and generates data on a petabyte (PB) scale. One of the most significant challenges with current data-consumption systems is the requirement for backroom processing. Before consumption, engineers and analysts must build dashboards, reports, and other items. This creates a lower data time value, affecting Twitter\u2019s ability to make timely data-driven decisions.\n\nThe entire cost of obtaining insights from additional traits, features, and dashboards has increased. Current technologies don\u2019t foresee and proactively uncover insights from exabytes of data based on what our internal business customers could find beneficial, resulting in missed opportunities.\n\nMany studies suggest a comprehensive and resilient big data platform\u2019s infrastructure for data processing, storage, and data consumption. We have robust infrastructure across the industry for processing petabytes of data and storing large amounts of data, such as distributed blob stores. However, obtaining timely, meaningful, and actionable insights from these exabyte-scale data systems via dashboards, visualizations, and reports remains non-trivial.\n\nAdvances in natural language processing and machine learning have made it possible to make data consumption from exascale platforms for insights both easy and timely.\u00a0\n\nTwitter has recently released Qurious, a new in-house product that allows internal business customers to ask inquiries in natural language. The product consists of a web app and a Slack chatbot connected to BigQuery and Data QnA APIs. The Slack chatbot was created with node.js and the Express Framework, based on a Google Data QnA reference implementation. They are then offered real-time analytics without having to construct dashboards.\n\n[**Continue Reading**](https://www.marktechpost.com/2022/04/03/twitter-releases-qurious-for-next-generation-data-insights-using-natural-language-queries/)", "upvote_ratio": 0.95, "id": "t3_tvlu44", "created_utc": 1649024757.0}
{"sub": "LanguageTechnology", "title": "Glossary Maker", "selftext": "I'm looking for a tool that can analyze an old book that uses obscure words \u2014 say, Burton's Arabian Nights \u2014 pick out all the obscure words, and make a list of them. A tool that could be used to help create a glossary for a given text. It doesn't need to define the words: only find them. I found a thread here from seven years ago where someone was asking for something like that, but it didn't really help. It included various lists, but no good tool for actually using them.", "upvote_ratio": 1.0, "id": "t3_tvkg4j", "created_utc": 1649021205.0}
{"sub": "LanguageTechnology", "title": "Help me choose between NLP graduate programs", "selftext": "I have a choice between two universities for post graduate study.\n\nA: US uni, rank around 20 globally. 2 year program. I will be like any regular student and will be very broke/in debt.\n\nB: Rank around 100, not in the US (I speak the language of its country). Generous funding with no work/teaching obligations, and the department seems prepared to support me a lot with research. They want me to stay 5 years for a PhD.\n\nI am a US citizen and ultimately hope to have the option to work for US companies one day due to the high salary.\n\nI was going to go with B due to the extensive support, but a few people I know are insisting I'd be destroying my future prospects for mere transient comfort. Arguments include \"rank matters,\" \"US companies won't value a non-US degree,\" and \"PhDs are not worth it.\"\n\nShould I switch to A and suffer through the two rough years?", "upvote_ratio": 0.5, "id": "t3_tv510x", "created_utc": 1648976366.0}
{"sub": "LanguageTechnology", "title": "can NLP professor teach CV?", "selftext": "I'm 1st year in phd  and my interest lies in medicine/bio so i think probably CV fits me\n\nbut prof who's specializing in CV doesn't take me\n\nwhile prof whose specialty is NLP is willing to take me.\n\nCan I go for the latter prof?\n\n&amp;#x200B;\n\n&amp;#x200B;\n\np.s  if my subject is Clustering algorithm, can NLP prof teach me?", "upvote_ratio": 0.43, "id": "t3_tv06bu", "created_utc": 1648957626.0}
{"sub": "LanguageTechnology", "title": "NLP for music generation", "selftext": "Hi everyone! I have been trying to use NLP to generate music. My site is currently using GPT-3's API which is very slow and I would like to train my own model to speed up requests and remove a dependency on someone else's service. I have trained GPT-NEO from scratch for 1M steps on a custom dataset but the results have been... not very good. I am wondering what Transformer models/methods would be best. I am using this project as reference: [https://github.com/chrisdonahue/LakhNES](https://github.com/chrisdonahue/LakhNES)\n\nI have converted a subset of the LAKHMIDI dataset into text (about 17k songs) and am thinking of using that for training from scratch, before fine-tuning on my NES dataset (3k songs). Size is also an issue I have run into. A lot of these models only support 2048 tokens, and songs can be much longer. I'm not sure how to get around this other than using something like GPT-XL. I have used HuggingFace very sparingly and am very new to NLP.\n\nIf you want to see the current song quality, my website is [here](https://www.chiptune.app), keep in mind generation may take a while. Thanks!", "upvote_ratio": 1.0, "id": "t3_tuoc5v", "created_utc": 1648920664.0}
{"sub": "LanguageTechnology", "title": "Real-Time Speech-to-Text in less than 20 lines of Python Code", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_tu4002", "created_utc": 1648853723.0}
{"sub": "LanguageTechnology", "title": "The Token-Dropping Approach Used By ML Researchers From Google and NYU Reduces BERT Pretraining Time And Cost By 25%", "selftext": "The Pretraining of BERT-type large language models, which may scale up to billions of parameters, is essential to achieving best-in-class performance on various natural language processing (NLP) applications. However, the pretraining procedure is costly, and it has become a hurdle for the industrial deployment of big language models.\n\nIn a research paper, researchers from Google, New York University, and the University of Maryland recommend a simple but effective \u201ctoken dropping\u201d method that drastically reduces the pretraining cost of transformer models like BERT while maintaining downstream fine-tuning performance.\n\nToken dropping is a technique for speeding up the pretraining of transformer models like BERT without sacrificing their performance on downstream tasks. Starting with an intermediate layer in the model, they eliminate uninteresting tokens to let the model focus on key tokens more effectively, given its limited computing resources. The model\u2019s last layer then picks up the dropped tokens, producing full-length sequences. They use the built-in masked language modeling (MLM) loss and its dynamics to detect non-essential tokens with little computing complexity. According to their tests, this straightforward strategy decreases BERT\u2019s pretraining cost by 25% while yielding somewhat higher overall fine-tuning performance on conventional downstream tasks.\n\n[Continue Reading The Summary](https://www.marktechpost.com/2022/04/01/the-token-dropping-approach-used-by-ml-researchers-from-google-and-nyu-reduces-bert-pretraining-time-and-cost-by-25/)\n\nPaper: https://arxiv.org/pdf/2203.13240.pdf\n\nGithub: https://github.com/tensorflow/models/tree/master/official/projects/token\\_dropping", "upvote_ratio": 1.0, "id": "t3_tu2gn4", "created_utc": 1648849573.0}
{"sub": "LanguageTechnology", "title": "Topic classification in a dialogue corpora", "selftext": "I have many transcripts of interviews which are meant to be represented in an annotated linguistic corpora. Among other parameters as pos-tagging and for e.g. speech disfluencies annotation I need to tag topics inside each dialogue. I have a list from about 12 topics and I want to use such an algorithm which would detect  and classify topics according to my list. \n\nPart of the corpora is annotated. I tried TF-IDF to extract  key-words for each topic. It worked but I still have no strategy for what do I need to do next. Seems like I have to deal with multi class or multi label classification.\n\n Would much appreciate any advices!", "upvote_ratio": 0.84, "id": "t3_ttw5x3", "created_utc": 1648833533.0}
{"sub": "LanguageTechnology", "title": "Search system at LinkedIn (Research Paper Walkthrough)", "selftext": "This paper from LinkedIn discusses exactly that and tries to answer few interesting questions regarding deploying deep NLP systems in the production environment\n\nInterested to know more? then watch video summary at -\u00a0https://lnkd.in/dap-vQ4N\n\n\n\u23e9 Paper Title: Deep Natural Language Processing for LinkedIn Search Systems\n\u23e9 Paper: https://arxiv.org/abs/2108.08252\n\u23e9 Author: Weiwei Guo, Xiaowei Liu, Sida Wang, Michaeel Kazi, Zhoutong Fu, Huiji Gao, Jun Jia, Liang Zhang, Bo Long\n\u23e9 Organisation: LinkedIn", "upvote_ratio": 1.0, "id": "t3_tts1c2", "created_utc": 1648823330.0}
{"sub": "LanguageTechnology", "title": "Pattern Matching using Entities", "selftext": "I know you can search for patterns in text using Matcher and pos tags in spaCy. But is it possible to search for patterns using entities?\n\nI want to be able to extract phrases such as \"Mary (1990)\", \"Mary and Lily (2000)\", \"University of Reddit (2022)\". So, the patterns should be something like (PERSON, DATE), (ORG, DATE).\n\nWould appreciate some help or direction on how to go about doing this.", "upvote_ratio": 0.72, "id": "t3_ttkt4m", "created_utc": 1648797307.0}
{"sub": "LanguageTechnology", "title": "Pro simplification software", "selftext": "I\u2019m an NLP hobbiest and noob.  I\u2019m looking for a program that will simplify text to a particular grade/Lexie level.  I found some libraries based off of the Newsela corpus, but it is probably beyond me to get it all running.  I\u2019ve found a few websites as well but they are not great.  I\u2019m creating social studies content and my writers are bad at leveling.", "upvote_ratio": 0.67, "id": "t3_ttc760", "created_utc": 1648767733.0}
{"sub": "LanguageTechnology", "title": "Training BERT models on distinct text fields, best approach?", "selftext": "I am using BERT models for text classification. The application is literature classification, where I am utilizing both the title and abstracts of the papers for classification.\n\nI am wondering what is the best way to classify things that have distinct text fields. Currently, I am just using string concatenation to create a long string containing the two fields *e.g.* for a paper with the title: \"whatever the title is\" and abstract:\"this is the abstract\" it becomes \"whatever the title is this is the abstract\". But this loses the context of these fields being distinct. \n\nI have considered prepending the fields with a label *e.g.* \"TITLE: whatever the title is ABSTRACT: this is the abstract\". I feel this may provide more context to the model. But I am wondering if there is a more standard and better way to do this. I am sure a similar problem has been solved for instance with email subject vs. text. Do I just need to train separately on the different fields?", "upvote_ratio": 1.0, "id": "t3_tt4zha", "created_utc": 1648747822.0}
{"sub": "LanguageTechnology", "title": "How to find schools offering a masters in computational linguistics?", "selftext": "Is there a website or something with a master list of schools who offer a masters degree related to computational linguistics? I am just trying to broaden my horizons beyond the obvious top 10 schools before I start applying for grad school here in the next few months. It could be in Europe or North America, not very picky about location. Thank you in advance.", "upvote_ratio": 0.8, "id": "t3_tt4j6d", "created_utc": 1648746605.0}
{"sub": "LanguageTechnology", "title": "DeepL API", "selftext": "Tried using this command from the DeepL API and am getting nothing in response, no error message either.\n\nhttps://www.deepl.com/en/docs-api/\n\ncurl https://api-free.deepl.com/v2/translate \\\n\t-d auth_key=[yourAuthKey] \\\n\t-d \"text=Hello, world!\"  \\\n\t-d \"target_lang=DE\"\n\nI used my auth key but maybe there\u2019s something wrong with it.\n\nMight anyone who has had success with it know what the issue is?\n\nThank you", "upvote_ratio": 0.33, "id": "t3_tt0qwq", "created_utc": 1648736218.0}
{"sub": "LanguageTechnology", "title": "Debug, inspect and run semantic search applications with the txtai console", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_tsyzy8", "created_utc": 1648730890.0}
{"sub": "LanguageTechnology", "title": "Data set: Central Bank Speeches Corpus 1997-2022", "selftext": "Hi, I recently wrote a [paper](https://arxiv.org/abs/2109.10058) where I scraped speeches from central banks. The data set contains speeches held by central bank boards affiliated with the Bank for International Settlements (BIS) (118 institutions) over the period 1997-01-07 to 2022-03-02. I thought I'd share it here if anyone is interested!\n\n- [Github: Code for scraping](https://github.com/HanssonMagnus/scrape_bis)\n- [Kaggle: Data set](https://www.kaggle.com/datasets/magnushansson/central-bank-speeches)\n\nCheers!", "upvote_ratio": 1.0, "id": "t3_tsc9gg", "created_utc": 1648662958.0}
{"sub": "LanguageTechnology", "title": "Research using GPT-3?", "selftext": "Hi, I'm currently working on a project to generate short text using generative models such as GPT3 or BART. I was wondering how research on GPT3 is reported as?  You can't possibly just report how much accuracy your prompts are returning, it doesn't seem like much. Does anyone have any references on GPT3 (or text generation) research is done?", "upvote_ratio": 1.0, "id": "t3_tsc4yr", "created_utc": 1648662629.0}
{"sub": "LanguageTechnology", "title": "Hackernews classified by topic with Huggingface", "selftext": "nan", "upvote_ratio": 0.82, "id": "t3_ts9mp6", "created_utc": 1648655852.0}
{"sub": "LanguageTechnology", "title": "Generative Pseudo Labeling (GPL): Unsupervised Learning in Semantic Search", "selftext": "Hi all, I wanted to share an article I wrote covering [Generative Pseudo-Labeling (GPL)](https://www.pinecone.io/learn/gpl/). It's a really cool technique from Kexin Wang and co (N Reimers' UKPLab) that allows us to adapt sentence transformer models to new domains using nothing more than unlabeled text data. It works by generating synthetic (query, positive, negative) triplets and margin scores, which are then used with margin MSE loss to fine-tune the sentence transformer.\n\nVery interesting technique, if you're working in the space I'd recommend looking into it, I think there will be more work on it in the future too :)", "upvote_ratio": 1.0, "id": "t3_ts6c0h", "created_utc": 1648646533.0}
{"sub": "LanguageTechnology", "title": "BioScope corpus clinical notes, where to find", "selftext": "Hello,\n\nI am trying to find a **subset of the BioScope Corpus**, concretely the **Clinical notes.** This file was hosted in a webpage [http://www.computationalmedicine.org/catalog](http://www.computationalmedicine.org/catalog)\n\nAccording to BioScope [webpage](https://rgai.inf.u-szeged.hu/node/105):\n\n***Clinical free-texts****: The radiology report corpus that was used for the* [*CMC clinical coding challenge*](http://ncc.cchmc.org/prod/pestianlabdata/request.do)*. The negation/hedge annotated version of the corpus can be obtained (due to licencing issues) by downloading the original 'ICD-9-CM coding' corpus from Cincinatti Children's Hospital site and merge it with our annotation:* [*readme*](https://rgai.sed.hu/file/44#overlay-context=)*,* [*merger software*](https://rgai.sed.hu/file/43#overlay-context=)*.*\n\nThe links are dead:\n\n[https://ncc.cchmc.org/prod/pestianlabdata/request.do](https://ncc.cchmc.org/prod/pestianlabdata/request.do)\n\n[http://www.computationalmedicine.org/catalog](http://www.computationalmedicine.org/catalog)\n\n&amp;#x200B;\n\nIs there any available source for getting this corpora, \n\nThanks,", "upvote_ratio": 0.81, "id": "t3_ts33d2", "created_utc": 1648635005.0}
{"sub": "LanguageTechnology", "title": "NLP &amp; Lojban", "selftext": "It surprises me that there seem to be few people doing NLP with Lojban, a constructed language which is supposed to be syntactically unambiguous. \n\nI think it's logical to assume that if someone were successful with NLP with Lojban, it'd be a big breakthrough in NLP, because it would allow computers to better \"understand\" language.\n\nThoughts on this?", "upvote_ratio": 0.75, "id": "t3_ts2fsj", "created_utc": 1648632058.0}
{"sub": "LanguageTechnology", "title": "Need help getting started in Speech Diarization", "selftext": "I want to build a speech-to-text system for audio input having 2 speaker. The goal is to differentiate words spoken by two different speakers.\n\nI haven't worked with audio before. So want to know if there're any deep learning architectures for such task or do I have to do speech processing.", "upvote_ratio": 1.0, "id": "t3_ts1spw", "created_utc": 1648629100.0}
{"sub": "LanguageTechnology", "title": "Can I do more epochs on a saved HuggingFace model?", "selftext": "This can be pretty obvious, so thanks for bearing with me!\n\nLet's say I trained a huggingface model for 3 epochs and then I saved the model. Can I reload the model and do more epochs from where I left off?", "upvote_ratio": 1.0, "id": "t3_tryeav", "created_utc": 1648614980.0}
{"sub": "LanguageTechnology", "title": "AI Podcast: from NLP to Protein-folding", "selftext": "nan", "upvote_ratio": 0.73, "id": "t3_trft3x", "created_utc": 1648582976.0}
{"sub": "LanguageTechnology", "title": "Need help for recommendation system", "selftext": "Problem Statement: Assign a new task to a user based on tasks previously completed by a user.\n\nE.g. Let's assume User A has completed tasks related to \"mix batter\" and User B has completed tasks related to \"bake cake\". Then, whenever there is a task to \"mix batter\" or similar, then it should recommend User A as the correct person to complete that task.\n\nRequire an ML/DL approach.", "upvote_ratio": 0.5, "id": "t3_treo3s", "created_utc": 1648580420.0}
{"sub": "LanguageTechnology", "title": "Struggling with an Extremely Low-Resource ASR", "selftext": "**Background:** I am a Harvard CS+Ling student trying to develop an ASR model for an extremely low-resource language: North-Eastern Neo-Aramaic. The [dataset I'm using](https://nena.ames.cam.ac.uk/audio/185/) is \\~40 minutes. Each recording is 1 - 11 minutes long and the transcriptions are in a nearly-IPA-like alphabet. The transcriptions are not phonetically aligned.\n\nI am using CTC loss to train a [generic deep learning model](https://www.assemblyai.com/blog/end-to-end-speech-recognition-pytorch/) as a baseline and it is NaN-ing out. I think it's because the transcriptions are not exact and leave out repeated phrases and imperfections of the speaker? Regardless, I've given up and am trying to learn the model with a phonetically aligned Arabic dataset then transfer learn.\n\n**Help:** The [Arabic dataset](http://en.arabicspeechcorpus.com/) is phonetically aligned and \\~240 minutes. I want to train the deep learning model with this data but I don't know how to take advantage of the phoneme time stamps provided in TEXTGRID format. CTC and other Pytorch Audio losses don't seem to include an option to pass in this information. Any help would be so much appreciated.", "upvote_ratio": 0.86, "id": "t3_tqyv6i", "created_utc": 1648557664.0}
{"sub": "LanguageTechnology", "title": "New to Spacy and NLP: NUM pos tag for non Number", "selftext": "I'm running spacy ('es_core_news_sm') on the following spanish sentence: '\u00bf Seguro que quer\u00e9is dejarnos tan pronto ?' and get the POS tag of NUM for the token 'quer\u00e9is'.\n\nThis seems so wrong that I'm not sure what to make of it - what steps can I take to improve this tag? Is there something really basic I'm not doing but should be?", "upvote_ratio": 0.86, "id": "t3_tqxbff", "created_utc": 1648552069.0}
{"sub": "LanguageTechnology", "title": "Computer setup recommendations", "selftext": "I\u2019m in academia (in LT obviously) and have a couple of grants that I need to spend this year. Some is earmarked for conferences, but a huge chunk is for hardware. \n\nI currently have a 27\u201d 4k display and a MacBook Pro from 2021 but not M1. I\u2019ve been considering the new Mac studio and adding an ultra wide curved 34\u201d-49\u201d 4k display. My budget is around 7-10k USD for hardware, but I certainly don\u2019t have to spend all of it on hardware and could use any leftover for a research assistant for a few months. I do use Google Colab quite a bit so I\u2019m wondering if the Mac studio would be overkill. I don\u2019t mind a Linux setup but as I use my iPad Pro a lot when creating teaching videos I\u2019m thinking a Mac addition might be more seamless. \n\nMainly the setup would be used for Python, some R, machine learning applications including language model creation, but also for creating teaching videos and such. I always have a ton of windows and programs open so RAM and screen space are a must.\n\nI\u2019d love to hear thoughts, suggestions, recommendations, and caveats regarding setups.", "upvote_ratio": 1.0, "id": "t3_tqrhd2", "created_utc": 1648527372.0}
{"sub": "LanguageTechnology", "title": "Need help finding the right approach", "selftext": "Hey everyone. I'm Lukas a MD student and I'm working on a anamnesis app, which should recognize, which symptom the doctor is asking for. For example \"Do you have pain when peeing?\" should be labeled as asking for dysuria. Could you point me in the right direction, on how to train a model for that?\n\nThanks for the help in advance!", "upvote_ratio": 1.0, "id": "t3_tqj21d", "created_utc": 1648500702.0}
{"sub": "LanguageTechnology", "title": "Spotify Employs Natural Language Search/Semantic Search For Podcast Episodes", "selftext": "Users don\u2019t always input the precise words they are searching for. This requires search algorithms to compensate using fuzzy matching, normalization, and even manual aliases. While these strategies are extremely beneficial to the user, they have limitations in that they cannot capture all possible ways of expressing yourself in natural language, particularly when employing natural language sentences.\n\nUntil recently, Spotify\u2019s search was primarily based on phrase matching. For example, if a user searches for \u201celectric vehicles climate impact,\u201d Elasticsearch will return search results. This returned result includes everything in its indexed metadata that contains each of the query words. However, such results do not guarantee that the relevant material for this query will be returned to the user.\n\n[Continue Reading](https://www.marktechpost.com/2022/03/27/spotify-employs-natural-language-search-semantic-search-for-podcast-episodes/)", "upvote_ratio": 0.6, "id": "t3_tq3e2k", "created_utc": 1648450516.0}
{"sub": "LanguageTechnology", "title": "Is it possible to measure the intensity of emotion in text using BERT?", "selftext": "Hello there, I'm a newbie and still learning NLP, so correct me if I am wrong! It will be a huge help.\n\nI'm trying to measure the intensity of anger and/or fear in e-petition data, then analyze whether the intensity of emotion affects the number of signatures.\n\nSince I could not find a quality emotion dictionary in my language, I have to create one myself.\n\nInitially, I was going to use word2vec and make a list of words closest to \"anger\" and \"fear\", then do the usual dictionary-based sentiment analysis (tagging the intensity of emotion of each petition based on the frequency of word appearances).\n\nThen I found out BERT has become the SOTA nowadays, and now I am wondering whether I should use BERT in my case.\n\nFrom what I have read, BERT seems to be used for classifying emotion, not measuring the intensity of emotion. But since I am not an expert in this field, I wanted to make sure and ask smarter people to be sure!\n\n**TL; DR: Is it possible to measure the intensity of emotion in text using BERT?**\n\nThank you so much in advance!", "upvote_ratio": 1.0, "id": "t3_tpzv2y", "created_utc": 1648436939.0}
{"sub": "LanguageTechnology", "title": "GloVe Question", "selftext": "Hi all! I\u2019m trying to use visual studio to run GloVe but it can\u2019t run the demo shell script\u2026 What does anyone here use to run GloVe? Or, is there any resources for someone who doesn\u2019t know much about programming to follow along with? Thanks.", "upvote_ratio": 0.4, "id": "t3_tpq4a8", "created_utc": 1648406299.0}
{"sub": "LanguageTechnology", "title": "Help with finding a master's thesis topic", "selftext": "Hi.\n\nI'm having a hard time finding a topic for my final thesis. I've been overwhelmed by different areas, approaches, models etc. I don't want to end up with an overdone topic, but I'm also not aiming for something revolutionary that I might struggle with working on and finishing.\n\nI started searching for a good dataset, as well as reading research papers, to hopefully find an intersection. There's always twitter data, but I still need to find an analysis or prediction that's not been overdone, or has been done but would be interesting to see with XYZ models.\n\nI would really appreciate any advice or help to at least narrow it down to an area with a dataset and then think about models or additional steps I could add (eg data augmentation myb?).\n\nThanks!", "upvote_ratio": 0.91, "id": "t3_tppznu", "created_utc": 1648405926.0}
{"sub": "LanguageTechnology", "title": "List of Negative words for financial data", "selftext": "Does somebody know any resource for a list of Negative words from a finance perspective? Even domain agnostic words would work, but the list should be holistic.\n\nI already am using [this](https://ptrckprry.com/course/ssd/data/negative-words.txt) but it lacks words like \n- miss\n- discrepancy\n- disappear\n- delete\n- duplication\n\nI am trying to find negative phrases in a document and trying to increase Recall\n\nThanks!", "upvote_ratio": 1.0, "id": "t3_tplxog", "created_utc": 1648393978.0}
{"sub": "LanguageTechnology", "title": "Few-shot NER", "selftext": "https://youtu.be/DnQKOuG-I_0\n\nFew-shot NER is the task of making work named entity recognition (NER) systems when a small number of in-domain labeled data is available.\n\nIn this video, I discuss in details the inner workings of \"Concise Concepts\" python library that facilitates the FSL NER. \ud83e\udd29", "upvote_ratio": 0.96, "id": "t3_tpijok", "created_utc": 1648382630.0}
{"sub": "LanguageTechnology", "title": "Microsoft Enhances its Translator, a Microsoft Azure Cognitive Service, with Z-code Mixture of Experts (MoE) models, to Boost Efficiency and Quality", "selftext": "Recent advancements in machine translation are breaking language barriers and bringing people worldwide together. However, human language is so flexible that MT tasks are considered to be one of the most difficult artificial intelligence projects.\n\nMicrosoft researchers have been working on enhancing existing AI methods to build multilingual, large-scale language models that can be used in multiple settings. Their current work significantly improves the quality of production translation models by adopting a more holistic, human-centric approach to learning and understanding. \n\nThey have recently released a new service named Translator, which employs Z-code Mixture of Experts models supporting the creation of AI systems that can speak, see, hear, and understand.\n\n**Continue Reading This Article** [**Here**](https://www.marktechpost.com/2022/03/26/microsoft-enhances-its-translator-a-microsoft-azure-cognitive-service-with-z-code-mixture-of-experts-moe-models-to-boost-efficiency-and-quality/)\n\nPaper: https://arxiv.org/pdf/2109.10465.pdf\n\nGithub: https://github.com/microsoft/DeepSpeed", "upvote_ratio": 0.9, "id": "t3_tp4uz7", "created_utc": 1648331003.0}
{"sub": "LanguageTechnology", "title": "Using Reddit commentary to derive consumer insights", "selftext": "nan", "upvote_ratio": 0.81, "id": "t3_tp3ugx", "created_utc": 1648327918.0}
{"sub": "LanguageTechnology", "title": "Universities for Masters In NLP or CL", "selftext": "Hi, I am a junior currently pursuing my undergrad in Computer Science. I've developed an interest and fascination for machine/deep learning, specifically natural language processing. I've enjoyed doing personal projects, participating in hacks, and working on research projects, where I could utilize the technology for creating impactful projects on NLP for social good. NLP for social good is something I'm very passionate about, and as my undergraduate studies come to an end, I wish to pursue a more formal and specialized course for NLP (for social good, if possible). However, I'm unable to find as many courses with a course plan for the same.\n\nSo I wanted to ask you guys if anyone knows any good universities which offer a good research-based NLP program (for social good, if possible). I wish to pursue this subject from an academic point of view, but I'll gladly listen to all suggestions, irrespective of their research orientation. For the location, I'm willing to relocate internationally (USA, Europe, Canada, Australia, etc.) for my courses.\n\nLooking forward to your suggestions!", "upvote_ratio": 1.0, "id": "t3_toj56o", "created_utc": 1648278696.0}
{"sub": "LanguageTechnology", "title": "Generating Human-like Deep Questions and Fact Verification using NLP Techniques", "selftext": "nan", "upvote_ratio": 0.89, "id": "t3_tnkqgo", "created_utc": 1648210943.0}
{"sub": "LanguageTechnology", "title": "Explainability for Semantic Search", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_tn72pw", "created_utc": 1648160228.0}
{"sub": "LanguageTechnology", "title": "Researchers From UF Health and NVIDIA Build World\u2019s Largest Clinical Language Generator, \u2018SynGatorTron\u2019, To Develop Better AI For Rare Disease Research and Clinical Trials", "selftext": "A neural network that generates synthetic clinical data is in high demand, as it is a valuable resource that researchers may use to train other AI models in healthcare. Synthetic data allows developers to create simulated patient data free of sensitive protected health information and manufactured with statistical realism. Team NVIDIA and UF Health, The University of Florida\u2019s academic health institution, have teamed up to do precisely that.\n\nS[ynGatorTron](https://blogs.nvidia.com/blog/2022/03/22/uf-health-syngatortron-ai-synthetic-clinical-data/) is a language model developed by the team that can construct synthetic patient profiles based on the health records it has learned from. This model is ranked #1 among the language generators available in the healthcare field, with the ability to handle 5 billion parameters.\n\n[**Continue Reading**](https://www.marktechpost.com/2022/03/24/researchers-from-uf-health-and-nvidia-build-worlds-largest-clinical-language-generator-syngatortron-to-develop-better-ai-for-rare-disease-research-and-clinical-trials%ef%bf%bc/)", "upvote_ratio": 1.0, "id": "t3_tmnvxx", "created_utc": 1648141334.0}
{"sub": "LanguageTechnology", "title": "CU Boulder vs Penn State for Natural Language processing research", "selftext": "Hello All, I have got admits from CU Boulder and Penn State (UP) for MS in CS. My research interests are in NLP.  Could anyone tell me how the two colleges compare against each other ? My goal is to pursue Ph.D after MS (in NLP) and later work in industry in NLP-based roles.\n\nKindly advise on this.", "upvote_ratio": 1.0, "id": "t3_tmgrjs", "created_utc": 1648135401.0}
{"sub": "LanguageTechnology", "title": "Few-shot NER: entity extraction without annotation and training based on GPT", "selftext": "Hello all,\n\nAfter 1 year working extensively with GPT models (GPT-3, GPT-J, and GPT-NeoX), I think I now have a good view on what these NLP models are capable of. It appears that many traditional NLP tasks can  now be achieved thanks to these large language models thanks to few-shot learning (aka \"prompting\", or \"prompt engineering\").\n\nNER is a very good candidate because, thanks to these models, it is possible to extract any type of entity without ever annotating and training a new model. Annotation has always been a challenge that has caused many entity extraction projects to simply fail, because it is a long and tedious process.\n\nIn this article, I'm showing how easy it is to perform NER thanks to GPT and few-shot learning, without any annotation process: [https://nlpcloud.io/few-shot-ner-entity-extraction-without-annotation-training-based-on-gpt.html](https://nlpcloud.io/few-shot-ner-entity-extraction-without-annotation-training-based-on-gpt.html?utm_source=reddit&amp;utm_campaign=fe5u8885-ed8e-21eb-ba80-5242ac13d5ja)\n\nIf you also experimented with entity extraction with GPT models, I would love to hear your thoughts. Are you, like I am, impressed by the results? And do you think it means that annotation is a thing from the past?\n\nThanks!", "upvote_ratio": 0.93, "id": "t3_tm3z4z", "created_utc": 1648122006.0}
{"sub": "LanguageTechnology", "title": "Is my laptop good enough?", "selftext": "Hello,\n\nI am new here in the subreddit, so, Hello!\n\nI'll be taking NLP and Python courses in a Translation Technologies MA program.\n\nHere are the courses if you would like to take a closer look. I will be attending to Ghent University for my 1st year, so the relevant list of courses is 1.4\n\n([https://studiekiezer.ugent.be/master-of-arts-in-technology-for-translating-and-interpreting-en/programma/2022](https://studiekiezer.ugent.be/master-of-arts-in-technology-for-translating-and-interpreting-en/programma/2022))\n\nI have a macbook air 2020 i3 - when I asked them about the required specs for the coursei they said it was fine.\n\nHowever, I was thinking of upgrading my laptop lately and I was considering a new apple silicon macbook.\n\nI don't know what is the python situation with the new M1 machines.\n\nI did some research but I couldn't be sure since I am new to computational linguistics.\n\nMy friend told me, he is an AI programmer and he graduated from Kent UK, that I would be using cloud computing platforms and unix-based machines such as macbooks offer a better experience in this regard.\n\nMy girlfriend also confirmed this, she did her MA in microbiology and used cloud computing for data science.\n\nIn the end, I want a bigger screen and I feel the i3 will slow me down.\n\nAre M1's good for this stuff or should I stick to intel.\n\nAlso, maybe I can get a second hand 15\" MacBook Pro.\n\nIf that is a viable option which generation of CPUs should I stay away from?", "upvote_ratio": 0.5, "id": "t3_tlzqsk", "created_utc": 1648104438.0}
{"sub": "LanguageTechnology", "title": "Laptop reccomendation to do NLP project with combination of Text Mining and ML?", "selftext": "Hi guys, I apologize if this topic isn't allowed here. I've researched everywhere but no one has any clue. I recently find out that my uni requires student to use a laptop with 32GB RAM and 500GB memory at least. I'm studying Text Mining. \n\nWe are going to run NLP project, load and save many data using python and jupiter notebook. I have a laptop with 4GB RAM and it has many issues with importing NLTK package back then. Not sure if it's even capable to load or save data for data analysis projects. \n\nBut when I look up, laptop with 32GB RAM isn't common and they are expensive. Do you think my old laptop would be able to handle uni work and does a laptop with 16GB RAM be enough replacement instead? And what would you reccomend?\n\nBudget range: 700-1000 euro\n\nI found Yoga Slim 7 PRO 14-Ryzen7 to be a match but I'm not fan of Lenovo. Mac is an option but the most affordable one only has 8GB RAM and 200GB in space.", "upvote_ratio": 1.0, "id": "t3_tlvxoq", "created_utc": 1648090389.0}
{"sub": "LanguageTechnology", "title": "Open Relation Extraction is difficult", "selftext": "Hi everyone,\n\nI am currently trying to figure out what kind of relation extraction task / approach would be best for  the following problem:\n\nGiven a sentence, I want to extract triples (named entity 1, relation, named entity 2). The named entity extraction step is taken care of, so now I only need the relation. So the question becomes: What should the relation look like / What relations am I looking for?\n\n\"Classical\" supervised Relation Extraction is mostly focused on n classes (Born\\_In, Located\\_In etc.) and because of that too restricted, IMO. At the same time 96 different relations (DocRed Dataset) would capture a lot of information as well.A supervised approach will also fail to find new/unseen relations. Zero Shot Adaptability could prove handy here and I have seen models performing reasonably good but the model would not be able to just generate a new relation if it encounters something unseen.\n\nOn the other side of the spectrum, we have Open Information Extraction (OpenIE) or Open Relation Extraction (OpenRE) which focuses on extracting \"any\" relation and these relations would probably be represented by the verbs in the sentence.I like the idea of that because all the information would be captured (high recall) but there would probably be a lot of noise. The problem here is that I want to transfer the triples into a knowledge graph and so I would need a mapping for each extracted Relation to its corresponding knowledge graph relation. I read about Canonicalization but I am not too familiar with the concept.\n\nI am leaning towards OpenIE / OpenRE, but I am having difficulties finding relevant papers and the SOTA models.  OpenIE6 and  M2OIE seem to be the newest / best ones. But the BenchIE dataset reports them to be worse than ClausIE, a model that extracts VP-mediated facts from 2013 ([source](https://arxiv.org/pdf/2109.06850.pdf))  and the Github implementation is quite bad tbh.Has anyone worked with OpenIE models before? What are your experiences?\n\nIs anyone aware of any other approaches that are worth taking a look at? I am open to any remotely connected idea or approach to the problem because I am kind of losing my mind over finding something good.\n\nOtherwise, I will spend the next year implementing my own OpenIE/OpenRE system. (/s hopefully)  \n\n\nEDIT: I will probably use AllenNLP for the OpenIE task. It seems way faster than CoreNLPs Model.\n\n&amp;#x200B;", "upvote_ratio": 0.92, "id": "t3_tl0o1p", "created_utc": 1648054101.0}
{"sub": "LanguageTechnology", "title": "Check if a detected sentence is complete?", "selftext": "For my NLP project I build my own model to identify sentences in a PDF document. Now I would like to check if my extracted sentences are complete sentences. Is there a simple but reliable approach or library that can do this?", "upvote_ratio": 1.0, "id": "t3_tkzzrj", "created_utc": 1648053431.0}
{"sub": "LanguageTechnology", "title": "What kind of technology should I use to get pronunciation, pos on top of tokenization?", "selftext": "New to ML, please bear with me if I am missing something obvious. I am writing a convertor that parses Cantonese/Chinese sentence, turning them from a simple string to objects that describe their POS and pronunciation correctly.\n\nI am using a hashmap based bi-directional maximal segment algorithm right now, it can segment correctly, but it cannot solve the ambiguity of pos and romanization, which would depends on characters before the one with ambiguity.\n\nExample:\n\n    Example Input: \u98df\u98ef\u67b1\u597d\u9577\n    \n    Expected output: [\n    { word: \"\u98df\u98ef\u67b1\", pos: \"noun\", romanization: \"sik6faan6toi2\" },\n     { word: \"\u597d\", pos: \"adv\", romanization: \"hou2\" },\n     { word: \"\u9577\",\n     // should be \"adj\",\n     because \"\u597d\" is used before pos: \"adj/verb\",\n     // should be \"coeng4\", because \"\u597d\" is used before\n      romanization: \"zoeng2/coeng4\"\n    }]\n\nIf I want to handle this, can HMM or CRF(if not, what other algorithm should I be using?) handle this? Because based on the example I can found online, it seems like they can only handle label of a single facet(segmentation). Can it handle segmentation, pos and romanization at the same time? What option do I have?", "upvote_ratio": 1.0, "id": "t3_tkv51h", "created_utc": 1648045612.0}
{"sub": "LanguageTechnology", "title": "Retrieval Augmented Generation (RAG) explained", "selftext": "In this video, we explain RAG or Retrieval Augmented Generation. We discuss **how to do Open-QA using a generative QA model**. We discuss in detail both formulations mentioned in the paper, RAG Sequence Model and RAG Token Model. We cover the mathematical formulations and how they are done in code.\n\nThe video is part 4 of 8 video series on Open Domain Question Answering. If you are interested in Open-QA or want to know more about it do check out the playlist on \"Open-Domain Question  Answering\" on the channel.\n\nI will really appreciate any feedback. Thanks.\n\n[https://www.youtube.com/watch?v=G-AV-kU6qbk](https://www.youtube.com/watch?v=G-AV-kU6qbk)", "upvote_ratio": 0.81, "id": "t3_tktsxz", "created_utc": 1648041700.0}
{"sub": "LanguageTechnology", "title": "Reduce a set of rules", "selftext": "Hy guys. I am working on my final project and i got a bit stuck. I manage to generate a set of about 4000 if rules ( e.q. If a &gt; 23 and b &lt;3 then c). I am trying to find a way to reduce the number of rules to get more general ones. Any idea or framework to help would be greatly appreciated. Thanks in advance guys :3.", "upvote_ratio": 1.0, "id": "t3_tksx4p", "created_utc": 1648038875.0}
{"sub": "LanguageTechnology", "title": "Deep Learning on Electronic Medical Records is doomed to fail", "selftext": "nan", "upvote_ratio": 0.82, "id": "t3_tke72a", "created_utc": 1647986024.0}
{"sub": "LanguageTechnology", "title": "Researchers From China Propose A New Pre-trained Language Model Called \u2018PERT\u2019 For Natural Language Understanding (NLU)", "selftext": "PLMs (pre-trained language models) have excelled at a variety of natural language processing (NLP) tasks. Auto-encoding and auto-regressive PLMs are the most common classifications based on their training processes. The Bidirectional Encoder Representations from Transformers (BERT), which models the input text through deep transformer layers and creates deep contextualized representations, is a representative work of auto-encoding PLM. The Generative Pre-training (GPT) model is a good example of auto-regressive PLM.\n\nThe masked language model is the most common pre-training job for auto-encoding PLM (MLM). The goal of the MLM pre-training job is to recover a few input tokens in the vocabulary space by replacing them with masking tokens (i.e., \\[MASK\\]). MLM has a simple formulation, yet it can represent the contextual information around the masked token, which is akin to word2vec\u2019s continuous bag-of-words (CBOW).\n\nContinue Reading The Full Research Summary [**Here**](https://www.marktechpost.com/2022/03/22/researchers-from-china-propose-a-new-pre-trained-language-model-called-pert-for-natural-language-understanding-nlu/) \n\nor \n\nyou can also read the paper [**here**](https://arxiv.org/pdf/2203.06906v1.pdf)", "upvote_ratio": 0.77, "id": "t3_tke5vh", "created_utc": 1647985933.0}
{"sub": "LanguageTechnology", "title": "BERT Information Extraction on unseen documents", "selftext": "Hi everyone, I am currently working on a project in order to extract data out of legal papers.\n\nMy approach (so far) was training a Q&amp;A BERT as the set of questions I have for each document is fixed. One thing I am confused about at the moment is, by training my model with these questions, I can not get answers on solely new unseen documents, am I right?\n\nI wanted to train the model the nature of questions and \"what to look for\" and then use this knowledge on new unseen documents in order to find similar answers in the new document.\n\nAny clarification is highly welcomed, also feel free to roast my approach, I am happy for any feedback :))", "upvote_ratio": 1.0, "id": "t3_tjzogj", "created_utc": 1647943979.0}
{"sub": "LanguageTechnology", "title": "Weighting embedding similarity by frequency/saliency", "selftext": "It seems like the new standard in search is sentence or document similarity, using things like BERT sentence embeddings. However, these don't really have a way to consider the salience of sentences, which can make it hard to compare different searches.\n\nFor example, when using [concept embeddings](https://github.com/cambridgeltl/sapbert), I'd like to be able to score \"Exam\" &lt;-&gt; \"Exam\" as less important than \"Diabetes\" &lt;-&gt; \"High blood sugar\". But obviously, the former has a similarity of 1.\n\nI've tried using weighting with inverse document frequency of terms. But with the latter being unbounded, it's really hard to figure out how to weight similarity and frequency scores. It's also just not sophisticated enough to handle synonymy and such.\n\nHas anybody come up with any solutions to this?", "upvote_ratio": 1.0, "id": "t3_tjp1d8", "created_utc": 1647904965.0}
{"sub": "LanguageTechnology", "title": "Do i always need to finetune hugging face models?", "selftext": "I am using models as is to try classification,\n\nUnfortunately result for phrase \"i do not like you\" is \\[0.49945366 0.50054634\\] meaning that 49.99% if thinks it's negative and 50.01% it thinks it's positive.\n\nAll tutorials i saw doo pretraining first on IMDb. Do i need to do that? I thought it's already pretrained and i only need to finetune it.\n\n    tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n    model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n    text = \"i do not like you\"\n    encoded_input = tokenizer(text, return_tensors='tf')\n    output = model(encoded_input)\n    res = tf.nn.softmax(output.logits, axis=1).numpy()\n    print(res)\n\nEdit: Looks like that model is expected to be finetuned. I used \"[**distilbert-base-uncased-finetuned-sst-2-english**](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)**\"** model which is finetuned for sentiment analysis and it produces good results. ", "upvote_ratio": 1.0, "id": "t3_tjjtr7", "created_utc": 1647890895.0}
{"sub": "LanguageTechnology", "title": "OpenAI Releases New Version of GPT-3 and Codex That Can Edit or Insert Content Into Existing Text", "selftext": "Text processing is a common task in many machine learning applications. These tasks deal with large amounts of text to conduct classification or translation, which necessitates a lot of back-end labor. It\u2019s challenging to turn text into something that an algorithm can understand.\n\nThe OpenAI team has launched new versions of GPT-3 and Codex that can update or insert stuff into it. Instead of only completing the existing text, [OpenAI API](https://openai.com/api/) can now be used to alter existing content. This includes rewriting a paragraph of text or reforming code, thanks to these additional capabilities. The new work has opened up new possibilities while improving existing ones; for example, insertion is currently being tested in GitHub Copilot, promising early results.\n\n[**Continue Reading Our Summary**](https://www.marktechpost.com/2022/03/21/openai-releases-new-version-of-gpt-3-and-codex-that-can-edit-or-insert-content-into-existing-text%ef%bf%bc/)", "upvote_ratio": 1.0, "id": "t3_tjgdqu", "created_utc": 1647881790.0}
{"sub": "LanguageTechnology", "title": "Need help in extracting relationships and values", "selftext": "I have a dataset consisting of words like \n\n\\- The salary of this person is $10K.\n\n\\- The revenue is $100K.\n\n\\- The loss is 50K.\n\n&amp;#x200B;\n\nThese are some of the easier examples, I wanna extract the word and the numeric values its refeering to like \n\n\\- salary: $10K\n\n\\- revenue: $100K and so on...\n\n&amp;#x200B;\n\nIs anyone aware of such techniques ? I need to do this for analysis of these numbers.", "upvote_ratio": 0.81, "id": "t3_tjd7oh", "created_utc": 1647873303.0}
{"sub": "LanguageTechnology", "title": "Contribution statements in research papers", "selftext": "What is an effective algorithm (rule-based) which can detect contributing statements in a research paper? Without necessarily diving into the nlp/nlu aspect I was wondering if anyone can help me come up with an efficient way to detect contributing statements in a paper. I think the reason why this is proving to be difficult is because I am unable to fully understand how to identify contributing statements so any explanation in that direction would help too.   \n\n\nIncase any specific context is required, please ask and I will go ahead and update this post.   \n\n\n**Reference :**  \nSemeval 2021 task 11: [https://ncg-task.github.io/](https://ncg-task.github.io/)", "upvote_ratio": 1.0, "id": "t3_tjbtlg", "created_utc": 1647869260.0}
{"sub": "LanguageTechnology", "title": "Useful Tools and Programs list for NLP", "selftext": "nan", "upvote_ratio": 0.89, "id": "t3_tiolcm", "created_utc": 1647793143.0}
{"sub": "LanguageTechnology", "title": "Extracting topic from single document from list of possible topics", "selftext": "I'm trying to extract the 'primary' topic from a single document where only words in a predefined list of topics are eligible. I.E., out of all of the words in this list, which word is \"most relevant\" or most likely to be the topic of the document. \n\nI hope this question makes sense, as I'm new to NLP.\n\nBonus points if it's easily implementable in python.\n\nAny resources or guidance would be greatly appreciated.\n\nThanks!", "upvote_ratio": 1.0, "id": "t3_timoa1", "created_utc": 1647787721.0}
{"sub": "LanguageTechnology", "title": "Is it possible to train a model to understand English and then fine tune it for a specific purpose?", "selftext": "I want to start this project to learn more about NLP but I want to know if it is also possible to do so. I want to create a recipe generator in my native language (not english) but the dataset that I have for it is really small (max like 40k recipes) so I was thinking of teaching the model how to understand my native language first and then fine tune it using the dataset that I have for the final target purpose that is predicting a recipe and the steps given some recipe ingredients. Thanks in advance if you're reading this.", "upvote_ratio": 0.93, "id": "t3_tijs5i", "created_utc": 1647778545.0}
{"sub": "LanguageTechnology", "title": "Code + Data for Paper (AAAI 2022): Multiple-Source Domain Adaptation via Coordinated Domain Encoders and Paired Classifiers", "selftext": "You can find the paper here:\n\n[https://arxiv.org/abs/2201.11870](https://arxiv.org/abs/2201.11870)\n\nAnd the code and the data here:\n\n[https://github.com/p-karisani/CEPC](https://github.com/p-karisani/CEPC)", "upvote_ratio": 1.0, "id": "t3_thyy8s", "created_utc": 1647707516.0}
{"sub": "LanguageTechnology", "title": "Paraphraser tool", "selftext": "Hi everyone, I need your suggestions on a tool project to paraphrase articles.\n\nHere is what I want to do.\n\n\\-Input: text/URL (500-10.000 words per article)\n\n\\-Output: Paraphrased text/article\n\nI googled it and  found some proposed solutions which seems ok for my purpose, for example using Parrot Paraphraser:\n\n[https://github.com/PrithivirajDamodaran/Parrot\\_Paraphraser](https://github.com/PrithivirajDamodaran/Parrot_Paraphraser)\n\n[https://huggingface.co/prithivida](https://huggingface.co/prithivida)\n\n[https://www.youtube.com/watch?v=C6gBcL9sAIw](https://www.youtube.com/watch?v=C6gBcL9sAIw)\n\nOr using an API:\n\n[https://healthytechguy.medium.com/i-built-a-paraphrasing-tool-that-can-rewrite-text-and-made-it-opensource-3833e1b93c07](https://healthytechguy.medium.com/i-built-a-paraphrasing-tool-that-can-rewrite-text-and-made-it-opensource-3833e1b93c07)\\]([https://healthytechguy.medium.com/i-built-a-paraphrasing-tool-that-can-rewrite-text-and-made-it-opensource-3833e1b93c07](https://healthytechguy.medium.com/i-built-a-paraphrasing-tool-that-can-rewrite-text-and-made-it-opensource-3833e1b93c07))\n\n[https://rapidapi.com/healthytechguy/api/paraphrasing-tool1/](https://rapidapi.com/healthytechguy/api/paraphrasing-tool1/)\n\n\\-I do not want an expensive tool coded from scratch\n\n\\-I do not want to use paid tools\n\n\\-I want paraphrased text to be readable (no loss of meaning)\n\n\\-Without grammatical errors\n\n\\-With proper punctuation\n\n&amp;#x200B;\n\nI guess I need to find  existing trained models for every niche my articles in or train my models from scratch? I am not sure...\n\nBut, I am not technical to judge whether those proposed solutions is good for my purpose or not.\n\nExpert suggestions would be highly appreciated.\n\n&amp;#x200B;\n\nThanks in advance.", "upvote_ratio": 0.81, "id": "t3_thyb5o", "created_utc": 1647705766.0}
{"sub": "LanguageTechnology", "title": "Are these scores ok to get published in ACL workshop (RepL4NLP)", "selftext": "I got score 2.5/2.5/2.0 on my short paper. They were very impressed with my results but they are unsure where the improvement comes from.  Even though the metareview says that the paper needs a lot of work for the paper to be publishable, the comments from the reviewers can be straightforwardly addressed in the comments for committment to the workshop. **With that score, how likely will my paper be accepted to that workshop RepL4NLP?**\n\n**Also, does is a paper related to quantization and/or pruning a good fit for that workshop (link below)?** The third point in \"Topics\" section kinda convince me it is.\n\n[https://sites.google.com/view/repl4nlp2022/](https://sites.google.com/view/repl4nlp2022/)", "upvote_ratio": 1.0, "id": "t3_thpca3", "created_utc": 1647671636.0}
{"sub": "LanguageTechnology", "title": "need some help in our collage \ud83e\udd1e project its 80% is completed", "selftext": " \n\nthis is an NLP and python based project we are trying to achieve something new this project is almost there but the only file connecting thing is leftover\ud83d\ude0c\n\nplease dm me for more information/collaboration\ud83d\ude0a\n\nwe are open to welcoming you to this project**\ud83e\udd17**\n\n**not a paid work\ud83d\ude44**", "upvote_ratio": 0.14, "id": "t3_thco9t", "created_utc": 1647630679.0}
{"sub": "LanguageTechnology", "title": "Need Help Choosing a Language Model for a Script Generating AI", "selftext": "Hello all, I'm a Computer Science major senior whos just finished a Natural Language Processing class. basically I want to build a very specific kind of text generator to create Star Trek (and potentially other) scripts based off scripts from the different shows. Basically what I'm looking for is  \n\n\nGrammar: Perfect, or at least as perfect as possible  \nStructure: There is a clear story/plot that can be followed  \nStory: It makes absolutely no sense and is hilarious.  \n\n\nBasically I'm looking for something like this: [https://geektyrant.com/news/an-ai-bot-writes-a-hilarious-episode-of-star-trek-the-next-generation](https://geektyrant.com/news/an-ai-bot-writes-a-hilarious-episode-of-star-trek-the-next-generation) or this: [https://twitter.com/keatonpatti/status/1014220692936589312?lang=en](https://twitter.com/keatonpatti/status/1014220692936589312?lang=en) or a personal favorite, Harry Potter and the Portrait of What Looked Like a Large Pile of Ash ( [https://botnik.org/content/harry-potter.html](https://botnik.org/content/harry-potter.html) )  \n\n\nI already have the scripts for several series in the form of:  \nTitle: \\[TITLE\\]  \nStardate: \\[DATE\\]  \nOriginal Airdate: \\[DATE\\]  \n\\[some newlines then the entire script in a standard format, then a few more newlines\\]  \n(Roll Credits)  \n\n\nFor my NLP class I've already build an AI which generates DnD spells using GPT2, and I have the skills to format/tokenize the scripts however I need to to train a model. So basically right now I'm in the market for a good model I can use. Ideally something that's not to hard to get working (or is at least very well documented to get it working without much headache), can run locally on Linux or Windows 10 (been having problems with Google Collab's hardware limitations/timeouts and I don't want to pay for Pro), and get the kind of plots I want. GPT2 seems like it might be good from example's I've seen, but our DnD spells don't really have that comedy level I'm looking for or see in the examples. But maybe that's just because magic is inherently a bit nonsensical and weird. I also don't particularly care about the time it take to train a model as I can just leave it running overnight.  \n\n\nI also wouldn't mind any tips in general for this project. For example with our DnD one we couldn't find a way to get it to generate 1 spell, so instead we tell it to generate X characters and just hope there's a spell in there and we trim it down. So some model that could generate until \"(Roll Credits)\" or that also learns the length of a document would be great. I have all the scripts as separate .txt files right now, but the only method I know of training a model involves combining them into one document. But with my DnD model I would have problems where sometimes the tags I used to start/end spell components got generated when they shouldn't and ruined the text. I'm worried this might also happen if I combine all the scripts into one document. So a perfect model would maybe read all the files in a specific folder and take stuff like the length and how they start/end into account?  \n\n\nFinally if GPT2 really is a great model for what I want, some suggestions on good training parameters would be appreciated. Actually for any model this would probably be helpful. As much as I took an NLP class, it wasn't exactly the best class I've ever had and while I understand a decent amount of how it all works messing with stuff like that would pretty much be stabbing in the dark for me.", "upvote_ratio": 0.9, "id": "t3_tguqyi", "created_utc": 1647583875.0}
{"sub": "LanguageTechnology", "title": "Question about unsupervised fasttext learning", "selftext": "Hi,\n\nI'm not sure if this is the right place but I thought I'd ask. I'm trying to implement fasttext (or something similar) myself to try and go beyond some simple DL models. I think there are really two tasks, the supervised one to classify and the unsupervised to get embeddings like with word2vec.\n\nI think the classification one is pretty simple. You get the embeddings of all the words/subwords in a sentence, average them and use that to compare to the class.(like in the old Keras example here: [https://github.com/keras-team/keras/blob/keras-1/examples/imdb\\_lstm.py](https://github.com/keras-team/keras/blob/keras-1/examples/imdb_lstm.py))\n\nI am not entirely sure about the unsupervised learning though. Is it essentially like skipgram? I've been trying to build my own to make sure I get it... is the only difference now that the target vector is actually the sum of the vector for the word and subwords?\n\nUsing the Keras code below as a basis (from [https://www.kaggle.com/kesarianubhav/skipgram-word2vec](https://www.kaggle.com/kesarianubhav/skipgram-word2vec)) is   the difference that my target embeddings (w) are now multiple ones that I would sum after the \"Dot\" layer (essentially the loss function)? It seems almost too simple!\n\n    dim_embedddings = 128\n    \n    # inputs\n    w_inputs = Input(shape=(1, ), dtype='int32')\n    w = Embedding(V, dim_embedddings)(w_inputs)\n    \n    # context\n    c_inputs = Input(shape=(1, ), dtype='int32')\n    c  = Embedding(V, dim_embedddings)(c_inputs)\n    o = Dot(axes=2)([w, c])\n    o = Reshape((1,), input_shape=(1, 1))(o)\n    o = Activation('sigmoid')(o)\n    \n    SkipGram = Model(inputs=[w_inputs, c_inputs], outputs=o)\n    SkipGram.summary()\n    SkipGram.compile(loss='binary_crossentropy', optimizer='adam')\n\nAlso, as an aside - if I was limited in memory could I use a common embedding layer?\n\nThanks!", "upvote_ratio": 0.81, "id": "t3_tgoog3", "created_utc": 1647563412.0}
{"sub": "LanguageTechnology", "title": "Domain-specific pre-training of GPT? Help!", "selftext": " \n\nI am looking to adapt GPT-2 to generate dialogue utterances in the style of a certain demography/population (let's call them Ogres). However, there are no large datasets that are both 1) dialogue datasets, and 2) are generated by this target demography.  \nIn the absence of such data, I have been considering a few approaches for data augmentation purposes. Many of those approaches would benefit from a GPT-Ogre, which is at least capable of generating text similar to Ogres, if not necessarily dialogic.  \nApproach 1  \n**==========**  \nFor this, I am considering performing additional pre-training of, say, GPT-2 on some medium-sized corpora generated by Ogres. This sounds like something that should have been done by a lot of people for a lot of different things by now, but except for some papers that have tried to do this with BERT in the Medical domain, I was not able to find any papers/GitHub repos that have done this with additional unsupervised pre-training GPT.  \nIt would be helpful if someone could point me to some resources around this as I feel the space of hyperparameters to figure out the best learning rate, etc. is too large, and if somebody has already done this, it would be easy to replicate it.  \nApproach 2  \n**==========**  \nThere are some dialogue-specific GPT models such as DialoGPT that have been fine-tuned (in a supervised way; mind you, not pretrained in an unsupervised way). However, it is not in the Ogre style. I am wondering if it's a ridiculous idea to perform additional pre-training of a fine-tuned GPT-2 model?", "upvote_ratio": 0.81, "id": "t3_tghp2v", "created_utc": 1647544080.0}
{"sub": "LanguageTechnology", "title": "Classification and Zero Shot Learning in dataframe", "selftext": " Hello. I already know how to use Zero-Shot-Learning. I would like to know how do I make it apply sentiment classification in dataframe in pandas.", "upvote_ratio": 0.67, "id": "t3_tgbmri", "created_utc": 1647527685.0}
{"sub": "LanguageTechnology", "title": "Multilingual NLP: how to perform NLP in non-English languages", "selftext": "Hello,\n\nNLP has made great progress these last years but the main focus is on the English language (understandably). I think that many people are trying to do NLP in non-English languages but are disappointed by the results. It is especially hard with text generation models like GPT-3, GPT-J, GPT-NeoX...\n\nIn this article, I'm trying to quickly summarize what the options are today for people trying to perform multilingual NLP:\n\n[https://nlpcloud.io/multilingual-nlp-how-to-perform-nlp-in-non-english-languages.html](https://nlpcloud.io/multilingual-nlp-how-to-perform-nlp-in-non-english-languages.html?utm_source=reddit&amp;utm_campaign=le5u8885-ed8e-11eb-ba80-5242ac13d5jv)\n\nIf you can think of additional solutions not mentioned in this article please let me know!", "upvote_ratio": 0.97, "id": "t3_tg82du", "created_utc": 1647516631.0}
{"sub": "LanguageTechnology", "title": "Topic Modeling: Use SVD &amp; NMF in Python to Find Topics in Text", "selftext": "nan", "upvote_ratio": 0.92, "id": "t3_tg489o", "created_utc": 1647500534.0}
{"sub": "LanguageTechnology", "title": "Explanation video of how to do Open-QA using ORQA formulation", "selftext": "Hi, in this video, I explain ORQA which uses a retriever to find the right context from the entire Wikipedia and then uses an extractive QA model to give a final answer. We discuss the task setup, architecture, and loss function.\n\nThe video is part of 8 video series on Open-QA, how it is different from normal QA, the difference in loss formulations, and key papers on different Open-QA architectures.\n\nI will really appreciate any feedback. Thanks.\n\n[https://www.youtube.com/watch?v=9bL2VbwZ9G8](https://www.youtube.com/watch?v=9bL2VbwZ9G8)", "upvote_ratio": 0.88, "id": "t3_tfh9zu", "created_utc": 1647436068.0}
{"sub": "LanguageTechnology", "title": "Eigendecomposition appears repeatedly in machine learning, sometimes as the key step of the learning algorithm itself. This video intuitively explains the maths behind one of the most important topics in linear algebra - Eigendecomposition. #MathsforMachineLearning", "selftext": "nan", "upvote_ratio": 0.69, "id": "t3_tf61s1", "created_utc": 1647394457.0}
{"sub": "LanguageTechnology", "title": "Relation clustering between entities", "selftext": "Given two SVO triplets (U.S, is a, Country) and (U.S, is name of, Country). The relations between two entities U.S. and Country in two triplets represent 'is-a' relation (I believe its fair to say that). What methods exist to cluster such relations e.g. clustering 'is-a' and 'is name of' in same cluster? Any papers that point out to this direction, any resources would be super helpful. Thank you in advance.", "upvote_ratio": 1.0, "id": "t3_tf56ow", "created_utc": 1647391827.0}
{"sub": "LanguageTechnology", "title": "How to find different approaches for a NLP task within the industry?", "selftext": "Hi\n\nI got my first job in NLP and am tasked with doing a in-house search engine, that could find various products and product details in a collection of documents.\n\nI would like to have an overview of how others have tackled this and what the current state of the art is within the industry( not necessary in academia, since approaches  used there  often  cannot be deployed in industry, due to limitations on inference, memory  etc)\n\nWhat is a good way to  do competitor analysis/ find out how others have approached a task in NLP?\n\nFor example in Business Analytics different consultancy houses like McKinsey offer a good insight. Is there something similar for NLP?", "upvote_ratio": 1.0, "id": "t3_tev94m", "created_utc": 1647367250.0}
{"sub": "LanguageTechnology", "title": "Request for an alternative to 'Annotations' app for OSX", "selftext": "Hi, \n\nI'm not quite sure if this is the appropriate subreddit for this request, but I am searching for an application compatible with OSX that has the same functionality of [https://www.annotationsapp.com](https://www.annotationsapp.com), which stopped receiving updates in 2018. It's an app that allows you to highlight and annotate text documents in a simple user interface not unlike the Notes app. \n\nI do close readings of short fiction and perform analyses of the stylistic and narratological aspects of the texts that require me to make 100s of annotations. I frequently tag the same lines of text with different labels or annotations.\n\nIf anyone could direct me to an alternative application or another subreddit community that can offer advice, I would be eternally grateful.", "upvote_ratio": 1.0, "id": "t3_tes2py", "created_utc": 1647359459.0}
{"sub": "LanguageTechnology", "title": "What linguistic theories are most widely underpinning semantic analysis techniques today?", "selftext": "I\u2019d like to try making a custom algorithm (over a long period of time) for semantic analysis. Mostly for fun and learning but if it ends up a viable option for my work, then hey, bonus.\n\nWhat theoretical frameworks are most common in NLP today? I\u2019m tempted to look to Structuralism given that computers work fundamentally on binary logic, but I remember reading somewhere that Chomskian Linguistics was the go to for most computer based semantic processing research for most of the latter half of the 20th century.\n\nAnyone know which linguistic theories hold the most water or have the most practical use in NLP today?", "upvote_ratio": 0.95, "id": "t3_tersx7", "created_utc": 1647358721.0}
{"sub": "LanguageTechnology", "title": "MSc. Program in Natural Language Processing Universit\u00e9 de Lorraine, Nancy (France)", "selftext": "Did somebody start the 2nd year of the MSc. Program in Natural Language  Processing Universit\u00e9 de Lorraine, Nancy (France) without attending the 1st year? Could you tell me what courses you had to take in 1 semester of the 2nd year? Do you have to catch up a lot?", "upvote_ratio": 1.0, "id": "t3_te2cbm", "created_utc": 1647277588.0}
{"sub": "LanguageTechnology", "title": "Entity Extraction with Large GPT Models", "selftext": "nan", "upvote_ratio": 0.75, "id": "t3_tdzgwy", "created_utc": 1647270072.0}
{"sub": "LanguageTechnology", "title": "HealthPrompt: A Zero-shot Learning Paradigm for Clinical Natural Language Processing (Paper Summary)", "selftext": "HealthPrompt is a novel Zero-Shot Learning(ZSL) clinical NLP framework applied the paradigm of prompt-based learning on clinical texts. \n\nZero-Shot Learning(ZSL) refers to the use of deep learning models to classify instances from new classes without having seen any training data for those classes.\n\nThe authors show that prompts effectively capture the context of clinical texts and perform remarkably well without any training data.\n\nPaper summary at https://youtu.be/1SJm6Zr5yAU\nPaper link at https://arxiv.org/pdf/2203.05061v1.pdf", "upvote_ratio": 1.0, "id": "t3_tdxsxy", "created_utc": 1647265398.0}
{"sub": "LanguageTechnology", "title": "Master's degree in Computational Linguistics?", "selftext": "Hi everyone. I know close to nothing about Computational Linguistics, but a professor gave a presentation about this Master's degree and I was kinda interested. It looks to me like a very specific degree with niche job opportunities. Is it worth the effort? And could you recommend me sources to know this topic better? Thanks", "upvote_ratio": 0.88, "id": "t3_td8xqf", "created_utc": 1647184355.0}
{"sub": "LanguageTechnology", "title": "Confused about what my goal is in terms of recall &amp; precision", "selftext": "So I'm studying a linguistic pattern and want to automatically compare its frequency in different parts of a corpus. The desired insight should be along the lines of \"it appears significantly more often in this part than in the other\". It can be detected in a classic binary fashion: a list of candidates can be extracted and each candidate either is or isn't relevant (true/false positive/negative).\n\nI wonder how well a model must perform for the comparison to be valid.\n\nA randomly guessing model naturally gets a recall of around 0.5 and a precision that's around equal to the proportion of Positives in the dataset \u2013 is that correct so far?\n\nMy question then is: What should I aim for so that my comparison holds water? What is more important for its validity, precision or recall?\n\nMy intuition says that if the model performs only a little better than randomly guessing, I should analyze lots of data from each part of the corpus. In this case, the corpus might not be large enough. And if the model performs reasonably well (not sure what values that would be), less data can be used. A perfect model, therefore, needs little data to get reliable results.\n\nFollowing my intuition, even a bad model would be enough to get an insight about the distribution of that linguistic pattern, provided that the corpus is huge. I should still analyze some samples to see if there is a bias but basically: If the model is better than random, I just need enough data to compensate for the model's bad performance. I'm still not sure what to prioritize though, precision or recall.\n\nI feel like maybe this is a complex topic with lots of literature about it, anyone have a good resource for me to read?", "upvote_ratio": 0.86, "id": "t3_td72sa", "created_utc": 1647178627.0}
{"sub": "LanguageTechnology", "title": "[D] Will Attention Based Architecture / Transformers Take Over Artificial Intelligence?", "selftext": "A [well popularized article](https://www.quantamagazine.org/will-transformers-take-over-artificial-intelligence-20220310) in Quanta magazine ask the question \u00ab Will Transformers Take Over Artificial Intelligence? \u00bb.  Since having revolutionized NLP, attention is conquering computer vision and reinforcement learning. I find pretty unfortunate that the  attention mechanism  was totally eclipsed by Transformers which is just a funny name (animation  movie/ toy) for self-attention architecture, although the Google's paper title on Transformers was \u00ab[Attention is all you need](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\u00bb.", "upvote_ratio": 0.63, "id": "t3_td1ybd", "created_utc": 1647157783.0}
{"sub": "LanguageTechnology", "title": "Best Sentence Rephrasing Libraries Python NLP / NLU", "selftext": "Hi team, \n\n&amp;#x200B;\n\nWhich tools are seen as best in industry for rephrasing sentences whilst retaining the intent of the sentence?\n\n&amp;#x200B;\n\nI'm looking for the best libraries to rephrase paragraphs of text in Python. \n\n&amp;#x200B;\n\nSo far I have identified 12 tools and tested two in Natural Language Understanding (NLU) over [here](https://huggingface.co/models?pipeline_tag=text2text-generation&amp;search=paraphrase). From the list I have compared Pegasus and Parrot for text rephrasing. Pegasus had a better vocabulary however Parrot was more aligned to the **intent** of each sentence was and was selected for the use case. \n\n&amp;#x200B;\n\nSimilar to CNN's for object recognition, Naive Bayes for Categorical Text Classification etc.\n\n&amp;#x200B;\n\nAre there best known solution as the best sentence rephraser in industry at 2022? \n\n&amp;#x200B;\n\n\\* Outside of a pure NLTK library naturally.", "upvote_ratio": 0.92, "id": "t3_tcg9cl", "created_utc": 1647091227.0}
{"sub": "LanguageTechnology", "title": "Machine learning generated Regex", "selftext": "Has anybody tried generating extremely complicated Regex or other rule systems for some linguistic parsing operation line sentence boundary detection or syntax parsing?\n\nLike creating something explicit but leveraging AI to make something far more sophisticated than a human would make, but taking out the blackbox of a neural network handling the entire task.\n\nThanks very much", "upvote_ratio": 0.85, "id": "t3_tcfnvj", "created_utc": 1647089093.0}
{"sub": "LanguageTechnology", "title": "Resources for social listening (preferably but not limited to Spanish)", "selftext": "Hi, guys! I'm getting into social listening for personal projects and for a new job, and I wanted to get input from anyone kind enough to provide it.\n\nI have experience with data preprocessing, topic modeling with LDA and NMF, data visualization, different types of vectorizers... Basically the fundamentals of NLP for this area. But I'm trying to dive deeper, so I would greatly appreciate if you could share any resources you may have. Bonus points for resources of how to work in Spanish!\n\nBooks, articles, guides, videos, GitHub repositores, anything you can recommend or share will be greatly appreciated.\n\nThanks a lot and enjoy your weekend!", "upvote_ratio": 1.0, "id": "t3_tc8z4k", "created_utc": 1647061476.0}
{"sub": "LanguageTechnology", "title": "Sentence classification problem", "selftext": "Hello!\n\nI'm looking to solve a multi label text classification problem but I don't really know how to formulate it correctly so I can look it up.. Here is my problem :\n\nSay I have the document \"`I want to learn NLP. I can do that by reading NLP books or watching tutorials on the internet. That would help me find a job in NLP`.\"\n\nI want to classify the sentences to 3 labels (for example) *objective*, *method* and *result*. The result would be :\n\n`objective : I want to learn NLP`\n\n`method : I can do that by reading NLP books or watching tutorials on the internet.`\n\n`result : That would help me find a job.`\n\nAs you would have noticed, it's not a classical classification problem, since the classification here depends on the document structure (unless I'm wrong?)\n\nAny idea of the key words to better describe the problem ? or how I might solve it ?\n\nMany thanks!", "upvote_ratio": 0.81, "id": "t3_tbxxfx", "created_utc": 1647026633.0}
{"sub": "LanguageTechnology", "title": "Researchers From the University of Hamburg Propose A Machine Learning Model, Called \u2018LipSound2\u2019, That Directly Predicts Speech Representations From Raw Pixels", "selftext": "The purpose of the paper presented in this article is to reconstruct speech only based on sequences of images of talking people. The generation of speech from silent videos can be used for many applications: for instance, silent visual input methods used in public environments for privacy protection or understanding speech in surveillance videos.\n\nThe main challenge in speech reconstruction from visual information is that human speech is produced not only through observable mouth and face movements but also through lips, tongue, and internal organs like vocal cords. Furthermore, it is hard to visually distinguish phonemes like \u2018v\u2019 and \u2018f\u2019 only through mouth and face movements.\u00a0\n\nThis paper leverages the natural co-occurrence of audio and video streams to pre-train a video-to-audio speech reconstruction model through self-supervision.\n\n[**Continue Reading my Summary on this Paper**](https://www.marktechpost.com/2022/03/11/researchers-from-the-university-of-hamburg-propose-a-machine-learning-model-called-lipsound2-that-directly-predicts-speech-representations-from-raw-pixels/)\n\nPaper: [https://arxiv.org/pdf/2112.04748.pdf](https://arxiv.org/pdf/2112.04748.pdf)", "upvote_ratio": 0.88, "id": "t3_tbx1j8", "created_utc": 1647024153.0}
{"sub": "LanguageTechnology", "title": "[Resource] How to Install Kaldi (Automatically)", "selftext": "Hey guys,\n\nI was working with **Kaldi** about a month ago for the first time, and I found the installation process really tricky, so I created an [**automated Kaldi installation script**](https://www.assemblyai.com/blog/kaldi-install-for-dummies/) to take care of it in 3 lines of code.\n\nI also lay out steps for a **manual installation** if anyone prefers that, but I thought I'd drop this tutorial in here for anyone struggling with Kaldi!", "upvote_ratio": 1.0, "id": "t3_tbu3ro", "created_utc": 1647016776.0}
{"sub": "LanguageTechnology", "title": "Snorkel weak labeling for NER. When a token does not fall under any of the class labels or abstains in NER?", "selftext": "I have a program that labels a sequence of words using ontologies. I have labeling functions for class negative, class positive, and class abstain. Should I keep the word unlabeled if it does not fall under any of these class labels and ignore it or should I force-label them under either class negative or abstain? I will be grateful for any hints or help.\n\n&amp;#x200B;\n\n&amp;#x200B;", "upvote_ratio": 0.9, "id": "t3_tbrfn8", "created_utc": 1647009267.0}
{"sub": "LanguageTechnology", "title": "One Human Language | Noam Chomsky", "selftext": "nan", "upvote_ratio": 0.38, "id": "t3_tb3p23", "created_utc": 1646932844.0}
{"sub": "LanguageTechnology", "title": "Comparing accuracy of two sentence similarity algirithm.", "selftext": "Hi, we were to implememt sentence semantic similarity matching algorithm. basically what we were supposed to do is to given database of about 300k sentence, we have to find 10 most similar sentence to the query sentence given by the user.\n\nThe approach we tried:\nwe created embedding vector from the sentence and created annoy index, to do approximate nearest neighbour  search, so every time user puts query, we create sentence embedding of that query sentence (using same technique we used create embeddings of all question in database) and find its approximate neighbours\n\nThe problem:\nsay we used two  methods to create sentence embeddings to see which one gives better result, lets say method A gives good similarity matching but method B is slightly not good as method A ( verified by human inspection) How do i compare which one is better mathematically? like which result have better accuracy.", "upvote_ratio": 0.93, "id": "t3_tb2brd", "created_utc": 1646929266.0}
{"sub": "LanguageTechnology", "title": "txtai 4.3 released - machine learning pipelines as SQL", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_tan05g", "created_utc": 1646875449.0}
{"sub": "LanguageTechnology", "title": "Open-source massively multilingual speech recognizer", "selftext": "nan", "upvote_ratio": 0.76, "id": "t3_tahdri", "created_utc": 1646859365.0}
{"sub": "LanguageTechnology", "title": "15 Datasets for Word Segmentation on the Hugging Face Hub", "selftext": "nan", "upvote_ratio": 0.93, "id": "t3_tafr58", "created_utc": 1646854849.0}
{"sub": "LanguageTechnology", "title": "Brat annotation: How to add URL links to each annotations", "selftext": "I have added some annotations to my file using brat annotation tool. The annotations display some text. I need to add URL links to each annotation text. How can I add URL links to annotations in brat annotation tool. Is there any configuration to do that?\n\n[https://i.stack.imgur.com/pII3d.png](https://i.stack.imgur.com/pII3d.png)", "upvote_ratio": 1.0, "id": "t3_ta3rqu", "created_utc": 1646816511.0}
{"sub": "LanguageTechnology", "title": "Story Ending selection between two choices - How to create a model for it", "selftext": "Hello everyone, I am new to NLP and am working on a problem where users the model needs to return the correct ending of the two (either by giving it a 0 or a 1) based on the previous 4 sentences. I am using the custom version of the ROCstories datasets and the StoryClozeTest but the general structure is the same.\n\nIn the training set, there are 5 sentences (the last sentence is the ending sentence which fits the previous sentences). Each row/story has a storyID.\n\nIn my testing set, there are 4 story setences and two potential endings (ending 0 and ending 1). Each row/story has a storyID.\n\nI am lost as to how to train the model properly and how to get a prediction from it when it is given the first four setences from the training set. \n\nHere is my Google Colab link - it contains my attempt so far: [https://colab.research.google.com/drive/1biFwBrTfu7ce3hqUmjA2NqzJuUcWo98h](https://colab.research.google.com/drive/1biFwBrTfu7ce3hqUmjA2NqzJuUcWo98h?usp=sharing)\n\n I am not able to upload the data here so please let me know if you would like it.\n\nAny help would be much appreciated!", "upvote_ratio": 0.67, "id": "t3_t9pxh6", "created_utc": 1646770909.0}
{"sub": "LanguageTechnology", "title": "How does one objectively quantify the quality of automatic Text Normalization?", "selftext": "I scraped sentences from the internet that I want to use for Text To speech input. For this purpose, I've done some text normalization. E.g. converting alphanumeric and numeric data in non-ambiguously words. (512 --&gt; five hundred and twelve)\n\nThe purpose of this process of text normalization, however, is not this particular dataset, but to improve the method for automatic text normalization. There will always be manual checking after normalization, but the better the automatic text normalization is, the less corrections have to be made.\n\nWould it be viable to manually normalize a bunch of sentences (that represent much of the variation in the intended text that will have to be normalized) and compare these 'perfectly' normalized sentences to the sentences that were normalized automatically (a duplication of the manual normalized text/sentence would be a 100% automatic normalization score).\n\nIf so, what would be viable metrics next to e.g. minimum edit distance.", "upvote_ratio": 1.0, "id": "t3_t9p261", "created_utc": 1646768543.0}
{"sub": "LanguageTechnology", "title": "How to properly use a custom Trainer from the HuggingFace library?", "selftext": " \n\nDoes anyone know how a custom trainer from the hugging face library is meant to be used ? I have found some code to creat a trainer with a custom loss function. But I get an error saying NameError: name 'self' is not defined  \n . Any ideas how to fix it?\n\nThe code is following:\n\n    class MultilabelTrainer(Trainer):\n        def compute_loss(self, model, inputs, return_outputs=False):\n            labels = inputs.pop(\"labels\")\n            outputs = model(**inputs)\n            logits = outputs.logits\n            loss_fct = torch.nn.BCEWithLogitsLoss()\n            loss = loss_fct(logits.view(-1, self.model.config.num_labels), \n                            labels.float().view(-1, self.model.config.num_labels))\n            return (loss, outputs) if return_outputs else loss\n\nMultilabelTrainer.train()", "upvote_ratio": 1.0, "id": "t3_t9mpn2", "created_utc": 1646762351.0}
{"sub": "LanguageTechnology", "title": "Any good resources to learn BART?", "selftext": "I'm working on a personal project and want to use BART as a generative model. Does anyone know any good resources to learn how to use or fine-tune BART?", "upvote_ratio": 0.81, "id": "t3_t9li4j", "created_utc": 1646759187.0}
{"sub": "LanguageTechnology", "title": "GenQ: Fine-tune models for semantic search with unstructured text data", "selftext": "Hi all, I wrote about a super interesting technique called [GenQ](https://www.pinecone.io/learn/genq) for training models to do semantic search with *nothing more* than unstructured text data.\n\nAt a high level it works by generating synthetic queries for the unstructured text, producing (query, passage) pairs - that we then use to fine-tune a retrieval model.\n\nIn short, it allows us to build models for domains that we simply *could not* build for in the past due to a lack of labeled data. There's a lot of potential for a technique like this.\n\nI hope some of you find it useful, let me know if you have questions, thanks!", "upvote_ratio": 0.95, "id": "t3_t9j24m", "created_utc": 1646752808.0}
{"sub": "LanguageTechnology", "title": "Retraining Stanza on new data", "selftext": "I've been tried to retrain Stanza NER by following the instructions from this page [Stanza - Model Training and Evaluation](https://stanfordnlp.github.io/stanza/training.html#training-with-scripts) and ran the following code after downloading everything from GitHub:\n\n    ~ /stanza train/stanza-train$  source stanza/scripts/config.sh\n    ~ /stanza-train/stanza-train/stanza/stanza/utils/datasets/ner$ python3 prepare_ner_file.py  enp_DE.lft.iob tessmann.json\n    ~ /stanza-train/stanza-train/stanza$ python3 -m stanza.utils.training.run_ner de_tessmann\n\nBut I get the following error\n\n     ValueError: dataset de_tessmann currently not handled \n\nHas anybody been able to successfully retrain Stanza? If so, how?\n\nEdit: at this link [https://stanfordnlp.github.io/stanza/training.html#training-with-scripts](https://stanfordnlp.github.io/stanza/training.html#training-with-scripts) I don't understand what `${corpus}` is supposed to be. A folder containing .json files? Where is it supposed to be?", "upvote_ratio": 1.0, "id": "t3_t9da9i", "created_utc": 1646732545.0}
{"sub": "LanguageTechnology", "title": "Hey Siri|Hey Google - What can/should we expect from personal assistants next versions?", "selftext": "We haven't seen any major breakthroughs on these over the past years, so what kind of advances are *reasonable* the next years? What pieces are missing for a more natural/advanced interaction with personal assistants? Can this tech get stuck for another decade?", "upvote_ratio": 0.92, "id": "t3_t8vfpg", "created_utc": 1646676667.0}
{"sub": "LanguageTechnology", "title": "*ACL Findings vs ACL Workshop", "selftext": "Hey, I'm doing my PhD in NLP and this is the first time I have attempted an ACL (main conference) submission (I have submitted in EACL though), my paper got rejected and I have worked on a revised version. As this year coincided with the ARR system, allowing revised versions of the same paper to be submitted to different venues, I have the following question:   \n\n\nFor a paper with average reviews (3, 2.5, 2.5) and a metareview of 3, acceptance at the main conference is most probably out of the question.   \nHowever, there's a chance that it can be accepted to the NAACL2022 findings or to a relevant ACL2022 workshop.   \nIn that case, what would you prefer and why? My personal opinion is that, while the Findings sounds better, you don't even get to attend a poster session. On the other hand, workshops usually attract less polished papers (although I am not sure in the case of ACL), hence having a mixed reputation.", "upvote_ratio": 0.76, "id": "t3_t8v2za", "created_utc": 1646675800.0}
{"sub": "LanguageTechnology", "title": "Mahalanobis Distance for Sentence Similarity?", "selftext": "Someone recommended using Mahalanobis Distance ([https://en.wikipedia.org/wiki/Mahalanobis\\_distance](https://en.wikipedia.org/wiki/Mahalanobis_distance)) instead of cosine similarity when investigating the similarity of sentence embeddings (e.g., obtianed using [https://www.sbert.net](https://www.sbert.net)).\n\nHowever, I fail to find any source (neither, website, blog or paper) that recommend doing so. Am I missing something or is the idea not really sensible for some reason?", "upvote_ratio": 0.76, "id": "t3_t8pfay", "created_utc": 1646660350.0}
{"sub": "LanguageTechnology", "title": "Accuracy didn't improve at all and loss keeps getting higher when training LSTM model. How could I fix it?", "selftext": "I am training a LSTM model for next word prediction in Indonesian Language. The Dataset used is around 100,000 sentences taken from wikipedia public corpora. Each sentence for the LSTM is tokenized and then sequenced into a maximum length of 9 words, 8 words for the input, and 1 word for the output.\n\n    tokenizer = Tokenizer()\n    \n    tokenizer.fit_on_texts(sentences)\n    total_words = len(tokenizer.word_index) + 1\n    \n    input_sequences = []\n    LENGTH_WORD = 8\n    for line in sentences:\n    \ttoken_list = tokenizer.texts_to_sequences([line])[0]\n    \tfirst_index = 0\n    \tfor i in range(1, len(token_list)):\n    \t\tn_gram_sequence = token_list[first_index:i+1]\n    \t\tif i&gt;=LENGTH_WORD:\n    \t\t\tfirst_index = first_index+1\n    \t\tinput_sequences.append(n_gram_sequence)\n    \t\n    # pad sequences \n    max_sequence_len = max([len(x) for x in input_sequences])\n    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n    \n    # create predictors and label\n    xs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n\nE.g. -&gt; Input Sentence: 'I am eating an apple pie and a fried chicken inside my house' Sequence:\n\n\\['I'\\]\n\n\\['I','am'\\]\n\n......\n\n\\['I','am','eating','an','apple','pie','and','fried'\\] \\['am','eating','an','apple','pie','and','fried','chicken'\\]\n\n....\n\n&amp;#x200B;\n\nThe predictor and the label is then created from the input sequences. Since it is not possible to encode the output using one hot encoding due to the vocabulary size (around 50000 words), I decided to use binary encoding.\n\nThe model code is as below\n\n    model = Sequential()\n    \n    # size of the input dimensions = length of the longest sequence minus 1.\n    # subtract one since last word is used for ys.\n    model.add(Embedding(total_words, 20, input_length=max_sequence_len-1))\n    \n    model.add((LSTM(64)))\n    # Sense layer sized as the total words, because we have as many outputs as the total word count.\n    model.add(Dense(max_length, activation='softmax'))\n    adam = Adam(learning_rate=0.001)\n    \n    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n    print(model.summary())\n    #earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\n    \n    history = model.fit(xs, ys, epochs=100, verbose=1,\n                    callbacks=[checkpoint])\n    #print model.summary()\n    print(model) \n\nThe accuracy keeps getting stuck betweeen 0.038 and 0.041 and the loss always increase in every epoch. I have tried changing the metrics to other accuracy such as categorical\\_accuracy, the LSTM unit, and the embedding output size, but the accuracy never improved at all and the loss keeps skyrocketing. Is there any way to fix this?", "upvote_ratio": 1.0, "id": "t3_t8j8jc", "created_utc": 1646635872.0}
{"sub": "LanguageTechnology", "title": "Here is a cool research by Dartmouth researchers where they proposed a deep learning model for emotion-based modeling of mental disorders using Reddit conversations", "selftext": "According to the World Health Organization (WHO), [mental diseases impact](https://arxiv.org/pdf/2201.09451.pdf) [one ](https://arxiv.org/pdf/2201.09451.pdf)out of every four people at some point in their lives, according to the World Health Organization (WHO). However, due to the social stigma associated with seeking professional care, patients in many parts of the world do not actively seek it. As a result, there is a need to passively use interactions to detect mental problems.\n\nIt is feasible to persuade patients to seek diagnosis and treatment for mental problems through passive (i.e., unprompted) detection. That is precisely what Dartmouth University academics have proposed. The objective is to concentrate on a subgroup of mental illnesses marked by different emotional patterns.\n\nThe research team\u2019s proposed model is entirely based on emotional states and their transitions using conversations on Reddit. Content-based representations, such as language model embeddings, have long been the focus of research in this area.\n\n[**Continue Reading My Summary On This Research**](https://www.marktechpost.com/2022/03/06/dartmouth-researchers-propose-a-deep-learning-model-for-emotion-based-modeling-of-mental-disorders-using-reddit-conversations/)\n\nPaper: https://arxiv.org/pdf/2201.09451.pdf", "upvote_ratio": 0.96, "id": "t3_t84aqq", "created_utc": 1646589423.0}
{"sub": "LanguageTechnology", "title": "[P] Small-Text: Active Learning for Text Classification in Python", "selftext": "nan", "upvote_ratio": 0.67, "id": "t3_t82flg", "created_utc": 1646584320.0}
{"sub": "LanguageTechnology", "title": "zero-shot domain adaptation for machine translation", "selftext": "Hi! My problem is to adapt the translation domain with no extra bilingual corpus (but may have monolingual corpus) of the adapted domain.\n\nFor instance, I have plenty of English-French bilingual pairs in the news domain, and I also have adequate English or French monolingual sentences in the biology domain. My goal is to conduct English-French biology translation system.\n\n&amp;#x200B;\n\nI wonder is there a specific term for this problem? Are there any papers on this subject? Thank you.", "upvote_ratio": 1.0, "id": "t3_t8053r", "created_utc": 1646577739.0}
{"sub": "LanguageTechnology", "title": "Representing multiple words with word embeddings", "selftext": "Hi!\nI have short texts, but they are with different lengths (1-14 words). \nMy goal would be to represent them with word embeddings (fastText). I've tried to use mean, so every text is represented by a mean vector of all the vectors in the text, but there must be a better way. Do you have any idea/resources about this issue? \nThank you very much.\n\nEdit1:  Problem is, those texts are not always sentences (and they are not in English). Those are responses by humans for questions like \"What do you see on this picture\", so sometimes it's longer description, sometime just a single word.", "upvote_ratio": 0.81, "id": "t3_t7y4tm", "created_utc": 1646571148.0}
{"sub": "LanguageTechnology", "title": "Detecting English names without context", "selftext": "Hi. Is it possible to detect if a given string contains a person's name without additional context?\n\nFor example, if the string was a sentence or phrase like \"I gave John Doe my thanks\", then there is enough context to use an NER model. But what if the string only contains \"John Doe\"? In another words, the string is not a sentence or a phrase, but rather just 1-3 nouns with title/uppercase text. Is there an algorithm that can tell if the string is a name without any (or minimal) additional info? The only assumption is that the name is in title or uppercase text.", "upvote_ratio": 1.0, "id": "t3_t7d8fm", "created_utc": 1646498198.0}
{"sub": "LanguageTechnology", "title": "Language technology master\u2019s programs", "selftext": "Hi everyone! I\u2019m still looking at LT master\u2019s programs and I was wondering if there\u2019s any difference between a Master in Arts or a Master of Science when it comes to LT? I\u2019ve seen most programs in the US and the one in Edinburgh are MS while the two I\u2019ve found in Sweden are MA. Thanks!!", "upvote_ratio": 0.8, "id": "t3_t74yw0", "created_utc": 1646468442.0}
{"sub": "LanguageTechnology", "title": "Gadsby - Constrained Text Generation with Transformers", "selftext": "nan", "upvote_ratio": 0.67, "id": "t3_t71nr2", "created_utc": 1646455302.0}
{"sub": "LanguageTechnology", "title": "Reg: Self-studying CS224n", "selftext": "Hello everyone! I'm looking for some help. Has anyone gone through Stanford CS224n (Deep Learning for Natural Language Processing). If yes, could you pls recommend if there's a study group or forum that I can consider joining? I really need some peer support + discussions. Thanks in advance!", "upvote_ratio": 1.0, "id": "t3_t6okot", "created_utc": 1646416046.0}
{"sub": "LanguageTechnology", "title": "Experimental methodology NLP", "selftext": "Hi!\nI have a question regarding the experimental methodology in NLP/ML.\nAre there any methodological recommendations regarding this research method?\nI'm from social science, where this research method has a strong foundation and specific \"guidelines\". Is there something similar in NLP, or is used loosely? Any paper would be appreciated! Thanks!", "upvote_ratio": 1.0, "id": "t3_t6kwcr", "created_utc": 1646406197.0}
{"sub": "LanguageTechnology", "title": "Interesting paper on zero shot classifiers | Metadata-Induced Contrastive Learning for Zero-Shot Multi-Label Text Classification", "selftext": "nan", "upvote_ratio": 0.91, "id": "t3_t6j1ku", "created_utc": 1646400736.0}
{"sub": "LanguageTechnology", "title": "Researchers from Tel Aviv Propose Long-Text NLP Benchmark Called SCROLLS", "selftext": "Although lengthier texts contain a significant quantity of natural language in the wild, NLP benchmarks have always primarily focused on short texts, such as sentences and paragraphs. Short text classification has consistently been a driving force behind standard benchmarks like GLUE, WMT, and SQuAD. A considerable amount of natural language is produced in the context of lengthier discourses, such as books, essays, and meeting transcripts, as is widely known. As a result, model structures are required to address the computing constraints associated with processing such long sequences.\n\nResearchers from Tel-Aviv University, Meta AI, IBM Research, and Allen Institute for AI (AI2) introduce Standardized CompaRison Over Long Language Sequences (SCROLLS) to tackle this issue. SCROLLS is a collection of summarization, question-answering, and natural language inference tasks that span a variety of topics, including literature, science, commerce, and entertainment.\n\n[**Continue Reading My Full Summary On This Paper**](https://www.marktechpost.com/2022/03/03/researchers-from-tel-aviv-propose-long-text-nlp-benchmark-called-scrolls/)\n\nPaper: [https://arxiv.org/pdf/2201.03533.pdf](https://arxiv.org/pdf/2201.03533.pdf)", "upvote_ratio": 0.87, "id": "t3_t6abfi", "created_utc": 1646367326.0}
{"sub": "LanguageTechnology", "title": "Looking for dictionary of common English words", "selftext": "I'm doing some text parsing work in Python and trying to detect and ignore phrases that use common English words.  I've used NLTK and some spell checker modules but the problem is that these recognize too many words, such as names of people and places.  \n\nDoes anyone know of a lighter weight or \"dumber\" dictionary that's readily available?  I can probably build one using word frequency on some publicly available corpus, but trying to avoid that.", "upvote_ratio": 0.67, "id": "t3_t69ogr", "created_utc": 1646365261.0}
{"sub": "LanguageTechnology", "title": "Detecting random character sequences in tabular strings", "selftext": "Hey everyone,\n\nI started a project with regard to analyzing **open-text answers** of surveys. I mainly deal with single words, half-sentences and single sentences, however sometimes people just face-roll over their keyboards (xP) and input an arbitrary character sequence (e.g. jdfaskl, adskfls, etc.). I do know the language, so there is not the problem that these character sequences could have a meaning in other languages.\n\nAmong other problems, I have to detect these sequences and flag them accordingly. While I have some approaches in mind on how to solve them (checking words against dictionaries, detecting recurring character sequences, checking for vowels, etc.), I still struggle to find a good solution for that. Especially because sometimes people use common abbreviations (like MS for Microsoft) and I really would like to first detect random character sequences and in a second step check against common abbreviations.\n\nI do have some experience in this field, but this is my first big project and I would really like to explore options on how to solve this problem thoroughly. \n\nAny hint to how this could be solved is appreciated ;)", "upvote_ratio": 1.0, "id": "t3_t5zrmt", "created_utc": 1646336337.0}
{"sub": "LanguageTechnology", "title": "Vanilla Transformer", "selftext": "Hello, do you guys have any resources for training a vanilla tramsformer model for machine translation? \n\n\nI tried the Tensorflow tutorial(en-pt) , but it seems like I cant get it to work when using a custom dataset from text files. I get confuse with the l[\"setup input pipeline stage](https://www.tensorflow.org/text/tutorials/transformer#setup_input_pipeline)\"\n\ncan someone explain why it has a tokenizers.pt and tokenizers.en and why cant i use a simple text vectorization layer and such?\n\n if any of u guys can help itll be very much appreciated, much thanks!", "upvote_ratio": 1.0, "id": "t3_t5sms8", "created_utc": 1646317264.0}
{"sub": "LanguageTechnology", "title": "Document-Term Matrix in NLP: Count and TF-IDF Scores Explained", "selftext": "nan", "upvote_ratio": 0.64, "id": "t3_t5rskc", "created_utc": 1646314759.0}
{"sub": "LanguageTechnology", "title": "What language is this?", "selftext": "I'm apart of an ARG and I need an idea of what this code/cipher/language is:  \n\n\n796f 2c20 706c 6561 7365 2064 6f6e 2774 2064 656c 6574 6520 6d65\n\n  \nPLEASE HELP!", "upvote_ratio": 0.5, "id": "t3_t5eebi", "created_utc": 1646267021.0}
{"sub": "LanguageTechnology", "title": "Semantic treebank of English Holy Bible?", "selftext": "Hi.  Does anyone know of a version of an English language bible (preferrably Douay-Rheims, but beggars can't be choosers) that has been parsed into a semantic treebank?  I've seen syntactic treebanks of ancient languages (Latin, Greek, etc.) using dependency grammars, but I haven't come across an English bible in a semantic treebank.\n\nAny idea where to look?", "upvote_ratio": 1.0, "id": "t3_t56chi", "created_utc": 1646244821.0}
{"sub": "LanguageTechnology", "title": "[Mozilla + Coqui] Speech Technology Hackathon", "selftext": "nan", "upvote_ratio": 0.75, "id": "t3_t4swgi", "created_utc": 1646199428.0}
{"sub": "LanguageTechnology", "title": "Tutorial: Apply sparsity and quantization to BERT question answering for up to 14x better performance on CPUs", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_t4gbxs", "created_utc": 1646163586.0}
{"sub": "LanguageTechnology", "title": "I am considering a master in Language Technology - advice?", "selftext": "[https://www.gu.se/en/study-gothenburg/master-in-language-technology-one-year-or-two-years-h2mlt](https://www.gu.se/en/study-gothenburg/master-in-language-technology-one-year-or-two-years-h2mlt)\n\n&amp;#x200B;\n\nI am looking at this one specifically. I have a bachelor in languages but I have been working in software for 5 year. This would require me to pause my career for 1-2 years, so I wanna make sure it's the right move. Thoughts?", "upvote_ratio": 1.0, "id": "t3_t4el8r", "created_utc": 1646159112.0}
{"sub": "LanguageTechnology", "title": "Hey Folks, A team of researchers at Meta AI is working on developing language and machine translation capabilities that will cover most of the world\u2019s languages. Their work includes two projects: No Language Left Behind and Universal Speech Translator.", "selftext": "A team of researchers at Meta AI is working on developing language and machine translation capabilities that will cover most of the world\u2019s languages. Their work includes two projects: No Language Left Behind, a new AI model that can be trained to learn languages with fewer examples allowing expert-quality translations in hundreds of languages, from Asturian to Luganda to Urdu. The second project is Universal Speech Translator, which includes unique methods for translating from one language\u2019s speech to another in real-time. This will enable models to support languages that do not have a regular writing system.\n\n**You can read my full summary** [**of this research here**](https://www.marktechpost.com/2022/03/01/meta-ai-introduces-no-language-left-behind-project-an-ai-model-to-support-machine-translation-for-low-resource-languages/) **or even check out** [Meta's Blog](https://ai.facebook.com/blog/teaching-ai-to-translate-100s-of-spoken-and-written-languages-in-real-time)", "upvote_ratio": 0.75, "id": "t3_t4bwt2", "created_utc": 1646152264.0}
{"sub": "LanguageTechnology", "title": "[D] Synthetic data for AI among the 10 Breakthrough Technologies 2022 of the MIT Tech Review", "selftext": "[Synthetic datasets](https://www.technologyreview.com/2022/02/23/1044965/ai-synthetic-data-2/) are computer-generated samples with the same statistical characteristics as the samples from the original dataset. Synthetic datasets are becoming common to train AIs in areas where real data is scarce or too sensitive to use, as in the case of medical records or personal financial data.\u00a0 I was involved in [textual data augmentation](https://arxiv.org/pdf/1812.04718.pdf) for my thesis.", "upvote_ratio": 0.82, "id": "t3_t425fq", "created_utc": 1646118903.0}
{"sub": "LanguageTechnology", "title": "Dropout pre-training vs. fine-tuning", "selftext": "Is it okay to use a higher dropout during fine-tuning than was used during pre-training a transformer? Are there any best practices around this or any related literature?", "upvote_ratio": 1.0, "id": "t3_t41oo6", "created_utc": 1646117123.0}
{"sub": "LanguageTechnology", "title": "Scores in ACL rolling review", "selftext": "I heard that if I want to respond to the reviews then I have to make a resubmission. How fast will they respond to the resubmissions?\u00a0 \n\nSo if I submitted to ACL rolling review 1 month before a targetted conference and I don't get the scores I wanted, I basically kinda miss an opportunity to submit a target conference?\n\nMost conference that I know like Neurips and ICLR, you get to respond to the reviews without making a resubmission and the reviwers will have enough time to read the responses and make changes to the score before the commitment deadline.\n\nWhat's the difference between a \"review\" and a \"meta-review\"?\n\n How bad is 2.5/2.5/2 for NAACL or even COLING?", "upvote_ratio": 1.0, "id": "t3_t3u22d", "created_utc": 1646093370.0}
{"sub": "LanguageTechnology", "title": "WMT22 announced to take place in Abu Dhabi", "selftext": "This year, WMT22 will take place in December [in Abu Dhabi](https://machinetranslate.org/wmt22)!", "upvote_ratio": 0.5, "id": "t3_t3obb0", "created_utc": 1646078208.0}
{"sub": "LanguageTechnology", "title": "Resources for sentiment analysis", "selftext": "What are the resources you have used to learn sentiment analysis?\nIf you could suggest any resources on building a sentiment dictionary that would be great. \n\nThank you for your kind replies.", "upvote_ratio": 1.0, "id": "t3_t3lffl", "created_utc": 1646070814.0}
{"sub": "LanguageTechnology", "title": "Hey folks, here is another cool Natural language research from Meta AI where they introduce Project CAIRaoke and using this project they built an end-to-end neural network-based model that can power much more personal and contextual conversations", "selftext": "The need of the hour is a better conversational AI, not just AI assistants who can\u2019t do more than what has been fed. AI assistants are underwhelming irrespective of whether we interact with them via text or voice. They are easily stumped by a bit of complexity added to the conversation. Imagine how it would be to converse with AI assistants the same way we do regularly, to our people, most naturally and colloquially.\u00a0\n\nResearchers from Meta AI come to save the day with their project CAIRaoke. The team has created an end-to-end brain model capable of considerably more intimate and contextual dialogues than current systems. The researchers have already used the model that evolved from this effort. Portal is what they call the product, and the purpose is to connect it to augmented and immersive virtual devices. This integration would benefit the community because it would allow for more comprehensive, multi-modal interactions with AI helpers.\n\nYou can [**continue reading my summary here**](https://www.marktechpost.com/2022/02/28/meta-ai-introduces-project-cairaoke-an-end-to-end-neural-network-based-model-that-can-power-much-more-personal-and-contextual-conversations-for-the-future-augmented-and-virtual-reality-devices/) or even check out the [Meta AI blog page](https://ai.facebook.com/blog/project-cairaoke)", "upvote_ratio": 0.88, "id": "t3_t3lcd4", "created_utc": 1646070591.0}
{"sub": "LanguageTechnology", "title": "Wish list for a data engineer?", "selftext": "Let's say you were an ML scientist working at Mega Corp FAANG, and you had a team of data engineers working for you\n\nWhat would you ask them to do?\n\nWhat are some of your most common issues that you think the dev tooling and engineering teams could help with?", "upvote_ratio": 1.0, "id": "t3_t3a752", "created_utc": 1646034364.0}
{"sub": "LanguageTechnology", "title": "Google USE versus BERT", "selftext": "Hey folks - first post here! I\u2019ve been reading a lot about different techniques to build chatbots, and I\u2019m struggling to understand how something like a Google Universal Sentence Encoder is related to BERT? I know USE has a transformer based architecture option and basically pretrained embeddings, but BERT seems lower level than that? When would I use each? Is USE simpler than BERT?", "upvote_ratio": 1.0, "id": "t3_t312um", "created_utc": 1646005078.0}
{"sub": "LanguageTechnology", "title": "Hey Folks, Here is a really cool research by Deepmind researchers where they probe image-language transformers and propose SVO probes for verb understanding", "selftext": "Fine-tuning is required for a range of functions performed by multimodal image\u2013language transformers. Researchers worldwide are interested in whether these algorithms can detect verbs or merely use the nouns in a phrase. A dataset of image-sentence pairs with 447 verbs that are either visible or widely encountered in the pretraining data was compiled to perform this task.\u00a0\n\nResearchers from DeepMind propose to evaluate the pretrained models in a zero-shot manner using this dataset. In comparison to other elements of speech, it is found that the pretrained models underperform more in scenarios that demand verb interpretation. An investigation into which verbs are the most difficult for these models is underway. \n\nYou can read this [**full summary here**](https://www.marktechpost.com/2022/02/27/deepmind-researchers-probe-image-language-transformers-and-propose-svo-probes-for-verb-understanding/) But if you are interested in only paper then you [check the paper here](https://arxiv.org/pdf/2106.09141.pdf)", "upvote_ratio": 1.0, "id": "t3_t2tn1x", "created_utc": 1645984326.0}
{"sub": "LanguageTechnology", "title": "Can you help solve a mystery? \"Sync and corrections by n17t01\"", "selftext": "I apologise if this doesn't fit this subreddit, but I can't think where else too look.\n\nI was using the DeepL translator app on Android today and translated the German word \"genauso\" to English, which should translate to \"just the same\". instead I got the response \"Sync and corrections by n17t01\". \n\nOdd. then I searched for that phrase on duckduckgo &amp; Google and it turns up on web pages all over the Internet in nonsensical ways. there must be an explanation. any suggestions?\n\nthanks.", "upvote_ratio": 1.0, "id": "t3_t2bgqj", "created_utc": 1645922110.0}
{"sub": "LanguageTechnology", "title": "Database/Pretrained model for Confident Messages?", "selftext": "Hey y'all! I'm trying to use a ML model to analyze text and give me the confidence level of the text itself. Most places I look for guidance I get confidence intervals of other pretrained models and predictions, but I am looking for a pretrained model (or a database) of text that is confident.\n\nAn example would be \"The answer is X\" as confident vs. \"I'm not entirely sure, but I think the answer is X\" as unconfident.\n\nWould love to see if someone can help point me in the right direction!", "upvote_ratio": 0.88, "id": "t3_t270tc", "created_utc": 1645909379.0}
{"sub": "LanguageTechnology", "title": "Quality metrics for text dataset", "selftext": "Hi guys, i'm Data science student and i'm doing a nlp project. For this, i must measure the quality of my 4 text dataset to understand how the input  influence the model output.\n\nReading various papers and surveys on the similar nlp task, I found the metrics proposed in this work interesting: [https://btw.informatik.uni-rostock.de/download/workshopband/C2-5.pdf](https://btw.informatik.uni-rostock.de/download/workshopband/C2-5.pdf)\n\n&amp;#x200B;\n\nany suggestions? Thanks all.", "upvote_ratio": 1.0, "id": "t3_t1yqiu", "created_utc": 1645886920.0}
{"sub": "LanguageTechnology", "title": "Is \"the\" included in the mention? e.g. \"the Channel Tunnel\", \"the tunnel\"", "selftext": "When we have \n\n&gt; \"the Channel Tunnel\"\n\n&gt; \"the Tower of London\"\n\nis \"the\" included in the span of the mention? The names of the entities are \"Channel Tunnel\" and \"Tower of London\" according to wikipedia, but not prefixing the names with \"the\" would not be grammatical in any context (I think).\n\nThen what about noun phrases? \n\n&gt; \"the dog\"\n\n&gt; \"the tunnel\"\n\nShould the mentions include \"the\" or not? Is there anywhere I can read more rules on what constitutes a mention according to some well defined rules?\n\nedit: I am leaning towards \"the\" being part of the mention for definite noun phrases, as otherwise they may not be distinguished from eg another indefinite noun phrase, eg if we had \"Spot came home for tea. The cat had met another cat while out.\" Well \"cat\" and \"cat\" would not tell the interpreter much, but \"the cat\" and \"another cat\" clearly refer to different cats.\n\nedit 2: I am leaning towards not including \"the\" in the case of \"the Tower of London\", but would in the case that the definite article was part of the entity's name eg \"The Hague\". I justify this by the fact that this is the wikipedia article naming convention https://en.wikipedia.org/wiki/Wikipedia:Naming_conventions_(definite_or_indefinite_article_at_beginning_of_name) and users of my software may want to perform wikification.\n\nBut I could be wrong, these are just my guesses.", "upvote_ratio": 0.76, "id": "t3_t1c1uu", "created_utc": 1645816764.0}
{"sub": "LanguageTechnology", "title": "Using Unsupported Huggingface Models in SpaCy", "selftext": "Hey,\n\nI've been tinkering a lot with SpaCy as of late, and I was wondering whether it is possible to use any of the models published on the Huggingface website apart from those that are already SpaCy compatible. More specifically, I for example would like to use the Dutch BERT model, BERTje ( [GroNLP/bert-base-dutch-cased \u00b7 Hugging Face](https://huggingface.co/GroNLP/bert-base-dutch-cased)).\n\n&amp;#x200B;\n\nI've searched a lot for info about this, but to no avail. Would appreciate it if anyone could tell me whether this is possible without too much hassle (and without any required retraining of the model) and perhaps point me in the right direction.\n\n&amp;#x200B;\n\nCheers,", "upvote_ratio": 0.96, "id": "t3_t16scr", "created_utc": 1645803295.0}
{"sub": "LanguageTechnology", "title": "txtai 4.2 released - Build once, run anywhere serverless NLP applications", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_t134do", "created_utc": 1645792944.0}
{"sub": "LanguageTechnology", "title": "Corpus for sentiment in Dutch tweets", "selftext": "Hi,\n\nDoes anyone know about an annotated corpus for sentiment in Dutch tweets?\n\nThanks", "upvote_ratio": 1.0, "id": "t3_t11ce8", "created_utc": 1645786811.0}
{"sub": "LanguageTechnology", "title": "Multi-Class Text Classification", "selftext": "So I'm working on my graduation project and our main NLP idea is about classifying graduation projects into categories like someone writes (&gt; 1000 words) description and the application model should classify the project into (Robotics, IoT, Machine Learning, Blockchain, etc...). I've made my search and I found many ways some with Naive Bayes some with (CNN + Word2Vec) or (LSTM + Word2Vec). I just want 2 models to compare and pick the best results but I don't know what are the best 2 suitable models in this case. Can someone suggest?\n\n&amp;#x200B;\n\n***NOTE: I'll be using a research topics dataset that is labeled.***", "upvote_ratio": 1.0, "id": "t3_t0xvk0", "created_utc": 1645773357.0}
{"sub": "LanguageTechnology", "title": "For the ones who are overwhelmed by the maths for machine learning...", "selftext": "nan", "upvote_ratio": 0.85, "id": "t3_t0sth0", "created_utc": 1645757270.0}
{"sub": "LanguageTechnology", "title": "[Research] Looking for volunteers to evaluate multilingual dialogue models (chatbots)!", "selftext": "[cross-post]\nHi! Are you frustrated that most chatbots are only in English? Would you want your native language to be part of a conversational AI system as well? I certainly do! That\u2019s why I\u2019m doing my master\u2019s thesis on multilingual open-domain dialogue systems. \n\nIf you also feel the same and want to contribute to research on multilingual, intelligent chatbots, then you\u2019re the perfect fit for my thesis project!  \n\nI\u2019m currently looking for people who can speak one of the following languages fluently:   \n\n* Arabic\n* Bengali\n* Finnish\n* Japanese\n* Korean \n\nThe task is to \n\n1. post-edit some data in your language (after I used machine translation to translate it from English to the languages) or\n2. evaluate the models\u2019 responses in your language.\n\nThere are 30 short dialogue exchanges in the post editing task. Your task is to check the post machine translated text and correct the grammatical and/or fluency issues if needed. \n\nAs for the system evaluation, you will chat with the chatbots and rate them on a scale of 5 about your view of their fluency, engagingness, naturalness, etc. \n\nI\u2019m looking for 1 volunteer in each language for the post editing task and 2 volunteers in each language for the evaluation task. The tasks are pretty small so they won't take too much of your time. And it'd be fun to chat with the bots (I think) :) The post editing task preferably starts asap and the evaluation task will kick-start by the end of March! If you\u2019re interested, please fill out [this form](https://forms.gle/eA5vwGGxbos9s4jY8) :)  \n\nI\u2019ll of course credit you in the paper unless you would like to stay anonymous. This is a project I\u2019m really passionate about, and I hope it\u2019ll encourage more research on multilingual dialogue systems in the community! \n\nPlease feel free to let me know if you\u2019d like to know more! I really, really appreciate it. You can dm me or just leave a comment here. \n\nTo sign up, please kindly fill out [this google form](https://forms.gle/eA5vwGGxbos9s4jY8). \n\nThank you so much for reading this post patiently :) Hope you have a great day!", "upvote_ratio": 0.89, "id": "t3_t0ndy9", "created_utc": 1645741928.0}
{"sub": "LanguageTechnology", "title": "How do you extract structured data from NL?", "selftext": "Specifically, people's current weight, goal wight, height, and weight change from their posts in r/loseit. How do I tokenize and vetorize numbers? What ML models are used for these kind of tasks?", "upvote_ratio": 0.83, "id": "t3_szmsiq", "created_utc": 1645637042.0}
{"sub": "LanguageTechnology", "title": "Best algorithm for grammar checking", "selftext": "From a computational linguistics perspective, what is the currently highest performing algorithm for either identifying grammar mistakes in a language or also that plus suggesting corrections?\n\nThank you", "upvote_ratio": 0.72, "id": "t3_szl2v9", "created_utc": 1645632650.0}
{"sub": "LanguageTechnology", "title": "What state-of-the-art NLP systems utilize bayesian modeling?", "selftext": "Additionally, for which tasks is bayesian modeling more useful?", "upvote_ratio": 1.0, "id": "t3_szl240", "created_utc": 1645632597.0}
{"sub": "LanguageTechnology", "title": "JS library for tagging words emotion in text", "selftext": "Hello everyone. \nI found many javascript library for \"sentiment analysis\", but they only seem to provide some kind of general \"score\" of positivity/negativity. Instead, I'm looking for a more refined library that could have more emotions into which to tag each words (for example, anxiety, excitement, etc, not just a number score). \n\nIs there such a thing? Is it also called \"sentiment analysis\" or is there a more appropriate term that I should use to search for that?", "upvote_ratio": 1.0, "id": "t3_szc20r", "created_utc": 1645603442.0}
{"sub": "LanguageTechnology", "title": "\ud83c\udf88 Build a custom Q&amp;A app with Streamlit and Pinecone to revolutionize your search systems! Join this week's webinar to learn how. \ud83d\udc47", "selftext": "nan", "upvote_ratio": 0.5, "id": "t3_szam6i", "created_utc": 1645598085.0}
{"sub": "LanguageTechnology", "title": "Any pretrained models that can detect categories?", "selftext": "I'm looking at Google's NLP and they can detect categories based on text like:\n\n\n\n/Games/Computer &amp; Video Games/Sandbox Games\n\n/Games/Computer &amp; Video Games/Shooter Games\n\n/Games/Computer &amp; Video Games/Simulation Games\n\n/Games/Computer &amp; Video Games/Sports Games\n\n/Games/Computer &amp; Video Games/Strategy Games\n\n/Games/Computer &amp; Video Games/Video Game Emulation\n\n\n\n\nDo any free models or libraries exist that I can play around with?\n\nGoogle NLP is too expensive for me.\n\n\nThank you.", "upvote_ratio": 0.86, "id": "t3_sz4v47", "created_utc": 1645580980.0}
{"sub": "LanguageTechnology", "title": "[N] Who Is Behind QAnon? Linguistic Detectives Find Fingerprints using statistics and machine learning", "selftext": "According to the [New-York Times,](https://nyti.ms/36s3szj) using machine learning, stylometry, and statistics on Q texts, two separate teams of NLP researchers from [France](https://zenodo.org/record/6164620#.YhWRPHX0mCV) and [Swiss](https://www.orphanalytics.com/en/news/whitepaper202201) have identified the same two men as likely authors of messages that fueled the QAnon movement. First the initiator, Paul Furber, a South African software developer and then Ron Watkins took over, who operated 8chan website where the Q messages began appearing in 2018 and is now running election for Republican in Arizona.", "upvote_ratio": 0.97, "id": "t3_sz4qyt", "created_utc": 1645580652.0}
{"sub": "LanguageTechnology", "title": "Top American Universities on Reddit - a corpus of posts and comments to date on the subreddits of Forbes' top 10 US colleges", "selftext": "nan", "upvote_ratio": 0.67, "id": "t3_syy6vw", "created_utc": 1645563610.0}
{"sub": "LanguageTechnology", "title": "CoNLL-U token annotations", "selftext": "I would like to access the annotations of individual tokens in a CoNLL-U file.\n\nThe code in the file I am running (I followed the documentation found in [github](https://github.com/EmilStenstrom/conllu)):\n\n    import conllu\n    \n    f = open('en_gum-ud-test.conllu', 'r', encoding='utf-8')\n    annotations = f.read()\n    \n    sentences = conllu.parse(annotations)\n    \n    sentence = sentences[0]\n    print(sentence) # prints TokenList of first sentence in file\n    # TokenList&lt;The, prevalence, of, discrimination, across, racial, groups, in, contemporary, America, :&gt;\n    \n    token = sentence[0] \n    print(token) # prints the first token of the TokenList above, The\n\nInstead of the actual token (The) I would like to get the ordered dictionary:\n\n    {\n        'id': 1,\n        'form': The,\n        'lemma': the,\n        ...\n    }\n\nI get this only if I run the code line by line in the console and without the print statements, so by running just what's inside the print statements.\n\nHow can I get the ordered dictionary by running the file where the code is?", "upvote_ratio": 1.0, "id": "t3_syiqa9", "created_utc": 1645519526.0}
{"sub": "LanguageTechnology", "title": "Best textbook on coreference stuff?", "selftext": "What is a good textbook for someone writing coreference code? I need sources I can cite in my dissertation. Currently I am using [Stanford's Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/), which has been very useful so far.", "upvote_ratio": 0.81, "id": "t3_sy23h8", "created_utc": 1645471825.0}
{"sub": "LanguageTechnology", "title": "Does this count as a mention?", "selftext": "I am doing some coreference stuff and I am uncertain if the noun phrase in a noun phrase possessive constitutes a mention, eg is \"Alex\" in\n\n&gt; Alex's\n\na mention? I am leaning towards no, as I think it will be part of a larger noun phrase that constitutes the actual mention.", "upvote_ratio": 1.0, "id": "t3_sy1y6k", "created_utc": 1645471444.0}
{"sub": "LanguageTechnology", "title": "Introduction to Sentence-BERT (SBERT)", "selftext": "nan", "upvote_ratio": 0.93, "id": "t3_sxt6or", "created_utc": 1645449037.0}
{"sub": "LanguageTechnology", "title": "How do CAT tools write segments to an output file", "selftext": "When you import text to a CAT tool, it gets converted to something like .xlf, where all the recognised sentences are marked up.\n\nThen I assume when using the tool the program has loaded in text into a list from that markup file and is somehow writing each segment back to the output file in the XML.\n\nBut how does that work? You could overwrite the file when you save the project in the program. But most CAT tools immediately save each segment. How do they connect the segment to its location in the XML to auto-write there every time you translate another segment?\n\nThank you very much", "upvote_ratio": 1.0, "id": "t3_sxhtzp", "created_utc": 1645409255.0}
{"sub": "LanguageTechnology", "title": "ASAPP AI Researchers Propose a Family of Pre-Trained Models (SEW) for Automatic Speech Recognition (ASR) That Could be Significantly Better in Performance-Efficiency Than the Existing Wav2Vec 2.0 Architecture", "selftext": "Recent research in natural language processing and computer vision has aimed to increase the efficiency of pre-trained models to reduce the financial and environmental expenses of training and fine-tuning them. We haven\u2019t seen comparable attempts in speaking for whatever reason. Efficiency advances in speech might mean better performance for similar inference times, in addition to cost savings related to more efficient training of pre-trained models.\n\nDue to a self-supervised training paradigm, Wav2Vec 2.0 (W2V2) is one of the current state-of-the-art models for Automatic Speech Recognition. This training method enables us to pre-train a model using unlabeled data, which is always more readily available. The model may then be fine-tuned for a specific purpose on a given dataset. It has attracted a lot of interest and follow-up work for using pre-trained W2V2 models in various downstream applications, such as speech-to-text translation (Wang et al., 2021) and named entity recognition (Shon et al., 2021). However, researchers believe that the model architecture has several sub-optimal design decisions that render it inefficient. To back up this claim, researchers ran a series of tests on various components of the W2V2 model architecture, revealing the performance-efficiency tradeoff in the W2V2 model design space. Higher performance (lower ASR word mistake rate) necessitates a bigger pre-trained model and poorer efficiency (inference speed). [**CONTINUE READING**](https://www.marktechpost.com/2022/02/20/asapp-ai-researchers-propose-a-family-of-pre-trained-models-sew-for-automatic-speech-recognition-asr-that-could-be-significantly-better-in-performance-efficiency-than-the-existing-wav2vec-2-0-arch/)\n\nPaper: https://arxiv.org/pdf/2109.06870.pdf\n\nGithub: https://github.com/asappresearch/sew", "upvote_ratio": 0.92, "id": "t3_sx7l1o", "created_utc": 1645381204.0}
{"sub": "LanguageTechnology", "title": "I trained the GPT-2 model on the tao te ching. It made some interesting samples.", "selftext": "nan", "upvote_ratio": 0.82, "id": "t3_swjkre", "created_utc": 1645303868.0}
{"sub": "LanguageTechnology", "title": "5 Scholarships for corise Applied NLP course", "selftext": "We are giving away 5 scholarships to the upcoming (starts March 14) Applied NLP courses taught by Sourabh Bajaj, former Google Brain and Coursera. Pls ping me if interested!", "upvote_ratio": 0.95, "id": "t3_svyegh", "created_utc": 1645235358.0}
{"sub": "LanguageTechnology", "title": "Why does Zero-Shot-Classification not work in this simple use-case?", "selftext": "I'm trying to classify **adjectives** to see if they apply to **humans** or **things**. \n\nI tried several pretrained models for this task on huggingface, such as [typeform/distilbert-base-uncased-mnli](https://huggingface.co/typeform/distilbert-base-uncased-mnli?candidateLabels=personality+trait%2C+object&amp;multiClass=true&amp;text=physical) (and many of [these](https://huggingface.co/models?pipeline_tag=zero-shot-classification&amp;sort=downloads)), but the classification is very unreliable when it comes to single adjectives. I tried all possible types of candidate labels, even aggregated scores for clusters, but cannot seem to solve this reliably. I have about 6 000 adjectives that should be classified. So far I haven't bothered to calculate validity, but intuitively I'd say \"slightly better than chance\".\n\nHowever, the examples provided on the model cards seem to work really well. Any ideas on why this doesn't work and how to fix it?", "upvote_ratio": 0.92, "id": "t3_svqw4t", "created_utc": 1645214275.0}
{"sub": "LanguageTechnology", "title": "Top 10 Language Detection APIs", "selftext": "nan", "upvote_ratio": 0.5, "id": "t3_svhrnz", "created_utc": 1645190308.0}
{"sub": "LanguageTechnology", "title": "Dodiom - Telegram bot to collect idiom corpus", "selftext": "Hi everyone,\n\nI've developed a Telegram bot to help you learn English idioms as a multiplayer competitive game while collecting MWE (multi-word expressions) corpus.\n\nBot link: https://t.me/dodiom_en_bot\n\nYou can see the results for Turkish version of the bot in this [journal article](https://www.cambridge.org/core/journals/natural-language-engineering/article/gamified-crowdsourcing-for-idiom-corpora-construction/A69DC2EC025689C5495A3859387468A3). You can also find the source code and collected corpus on [Github](https://github.com/Dodiom/dodiom/). Right now we are collecting corpus for English language and our aim is to compare it to existing human annotated corpora. I'd be very grateful if you try it.\n\nAll feedback is welcome and encouraged. I hope this post does not break subreddit rules.", "upvote_ratio": 1.0, "id": "t3_svctbx", "created_utc": 1645172187.0}
{"sub": "LanguageTechnology", "title": "Dutch Lexical Simplification with RobBERT", "selftext": "Hi all,\n\nI'm working on a lexical simplification system for Dutch. I've found the following for English: \n\n[https://github.com/qiang2100/BERT-LS](https://github.com/qiang2100/BERT-LS)\n\nWould it be doable to adapt this to Dutch? I would use RobBERT (pretrained Dutch BERT), and would have to apply some changes so that the output would be grammatical in Dutch, I suppose?\n\nSeems like it may be more efficient to create a \"handcrafted\" pipeline approach for Dutch, and add in trigram verification of the output using RobBERT or similar. The output may not be as good, though.\n\nPlease let me know what you think!", "upvote_ratio": 0.9, "id": "t3_suo2ln", "created_utc": 1645102265.0}
{"sub": "LanguageTechnology", "title": "Warning: Invalid line when computing TER metric", "selftext": "Hi guys, i'm trying to compute the ter metric using this repository:  [https://github.com/jhclark/tercom](https://github.com/jhclark/tercom)\n\nI have two txt files for the ref. and hypothesis, with this two i have already compute the Bleu ([https://raw.githubusercontent.com/moses-smt/mosesdecoder/master/scripts/generic/multi-bleu.perl](https://raw.githubusercontent.com/moses-smt/mosesdecoder/master/scripts/generic/multi-bleu.perl)), Meteor ([https://www.cs.cmu.edu/\\~alavie/METEOR/README.html#about](https://www.cs.cmu.edu/~alavie/METEOR/README.html#about)) and Rouge ([https://github.com/pltrdy/rouge.git](https://github.com/pltrdy/rouge.git)).\n\nFor compute the ter metric i use this code :\n\n    !java -jar tercom.7.25.jar -h /content/pred.txt -r /content/tgt-val.txt\n\nBut it gives me this error:\n\n Warning: Invalid line for every line of the txt file and the final results are:\n\n    Total TER: NaN (0.0/0.0)\n    \n    Number of calls to beam search: 0 \n    \n    Number of segments scored: 0 \n    \n    Number of shifts tried: 0\n\nHow can i solve this problem?\n\nThanks all", "upvote_ratio": 1.0, "id": "t3_sumgy0", "created_utc": 1645096689.0}
{"sub": "LanguageTechnology", "title": "Deploying GPT-NeoX 20B: lessons learned and a focus on Deepspeed", "selftext": "Hello all,\n\nDeploying  and using GPT-NeoX 20B reliably in production has been quite a  challenge. You basically have 2 choices: have it run on a single huge  GPU, or on multiple smaller GPUs. Here are a couple of lessons I learned  during this interesting journey:\n\n[https://nlpcloud.io/deploying-gpt-neox-20-production-focus-deepspeed.html](https://nlpcloud.io/deploying-gpt-neox-20-production-focus-deepspeed.html?utm_source=reddit&amp;utm_campaign=ehyiek56-ed8e-11eb-ba80-5242ac13d5jv)\n\nIf  some of you used a different strategy, I'd love to hear about it. Also,  if you have an idea about how to perform batch inference on GPT-NeoX  20B, I'm interested in hearing your thoughts.\n\nThanks to EleutherAI for their amazing work. Can't wait to see what's next!", "upvote_ratio": 0.97, "id": "t3_sujgxo", "created_utc": 1645084772.0}
{"sub": "LanguageTechnology", "title": "Relative Position Representation/Encoding for Transformer", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_sucfg7", "created_utc": 1645061880.0}
{"sub": "LanguageTechnology", "title": "How to handle text data for sentence boundary detection with LSTMs?", "selftext": "First of all, i am complete new to the topic of NLP and in general new to machine learning. \nI want to build a sentence boundary detection model for texts using Keras LSTMs. The goal is that I can give the model a text and then I get a list of labels representing each token in the given text. I have the following training data: 6 texts of different lengths, each of these texts are annotated at word level with one of the following labels: [\"B-SEN\", \"E-SEN\", \"O\"]. Example data:\n\nX = [[\"This\", \"is\", \"text\", \"one\", \".\", ...], [\"This\", \"is\", \"text\", \"two\", \".\", ...], ... [\"This\", \"is\", \"text\", \"six\", \".\", ...]]\n\ny = [[\"B-SEN\", \"O\", \"O\", \"O\", \"E-SEN\", ...], [\"B-SEN\", \"O\", \"O\", \"O\", \"E-SEN\", ...], ... [\"B-SEN\", \"O\", \"O\", \"O\", \"E-SEN\", ...]]\n\nMy training data X I have now converted that each token/word is converted to a feature vector of 8 features. My labels y I have also encoded starting at 1 for B-SEN. Since the texts have different lengths I have padded the training data based on the longest of my samples. The longest sample/text of these 6 texts is 80451 tokens long. The shorter texts were padded with 0-vectors in X representing the padding and the y arrays with 0 entries.\n\nSince I am completely new to this topic, I wanted to start with a simple model consisting of one LSTM layer and then extend it later on. Now I have the question of how to model this problem/approach best? The most trivial approach in my opinion would be to say I choose my input_shape = (6, 80451, 8)\n\nmodel = Sequential()\nmodel.add(LSTM(128, input_shape=(6, 80451, 8), return_sequences=True))\nThat would mean that I would look at a whole sample at one timestep, right? Does this make sense for this problem? Most sentences have a length of &lt; 100 tokens. For example, if I wanted my model to look at only 100 tokens per timestep per sample, how could I do that, would I have to restructure my training data?\nMany thanks in advance!", "upvote_ratio": 1.0, "id": "t3_su8hsa", "created_utc": 1645051010.0}
{"sub": "LanguageTechnology", "title": "How to assess and statistically test the difference between two embedding models (applied on the same dataset)?", "selftext": "I am working on a supervised multi-class text classification problem. A number of phrases/responses are classified into one or more categories (with the following labels: 1 noting that it is in that category, 0 noting that it is not).\n\nI used BERT + NN, then tried the Universal Sentence Encoder to embed again + NN. How is the difference between models going to be assessed and statistically tested? \n\nUntil now, I've seen the following metrics: Accuracy, F1, Precision and Recall. Do I just put the numbers side by side? Because the difference doesn't seem grand that way. Or is another test needed?\n\nFinally, as another evaluator I can use an inter-rater reliability between human experts and the algorithm. I was thinking of calculating the hamming-loss of the model predictions and the humans. How can I compare both models though? Looking at both hamming loss scores?\n\nThere are a lot of sub-questions in one, but I would love to hear your ideas.", "upvote_ratio": 0.91, "id": "t3_su4au1", "created_utc": 1645040172.0}
{"sub": "LanguageTechnology", "title": "Applications of DeBERTa for Sentiment Analysis", "selftext": "I was wondering if anyone had any cool projects that applied the DeBERTa model for a sentiment analysis task; I'm starting a new project in the field, and I'm just trying to get a good scope of the terrain. Thanks!", "upvote_ratio": 1.0, "id": "t3_su2y7c", "created_utc": 1645036691.0}
{"sub": "LanguageTechnology", "title": "Similarity of answers across groups to open-ended survey questions", "selftext": "Say I have some data in which vegetarians, vegans and omnivores all responded to the same set of open ended questions (presumably about eating habits) and I wanted to determine the strength / difference in their answers.\n\nOf course I could simply look at relative word frequencies, topic models etc. But if i wanted to determine the similarity between the answers, would vectorizing each  group's answers and comparing them with cosine similarity make sense? \n\nConceptually, I think this is interesting as I would like to see, with some quantitative value, how 'similar' vegans are to vegetarians to omnivores? Does this approach make sense?", "upvote_ratio": 1.0, "id": "t3_su15x3", "created_utc": 1645031742.0}
{"sub": "LanguageTechnology", "title": "data format to fine tune gpt-2 for code generation", "selftext": " \n\nI'm following this [https://github.com/nshepperd/gpt-2](https://github.com/nshepperd/gpt-2) repo to fine tune the gpt-2 355M model, i've collected (comment,code) pairs from github into a text file where data have the following format :\n\n    #comment \n    code\n    &lt;|endoftext|&gt; \n\nis this the correct format for fine tuning the gpt-2 model?", "upvote_ratio": 1.0, "id": "t3_sttkig", "created_utc": 1645010493.0}
{"sub": "LanguageTechnology", "title": "Dialects in Q&amp;A models", "selftext": "Hey everyone, I have been trying to implement a Q&amp;A model for a low resource language and I've read up a few papers for the same. I want to know if different dialects across a country might affect the model. And how I can avoid this. Since I'm going to be crowdsourcing my data for this! \n\nI thought it would be an issue with GottBERT but they haven't mentioned anything with regard to dialects. \n\nAny insights would be appreciated!", "upvote_ratio": 1.0, "id": "t3_stret9", "created_utc": 1645002075.0}
{"sub": "LanguageTechnology", "title": "8 Completely FREE Courses to Learn AI (Artificial Intelligence)", "selftext": "nan", "upvote_ratio": 0.25, "id": "t3_stqc3t", "created_utc": 1644997851.0}
{"sub": "LanguageTechnology", "title": "Collaborate with an up-and-coming NLG project!", "selftext": "Hey NLP community! We've just launched a project called \"Friday\", a GPT-3 based content generator. We are looking for users to help collaborate and provide feedback on the project. Direct message me for a 3-month trial account of Friday service, we hope our work together can create a stronger generator for the whole community to use. Thanks!", "upvote_ratio": 1.0, "id": "t3_stkhcp", "created_utc": 1644978399.0}
{"sub": "LanguageTechnology", "title": "Which vector database to use for storing embeddings? A benchmark", "selftext": "If you are wondering which vector databases to use, hope these blog posts serve as a reference for you:\n\n[https://farfetchtechblog.com/en/blog/post/powering-ai-with-vector-databases-a-benchmark-part-i/](https://farfetchtechblog.com/en/blog/post/powering-ai-with-vector-databases-a-benchmark-part-i/)\n\n[https://www.farfetchtechblog.com/en/blog/post/powering-ai-with-vector-databases-a-benchmark-part-ii/](https://www.farfetchtechblog.com/en/blog/post/powering-ai-with-vector-databases-a-benchmark-part-ii/)\n\nThe analysis is conducted by Farfetch, an e-commerce company, which surveyed the most recent, popular, and reportedly robust large-scale vector databases that can sustain conversational AI systems including Vespa, Milvus, Qdrant, Weaviate, Vald, and Pinecone.", "upvote_ratio": 0.92, "id": "t3_stp26x", "created_utc": 1644993014.0}
{"sub": "LanguageTechnology", "title": "precision recall calculation for key phrase extraction", "selftext": "suppose I have 5 documents and I ranked keyphrases extracted. How do I calculate precision, recall @ k? The number of keyphrases vary across each document. How does it work? \n\n1. Is precision@k is average precision at k? 1/k(precision@1+....precision@k) or its just precision@k?\n2. If number of keyphrases extracted vary. For example: doc 1 has 2 keyphrases and doc 2 has 3 keyphrases? How do I calculate precision@3? is it 1/3((doc1 p@1+ doc2@ p@1)/2+.......+ (doc 1 p@3 (0)+ doc2 p@3)/2)? Please bear somehow unclear example.", "upvote_ratio": 1.0, "id": "t3_stccso", "created_utc": 1644956531.0}
{"sub": "LanguageTechnology", "title": "XLMRoberta validation loss increases with mcc score.", "selftext": "I am trying to fine tune a bit complex problem with XLMRoberta model and using the parameters as prescribed in the paper, lr of 2e-5 , etc. However, the validation loss (crossentropy) keeps on increasing   with epochs along with other matrices like f1 score , mcc score! Although everyone suggests to use just 2-4 epochs of training but when i try to use early stopping (on mcc score), I noticed that the accuracy keeps on improving with epochs. Is it a valid scenario ? Because as per Machine Learning logics it shouldnt be the case. And what is expected if i fine tune BERT model for large epochs  ?", "upvote_ratio": 1.0, "id": "t3_stc0d7", "created_utc": 1644955621.0}
{"sub": "LanguageTechnology", "title": "Embedding 3 Segments, Each With Its Own [CLS] Token in BERT", "selftext": "My goal is to build a model that takes embeddings of 3 segments, concatenates them, and passes them through a single linear layer for some classification.  \n\nMy strategy right now is to surround each segment with a \\[CLS\\] and \\[SEP\\] token, so my input to BERT looks like:\n\n\\[CLS\\] segment 1 \\[SEP\\] \\[CLS\\] segment 2 \\[SEP\\] \\[CLS\\] segment 3 \\[SEP\\]\n\nI think grab the final hidden state from each \\[CLS\\] token and concatenate before passing through final layer.  My question is, does this strategy make sense, i.e.:\n\n\\-will the first \\[CLS\\] token encode something meaningful about segment 1?\n\n\\-will the second \\[CLS\\] token encode something meaningful about segment 2?\n\n\\--will the third \\[CLS\\] token encode something meaningful about segment 3?\n\nI understand BERT was trained with two segments, but I've also seen work where people are inserting a bunch of CLS tokens.\n\nThanks in advance, and if anyone has any suggestions for how this can be done better that'd be greatly appreciated as well!", "upvote_ratio": 1.0, "id": "t3_st8ey2", "created_utc": 1644946519.0}
{"sub": "LanguageTechnology", "title": "Named Entity Recognition - Label-Label Transitions", "selftext": "In NER, what are label-label transitions? And what do the weights refer to?\n\nAm I right in thinking that this is the relationship strength between the labels (e.g. I-LOC and B-LOC)?\n\n&amp;#x200B;\n\nThanks", "upvote_ratio": 0.5, "id": "t3_st7nbq", "created_utc": 1644944523.0}
{"sub": "LanguageTechnology", "title": "can someone point me to research of a minimal language word set that can be used to describe most other words?", "selftext": "Has anyone done research like this?\nFor example  \"to run\" can be described as move fast.\nmove is a base word used in many other word definitions like:\ndrive \"to move in fast object with wheels\"\nI've been trying to find this research, but can't find anything good, can anyone help point me to some good research?", "upvote_ratio": 1.0, "id": "t3_st3rng", "created_utc": 1644934096.0}
{"sub": "LanguageTechnology", "title": "Using Transformer for Topic Modeling - what are the options?", "selftext": "I need to generate labels that semantically describe groups of words as fittingly as possible, so I've been looking into topic modeling and want to know what the options are.\n\nWhat are the state-of-the-art libraries/models out there? So far I've looked into BERTopic ([https://github.com/MaartenGr/BERTopic](https://github.com/MaartenGr/BERTopic)).\n\nThanks\n\n**EDIT:** I'm especially interested in finding **words/sequences that best describe** topics (i.e., finding the word \"animal\" for \"cat\", \"dog\", \"bear\").", "upvote_ratio": 0.85, "id": "t3_st2zh7", "created_utc": 1644931819.0}
{"sub": "LanguageTechnology", "title": "Averaging sentence embeddings to create multi-sentence embeddings?", "selftext": "Hey everyone. \n\nSo, I'm getting started on a project where I'm trying to extract information from short texts with an unsupervised approach. The plan is for me to cluster these texts. I know of Topic Modeling but I'm looking into other options too.\n\nWhile the texts are short, they can contain muliiple sentences. I'm basically wondering if it would be considered fine to use a sentence transformer like Sentence-BERT to create sentence embeddings, and for each text with multiple sentences, average the embeddings to create a single one? \n\nFeel free to suggest other ideas to create a single embedding for something that contains multiple sentences.", "upvote_ratio": 0.94, "id": "t3_st1si5", "created_utc": 1644927902.0}
{"sub": "LanguageTechnology", "title": "System for Semantic Role Labeling", "selftext": "Anyone know of a decent SRL parser?  I'd prefer one that produces output based on the Propbank v3 frameset, not the v2 used in the original OntoNotes release but I'm not sure I'm going to get to be picky.  If you know of a decent one, let me know.  \"Papers with Code\" has some links but they require training.  I was hoping to find something in a little more of a released state.  It doesn't look like Spacy or Stanford's NLP suites offer this functionality.", "upvote_ratio": 0.88, "id": "t3_ssumex", "created_utc": 1644900186.0}
{"sub": "LanguageTechnology", "title": "Are there any datasets/models that address the connotation of a word?", "selftext": "For example, the word \"blood\", with the same meaning, can connote:\n\n1. Family: \"blood\" is thicker than water\n2. Violence: there was \"blood\" in the streets\n3. Passion: he got my \"blood\" racing\n4. Biological substance: \"blood\" pressure\n\nThe languages models I know have a single meaning for a word, and attention models don't address meaning, but rather usage \nand prediction.\n\nBefore I go down this research path, I'd like to know if there is already literature surrounding this problem.", "upvote_ratio": 1.0, "id": "t3_ssatsr", "created_utc": 1644845901.0}
{"sub": "LanguageTechnology", "title": "Dependency Parsing(python)", "selftext": "I have predicted dependency relation for each of the words in a sentence. How do I find syntactical head of each of these words if my data now is of form list(deprel) for a sentence inorder to construct dependency tree?", "upvote_ratio": 0.81, "id": "t3_ss898f", "created_utc": 1644837261.0}
{"sub": "LanguageTechnology", "title": "Knowledge Base for Portuguese", "selftext": "Hi,\n\nI want to do Entity Linking in text written in Portuguese and just find out that Spacy has an [EntityLinker](https://spacy.io/api/entitylinker) component in its pipeline. I only need a Knowledge Base database. \n\nDo you about Knowledge Base repositories? I'm looking for one more targeted to Portuguese, in particular European (most of the entities I will find in the texts are from Portugal).", "upvote_ratio": 1.0, "id": "t3_ss744n", "created_utc": 1644832852.0}
{"sub": "LanguageTechnology", "title": "Using LDA to categorize my blog posts", "selftext": "My problem is that the lda/js algo i'm using returns terms with probabilities. I would like to feed the algo a list of allowed categories and it should return N categories with probas, not terms.\n\nI'm using this : [https://github.com/primaryobjects/lda](https://github.com/primaryobjects/lda) . I don't know if Gibbs sampling is the answer.\n\nExample of what I have at the moment: categories should be like \"Life\", \"Society\", \"Art\", \"Business\" etc... Right now the categories I have are \"Traffic\", \"Music\" and \"Money\".\n\n    [\n      [\n        { term: 'traffic', probability: 0.072 },\n        { term: 'people', probability: 0.07 },\n        { term: 'reasons', probability: 0.056 },\n        { term: 'website', probability: 0.048 },\n        { term: 'readers', probability: 0.044 }\n      ],\n      [\n        { term: 'music', probability: 0.047 },\n        { term: 'feel', probability: 0.035 },\n        { term: 'problem', probability: 0.027 },\n        { term: 'partners', probability: 0.027 },\n        { term: 'jobs', probability: 0.025 }\n      ],\n      [\n        { term: 'money', probability: 0.055 },\n        { term: 'blog', probability: 0.045 },\n        { term: 'started', probability: 0.034 },\n        { term: 'give', probability: 0.028 },\n        { term: 'business', probability: 0.028 }\n      ]\n    ]", "upvote_ratio": 0.84, "id": "t3_srqflu", "created_utc": 1644779743.0}
{"sub": "LanguageTechnology", "title": "Noob question: how hard is it to pair quotes and names from the text of a news article?", "selftext": "I was reading an article on a political website that was describing how a particular politician had said something that more or less contradicted something they had said a year or two ago.  I thought, wouldn't it be nice to have a public website where you could look quotes that appeared in news articles to see a persons' public statements over time, but also with the context of the time.  As a web developer, building a site to scrape articles and a database to hold the person, quote, and original source article is something I understand, but to automate the process of pairing a quote in the article to the speaker, and do it *reliably*, is more difficult (at least to me).  Any thoughts on how hard that would be with NLP or a similar approach?  Thanks in advance.", "upvote_ratio": 0.93, "id": "t3_srmrqt", "created_utc": 1644770461.0}
{"sub": "LanguageTechnology", "title": "What are the State of the art Approaches for Long Text Similarity?", "selftext": "Hi everyone,\n\nWhat are the state of the art approaches for measuring long text similarity? I've been working on this subject and noticed that measuring semantic similarity between short texts are some what easier but I haven't achieved good results for longer texts. Do you guys have any suggestions on what I could try for this task? It would be awesome if you could even share some code. \n\nI tried sentence BERT for example, but even for BERT I am not getting very good results and I thought that I'd get good results since BERT is the state of the art for most of the other tasks. (I sentence tokenized the texts, took the mean of the sentence BERT representations for all the sentences in a single long text and finally took the cosine similarity between the resulting vectors)", "upvote_ratio": 0.95, "id": "t3_sr0z3i", "created_utc": 1644699231.0}
{"sub": "LanguageTechnology", "title": "The Best Machine Learning Courses on Udemy (2022)", "selftext": "nan", "upvote_ratio": 0.4, "id": "t3_sqo123", "created_utc": 1644659336.0}
{"sub": "LanguageTechnology", "title": "3Blue1Brown Solving Wordle using information theory - YouTube", "selftext": "nan", "upvote_ratio": 0.98, "id": "t3_sqgr1y", "created_utc": 1644633542.0}
{"sub": "LanguageTechnology", "title": "NLP: Evaluation of Summarization Task", "selftext": "ROUGE score is heavily used for the evaluation of summarization task in NLP but it only considers n-gram overlap rather than contextual similarity. \n\nWe want to check the feasibility of BertScore(arXiv:1904.09675) as an evaluation metric.  \n\nwe collected some samples from our in-house dataset (with human-generated ground truth summaries) and generated their summaries using abstractive models like pegasus and Bart. For those summaries, we got some people to rate them between 1-5. \n\nQuery: Now, to check if BertScore is a better evaluation metric, we plan to compare the correlation of human ratings with both rouge score and BertScore. Is it the right way to proceed? Any suggestions? \n\n(we are focused on financial news data, and we cater to an audience with finance background mainly)", "upvote_ratio": 0.67, "id": "t3_sq6cgy", "created_utc": 1644604786.0}
{"sub": "LanguageTechnology", "title": "Deepmind\u2019s Latest Machine Learning Research Automatically Find Inputs That Elicit Harmful Text From Language Models", "selftext": "Large generative language models (LMs) such as GPT-3 and Gopher have proven the ability to generate high-quality text. However, these models run the danger of producing destructive text. Therefore, they are challenging to deploy due to their potential to hurt people in ways that are nearly impossible to forecast.\u00a0\n\nSo many different inputs can lead to a model producing harmful text. As a result, it\u2019s challenging to identify all scenarios in which a model fails before it\u2019s used in the actual world. [**Continue Reading**](https://www.marktechpost.com/2022/02/11/deepminds-latest-machine-learning-research-automatically-find-inputs-that-elicit-harmful-text-from-language-models/)\n\nPaper: https://arxiv.org/pdf/2202.03286.pdf", "upvote_ratio": 1.0, "id": "t3_sq4wbd", "created_utc": 1644600913.0}
{"sub": "LanguageTechnology", "title": "Document type reckognition.", "selftext": "I am dealing with documents, 100 different documents. Some are standard (1040, W2) and some are not (Insurance Policy, bank statement....)\n\nI need to develop a system that I would feed a document and it will spit out which doc type it is.\n\nNot sure how to go about it. Should I vectorize all words, then my input layer will have (let say) 2000 words and output layer will have 100 SoftMax neuron showing probability of those 2000 words belonging to specific doc type?\n\n&amp;#x200B;\n\nPS: Sorry, misspelled the title, should be \"recognition\"", "upvote_ratio": 1.0, "id": "t3_sq2sb1", "created_utc": 1644595399.0}
{"sub": "LanguageTechnology", "title": "NER - How to determine covariance of entities in named entity recognition?", "selftext": "How would I go about deriving the correlation or covariance between entities within text? Relation extraction seems to be more focussed on providing more context to the entities, whereas I am looking to calculate the correlation of entities.\n\nThe entities I am looking to use in building the model are likely to have significant overlap, which I would be interested in identifying. For example, would I look for overlap in identified text for different entities, or look at word distance between co-occurring entities?\n\nFor example, I have a sample sentence of \"*Communication \\[entity1\\]* skills are key for *interpersonal \\[entity2\\]* development, *groupwork \\[entity3\\]* and *presentations \\[entity1\\]*.\" I would like to determine the strength of entity1, entity2 and entity3 but uncertain of how best to approach this problem. Would I look to e.g. calculate average word distance between entities, or something else?\n\nThe datasets I am looking to run this on will be large (multiple webpages of text, rather than sentences) and so I don't think collecting counts would be appropriate and requires some consideration of how close entities appear.\n\nThanks!", "upvote_ratio": 1.0, "id": "t3_spv4y1", "created_utc": 1644570633.0}
{"sub": "LanguageTechnology", "title": "How to add a global attribute to an input sentence of a pre-trained language model?", "selftext": "I want to add a global attribute (say text style) to the input sentence of a pre-trained model like BERT for downstream tasks. Should I\n\n1. replace the \\[CLS\\] token with the style embedding\n2. add the style embedding before or after the \\[CLS\\] token\n3. add the style embedding to each of  tokens in the sentence\n4. other methods\n\nWhich one is the recommended way? or it depends?", "upvote_ratio": 0.93, "id": "t3_spv25a", "created_utc": 1644570313.0}
{"sub": "LanguageTechnology", "title": "Attempting to rewrite BERT codebase into RoBERTA, running into shape issue.", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_spkuk1", "created_utc": 1644538373.0}
{"sub": "LanguageTechnology", "title": "Corpus of news articles about Politicians?", "selftext": "HI!   \nI've been looking around for something I could use but nothing has jumped out at me. I'm looking for a corpus of news articles about politicians. Specifically, I'm looking for a database I can use to feed a neural network the article, and the subject's sex (male/female).\n\nIf I can find a corpus about politicians, I can do the manual labor of storing it as male/female myself.  \n\n\nThese articles could be something with like \"Biden repeats earlier statement regarding....\", or \"Washington halts Greene's progress on...\", etc.\n\n&amp;#x200B;\n\nAny sort of guidance is helpful.\n\nThanks!", "upvote_ratio": 0.86, "id": "t3_spaigr", "created_utc": 1644510448.0}
{"sub": "LanguageTechnology", "title": "The computational cost of Text Classification with Universal Embeddings?", "selftext": "Hi. I am not experienced with AI but I am trying to use a simple model for one of my personal projects.\n\nThe question is: What would be the computational complexity ( just an estimation of it) of a Text Classification with Universal Embeddings for like 1000 data inputs? For example, I have 1000 sentences, and some of them may mean the same thing. So I want to create a list of groups of sentences with the same meaning. Therefore it would look like this -&gt; Input: 1000 sentences. Output: Group 1: 500 sentences mean this thing ( and showing the sentences), group 2: 300 sentences mean this thing, and group 3: 200 mean another thing ( whatever the sentences might be). I am interested in how computational expensive would that be? Would it run on a phone? Implying the input is 1000 sentences.\n\nProbably using this: [https://www.npmjs.com/package/@tensorflow-models/universal-sentence-encoder](https://www.npmjs.com/package/@tensorflow-models/universal-sentence-encoder) but I am not sure if that's the best way. Also, does this model make requests to google to do the heavy lifting? \n\nAlso, please don't hesitate to correct me if I got it all wrong... Most likely I got it all wrong.\n\nThanks.", "upvote_ratio": 1.0, "id": "t3_sp8tra", "created_utc": 1644505998.0}
{"sub": "LanguageTechnology", "title": "Is there a database of books for processing?", "selftext": "Hello everyone!\n\nI want to know whether exists a books repository for NLP processing? Here in Brazil, we have one database with most Brazilian literature [https://www.literaturabrasileira.ufsc.br/?locale=pt\\_BR](https://www.literaturabrasileira.ufsc.br/?locale=pt_BR).", "upvote_ratio": 1.0, "id": "t3_sp6asn", "created_utc": 1644498673.0}
{"sub": "LanguageTechnology", "title": "[P] What we learned by accelerating by 5X Hugging Face generative language models", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_sp2oy5", "created_utc": 1644485600.0}
{"sub": "LanguageTechnology", "title": "9 Best Courses to Learn to TensorFlow", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_sp1ery", "created_utc": 1644480591.0}
{"sub": "LanguageTechnology", "title": "Are there models like \"punctuation_en_bert\" from Nvidia for other languages, in particular German?", "selftext": "Hi guys, I want to perform some analysis on transcripts that miss punctuation. The lack of it distorts the results, so I found that there is \"punctuation_en_bert\" from Nvidia that inserts punctuation back. It does a great great job for English. I need something like this for German as well. I wonder if that exists though. Can you point me in the right direction ?", "upvote_ratio": 0.94, "id": "t3_soqap4", "created_utc": 1644446949.0}
{"sub": "LanguageTechnology", "title": "What Keeps You Going ? | Noam Chomsky", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_sofq8n", "created_utc": 1644419133.0}
{"sub": "LanguageTechnology", "title": "Transformer model comparison for binary sentiment classification", "selftext": "Hi everyone,\n\nOn two independent datasets, I am comparing XLNet and BERT models  with binary sentiment classification tasks: the Twitter dataset, where  sentences are short, and the IMDB review dataset, where sentences are  long.\n\nOn the Twitter dataset, BERT matches and slightly outperforms XLNet,  but XLNet outperforms BERT on the IMDB dataset. I understand that XLNet  captures longer dependencies due to the Transformer XL architecture and  so outperforms BERT; but, what additional reasons may exist for one to  outperform the other for a certain dataset? Why is BERT more successful,  or at least comparable to XLNet, in classifying social media sentiment?", "upvote_ratio": 1.0, "id": "t3_soe7gc", "created_utc": 1644415039.0}
{"sub": "LanguageTechnology", "title": "Brainstorming an Approach to Label Specific Narrative Text", "selftext": "Hi! I'm currently working on a project that involves labeling specific narrative text - for example, finding descriptions of COVID-19 hospital situations throughout Facebook posts from March 2020 to current. I'm not too well-versed with the neural models (know how they work, but haven't played around with transformers or anything) - currently thinking of employing something along the lines of a random forest using various engineered features (such as n-grams, whether certain phrases of interest occur, etc.) but believe that it will not be expressive enough to fit large text data.\n\nAny ideas on technologies and models to look into for problems like this? Thanks!", "upvote_ratio": 0.76, "id": "t3_so49dg", "created_utc": 1644379983.0}
{"sub": "LanguageTechnology", "title": "For those who get overwhelmed by the maths for machine learning \ud83d\udc47", "selftext": "nan", "upvote_ratio": 0.5, "id": "t3_so2xzs", "created_utc": 1644375995.0}
{"sub": "LanguageTechnology", "title": "Master's thesis topic? - Policy/Government related", "selftext": "Hi everyone,\nI'm struggling to frame the topic of my master's thesis, so I'm calling reddit to the rescue! \nIdeally, I'd like to work on something related to policy/government/democracy. I have a few government surveys (50k+ answers) with open ended questions, which were poorly analyzed. I would like to do something a bit more interesting than a simple clustering of the answers, any idea?\nOf course, any other suggestions are welcome!\n\nThanks!", "upvote_ratio": 1.0, "id": "t3_snux8b", "created_utc": 1644354526.0}
{"sub": "LanguageTechnology", "title": "I would like to put ancient Greek texts through a neural network, in order to individuate multiple authors within them. Where should I get started?", "selftext": "Disclaimer: I know absolutely nothing about NLP.\n\nI study ancient Greek texts and would love to analyse parts of a single text in order to test hypotheses on its multiple authors. I suspect the text, which is said to have been written by a single author, was actually written by multiple authors. \n\nIs there some online resource you would recommend, or some tips you might have as for testing such a hypothesis through NLP?", "upvote_ratio": 1.0, "id": "t3_snuwjz", "created_utc": 1644354479.0}
{"sub": "LanguageTechnology", "title": "What tools do you want when transforming raw text to dataset", "selftext": "Hi,\n\nI always feel it's a headache to transform raw text (like wiki data) to structured dataset for training, so I am thinking about contributing some open-source utility functions to ease NLP work. One thing I am thinking about is for Masked Language Modeling, it would be helpful if we have a function `mask_tokens(text)` that randomly mask tokens for us. \n\nMy knowledge is very limited...so I am posting to ask for your opinions: are people aware of any existing NLP data processing tools? Also what other tools you think maybe helpful? \n\nMany thanks!!", "upvote_ratio": 0.92, "id": "t3_snrfc8", "created_utc": 1644345637.0}
{"sub": "LanguageTechnology", "title": "Alternative to sentence semantic similarity?", "selftext": "How do we model the relationship between two sentences that are connected but not semantically related? Can BERT be used for this kind of modeling?\n\nEDIT: 'connected' can mean any relationship that may exist between two sentences 1 &amp; 2 (not necessarily semantics or meaning based). The task is around feeding sentence 1 as input to a bert-based model so that it outputs sentence 2.", "upvote_ratio": 1.0, "id": "t3_snr64z", "created_utc": 1644344989.0}
{"sub": "LanguageTechnology", "title": "AI-modified short story study", "selftext": "Hi all, I am doing a PhD on personalisation and narratives, and for this, I created a user study where a short story of about 4000-5000 words has been modified by AI. The participants should also do a very short personality test and answer a few questions on what they thought of the story. There are also  Amazon vouchers worth 5 GBP available for those doing this now! It's at [https://cci.arts.ac.uk/\\~wnybom/cloak.html](https://cci.arts.ac.uk/~wnybom/cloak.html)", "upvote_ratio": 0.67, "id": "t3_snj92r", "created_utc": 1644323997.0}
{"sub": "LanguageTechnology", "title": "Multi-Class Classification NLP", "selftext": "I am trying to build a Multi-class Text classification model with 90 classes.Data is quite imbalanced with some of the classes having less than 100 samples while some having over 1200 samples. Currently I am using Bert Base with Cross-Entropy as loss. However, I am seeing very low accuracy for some of the classes. Classes with low accuracy are not necessarily those which have low number of samples. \n\nI have already tried using Focal Loss, Dice Loss and Weighted Cross entropy as Loss functions.\n\nWhat else can be tried ?", "upvote_ratio": 1.0, "id": "t3_sncs76", "created_utc": 1644299984.0}
{"sub": "LanguageTechnology", "title": "Are translation models useful for generating synthetic data?", "selftext": "I am working on a project which requires data in low resource languages (mainly Indic). Would it be acceptable to use translated data for something like this? I unserstand there are some issues about validating the translation quality of the model. Would that be required even if we were using something like the google translate API? Any leads to resources/papers which have done this before will be much appreciated.", "upvote_ratio": 0.84, "id": "t3_sn6ar0", "created_utc": 1644281373.0}
{"sub": "LanguageTechnology", "title": "Unix utility for machine translation", "selftext": "Is there any Unix utility which performs machine translation from the command line without connecting to some company\u2019s API online?\n\nIt should be something that just works out of the box; it\u2019s ok if it only works in limited ways.\n\nThanks very much", "upvote_ratio": 1.0, "id": "t3_smz9n2", "created_utc": 1644263925.0}
{"sub": "LanguageTechnology", "title": "Any pointers as to creating a bot that generates silly quotes?", "selftext": "Title pretty much sums it up. I am looking for any pointers you might have to help me build a bot that generates silly quotes.", "upvote_ratio": 0.78, "id": "t3_smydor", "created_utc": 1644261693.0}
{"sub": "LanguageTechnology", "title": "Extracting useful information from product reviews", "selftext": "I'm trying to extract useful facts from product reviews, to make the buying decision easier.\n\nI do that by letting humans  annotate the phrases that contain the most common word n-groups, and then trying to find other n-groups, that come up in similar contexts as the  annotated ones.\n\n&amp;#x200B;\n\nAt the moment, what I'm generating looks like this: [https://www.rantyu.com/handcreams/productinfo.php?pc=B004RRH90Q](https://www.rantyu.com/handcreams/productinfo.php?pc=B004RRH90Q)\n\nWhen a fact seems to be common, I also use it as a search filter:\n\n[https://www.rantyu.com/handcreams/search.php](https://www.rantyu.com/handcreams/search.php)\n\n&amp;#x200B;\n\n1. How would you approach this problem?\n2. What other useful info would you add on the product page, besides the extracted facts, product info, frequent nouns and frequent adjectives?", "upvote_ratio": 1.0, "id": "t3_sms0pk", "created_utc": 1644245488.0}
{"sub": "LanguageTechnology", "title": "How to create a broad/representative sample from millions of records?", "selftext": "Hey all. \n\n\nI have millions and millions of small text documents. Within the project, I\u2019m looking to generate a relatively small sample to use as a training dataset.  \n\n\nI haven\u2019t been able to find anything really about this\u2026 which makes me think I\u2019m just searching the wrong terms. The one [stackoverflow comment](https://datascience.stackexchange.com/questions/81005/sampling-methods-for-text-datasets-nlp) I found goes unanswered\u2026 \n\n\n\nI do have some ideas if I have to push on blindly. Maybe do some doc2vec and then work on creating a uniform subset from the vectors? Haven\u2019t given this much thought yet, as you can tell. \n\n\nTldr is I\u2019m looking for blogs/papers/packages to sample a large text dataset, resulting in a (relatively) small sample to use.", "upvote_ratio": 1.0, "id": "t3_smd223", "created_utc": 1644195833.0}
{"sub": "LanguageTechnology", "title": "BERT: Understanding the Null response threshold in QA systems", "selftext": "nan", "upvote_ratio": 0.92, "id": "t3_smar5h", "created_utc": 1644189542.0}
{"sub": "LanguageTechnology", "title": "[R] PromptBERT: Improving BERT Sentence Embeddings with Prompts. tl/dr For sentence embeddings, an input text prompt out performs average pooling and the CLS token. Anyone else confused by this?", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_slkn3o", "created_utc": 1644105704.0}
{"sub": "LanguageTechnology", "title": "OpenAI Team Introduces \u2018InstructGPT\u2019 Model Developed With Reinforcement Learning From Human Feedback (RLHF) To Make Models Safer, Helpful, And Aligned", "selftext": "A system can theoretically learn anything from a set of data. In practice, however, it is little more than a model dependent on a few cases. Although pretrained language models such as Open AI\u2019s GPT-3 have excelled at a wide range of natural language processing (NLP) tasks, there are times when unintended outputs, or those not following the user\u2019s instructions, are generated. Not only that, but their outcomes have been observed to be prejudiced, untruthful, or poisonous, potentially having harmful societal consequences.\n\nOpenAI researchers have made substantial progress in better aligning big language models with users\u2019 goals using reinforcement learning from human feedback (RLHF) methodologies. The team proposed [InstructGPT](https://openai.com/blog/instruction-following/) models that have been demonstrated to produce more accurate and less harmful results in tests. \n\n[**Continue Reading**](https://www.marktechpost.com/2022/02/05/openai-team-introduces-instructgpt-model-developed-with-reinforcement-learning-from-human-feedback-rlhf-to-make-models-safer-helpful-and-aligned/)\n\nOpen AI Blog**:** [https://openai.com/blog/instruction-following/](https://openai.com/blog/instruction-following/)\n\nPaper: [https://cdn.openai.com/papers/Training\\_language\\_models\\_to\\_follow\\_instructions\\_with\\_human\\_feedback.pdf](https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf)", "upvote_ratio": 0.94, "id": "t3_sler0f", "created_utc": 1644089478.0}
{"sub": "LanguageTechnology", "title": "Locality-sensitive hashing in Reformer model", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_slci7r", "created_utc": 1644082664.0}
{"sub": "LanguageTechnology", "title": "Using Physics to Teach Computers to Speak! Unity3D + GPT Project Overview", "selftext": "nan", "upvote_ratio": 0.67, "id": "t3_skho3l", "created_utc": 1643992372.0}
{"sub": "LanguageTechnology", "title": "How to obbtain probability for entire sequence (Huggingface transformers)", "selftext": " \n\nI want to encode certain, predetermined sentences, such as e.g.\n\n    s1 = \"He is a good-hearted person.\" \n    s2 = \"He is a blockheaded person.\" \n\nand compare the **overall probabilities** for each of these sequences, to see which is more likely.\n\nHow do I obtain such probabilities using any of the huggingface pretrained transformers?", "upvote_ratio": 1.0, "id": "t3_skdpoj", "created_utc": 1643982522.0}
{"sub": "LanguageTechnology", "title": "Top 10 Sentiment Analysis APIs", "selftext": "nan", "upvote_ratio": 0.54, "id": "t3_skafa3", "created_utc": 1643971665.0}
{"sub": "LanguageTechnology", "title": "wav2vec2", "selftext": "How do we calculate the receptive field from the convolution configuration?\n\n eg. wav2vec2 paper says\n\nThe feature encoder contains seven blocks and the temporal convolutions in each block have 512 channels with strides (5,2,2,2,2,2,2) and kernel widths (10,3,3,3,3,2,2). This results in an encoder output frequency of 49 hz with a stride of about 20ms between each sample, and a receptive field of 400 input samples or 25ms of audio.", "upvote_ratio": 0.72, "id": "t3_sk8au5", "created_utc": 1643963311.0}
{"sub": "LanguageTechnology", "title": "auto-regressive decoder in Transformer", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_sk86yr", "created_utc": 1643962885.0}
{"sub": "LanguageTechnology", "title": "txtai 4.1 released - semantic search workflows with scheduling", "selftext": "nan", "upvote_ratio": 0.9, "id": "t3_sk0xqg", "created_utc": 1643939411.0}
{"sub": "LanguageTechnology", "title": "[Urgent]Training mean-teacher for token level classification", "selftext": "I am planning to apply mean-teacher for my problem of token classification. Since adding different noise for teacher and student is really important for the approach, i am confused about how to calculate consistency cost as length of active logits would differ. for e.g. if i use synonym noise then it can happen that it increases the length of the sentence (some tokens maybe replaces by synonym of len 2) when given to teacher model and same augmentation/Noise may generate different sentence(of different length) when given to student model. Can anyone please help as i am stuck. Paper link for ref. : [https://arxiv.org/abs/1703.01780](https://arxiv.org/abs/1703.01780)", "upvote_ratio": 0.76, "id": "t3_sjys2g", "created_utc": 1643933517.0}
{"sub": "LanguageTechnology", "title": "Holy $#!t: Are popular toxicity models simply profanity detectors? [OC]", "selftext": "nan", "upvote_ratio": 0.96, "id": "t3_sjxsem", "created_utc": 1643930842.0}
{"sub": "LanguageTechnology", "title": "How to represent sequential triples in an ontology?", "selftext": "I have the sentence \"The leaf was green before it was brown.\" and I would like to represent it using an ontology.\n\nObviously, we have two triples:\n\n     :leaf :was \"green\", \"brown\" .\n\nHowever, how do we represent that `:leaf :was \"green\"` comes before `:leaf :was \"brown\"`?\n\nI'm thinking there has to be an event like:\n\n    _:a a :event ; :changes :leaf ; :traitChanged :color ; :from \"green\" ; :to \"brown\" .\n\nWhat are people's thoughts on this?\n\nI tried comparing to Stanford CoreNLP annotation, but it just returns this:\n\n    1.0\tleaf\twas green\tbefore brown\n    1.0\tleaf\twas\tgreen\n    1.0\tleaf\twas green\tbrown\n    1.0\tit\twas\tbrown\n    1.0\tit\twas\tbefore brown", "upvote_ratio": 1.0, "id": "t3_sjxsba", "created_utc": 1643930834.0}
{"sub": "LanguageTechnology", "title": "Need advise for domain specific QA system.", "selftext": "I am trying to build a question answering system on instruction manuals (kind of one that comes with electrical appliances) . I am still at a very initial phase and I have some doubts.\n\n1. The manuals are in PDF format. They are highly unstructured with tables, images and text in no specific format (different appliance manufacturers have different formats). I have tried various techniques and ways to extract text and split the pdf into smaller sub-documents (like page-wise split, paragraph-wise split, section-wise split, etc.) but none of them seem to be \"the solution\" as each approach has its own drawbacks. Are there any better ways to extract text with any suggestions for pre-processing of data and building a document store?\n2. I have tried various pre-trained models on small paragraphs of text. They give near accurate answers to most of the short answer type questions but struggle if the queries are more technical. Moreover the models aren't fit for LFQA. I've been looking into Extractive QA but I am a little lost with so much of information. What adds to the problem is the lack of domain specific dataset that could've helped in fine-tuning the model.\n\nCan you please suggest any resources, code-implementations or projects I could refer to that might help. Any sort of suggestion or thought will be immensely appreciated. Thank you for taking time to read this.", "upvote_ratio": 1.0, "id": "t3_sjoct8", "created_utc": 1643907927.0}
{"sub": "LanguageTechnology", "title": "Looking for partners for Kaggle competition", "selftext": "Hello! I'm looking for partners for [an NLP Kaggle competition](https://www.kaggle.com/c/feedback-prize-2021/overview). I'm a BA in linguistics and have experience developing classical NLP models (ontologies and decision trees based on SEMs). I also have a good understanding of NNs and the mathematics that underlay the models, but I don't have any experience developing them. \n\n&amp;#x200B;\n\nI would love to team up with someone who's available to put in the work to have a great portfolio piece and maybe even win some money.", "upvote_ratio": 0.87, "id": "t3_sjoaf0", "created_utc": 1643907761.0}
{"sub": "LanguageTechnology", "title": "[Project] Refining the Natural language processing course - Feedback v2 and thank you", "selftext": "I\u2019m Sourabh, I lead one of the core Tensorflow teams at Google Brain and worked on data products at Coursera with Andrew Ng. Kaushik Rangadurai, ML Engineer at Facebook and I are leading a live, cohort based course on NLP starting March 14th. [https://corise.com/course/natural-language-processing](https://corise.com/course/natural-language-processing).\n\nThis is the second run of the class and we learned a lot from the feedback of the reddit community from the first run in November. Some of the changes we're making from the previous iteration:\n\n1/ More focus on transformers and less on RNN/LSTM as hugging face is becoming the defacto for any content.\n\n2/ Pytorch lightning has some really easy to use interfaces so better organizing the boiler plate code.\n\n3/ OpenAI has opened the GPT-3 API so a deeper dive into current possibilities.\n\nWould love to continue getting feedback and build this to be a great resource. The plan is to open the content after we refine it to a degree we're happy with. You can join the course (capped at about 30 students) at the link above. If you\u2019re open to giving feedback on the class on how we can do better, happy to give a discount.", "upvote_ratio": 0.87, "id": "t3_sjnp6o", "created_utc": 1643906372.0}
{"sub": "LanguageTechnology", "title": "References for multi-lingual digital dictionaries?", "selftext": "I see e-readers have very nice digital dictionaries. Where can we find digital dictionaries for multiple languages. Any note about digital dictionaries that can be consumed with code would be appreciated. It would be perfect if it include the POS tag and lemmas.", "upvote_ratio": 1.0, "id": "t3_sjmnwv", "created_utc": 1643903843.0}
{"sub": "LanguageTechnology", "title": "Local text generation (InferKit alternative)", "selftext": "[This Radiohead post](https://old.reddit.com/r/radiohead/comments/sbg4is/i_used_an_ai_program_to_predict_the_rest_of/) is a good example of what [InferKit's Text Generation](https://inferkit.com/docs/generation) can do with the [demo page](https://app.inferkit.com/demo).\n\nDoes anyone here know of any free software programs or frameworks that can do this sort of text generation locally with a free model?", "upvote_ratio": 1.0, "id": "t3_sjmexo", "created_utc": 1643903192.0}
{"sub": "LanguageTechnology", "title": "A library for storing and retrieving tag data", "selftext": "Here is a library that allows accessing different nlp taggers (esp. spacy, flair, stanza) from a unified API. Also allows saving / loading the results for larger document collections. Might be useful to other people.\n\n[https://github.com/poke1024/nlabel](https://github.com/poke1024/nlabel)", "upvote_ratio": 0.86, "id": "t3_sjj7xw", "created_utc": 1643894986.0}
{"sub": "LanguageTechnology", "title": "Reversible layer for Transformer", "selftext": "nan", "upvote_ratio": 0.67, "id": "t3_sjh2l8", "created_utc": 1643888295.0}
{"sub": "LanguageTechnology", "title": "11 Best Natural Language Processing Online Courses", "selftext": "nan", "upvote_ratio": 0.64, "id": "t3_sjf980", "created_utc": 1643881711.0}
{"sub": "LanguageTechnology", "title": "Really need some guidance with my dissertation - information extraction on political text", "selftext": "I am aiming to do information extraction on Hansard, a record of what is said in UK Parliament. I have 3 months and I am not even sure where to start. I did almost nothing last semester between a really shitty unit sucking up my time and depression. \n\nMy supervisor does not seem very in to the field and has other focuses.\n\nI have tried applying Stanford's OpenIE to some of the text. It outputs trash. I had intended to aim to make improvements on it, eg improved NER, coreference resolution, filtering input based on sentiment analysis, then maybe some kind of filtration on the output to reduce false positives. A major issue is that it is very poor at NER, outputting relations with fragmented boundaries. I don't know if attacking these problems is feasible in 3 months.\n\nI have 3 months. I am losing my fucking mind. Any pointers are appreciated.\n\nedit\n\nmaybe just scrapping all of that and instead doing some sentiment analysis to investigate MP's views on other MP's and parties over time would be more feasible. I would have a much smaller named entity domain.", "upvote_ratio": 1.0, "id": "t3_sj00by", "created_utc": 1643836257.0}
{"sub": "LanguageTechnology", "title": "Is it a good idea to combine encoders and decoders from different models?", "selftext": "I was looking at a problem where I only need the decoder of a specific architecture. So would it be a good idea to train my encoder and use a pre-trained decoder from that model?", "upvote_ratio": 1.0, "id": "t3_siars7", "created_utc": 1643762955.0}
{"sub": "LanguageTechnology", "title": "Stand-alone sentence segmenter", "selftext": "Does anyone know a good standalone sentence segmentation tool / method that isn\u2019t part of a broader NLP module like NLTK or Spacy?\n\nIdeally, just a single standalone function. I could pip install it or maybe even just copy and paste in some code into a file.\n\nThanks very much", "upvote_ratio": 1.0, "id": "t3_si7cs8", "created_utc": 1643753886.0}
{"sub": "LanguageTechnology", "title": "Espial - NLP tool to automate discovery and organization of personal knowledge", "selftext": "nan", "upvote_ratio": 0.76, "id": "t3_si604g", "created_utc": 1643750525.0}
{"sub": "LanguageTechnology", "title": "Treebanks with PTB style bracketing", "selftext": "Hi everyone!\n\nAre there any corpora or treebanks that are labeled with Penn Treebank-style constituency trees (other than the PTB itself)?\n\nI'm investigating the usage of a certain syntactic construction in English. I'm using the PTB constituency trees, but it would be nice to have as much data as possible. I found the Georgetown University Multilayer Corpus (GUM), and I'm wondering if there are others.\n\nThanks in advance!", "upvote_ratio": 1.0, "id": "t3_shq4rt", "created_utc": 1643704333.0}
{"sub": "LanguageTechnology", "title": "Where to find an up-to-date list of the top-k most common words in English", "selftext": "I need to identify the \\~ 10 000 most common words in the English language as of some reliable / well-established corpus. I intended to use this repo [https://github.com/first20hours/google-10000-english](https://github.com/first20hours/google-10000-english)\n\nbut I realized that it is somewhat outdated. Any ideas where I could find a comprehensive, up-to-date list of the most common words in English? Thanks :)", "upvote_ratio": 0.92, "id": "t3_sh7h2x", "created_utc": 1643651868.0}
{"sub": "LanguageTechnology", "title": "CKY algorithm", "selftext": "\n\nHey,\n\nCould someone please simply explain the CKY algorithm, how it works?\n\nlet the input be a string I consisting of n characters: a1 ... an.\n\nSo the input is just characters, not tokenized words?\n\nlet the grammar contain r nonterminal symbols R1 ... Rr, with start symbol R1.\n\nWhat are non terminal symbols?\n\nlet P[n,n,r] be an array of booleans. \n\nThis is a three dimensional array? Why does n occur twice?\n\nInitialize all elements of P to false.\n\nfor each s = 1 to n\n\n\n    for each unit production Rv \u2192 as\n        set P[1,s,v] = true\n\nWhat is unit production?\n\nfor each l = 2 to n -- Length of span\n    for each s = 1 to n-l+1 -- Start of span\n        for each p = 1 to l-1 -- Partition of span\n            for each production Ra    \u2192 Rb Rc\n                if P[p,s,b] and P[l-p,s+p,c] then set P[l,s,a] = true\n\nif P[n,1,1] is true then\n    I is member of language\nelse\n    I is not member of language\n\nhttps://en.m.wikipedia.org/wiki/CYK_algorithm\n\nThanks very much", "upvote_ratio": 1.0, "id": "t3_sh0cip", "created_utc": 1643632847.0}
{"sub": "LanguageTechnology", "title": "Current book about extracting text from structured and unstructured documents (PDFs, Word, Excel)?", "selftext": "I am quite used to the Machine learning aspects of NLP, but I am lacking knowledge on how to make raw texts accessible and how to handle meta data. Is there a good book on this - preferably in Python?", "upvote_ratio": 1.0, "id": "t3_sh08de", "created_utc": 1643632435.0}
{"sub": "LanguageTechnology", "title": "Searching participants for art project about AI", "selftext": "Hi,\n\nI\u2019m part of an art group from Switzerland currently studying at HSLU Design &amp; Arts ([https://www.hslu.ch/de-ch/design-kunst/studium/bachelor/camera-arts/](https://www.hslu.ch/de-ch/design-kunst/studium/bachelor/camera-arts/)).\n\nThe group consists of:\n\nKarim Beji ([https://www.instagram.com/karimbeji\\_/](https://www.instagram.com/karimbeji_/) [https://karimbeji.ch/](https://karimbeji.ch/))\n\nEmanuel Bohnenblust ([https://www.instagram.com/e.bohnenblust/](https://www.instagram.com/e.bohnenblust/))\n\nLea Karabash ([https://www.instagram.com/leakarabashian/](https://www.instagram.com/leakarabashian/))\n\nYen Shih-hsuan ([https://www.instagram.com/shixuan.yan/](https://www.instagram.com/shixuan.yan/) [http://syen.hfk-bremen.de/](http://syen.hfk-bremen.de/))\n\nAt the moment, we are working on a project on the topic if AI can augment the happiness of humans. To answer this question, we are mainly working with chatbots. The end result is going to be an exhibition at the end of March. \n\nFor that exhibition, we want to conduct a trial in which people from over the world chat with a chatbot to find out if and how it augments the mood of the participants. \n\nWe would give you access to a GPT-3 (OpenAI) chatbot and ask you to a) record yourself through a webcam (laptop) while you are chatting and b) simultaneously screen record the chat window. \n\nIn the exhibition we would have a) a book with all the chats and b) small videos with your faces (webcam) to assess your mood. \n\nWe would have a Zoom meeting beforehand to discuss everything.\n\nLooking forward to your message!", "upvote_ratio": 1.0, "id": "t3_sgypv1", "created_utc": 1643626972.0}
{"sub": "LanguageTechnology", "title": "Structure identified Named Entities in a meaningful manner", "selftext": "I have trained a SpaCy NER model which can identify Name, Address, Institute, Degree, Skill , Company, Designation, School, Society and Location in a Resume. Now I want to structure recognized entities in such a way that CV owners name, address, skills and other details are together &amp; Referees name , address, Designations separately. Is there a way to do it? I mean I want to have CV owners data together and Referees data together.", "upvote_ratio": 1.0, "id": "t3_sgxqg3", "created_utc": 1643623169.0}
{"sub": "LanguageTechnology", "title": "academic ethics issues in NLP", "selftext": "Hi all. I was wondering what are the most interesting academic ethics issues we have to account for while researching in this area?", "upvote_ratio": 0.92, "id": "t3_sg4teu", "created_utc": 1643531903.0}
{"sub": "LanguageTechnology", "title": "Using fasttext to load word embeddings", "selftext": "I want to use pre-trained fasttext model to load word embeddings for all words ( including out of vocab words ) in English. I have tried to read up how to do this , and so far haven't been able to get it to work.\n\nI would appreciate it if someone could give me the code for it or point me to some tutorial that would help me do it. \n\nI am from a Statistics background, and a new Python User. Trying to navigate through everything to get things done !", "upvote_ratio": 1.0, "id": "t3_sg20z5", "created_utc": 1643521198.0}
{"sub": "LanguageTechnology", "title": "Using embedding model in C++ app", "selftext": "I am looking for a good way to use pre-trained embedding models (from huggingface or tensorflow hub) in my C++ application running on a local CPU.\n\nMy solution so far: I am using a compiled Tensorflow C DLL in combination with cppflow (https://github.com/serizba/cppflow). However, I get problems when I take models which use operations from the tensorflow_text python module since I don\u2019t know how to get their C++ API.\n\nHas somebody experience in doing so? Or in general, has someone used local embedding models in a C++ app before?", "upvote_ratio": 1.0, "id": "t3_sff0hf", "created_utc": 1643450476.0}
{"sub": "LanguageTechnology", "title": "Replicate WebNLG 2017 challenge with OpenNMT-tf", "selftext": "Hello guys, i'm data science student, i'm trying to replicate WebNLG 2017 challenge with OpenNMT-tf.\n\nI have already performed the same challenge with OpenNMT-py and everything went well.\n\nWhen using the tensoflow version, some doubts arose:\n\n* how to build vocabularies from webnlg\\_baseline\\_input.py output files: \\['train-webnlg-all-delex.triple', 'train-webnlg-all-delex.lex', 'dev-webnlg-all-delex.triple' , 'dev-webnlg-all-delex.lex'\\]. since in the tensorflow version a transformation step in a bpe file is required;\n* how to build the default model of openNMT-py (LSTM with 2 layers of 500 units);\n\nI tried to follow this notebook but, given the doubts expressed above, the results were not the same.\n\n([https://github.com/Parkchanjun/OpenNMT-Colab-Tutorial/blob/master/OpenNMT\\_Tensorflow\\_Tutorial.ipynb](https://github.com/Parkchanjun/OpenNMT-Colab-Tutorial/blob/master/OpenNMT_Tensorflow_Tutorial.ipynb)) \n\nHow can I do? Thanks all.", "upvote_ratio": 1.0, "id": "t3_sesv7z", "created_utc": 1643382418.0}
{"sub": "LanguageTechnology", "title": "Confusion about BERT masking?", "selftext": "I am trying to understand the masking in BERT model.\n\nI have confusion in following line taken from paper\n\n&gt;The training data generator chooses 15% of the token positions at random for prediction. If the i-th token is chosen, we replace the i-th token with (1) the \\[MASK\\] token 80% of the time (2) a random token 10% of the time (3) the unchanged i-th token 10% of the time\n\nat point 3 it say unchanged token (i think it mean unmasked token) 10% time. If we have to use original token 10% of 15% tokens, then why we need to mask it.\n\nThis can be more clear in **Attempt 4: Masked LM with Random Words and Unmasked Words** section of [this guide](https://mlwhiz.com/blog/2021/07/24/bert-sketches/).\n\nThe guide say\n\n&gt;So if we have a sequence of length 500, we will mask 75 tokens(15% of 500), and in those 75 tokens, 7 tokens(10 % of 75) would be replaced by random words, and 7 tokens (10% of 75) will be used as it is.\n\nSo if we have to use 7 tokens as it is, then why we masked them first?", "upvote_ratio": 0.92, "id": "t3_ses9wt", "created_utc": 1643380762.0}
{"sub": "LanguageTechnology", "title": "Has anyone ever used spacy with a fit predict structure", "selftext": "Spacy is an industry standard, and I have been using it ever since I've been in the field. One thing I have always wanted is for spacy to have fit and predict methods, much like sklearn. I understand spacy has its own forms, like the evaluate method. Of course, it probably won't be hard to build a fit predict method based wrapper around spacy, but **I am wondering if anyone has ever come across any such wrapper?** \n\nBenefit of such a wrapper would be that when building retraining pipelines with models from various libraries, most of which use fit and predict, being able to call fit and predict on a spacy model would simplify things.", "upvote_ratio": 0.75, "id": "t3_se6nzq", "created_utc": 1643312255.0}
{"sub": "LanguageTechnology", "title": "Extracting information about entities automatically", "selftext": "I came across an interesting problem, Let's say given two sentences, \"Meet Harry and David and take them to London and Athens. These two cities are worth exploring. Here, the second sentence mentions that the two entities are cities. Is there any method to assign these two entities to category city? I am more concerned about different approaches we can use, may be rule based methods to deep learning based methods.", "upvote_ratio": 1.0, "id": "t3_se48zj", "created_utc": 1643305999.0}
{"sub": "LanguageTechnology", "title": "Benchmarking NLP Datasets", "selftext": "Hello Everyone,\n\nI am a newbie in NLP research. My question is - How should we benchmark a new Language dataset/corpus (ex. dialogue dataset, q/a dataset) when there is no publicly available dataset for that particular language? Also what are the possible directions to perform evaluation on the newly prepared dataset. Need suggestions, please.", "upvote_ratio": 0.76, "id": "t3_sdurp5", "created_utc": 1643276829.0}
{"sub": "LanguageTechnology", "title": "[R] ML &amp; NLP Reasearch Highlights of 2021 - by Sebastian Ruder", "selftext": "[ML and NLP Research Highlights of 2021](https://ruder.io/ml-highlights-2021) by Sebastian Ruder, actually research scientist at Google in London, ex-DeepMind: Universal Models, Massive Multi-task Learning, Beyond the Transformer (aka cross-attention), Prompting, Efficient Methods, Benchmarking, Conditional Image Generation, ML for Science, Code Synthesis, Bias, Retrieval Augmentation, Token-free Models, Temporal Adaptation, Importance of Data  and Meta-learning.", "upvote_ratio": 0.98, "id": "t3_sds8q8", "created_utc": 1643266290.0}
{"sub": "LanguageTechnology", "title": "OpenAI Releases Three Embedding Model Families To Optimize Text Search, Code Search and Text Similarity", "selftext": "In the last few decades, neural networks have been used for a wide range of tasks, including image segmentation, natural language processing, and time-series forecasting.\u00a0\n\nOne promising use of deep neural networks is embedding, a method for representing discrete variables as continuous vectors. An embedding is a low-dimensional space into which high-dimensional vectors can be translated, making it easy for computers to understand the relationships between those concepts. Numerically similar embeddings are also semantically identical. Word embeddings for machine translation and entity embeddings for categorical data are two applications of this approach.\u00a0[**Continue Reading**](https://www.marktechpost.com/2022/01/26/openai-releases-three-embedding-model-families-to-optimize-text-search-code-search-and-text-similarity/)\n\nPaper: https://arxiv.org/abs/2201.10005\n\nDocumentation: https://beta.openai.com/docs/guides/embeddings", "upvote_ratio": 0.9, "id": "t3_sdbq8w", "created_utc": 1643219157.0}
{"sub": "LanguageTechnology", "title": "Dependency parser from scratch in Python", "selftext": "I\u2019d like to challenge myself to write my own dependency parser for natural language (English) in Python.\n\nI\u2019m picturing taking in the sentence one word at a time and somehow working backwards to figure out the tree structure of the sentence.\n\nOf course, it should start with tokenization. Perhaps part-of-speech tagging is a necessary next step, to attempt to group certain parts of speech that never dislocate from their complements, like \u201cthe\u201d, or other words with predictable behavior, like \u201cand\u201d and conjunctions and so on?\n\nLike the game \u201cMindmaster\u201d, one can work backward layer upon layer to deduct what the original structure of the sentence is.\n\nHowever, maybe I need effective segmentation for this to work? I\u2019m wondering how periods will affect this procedure. I could ignore them and hope the dependency parse can still work. For example, \u201cand\u201d never appears at the end of a sentence.\n\nAnother idea is to try to design a neural network myself rather than using a library like Spacy. I just need to know what architecture is optimal and practice training it a bit.\n\nAnyone have any recommendations on this?\n\nThanks very much", "upvote_ratio": 1.0, "id": "t3_sd3sh6", "created_utc": 1643196266.0}
{"sub": "LanguageTechnology", "title": "Import part of Spacy library", "selftext": "Does anybody know if you can just import part of the Spacy library you need?\n\nI find \u201cimport spacy\u201d to be the slowest part of using spacy.\n\nWhat takes so long to load? All the pipeline scripts or something? Because loading the language model with spacy.load() and constructing the doc object with doc(text) are faster than the initial import.\n\nI\u2019d like to just load the pipelines I need or something from the module, like \u201cfrom spacy import \u2026\u201d\n\nDoes anyone know of this is possible?\n\nThanks very much", "upvote_ratio": 1.0, "id": "t3_sd2kmw", "created_utc": 1643191489.0}
{"sub": "LanguageTechnology", "title": "Visual Intro to Basic Semantic Search", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_sd1gbu", "created_utc": 1643186757.0}
{"sub": "LanguageTechnology", "title": "Search query suggestion/autocomplete", "selftext": "I\u2019m planning to add this feature to a search bar. Any starting package/model/tool suggestion?", "upvote_ratio": 0.76, "id": "t3_sczm2z", "created_utc": 1643179398.0}
{"sub": "LanguageTechnology", "title": "Question regarding the denominators in Kneser-Ney Smoothing", "selftext": "Hi everyone!\n\nI am currently studying smoothing techniques, specifically Kneser-Ney smoothing. I understand that it helps to handle the case where the next word hasn't appeared in the given context previously. For eg, the corpus could have non zero trigram counts of 'This is a', but no occurrence of the 4-gram 'This is a car'.\n\nThe count C(This is a) is captured in the denominator of the lambda term as well, and this lambda term is multiplied with the recursion term. My question is, what if that particular count is actually zero? I hope the following the mini example can make my question clearer - \n\nCorpus:\n\n&lt;s&gt; &lt;s&gt; You are my friend &lt;/s&gt; &lt;/s&gt;\n\n&lt;s&gt; &lt;s&gt; They are my enemies &lt;/s&gt; &lt;/s&gt;\n\n&lt;s&gt; &lt;s&gt; I have friends and enemies &lt;/s&gt; &lt;/s&gt;\n\nSay we would like to find the probability of the trigram 'are you friend', or P(friend | are you). As per the formulation given in page 9 of the document:\n\n[https://u.cs.biu.ac.il/\\~yogo/courses/mt2013/papers/chen-goodman-99.pdf](https://u.cs.biu.ac.il/~yogo/courses/mt2013/papers/chen-goodman-99.pdf)\n\nThe denominator of two terms consists of the count of the bigram 'are you'. But from the corpus this is zero, and at the same time, each of the individual words 'are' and 'you' aren't UNK, as their unigram counts are 1 and 2 respectively. So how does the recursion proceed now, since we cannot divide by zero?  \n\n\nThank you!", "upvote_ratio": 0.81, "id": "t3_scyef9", "created_utc": 1643175048.0}
{"sub": "LanguageTechnology", "title": "Interesting NLP project ideas", "selftext": "Hello, I'm a postgraduate student and I have a NLP project that I have to come up with and do (as part of a course, not a MSc thesis). What are some really interesting ideas that you could recommend?   \n\n\nAn example of an interesting and good project is: [https://www.ucl.ac.uk/computer-science/news/2019/oct/msc-machine-learning-paper-receives-international-acclaim](https://www.ucl.ac.uk/computer-science/news/2019/oct/msc-machine-learning-paper-receives-international-acclaim) .", "upvote_ratio": 0.62, "id": "t3_scfo4t", "created_utc": 1643122856.0}
{"sub": "LanguageTechnology", "title": "How to create a question answering model that can trigger specific actions?", "selftext": "I want to create a model that can trigger specific actions based on the input given to the model. For example, if the user asks where is the nearest petrol pump the model will trigger the google maps API and calculate the distance.", "upvote_ratio": 0.92, "id": "t3_sccj0j", "created_utc": 1643113459.0}
{"sub": "LanguageTechnology", "title": "How to tackle the problem of Dangling Modifier (Help Needed)", "selftext": "Hello Geeks, I am trying to solve the problem of dangling modifiers and have not been able to think of any solutions or a better way to put this would be from where to start to solve the problem. If you people have any solution or any pointers which you could share it will be really helpful.\n\nDangling Modifier  Examples -\n\n1. Orig - Fumbling in her purse, the keys could not be found.\n2. Modified - Fumbling in her purse, she could not find the keys.\n3. Orig -  Having injured his dominant hand, it was difficult to write the exam.\n4. Modified -  Having injured his dominant hand, John had difficulty writing the exam.\n\nThank you.", "upvote_ratio": 0.63, "id": "t3_sc54sq", "created_utc": 1643085110.0}
{"sub": "LanguageTechnology", "title": "Is there a way I can see connections between two senses in Wordnet?", "selftext": "For example, let's say I have the first sense of the verb \"go\" and the first sense of the verb \"walk\". I want to see the connection between these two words. In other words, if I start with go#1 (v), how can I arrive at walk#1 (v)?", "upvote_ratio": 0.72, "id": "t3_sc2u7d", "created_utc": 1643078132.0}
{"sub": "LanguageTechnology", "title": "How Alexa learned Arabic", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_sbs4lj", "created_utc": 1643047091.0}
{"sub": "LanguageTechnology", "title": "Is there no standard train/dev/test split for the Quora Question Pairs dataset of duplicate questions, *with labels for all splits*?", "selftext": "QQP is a dataset of duplicate and non-duplicate question pairs from Quora. I think it was originally developed as part of a Kaggle competition:\n\nhttps://www.kaggle.com/c/quora-question-pairs\n\nThe competition released a training set with labels, and a test set without labels. QQP has subsequently been used in many papers developing new architectures for document similarity and duplicate detection, but unfortunately, as far as I can tell, there is no standard train/dev/test split of the dataset *that has labels for each of train, dev, and test*, and therefore people make up their own splits on the Kaggle training set. Am I mistaken? Is there a widely agreed on train/dev/split of this dataset with labels for each split?", "upvote_ratio": 0.76, "id": "t3_sbpmnj", "created_utc": 1643040605.0}
{"sub": "LanguageTechnology", "title": "I made a tutorial on how to do Speech Recognition with Kaldi!", "selftext": " Hey everyone,\n\nKaldi is a really powerful toolkit for ASR and related NLP tasks, but I've found that the learning curve is a bit steep.\n\n[**Here's**](https://www.assemblyai.com/blog/kaldi-speech-recognition-for-beginners-a-simple-tutorial/) **a tutorial I made that takes you through installation and transcription using pre-trained models**, but the cool part is that you can decide how advanced you want it to be!\n\nIncluded are Python scripts to automate the entire process, so you can generate transcriptions in just a few lines of code, but I also dive into the code itself to explain what's going on under the hood!\n\nI'd love to hear any thoughts and feedback, or future topics you want to see covered!", "upvote_ratio": 1.0, "id": "t3_sbnic5", "created_utc": 1643034696.0}
{"sub": "LanguageTechnology", "title": "How to test statistical significance on text data?", "selftext": "So, I was in an interview and I was asked so many questions about statistical details on text data. For example \n1. How would you sample million sentences from billions of sentences? What strategies will you use for sampling?\n\n2. Having sampled, how would determine that the sampled data follows actual data distribution? (In nutshell how would you determine whether two text data distributions are similar or not)\n\nFollow up for these questions were, When will you decide to re-train your model. (Yet again how would you determine whether data distribution has changed)\n\nNow I am confused about how to perform such statistical analysis over text data. I have understanding about DL approaches within NLP, but stats is something bugging me a lot during the interviews. (I have worked less/no in stats than actual model building)\n\nPlease advice me how to solve these about mentioned questions as well as where should I start working/learning on stats for such questions. Will be very helpful.", "upvote_ratio": 0.96, "id": "t3_sbhz36", "created_utc": 1643015375.0}
{"sub": "LanguageTechnology", "title": "First foray into NLP at work, need feedback on the workflow.", "selftext": "Hi everyone, could use some input/thoughts on an NLP workflow I am helping to design based on a pretty unique(?) situation .\n\nEssentially this is a binary classification problem; we have around 1000 documents and because of the sheer messiness of the text, a team has provided us with the key phrases taken from these documents that denote whether the document should be a 'Yes'. As opposed to labeling the entire document itself, if that makes sense. \n\nBased on this, I was thinking that instead of using the entire set of documents as a corpus (standard approach), that we would take the collection of key phrases as a corpus instead. Then vectorize it using the standard TF-IDF approach, then use that as the input to the classifier.\n\nCouple questions on this:\n1) Firstly, does the construction of the corpus in this manner even make sense, since we're not using the entirety of the document?\n\n2) Is there a way to incorporate BERT embeddings into this? My supervisor is very keen on the cutting edge stuff and wants to incorporate BERT, but best I can tell there's not really a way to do this based on the workflows I have seen. The standard TF-IDF approaches seem different from the BERT workflow, which typically involves fine tuning on my corpus and then making predictions from there. \n\nThanks!", "upvote_ratio": 1.0, "id": "t3_sb6xiq", "created_utc": 1642979852.0}
{"sub": "LanguageTechnology", "title": "Run your own locally-hosted translation service with a couple lines of code", "selftext": "nan", "upvote_ratio": 0.94, "id": "t3_sat7e6", "created_utc": 1642942975.0}
{"sub": "LanguageTechnology", "title": "NLP architecture for paragraph extraction based on fixed question", "selftext": "Hi, I am looking for leads on how to tackle a NLP problem. \n\nIn short: I have 5 fixed (not changing) questions where I extracted in sum 160 answers to from 5 longer text documents (approx 21 pages of text per doc).\n\nI want to build a sort of question answering model that gets another text document (\\~20 pages of text input) and suggests the question answering paragraphs of the document.\n\n&amp;#x200B;\n\nDuring Research the following questions arised: \n\n1. Looking through question answering models I am not sure if my text input might be too big?\n2. Also is the problem too \"supervised\" for classical question answering approaches? \n3. Is there a way to transfer learn the supervised question answers from the document into existing models, any experiences on similar approaches?\n\n&amp;#x200B;\n\nThank you all, appreciate your feedback.", "upvote_ratio": 1.0, "id": "t3_saqjvi", "created_utc": 1642932846.0}
{"sub": "LanguageTechnology", "title": "Personalization for semantic search using vector db from another organization", "selftext": "I have a question around personalization for semantic search- when using semantic search api through a third party, for instance pinecone or any vector db company, how is personalization possible? Do you also pass the last 10 session actions/ or info about the user through the api?", "upvote_ratio": 1.0, "id": "t3_saiq21", "created_utc": 1642904455.0}
{"sub": "LanguageTechnology", "title": "Authorship Attribution dataset for short texts.", "selftext": "Hi, I'm looking for an Authorship Attribution dataset with small-medium texts (Mostly social media excerpts if possible). Looked everywhere but couldn't find any, found a great dataset for large texts (Blogs) but none for small texts. Would like to search if one exists to hopefully not have to scrap it myself.", "upvote_ratio": 0.75, "id": "t3_sac5np", "created_utc": 1642885609.0}
{"sub": "LanguageTechnology", "title": "[P] Finding and correcting text classification label errors with cleanlab and Rubrix | https://rubrix.readthedocs.io/en/master/tutorials/find_label_errors.html", "selftext": "nan", "upvote_ratio": 0.88, "id": "t3_sac14a", "created_utc": 1642885272.0}
{"sub": "LanguageTechnology", "title": "Starting to learn NLP, need some directions. (I'm using catalyst NLP, inspired by spacy)", "selftext": "I'm currently in the process of learning NLP. I am using catalyst on c#. \n\nI was able to run the sample programs and it was able to determine if the word is an noun, adjective, etc. But I can't find any sample for what I need.\n\nHere is a summary of what I would like to achieve. \n\nI would like to extract certain information on a sentence. Lets say i have the following texts:\n\n\"Sally ate an orange this morning. \"\nOr \n\"Sally is hiding behind the cabinet and she is eating an orange. \" \n\nHow do i use the nlp to extract what sally ate?", "upvote_ratio": 0.5, "id": "t3_saas64", "created_utc": 1642881778.0}
{"sub": "LanguageTechnology", "title": "Problem downloading - wikipedia split used for evaluating DPR", "selftext": "Hi all,  \n\n\nSo I've been trying to download the wikipedia split that is used for evaluating  [Dense Passage Retrieval (DPR)](https://github.com/facebookresearch/DPR) \n\n&amp;#x200B;\n\nI just pulled up a google colaboratory session and followed their instructions to download the dataset as shown below.\n\n`python data/download_data.py \\`\n\n\t`--resource {key from download_data.py's RESOURCES_MAP}  \\`\n\n\t`[optional --output_dir {your location}]`\n\n&amp;#x200B;\n\nI don't know why, but for some reason, the colab automatically exits the download cell (Output shows \"\\^C\"), and all I get is a .tmp file.\n\n&amp;#x200B;\n\nI believe I should be getting a .tsv file as instructed here (this is taken from the repo BPR - binary passage retrieval, this is an improvement over DPR) :\n\n[https://github.com/studio-ousia/bpr#:\\~:text=python%20data/download\\_data.py%20%2D%2Dresource%20data.wikipedia\\_split.psgs\\_w100](https://github.com/studio-ousia/bpr#:~:text=python%20data/download_data.py%20%2D%2Dresource%20data.wikipedia_split.psgs_w100)\n\n&amp;#x200B;\n\nI am attaching the .ipynb file here too for convenience.\n\n[https://colab.research.google.com/drive/1OFAV9yUO0khCdQCoaa6ukcZC-7WVvvgt?usp=sharing](https://colab.research.google.com/drive/1OFAV9yUO0khCdQCoaa6ukcZC-7WVvvgt?usp=sharing)\n\nAnyone know what I'm doing wrong?\n\n&amp;#x200B;\n\nThanks.", "upvote_ratio": 1.0, "id": "t3_sa9n1f", "created_utc": 1642878603.0}
{"sub": "LanguageTechnology", "title": "Google AI Introduces a Method Called Task-Level Mixture-of-Experts (TaskMoE), that Takes Advantage of the Quality Gains of Model Scaling While Still Being Efficient to Serve", "selftext": "Large-scale language model scaling has resulted in considerable quality gains in natural language understanding (T5), generation (GPT-3), and multilingual neural machine translation (M4). One typical method for creating a more extensive model is to increase the depth (number of layers) and breadth (layer dimensionality), essentially expanding the network\u2019s existing dimensions. Such dense models take an input sequence (split into smaller components known as tokens) and route each token through the whole network, activating every layer and parameter. While these big, dense models have shown cutting-edge outcomes on various natural language processing (NLP) applications, their training costs rise linearly with model size.\n\nBuilding sparsely activated models based on a mixture of experts (MoE) (e.g., GShard-M4 or GLaM), where each token supplied to the network follows a distinct subnetwork by bypassing some of the model parameters, is an alternative and more common technique. Small router networks that are educated with the rest decide how to distribute input tokens to each subnetwork (the \u201cexperts\u201d). This enables researchers to increase the model size (and hence performance) without increasing training costs proportionally. [***Continue Reading***](https://www.marktechpost.com/2022/01/21/google-ai-introduces-a-method-called-task-level-mixture-of-experts-taskmoe-that-takes-advantage-of-the-quality-gains-of-model-scaling-while-still-being-efficient-to-serve/)\n\nPaper: [https://arxiv.org/pdf/2110.03742.pdf](https://arxiv.org/pdf/2110.03742.pdf)", "upvote_ratio": 0.92, "id": "t3_s9sjqz", "created_utc": 1642820797.0}
{"sub": "LanguageTechnology", "title": "Questions from a clueless student", "selftext": "Hi everybody,\n\nI've become interested in NLP and would like to get started on preparing for some master's courses. My background is modern languages, and I've been teaching myself Python alongside my internship. I'd love to work in machine translation, or even build my own machine translation engine. What advice do you all have for a clueless boi on getting started in NLP? Thank you!", "upvote_ratio": 0.72, "id": "t3_s99cke", "created_utc": 1642766992.0}
{"sub": "LanguageTechnology", "title": "spacy ner introduction and usage ( english)", "selftext": "nan", "upvote_ratio": 0.67, "id": "t3_s8s3mx", "created_utc": 1642710478.0}
{"sub": "LanguageTechnology", "title": "Item2Vec - Word2Vec from gensim wrapped as sklearn estimator for GridSearchCV", "selftext": "Prod2Vec or Item2Vec produces embedding for items in a latent space. The method is capable of inferring item-item relations even when user information is not available. It's based on NLP model Word2Vec. [Click here](https://arxiv.org/pdf/1603.04259.pdf#:~:text=Inspired%20by%20SGNS%2C%20we%20describe,user%20information%20is%20not%20available.) to know more\n\nThis project provide a class that encapsulates Item2Vec model ([word2vec](https://radimrehurek.com/gensim/models/word2vec.html) gensim model) as a [sklearn estimator](https://scikit-learn.org/stable/developers/develop.html).\n\nIt allows the simple and efficient use of the Item2Vec model by providing :\n\n* metric to measure the performance of the model ([Precision@K](https://arxiv.org/pdf/0704.3359.pdf))\n* compatibility with [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) and [BayesSearchCV](https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html) to find the optimal hyperparameters\n\n&amp;#x200B;\n\n[https://github.com/MathieuCayssol/Item2Vec](https://github.com/MathieuCayssol/Item2Vec)", "upvote_ratio": 0.93, "id": "t3_s8o288", "created_utc": 1642699387.0}
{"sub": "LanguageTechnology", "title": "Questions about BigBird", "selftext": "Hello, people. I still have some questions after reading the paper about Big Bird model ( [https://arxiv.org/pdf/2007.14062v2.pdf](https://arxiv.org/pdf/2007.14062v2.pdf) ) and will be happy if some Big Bird specialists will help me to understand this model better.\n\n1. Is distribution of random attention (Figure 1 (a)) fixed from advance for all inputs, or it somehow can be different for different inputs even on the same head?\n2. In BIGBIRD-ETC, do they add some additional global tokens, aside of \\[CLS\\]?\n3. In BIGBIRD-ITC, how is the subset of tokens for global attention chosen?\n4. Why is infinite precision necessary for sparse transformer to be Turing complete?\n\nThank you.", "upvote_ratio": 0.86, "id": "t3_s8lxk7", "created_utc": 1642693719.0}
{"sub": "LanguageTechnology", "title": "Detecting the Presence of an Object in a Sentence", "selftext": "Is there any way to differentiate between the absence or inclusion of an object in a sentence? For example: there is not a cat in my yard, the cat disappeared from my yard, a cat does not exist in my yard, etc. versus there is a cat in my yard, the cat appeared in my yard, a cat exists in my yard.\n\nAny help would be greatly appreciated!", "upvote_ratio": 0.57, "id": "t3_s8j2qk", "created_utc": 1642685503.0}
{"sub": "LanguageTechnology", "title": "The Structure of Language | Noam Chomsky", "selftext": "nan", "upvote_ratio": 0.4, "id": "t3_s85d57", "created_utc": 1642638974.0}
{"sub": "LanguageTechnology", "title": "[D] Did you also feel that Snorkel's LabelModel is really slow?", "selftext": "Has anyone here used Snorkel AI's LabelModel for automatically labeling text? Have you found it to be super slow?", "upvote_ratio": 0.9, "id": "t3_s7vype", "created_utc": 1642614770.0}
{"sub": "LanguageTechnology", "title": "Seeking string \"Readability\" metric.", "selftext": "Hello fellow enthusiasts, \n\nI have a corpus of 150k documents, and their respective OCR outputs. \n\nI'd like to assign a Readability score to each document, is there a metric out there for something like that? \n\nIn retrospect to my OCR extraction, which took almost a month of runtime to run, I *could* have extracted an OCR-accuracy score along with my strings. I'd like to find an alternative solution instead of re-running it. Knowledge for next time, anyways...\n\nI'm open to all thoughts and considerations.", "upvote_ratio": 1.0, "id": "t3_s7r2pj", "created_utc": 1642601826.0}
{"sub": "LanguageTechnology", "title": "Simplest keyword extractor", "selftext": "I\u2019ve been trying to do some basic keyword extraction and finding it harder than expected.\n\nKeyBERT seems good but it requires a powerful GPU to be usably fast. That\u2019s possible with AWS, but there\u2019s a bit more set up.\n\nI just tried PyTextRank, and I was surprised at the quality of the output - I wouldn\u2019t say it was perfect either. Maybe I should set a threshold, like choose the top 200 ranked keywords? It\u2019s fine if we exclude potential good keywords just to have a smaller list of good ones.\n\nHere\u2019s a good article about 7 different methods, which is helpful -\n\nhttps://towardsdatascience.com/keyword-extraction-a-benchmark-of-7-algorithms-in-python-8a905326d93f.\n\nIn theory, Spacy and BERT seem like the best options but they\u2019re both a little complex. \n\nI think KW extraction really only needs a few layers or as Spacy would call them pipelines.\n\n1. accurate tokenization of words and punctuation symbols\n\n2. accurate recognition of multi-word expressions (phrases) - think of it as \u201cchunking\u201d\n\n3. Strong assessment of keyword \u201ccandidacy\u201d for each MWE (could be purely rule-based, corpus based, or machine learning based)\n\nOf course, a good algorithm can often skip steps. Like BERT is so smart it doesn\u2019t need anything but the input text.\n\nDoes anyone know of a simplest way to run a fast, effective keyword extraction?\n\nI\u2019m talking 200 keywords in one second on a fast CPU.\n\nThanks very much", "upvote_ratio": 0.91, "id": "t3_s7qml8", "created_utc": 1642600495.0}
{"sub": "LanguageTechnology", "title": "Create a good prompt for openAI text-generation model", "selftext": "Hi guys, I'm a data science student and i'm trying to build a new dataset using OpenAi framework.\n\nMy idea is to use 5000 phrases and triples to train a data-to-text OpenAi model which given a triple generates of the text.\n\nreading the OpneAi documentation, I found this script for reference:\n\n    import os\n    import openai\n    \n    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n    \n    response = openai.Completion.create(\n      engine=\"davinci\",\n      prompt=\"English: I do not speak French.\\nFrench: Je ne parle pas fran\u00e7ais.\\n\\nEnglish: See you later!\\nFrench: \u00c0 tout \u00e0 l'heure!\\n\\nEnglish: Where is a good restaurant?\\nFrench: O\u00f9 est un bon restaurant?\\n\\nEnglish: What rooms do you have available?\\nFrench: Quelles chambres avez-vous de disponible?\\n\\nEnglish: What time is breakfast?\\nFrench:\",\n      temperature=0.5,\n      max_tokens=100,\n      top_p=1.0,\n      frequency_penalty=0.0,\n      presence_penalty=0.0,\n      stop=[\"\\n\"]\n    )\n\ni would like that the above script follows this scructure: \n\nprompt = \"Triples: subject, predicate, object.\\\\nEnglish:text generated\"\n\n    import os\n    import openai\n    \n    openai.api_key = \"mykey\"\n    \n    response = openai.Completion.create(\n      engine=\"ada\",\n      prompt= \"Triples: Aarhus_Airport, cityServed, Aarhus_Denmark.\\nEnglish:The Aarhus     \n           is the airport of Aarhus, Denmark.\\n\n               Triples:Alan_Bean, almaMater,UT_Austin_B.S._1955.\\nEnglish:The Alma Mater \n           of Alan Bean is UT Austin, B.S. 1955.\\n\n               Triples:103_Colmore_Row, architecturalStyle, Brutalist_architecture.\\n \n           English:The architecture style of 103 Colmore Row falls under Brutalist \n           architecture.\\n\n               Triples:The_Phoenix, Fast_food, riverside.\\nEnglish:\",\n      temperature=0.5,\n      max_tokens=100,\n      top_p=1.0,\n      frequency_penalty=0.0,\n      presence_penalty=0.0,\n      stop=[\"\\n\"]\n    )\n\nexpected output from the last triple (The\\_Phoenix, Fast\\_food, riverside.):\n\n    The Phoenix is a fast food place in the riverside area.\n\nFor example, given the webnlg dataset (xml file) with this strcuture:\n\n     &lt;entry category=\"Airport\" eid=\"Id1\" size=\"1\"&gt;\n          &lt;originaltripleset&gt;\n            &lt;otriple&gt;Aarhus_Airport | cityServed | \"Aarhus, Denmark\"@en&lt;/otriple&gt;\n          &lt;/originaltripleset&gt;\n          &lt;modifiedtripleset&gt;\n            &lt;mtriple&gt;Aarhus_Airport | cityServed | \"Aarhus, Denmark\"&lt;/mtriple&gt;\n          &lt;/modifiedtripleset&gt;\n          &lt;lex comment=\"good\" lid=\"Id1\"&gt;The Aarhus is the airport of Aarhus, Denmark.&lt;/lex&gt;\n          &lt;lex comment=\"good\" lid=\"Id2\"&gt;Aarhus Airport serves the city of Aarhus, Denmark.&lt;/lex&gt;\n        &lt;/entry&gt;\n    \n\nI would like to randomly extract sentences and their triples from this dataset and use them as training. How can I insert them into the \"prompt\" variable automatically without writing by hand ?.\n\n&amp;#x200B;\n\nThanks!", "upvote_ratio": 0.81, "id": "t3_s7niat", "created_utc": 1642589457.0}
{"sub": "LanguageTechnology", "title": "Rewriting to Fit Author's Style", "selftext": "Hello all,\n\nI'm trying to get smarter on mimicing writing style based on sample input text. The hope is a system like this:\n\n**Inputs:**\n\n* Input tagged sample writing/letters/emails/dialogue from desired author.\n* Basic sentence to be rewrite (assuming some word embedding/transformer to abstract meaning).\n\n**Output:**\n\n* Translated sentence written in sample author's writing style.\n\nI'm assuming this is a bit of an ambitious lift and may require some training on my own. Curious if anyone has any insights on stylometry papers written. Even something on generative text that is just meant to replicate an author's style could be a helpful starting point.\n\nThanks!", "upvote_ratio": 1.0, "id": "t3_s7a0rx", "created_utc": 1642545475.0}
{"sub": "LanguageTechnology", "title": "Pyarabic: a python package for the Arabic language ( brief description with basic simplified in english)", "selftext": "nan", "upvote_ratio": 0.67, "id": "t3_s79qhb", "created_utc": 1642544731.0}
{"sub": "LanguageTechnology", "title": "Fine-tuning reader models for Question-Answering", "selftext": "Hi all, I put together some material on [fine-tuning reader models for open-domain question-answering](https://www.pinecone.io/learn/reader-models) (ODQA). ODQA is an increasingly popular approach to building more human/natural language information retrieval tools. Allowing users to store massive amounts of text data, and then search using natural language questions, it is one of the technologies that powers Google search. Reader models are the final step in an ODQA pipeline, allowing us to extract very specific answers to questions.\n\nLet me know what you think, I hope it's useful, thanks!", "upvote_ratio": 1.0, "id": "t3_s726uh", "created_utc": 1642525369.0}
{"sub": "LanguageTechnology", "title": "Sentence-transformer Bert model performs worse after fine-tuning", "selftext": "I'm using [symanto/sn-xlm-roberta-base-snli-mnli-anli-xnli](https://huggingface.co/symanto/sn-xlm-roberta-base-snli-mnli-anli-xnli) from HuggingFace. After multiple tries with different batch sizes, epochs, learning rates and even different unsupervised learning models methods such as [this](https://www.sbert.net/examples/unsupervised_learning/TSDAE/README.html), I couldn't get my sentence transformer to perform better than raw model straight from HuggingFace. I'm not sure what I'm doing wrong. I'm sure there are no bugs in my code since I followed the sentence transformer model documentation almost verbatim.\n\nbackground on my task: my datasets consists of a list of sentences(legal articles\u2014 around \\~300 small sentences) and a person will enter a query of say 3-5 sentences and I'm supposed to find the \"correct\" matches for the query.\n\nCurrently, the base model isn't amazing but it's also not too bad. Hence, I expected better performance once I fine-tune it. However, upon fine-tuning, cosine similarity scores all drop and the fine-tuned model has never made a better prediction(map from query to correct sentence in the dataset) than the original model with no fine tuning.\n\nI'd like to know why that might be the case and if that's a normal thing that usually happen with nlp models. My dataset is very small so my guess is my training parameters were bad? or is my training data so insignificant that my fine-tuning simply doesn't matter?", "upvote_ratio": 0.94, "id": "t3_s6y0ah", "created_utc": 1642513906.0}
{"sub": "LanguageTechnology", "title": "Unicode error in vectorizing text", "selftext": " \n\nI am trying to vectorize 20-news group data  \n using tensorflow TextVectorization layer  \n but in TextVectorization layer  \n if I limit the vocab size to some number say 10000 then it works fine. However if I preprocess the data or do not set the vocab size to some number then I get UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfe in position 2257: invalid start byte  \n error.\n\nMy question is have I done something wrong in preprocessing? Because if I set the vocab size to 10000 and do the same preprocessing then this wont work. Also, do I need to set vocab size in 'TextVectorization\\` but the docs says it can have unlimited size?\n\n&amp;#x200B;\n\n \n\nHere is what I did :\n\ni. Get the list of files:\n\n     train_dir_list = []\n     for i in os.listdir(train_dir):\n         f = os.path.join(train_dir, i)\n         for j in os.listdir(f):\n           train_dir_list.append(os.path.join(f, j)) \n\nii. Create tensorflow data\n\n    train_data = tf.data.TextLineDataset(train_dir_list) \n\niii. Preprocess data\n\n    def preprocess(text):\n       lower = tf.strings.lower(text)\n       # remove emails\n       email_removed = tf.strings.regex_replace(      lower, \"\\S*@\\S*\\s?\", \"\"   )\n       # remove numbers\n       number_removed = tf.strings.regex_replace(       email_removed, \"[0-9]\", ' '   )\n       # remove punctuations\n       punctuation_removed =  tf.strings.regex_replace(       number_removed, '[%s]' % re.escape(string.punctuation), ' '   )\n       # remove multiple blank spaces\n       multiple_space_removed =tf.strings.reduce_join(tf.strings.split(punctuation_removed),      separator=\" \")\n       return multiple_space_removed \n\niv. Create vectorizer: In here if I remove the standardize=preprocess  \n and keep the vocab\\_size and sequence\\_length  \n it works fine. But if I use standardize=preprocess  \n either with same vocab\\_size and sequence\\_length  \n or do not use standardize=preprocess  \n but keep the vocab\\_size and sequence\\_length  \n empty or as default then it gives the UnicodeDecodeError:\n\n    vocab_size = 10000 sequence_length = 300 \n\nThis works fine:\n\n    vectorize_layer = layers.TextVectorization(\n         # removed preprocess\n         max_tokens=vocab_size,\n         output_mode='int',\n         output_sequence_length=sequence_length ) \n\nThis will throw error:\n\n    vectorize_layer = layers.TextVectorization(\n         standardize=preprocess,\n         max_tokens=vocab_size,\n         output_mode='int',\n         output_sequence_length=sequence_length ) \n\nThis will also give error :\n\n    vectorize_layer = layers.TextVectorization(#used default parameter) \n\nv. calling adapt works fine\n\n    vectorize_layer.adapt(train_data.batch(1024)) \n\nvi. This is were the error is thrown\n\n    vectorize_layer.get_vocabulary() \n\nAlso, on looking up the vocab size when the error is produced, the vocab size is only around 30-40", "upvote_ratio": 0.67, "id": "t3_s6wvu7", "created_utc": 1642510480.0}
{"sub": "LanguageTechnology", "title": "Vocab size for word2vec implementation", "selftext": " \n\nI am trying to implement word2vec for large corpus may be in billions of words if possible. I am following [Word2vec tensorflow](https://www.tensorflow.org/tutorials/text/word2vec) as a reference, where they have used a vocab size of 4096 and sequence length of 10. My question is, if I use other corpus with billions of words should I limit the vocab size to some numbers like 10000 and sequence length or create vocab for all the unique words present in the corpus ?\n\nI want to know how did gensim and other library trained their model on large corpus, did they limit the size of vocabulary or trained on all the unique words present in the corpus ?", "upvote_ratio": 1.0, "id": "t3_s6qaog", "created_utc": 1642485582.0}
{"sub": "LanguageTechnology", "title": "searching for free available document for language processing", "selftext": "Hey Im searching the web for a document with following criteria for a nlp model \n\n30+ pages \n\ncontain :\n\nknowledge\n\nrules \n\ninstructions\n\n\\--- no speculation unclear content inside like research papers \n\n\\------ mostly text no relevant pictures or formulas \n\n&amp;#x200B;\n\nI would greatly appropriate any help and tips", "upvote_ratio": 0.76, "id": "t3_s6c319", "created_utc": 1642446741.0}
{"sub": "LanguageTechnology", "title": "What LSTM Baseline To Use?", "selftext": "Suppose you are writing a paper with some new transformer-variant that does well on classification tasks. You want to have a LSTM baseline. How would you go about choosing that LSTM architecture? What about training hyperparameters? Is there a standard, should it be grounded with respect to another paper, does it not matter as long as one explains what the architecture is?\n\n&amp;#x200B;\n\nThanks!", "upvote_ratio": 1.0, "id": "t3_s66i5v", "created_utc": 1642433105.0}
{"sub": "LanguageTechnology", "title": "I'm building a neural search plugin for Elastic/Opensearch", "selftext": "Hey Everyone,\n\nI'm building a plugin for end-to-end neural search (eg SBERT, Hugging Face, Doc2Vec etc) in Opensearch and I'd love to hear any suggestions from the NLP community of what you might find useful or about issues you've had with Opensearch/neural search in the past.\n\nDespite the Elasticsearch website claiming in many places that you can do \"machine learning\" with Elasticsearch, I've found that it's not straight forward at all to use neural search algos with ES/Opensearch. In most cases (putting to one side some specific cases like anomaly detection), you have to implement the ML algorithm yourself and you only get to use ES as a storage layer. Some 3rd party plug and play frameworks that support ES seem quite promising but also lack functionality in terms of data retrieval - for example, Cherche seems to enforce that users first retrieve documents using an algorithm like tf-idf before putting them through neural search.\n\nI want to build a plugin/service that will allow users to more readily take advantage of the vector functionality and neural search in general. I've found the following issues:\n\n1. Opensearch/ES have added a great deal in terms of functionality to allow for vector search (eg approximate knn algo), but it seems entirely up to the user to encode the word embeddings. Therefore, users must add code to manually encode any documents and queries into their chosen embeddings before searching/adding data. I think users having their embeddings is generally a good idea if they want a high level of optimisation, but for many use cases, pretrained embeddings should be a \"good enough\" solution.\n2. If the data is in text format, it cannot easily be converted into a format to be used with algorithms like SBERT etc without reindexing the entire index and running it through a custom script to change the data into a vector format.\n3. I'd suspect for many users who arn't NLP experts, navigating all of the potential options for embeddings/Neural Search architecture could be quite overwhelming. Having a configurable plugin where they can try different options would likely help them to accelerate getting started.\n\nI think letting users have their own embeddings makes sense from an optimisation perspective but I think also it would be amazing to have an end to end solution where you can connect different algos directly into Opensearch. I'm also exploring extending this and allowing users to refresh/update these embeddings to continually improve them.\n\nLet me know what you think, open to any suggestions! If you want to keep up to date with this, here is a google form [https://forms.gle/acmGTK1gPkPZbVJm8](https://forms.gle/acmGTK1gPkPZbVJm8)", "upvote_ratio": 0.91, "id": "t3_s5ethj", "created_utc": 1642348913.0}
{"sub": "LanguageTechnology", "title": "[P] Open-source tool for building NLP training sets with weak supervision and search queries", "selftext": "nan", "upvote_ratio": 0.71, "id": "t3_s5dmsy", "created_utc": 1642345368.0}
{"sub": "LanguageTechnology", "title": "Metric for text summarization", "selftext": "Recently, I was reading some literature about text summarization and came across its evaluation metric, the \"ROUGE\" score. From what I understood from preliminary reading, the ROUGE score only measures n-gram overlap between candidate summary and reference summary which wrongly penalizes abstractive summaries containing different n-grams but conveying the same meaning.  There's also a **BERTScore** metric (arXiv'19, ICLR'20) that does not suffer from these issues of ROUGE and computes contextual similarity rather than just n-gram overlap. How can I assess if BERTScore is a better evaluation metric compared to ROUGE?  (consider me a beginner in NLP)", "upvote_ratio": 0.88, "id": "t3_s5ao3z", "created_utc": 1642335236.0}
{"sub": "LanguageTechnology", "title": "How is multidimensional scaling plot supposed to be for word embeddings?", "selftext": "I plotted MDS plot for word embeddings obtained from BERT. The \\[CLS\\] token is plotted on the middle of the figure and other word embeddings are scattered . Is it supposed to be in middle? Is there any significance to it? MDS plot was plotted on the basis of pairwise cosine similarity.", "upvote_ratio": 0.71, "id": "t3_s51i8l", "created_utc": 1642301236.0}
{"sub": "LanguageTechnology", "title": "I'm conducting research in NLP with data pulled from multiple sources, primarily Reddit, Twitter, and Facebook. The data contains different categories which are mentioned in the description below. Is anyone familiar with the ethics or the problems that came up using data like this?", "selftext": "The different categories include:\n\n1. Posts that were deleted by the user themselves.\n2. Posts that were banned by the Community moderators.\n3. Posts that were banned by the Platform moderators.\n4. Pages or communities containing posts that were banned by the Platform moderators.\n\nI'm fairly uncertain about whether all the data that was pulled contains reasons for the ban they faced. In the case of deleted posts, there's no such label available.\n\nAny idea how to go about this? Any link to cited paperwork that has faced and dealt with similar problems would be great. Links or mentions of authors who might have faced this issue also help as I can try reaching out to them. I'm having some trouble finding sources.\n\nEven similar datasets links would be great as I can do a comparison study on this.\n\nThanks. :D", "upvote_ratio": 0.85, "id": "t3_s4cjy3", "created_utc": 1642223400.0}
{"sub": "LanguageTechnology", "title": "What is an example of a problem of a specific 2022 that NLP can solve?", "selftext": "For example a project similar to scanning for cyberbullying comments but one that has been done already", "upvote_ratio": 0.27, "id": "t3_s47pz3", "created_utc": 1642208404.0}
{"sub": "LanguageTechnology", "title": "Farsi &gt; English Translation Model", "selftext": "I\u2019m just wondering if anybody knows of any good Farsi (Persian) &gt; English translation models? I\u2019ve tried a few of the multilingual ones from Huggingface but the quality isn\u2019t the best", "upvote_ratio": 0.76, "id": "t3_s41hmv", "created_utc": 1642191320.0}
{"sub": "LanguageTechnology", "title": "Scientific Literature Review Generation v1.0", "selftext": "Hello ,\n\nI've developed after my PhD a first version of an algorithm to automatically generate a literature review : [https://www.naimai.fr](https://www.naimai.fr/) and many remarks were given. I just deployed a new version with much more papers and I'll be thankful if you have any remarks about it :)\n\nMore about the new version here : [https://yaassinekaddi.medium.com/scientific-literature-generation-ii-73628aebd4fb](https://yaassinekaddi.medium.com/scientific-literature-generation-ii-73628aebd4fb)\n\nHopefully that could be useful for the PhDs (and the non PhDs) !\n\n&amp;#x200B;\n\nCheers,", "upvote_ratio": 0.9, "id": "t3_s3whz6", "created_utc": 1642177989.0}
{"sub": "LanguageTechnology", "title": "Help needed. How to predict profession from short bio ?", "selftext": "Hi NLP community,\n\nProbably much of a newbie here and need some guidance. I am doing a personal project that aims to predict a person's industry from their short biography.\n\n&amp;#x200B;\n\nFor example:\n\n\" I am a retired engineer and company manager. I do not have a financial background or offer financial advice. blah blah \" =&gt; **Prediction:** ENGINEERING\n\nand\n\n\" Damon makes his living as a gap trader, an earnings trader, and an interday trader. In his free time, he writes for ABC, where he focuses on seasonal investing, market timing, and earnings analyses. \" =&gt; **Prediction:** FINANCE\n\n&amp;#x200B;\n\nI wanted to ask what approach should i do to make such predictions ? And what kind of public dataset would be useful to train a ML model for such task ?\n\n&amp;#x200B;\n\nThank you so much !", "upvote_ratio": 0.72, "id": "t3_s3iw6i", "created_utc": 1642133379.0}
{"sub": "LanguageTechnology", "title": "Using Stanford NLP want to get sentiment for full context with bias", "selftext": "I'm using the Stanford NLP to help with a personal project. Last week I couldn't tell you what a Verb or Adverb was to be quite honest. I've been feeding my C# program titles/comments from stock investing subs to get the \"Sentiment\". Someone posted:\n\nTo r/ALL: 1 Year ago, the most unprecedented move in the history of the stock market happened. \\[Positive\\]  \nThe Buy Button was turned off for a specific stock. \\[Negative\\]  \n1 year later and there have been NO CONSEQUENCES. \\[Neutral\\]  \nNo one went to jail. \\[Neutral\\]  \nWas there even a fine? \\[Neutral\\]   \nWhy? \\[Negative\\]   \nHow is the answer not going to sound like a conspiracy \\[Neutral\\]\n\nI understand the algorithm is evaluating each sentence. How would one go about determining the full context? Clearly this example has a neutral/negative assumption. \n\nFurther more, bias... so if I have a left or right leaning comment to which I could tell the program i'm in favour of L/R then the algorithm would deem that as a postitive to my bias.", "upvote_ratio": 0.89, "id": "t3_s392e0", "created_utc": 1642106307.0}
{"sub": "LanguageTechnology", "title": "NLP Bias &amp; (un)Fairness Recognizer App with Spacy | Day 2 of #8daysofstreamlit", "selftext": "nan", "upvote_ratio": 0.76, "id": "t3_s38zn9", "created_utc": 1642106108.0}
{"sub": "LanguageTechnology", "title": "Remote company looking for an NLP DS!", "selftext": "[This](https://apply.workable.com/bunnystudio/j/8E938022F3/) position is currently open and I wanted to share with you!", "upvote_ratio": 0.84, "id": "t3_s30ccv", "created_utc": 1642083152.0}
{"sub": "LanguageTechnology", "title": "Pretrained models for multi-label classification (transformer based)", "selftext": "Hi,\n\nI am looking at multi-lable classification for Twitter-like data. Does anybody know if any open source project (or Huggingface model hub) has any pre-trained models ready to use?\n\nI am looking at classification taxonomy such as [IAB v2 categories](https://www.iab.com/guidelines/content-taxonomy/) or Wikipedia categories.  \n\n\nThanks!", "upvote_ratio": 0.8, "id": "t3_s300v8", "created_utc": 1642082261.0}
{"sub": "LanguageTechnology", "title": "Making sentences of a dialogue simple and clear", "selftext": "I have a task where I need to come up with a system that takes sentences from dialogues and makes them more simple and clear.  Sentences during dialogues are often filled with filler words such as uhm, uh and repetitions and my goal is to extract the information of the sentence and present it in a clear form.\n\nE.g:\n\nOriginal: \"And then I drew it on the board uhm white board because that's easier to understand for the student uh i mean students\"\n\nSimplified: \"I drew it on the whiteboard because it's easier to understand for the students\"\n\nI just took an introductory NLP course in my undergraduate degree so I'm still relatively new to the subject. Any learning resources and advice on how to do this would be greatly appreciated.", "upvote_ratio": 1.0, "id": "t3_s2t02e", "created_utc": 1642056598.0}
{"sub": "LanguageTechnology", "title": "Understanding BLEU metric", "selftext": "In the BLEU paper [https://aclanthology.org/P02-1040.pdf#page=2](https://aclanthology.org/P02-1040.pdf#page=2) , why **In Example 1, Candidate 1 achieves a modified unigram precision of 17/18; whereas Candidate 2 achieves a modified unigram precision of 8/14.** ?", "upvote_ratio": 0.76, "id": "t3_s2neoo", "created_utc": 1642039194.0}
{"sub": "LanguageTechnology", "title": "Alternatives to Google Cloud Translate", "selftext": "A lot of cloud computing services have a lot of vendors in the field but is there any other API for machine translation like Google Translate? DeepL or any others?\n\nThanks very much", "upvote_ratio": 0.86, "id": "t3_s2gg6u", "created_utc": 1642020337.0}
{"sub": "LanguageTechnology", "title": "What do think of doing empirical study on open source model for my master thesis?", "selftext": "The problem with my university is we have to first write proposal and only after approved we get a supervisor on thesis domain. But the proposal take lot of effort and time. Even choosing topic takes lot of time. So reddit is the only option I have.\n\nI wanted my master thesis to be moderatly hard, so choosed to do empirical study instead and was finding hard to choose the topic. I found some research papers do empirical study, for eg: [https://aclanthology.org/P19-1493/](https://aclanthology.org/P19-1493/). What do you guys think? Is doing empirical study on open source model like BERT is appropriate for master thesis?", "upvote_ratio": 0.83, "id": "t3_s2gd3g", "created_utc": 1642020113.0}
{"sub": "LanguageTechnology", "title": "Suggestions for Simple Natural Language Projects that haven't been done yet?", "selftext": "Also possible in Python programming language", "upvote_ratio": 0.5, "id": "t3_s2egjq", "created_utc": 1642015224.0}
{"sub": "LanguageTechnology", "title": "Fine-tuning and implementing retrievers in open-domain Q&amp;A", "selftext": "Hi all, I've just published another article focusing on [fine-tuning a retriever model for open-domain question-answering](https://www.pinecone.io/learn/retriever-models/). The retriever is a big component of the open-domain QA pipeline, allowing us to retrieve relevant *contexts* from a vector database, which then help us answer a query (article includes vector db &lt;&gt; retriever setup too)\n\nLet me know if you have any questions or feedback, thanks!", "upvote_ratio": 1.0, "id": "t3_s26r9f", "created_utc": 1641995581.0}
{"sub": "LanguageTechnology", "title": "txtai 4.0 released - semantic search with SQL, content storage, object storage, reindexing and more", "selftext": "txtai 4.0 has been released with a number of new features.\n\n* **Content Storage** \\- Content can now be stored alongside embeddings vectors. No longer required to have external data storage.\n* **Query with SQL** \\- txtai supports both natural language queries and SQL queries  \n`embeddings.search(\"feel good story\")`  \n`SELECT id, text, score FROM txtai WHERE similar('feel good story') AND score &gt;= 0.15`\n* **Object Storage** \\- Store binary objects alongside embeddings vectors\n* **Reindex** \\- Indexes can be rebuilt using stored content, no need to resend data to txtai\n* **Index Compression** \\- Indexes can be compressed using GZ/XZ/BZ2/ZIP\n* **External Vectors** \\- Use external vector models from an API or an external library. Centralize building vectors on GPU servers leaving index servers to be powered by more modest hardware.\n\nMore information can be found in following links.\n\n* [GitHub Project](https://github.com/neuml/txtai)\n* [4.0 Release Notes](https://github.com/neuml/txtai/releases/tag/v4.0.0)\n* [What's new in txtai 4.0](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/24_Whats_new_in_txtai_4_0.ipynb)\n* [Documentation](https://neuml.github.io/txtai)\n* [Examples](https://neuml.github.io/txtai/examples/)", "upvote_ratio": 1.0, "id": "t3_s25i2x", "created_utc": 1641991728.0}
{"sub": "LanguageTechnology", "title": "Choosing a LT program", "selftext": "Hi everyone! I am a BA graduate with background in linguistics and self-taught programmer. I have been looking at two programs in language technology Msc. in Speech and Language Processing in Edinburgh ([https://www.ed.ac.uk/ppls/linguistics-and-english-language/prospective/postgraduate/msc/speech-language-processing](https://www.ed.ac.uk/ppls/linguistics-and-english-language/prospective/postgraduate/msc/speech-language-processing)) and Human Language Technology ([https://linguistics.arizona.edu/master-science-human-language-technology-hlt](https://linguistics.arizona.edu/master-science-human-language-technology-hlt)) in the States and I was wondering I anyone has any advice in which would be a better fit? I am hoping to start working in the industry after graduating (open to do a PhD but maybe more in the future) but I am worried about which one would help me be more prepared. The one in Edinburgh is a one-year program while the one in AZ is a two-year program. Would appreciate any help with your viewpoints on this :)", "upvote_ratio": 0.81, "id": "t3_s1wvwl", "created_utc": 1641960572.0}
{"sub": "LanguageTechnology", "title": "A dataset of parse trees generated from abstracts of arXiv articles", "selftext": "[https://github.com/l74d/scholarly-trees](https://github.com/l74d/scholarly-trees)\n\nI have put up some (not so few) parse trees online as a dataset. Not something as substantial as Penn Treebank, since the trees have NOT been human-edited, but still way more parse trees than from Penn to feed into your later-stage NLP algorithms, free of charge or hassle.\n\nThe current format is straight from where they were generated. Suggestions of alternative formats based on ease of use would be heavily appreciated!\n\n&amp;#x200B;", "upvote_ratio": 0.96, "id": "t3_s1d4p9", "created_utc": 1641908383.0}
{"sub": "LanguageTechnology", "title": "Twitter topics", "selftext": "Does anyone know how Twitter generates their \u201cTopics\u201d?\n\nIt seems like they could be machine generated (but human reviewed)? It would be a lot of labor simply brainstorming a huge ontology of trending concepts in the Twittersphere.\n\nThey must have some algorithms for analysing and clustering tweet topics.\n\nAnd possibly even for automatically suggesting the name of the cluster (the topic / label).\n\nAnyone have any guesses how they do it?\n\nThanks very much", "upvote_ratio": 0.67, "id": "t3_s103c7", "created_utc": 1641863189.0}
{"sub": "LanguageTechnology", "title": "Webinar: NLU Project Showcase", "selftext": "Stanford Online students will present original projects developed in the Natural Language Understanding professional course. Q&amp;A to follow. [Register here](https://learn.stanford.edu/WBN-AI-NLU-Project-Showcase.html).", "upvote_ratio": 0.67, "id": "t3_s0vkki", "created_utc": 1641851323.0}
{"sub": "LanguageTechnology", "title": "Research topics for a master\u2019s thesis?", "selftext": "I am doing a Master\u2019s conversion course in Computer Science. Previously I did Linguistics for my undergrad. Due to my background, I am interested in conducting my thesis on Natural Language Processing. However, as I\u2019ve never conducted a thesis, I don\u2019t know where to start.\n\nWhat are some good research topics I could read through to get a good idea? Any suggestions on where to start for a beginner in this field would be very much appreciated.", "upvote_ratio": 0.88, "id": "t3_s0uwco", "created_utc": 1641849650.0}
{"sub": "LanguageTechnology", "title": "UC Sandiego Researchers Propose A Controllable Voice Cloning Method That Allows Fine-Grained Control Over Various Style Aspects Of The Synthesized Speech For An Unseen Speaker", "selftext": "Text-to-Speech (TTS) synthesis is achieved using current voice cloning methods for a new voice. They do not, however, manipulate the expressiveness of synthesized sounds. The task of learning to synthesize the speech of an unseen speaker with the least amount of training is known as voice cloning.\n\nUC San Diego researchers propose a Controllable voice cloning method that offers fine-grained control over many style features of synthetic speech for an unseen speaker. The voice synthesis model is explicitly conditioned on a speaker encoding, pitch contour, and latent style tokens during training. [***Continue Reading***](https://www.marktechpost.com/2022/01/10/uc-sandiego-researchers-propose-a-controllable-voice-cloning-method-that-allows-fine-grained-control-over-various-style-aspects-of-the-synthesized-speech-for-an-unseen-speaker/)\n\nPaper: https://arxiv.org/pdf/2102.00151.pdf", "upvote_ratio": 0.81, "id": "t3_s0qfes", "created_utc": 1641838309.0}
{"sub": "LanguageTechnology", "title": "Where do you search for your state of the art NLP papers ?", "selftext": "Hi,  \nI'm looking for the latest papers on different subjects (explicit language detection, keywords/keyphrase detection ..etc)  .   \nTyping on google is not helping. I find only commercial solutions.  \n\n\nThank you", "upvote_ratio": 0.93, "id": "t3_s0q1m4", "created_utc": 1641837372.0}
{"sub": "LanguageTechnology", "title": "Using pre-trained BERT embeddings for multi-class text classification", "selftext": "What would be the steps involved in doing such a thing? Basically I have data from a research,   \n the form of a dataframe including: participant ID, response ID, single word object, and response, which consists of a description of the function of the single word object. Some paricipants have had more than one response from the same object. Object is the same in the whole dataset, and category codings have been done by someone else. Basically responses need to be categorized into these coded categories, denoting similar responses. I want to construct embeddings of the responses and feed them into a CNN. How can this be done quickest? I don't have much knowledge and all the information online is very overwhelming.\n\n&amp;#x200B;\n\nBasically, I want to use the bert uncased model, (so no s-bert or anything of that sort), but I guess I do need to average the world embeddings. Also, how do I tokenize the whole column of responses, and how to add the object into the mix since it is needed as the response and object are connected. Don't expect someone to give me a tutorial in the comments, but a list of general steps to take in this context would be incredibly helpful.  You will save my life and my graduation.", "upvote_ratio": 0.94, "id": "t3_s0p3yw", "created_utc": 1641835002.0}
{"sub": "LanguageTechnology", "title": "Is there a way to detect torn up words?", "selftext": "Example: Qual ity -&gt; quality\n\nI'm using pytesseract to transcribe pdfs, and unfortunately one of the issues is PDF often splits up words at the end of column in two parts . I'm trying to figure out a way to detect when words don't make sense separately but make a normal word combined (using python)", "upvote_ratio": 0.92, "id": "t3_s0l796", "created_utc": 1641824657.0}
{"sub": "LanguageTechnology", "title": "How to speed up inference of your Transformer-based NLP models?", "selftext": "Hello all,\n\nMany of us are having a hard time speeding up our Transformer based NLP models for inference in production.\n\nSo  I thought it would be worth writing an article that summarizes the  options one should consider (GPU, batch inference, export to ONNX or  Torchscript, using TensorRT, Deepspeed, Triton Inference Server...  etc.):\n\n[https://nlpcloud.io/how-to-speed-up-deep-learning-nlp-transformers-inference.html](https://nlpcloud.io/how-to-speed-up-deep-learning-nlp-transformers-inference.html?utm_source=reddit&amp;utm_campaign=ehyiek56-ed8e-11eb-ba80-5242ac130007)\n\nI hope you'll find it useful. If you can think of additional options, please let me know and I'll add them to the article!\n\nJulien", "upvote_ratio": 1.0, "id": "t3_s0h5nv", "created_utc": 1641811182.0}
{"sub": "LanguageTechnology", "title": "Why is Fine tuning a text model so influential on the results?", "selftext": "Newbie to this field, but nonetheless BERT was trained on 3.3 billion+ words, when I do a masked learning task it is fairly successful on my healthcare dataset without fine tuning. However, when I fine tune the dataset, maybe adding only additional 1 million words (only \\~0.02% more words), suddenly the same task is significantly more accurate.\n\nI understand that fine tuning is training the model on my specific task, so of course results will improve, but it is almost as if the model weights the fine-tuned additional words over itself. \n\nWhy can such a small number of additional words improve the model so drastically?", "upvote_ratio": 0.81, "id": "t3_rzuyqo", "created_utc": 1641744709.0}
{"sub": "LanguageTechnology", "title": "AI that can write advanced explicit segmentation rules", "selftext": "Usually you move from rule-based segmentation to just machine learning when it gets too complicated.\n\nBut if we consider all rule based methods at our disposal - not just regular expressions but also having a corpus of words and symbols loaded as identifiers, plus various statistical methods that were used before machine learning like N-grams and so on -\n\nI think it\u2019s conceivable that if we give an AI certain parameters to play with from the gamut of rule-based segmentation methods, it could think of extraordinarily complex yet relatively effective explicit segmentation scripts.\n\nSort of like how Ramanujan could think of extremely complicated formulae in number theory that produced perfect results, and DeepMind recently tried to emulate a complex mind like that to discover new theorems in mathematics.\n\nDoes anyone know of any projects like this?\n\nThank you", "upvote_ratio": 0.67, "id": "t3_rzuiy5", "created_utc": 1641743486.0}
{"sub": "LanguageTechnology", "title": "HMM tagger from scratch", "selftext": "Hey there! I'm trying to build an HMM tagger from scratch in Python, but I'm not so sure of how to go about it. Do you know of any good resources or guides that could be useful to me?", "upvote_ratio": 0.67, "id": "t3_rzs4ah", "created_utc": 1641736244.0}
{"sub": "LanguageTechnology", "title": "Best tools for Multi-GPU Model training?", "selftext": "nan", "upvote_ratio": 0.76, "id": "t3_rzpmd0", "created_utc": 1641727290.0}
{"sub": "LanguageTechnology", "title": "Are there any languages in the world which break some NLP fundamental assumptions?", "selftext": "So, for context, I've only just started learning NLP, and I've just encountered the word2vec algorithm for the first time. This algorithm calculates the probability of a word appearing at a position in a sentence as a function of what it's surrounding words are, weighted by the distance from that central word, learned from a large corpus of language. So for instance, if you fed it an incomplete sentence: \"the cat jumped over the ... \", it would assign high probabilities to words like \"table\", \"mat\", \"bed\", and assign low probabilities to words like \"blue\", \"boil\", \"running\". \n\nAre there any human languages in the world for which the assumptions which the algorithm are built on break? For example, any languages for which the context of a word is *inversely* proportional to it's semantic meaning, rather than proportional as this algorithm assumes? \n\nAre there any other interesting concepts in NLP which work for some languages, but not others?", "upvote_ratio": 0.98, "id": "t3_rzp9h6", "created_utc": 1641725921.0}
{"sub": "LanguageTechnology", "title": "Coqui Introduces \u2018YourTTS\u2019: A Zero-Sample Text-to-Speech Model With State-of-The-Art (SOTA) Results", "selftext": "Recent advancements in end-to-end deep learning models have enabled new and intriguing Text-to-Speech (TTS) use-cases with excellent natural-sounding outcomes. However, the majority of these models are trained on large datasets recorded with a single speaker in a professional setting. Expanding solutions to numerous languages and speakers is not viable for everyone in this situation. It is more challenging for low-resource languages not often studied by mainstream research.\n\nCoqui\u2019s team has designed \u2018[YourTTS](https://coqui.ai/blog/tts/yourtts-zero-shot-text-synthesis-low-resource-languages)\u2018 to overcome these limits and provide zero-shot TTS to low-resource languages. It can synthesize voices in various languages and drastically reduce data requirements by transferring information between the training set.\u00a0 [***Continue Reading***](https://www.marktechpost.com/2022/01/08/coqui-introduces-yourtts-a-zero-sample-text-to-speech-model-with-state-of-the-art-sota-results/)***....***\n\nPaper: https://arxiv.org/pdf/2112.02418.pdf\n\nGithub: https://github.com/coqui-ai/TTS", "upvote_ratio": 0.9, "id": "t3_rz8epu", "created_utc": 1641671793.0}
{"sub": "LanguageTechnology", "title": "Is there an equivalent concept in NLP to what high-level computer languages (e.g. Python) do to manage user error?", "selftext": "Is there an equivalent concept in NLP to what high-level computer languages (e.g. Python) do to manage user error?\n\nThat is:\n\n* natural languages may see users doing errors (grammar etc.)\n\nIn computer languages, however:\n\n* low-level languages (e.g. ASM, C) see users doing errors (e.g. in utilizing memory)\n* therefore people design higher level languages (e.g. Python) that have features that correct these errors\n\nIs there an equivalent concept for natural languages in NLP?", "upvote_ratio": 0.87, "id": "t3_ryyhke", "created_utc": 1641643791.0}
{"sub": "LanguageTechnology", "title": "How hard are automatic grammar checkers?", "selftext": "How hard are automatic grammar checkers?\n\nOver and over again I find that it'd be much smarter to have a program suggest whether your grammar is sound instead of trusting a person to keep track of all nuances over the course of a large number of sentences.\n\nBut I wonder how hard is it to write an automatic grammar checker.\n\nIs it easier for languages like Lojban?", "upvote_ratio": 0.78, "id": "t3_rywwfl", "created_utc": 1641637563.0}
{"sub": "LanguageTechnology", "title": "Just found out Google Translate doesn't provide romanizations of Farsi, and doesn't even have Tibetan at all...", "selftext": "This feels intentional... no?\n\nAlso: [https://www.k-international.com/blog/8-surprising-languages-you-wont-find-on-google-translate/](https://www.k-international.com/blog/8-surprising-languages-you-wont-find-on-google-translate/)\n\nFound some others that are sorely lacking too\u2014what a joke.", "upvote_ratio": 0.27, "id": "t3_rynylf", "created_utc": 1641605752.0}
{"sub": "LanguageTechnology", "title": "Possible interview questions for Research Assistant.", "selftext": "Hello all, \n\n&amp;#x200B;\n\nI am trying to pursue my MS in Computer Science and for funding I have been talking to Professor. He is asking me to brush up my knowledge on Deep Learning and NLP. I am scaring to my death. I have 2 years of working experience but working in company is different from research. In company we just use library with surface understanding and working mechanism to get things done.\n\n&amp;#x200B;\n\nSo, what might be the expectation of any Professor who is hiring any research assistant. What can be  the possible questions any Professors ask in Deep Learning and NLP ?\n\n&amp;#x200B;\n\nThanks in advance. Your help and support means a lot to me.", "upvote_ratio": 1.0, "id": "t3_rxwrax", "created_utc": 1641524304.0}
{"sub": "LanguageTechnology", "title": "Clone your voice and speak a foreign language", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_ry08s3", "created_utc": 1641534910.0}
{"sub": "LanguageTechnology", "title": "How can you do efficient text preprocessing?", "selftext": "Hello,\n\nI am trying to do some basic preprocessing on 2.5GB of text. More specifically, I want to do tokenization, lower casing, remove stop words and top-k words. I need to use spacy because the dataset is in greek and I think other libraries can't support this.\n\nHowever, when I try to apply what the spacy documentation or most of the guides/resources mention, it takes forever to complete even half of the techniques that I mentioned above. I stop the execution every time.\n\nCould you provide me with some resources that I might have missed, in order to make this procedure run faster?\n\nThanks in advance", "upvote_ratio": 0.88, "id": "t3_rxtn0v", "created_utc": 1641515584.0}
{"sub": "LanguageTechnology", "title": "How could I go about turning a bunch of text into a series of questions with answers?", "selftext": "nan", "upvote_ratio": 0.67, "id": "t3_rxt5kp", "created_utc": 1641514264.0}
{"sub": "LanguageTechnology", "title": "Named Entity Recognition (NER) Ensemble Method", "selftext": "Hi, I am trying to get into NLP with knowledge in CV. In CV, tasks like object detection have SOTA ensemble methods like Weighted Box Fusion. I was wondering if NER has the equivalent to WBF in terms of ensembling Transformer models of different folds.", "upvote_ratio": 0.78, "id": "t3_rxiklt", "created_utc": 1641486534.0}
{"sub": "LanguageTechnology", "title": "corpus visualization tools", "selftext": "Hello all,\n\nWhich are your favorite tools to visualize a corpus?  \nDo you prefer a desktop or web solution? And which kind of analyses can you perform with such tools (n-gram, pos, lemmatization ecc)?\n\nThanks in advance", "upvote_ratio": 0.83, "id": "t3_rxc8t8", "created_utc": 1641467912.0}
{"sub": "LanguageTechnology", "title": "Text Classification using Unsupervised Learning.", "selftext": "hi r/LanguageTechnology,\n\nI'm working on a text classification problem, where I want to classify the textual data into different domains/categories.\n\nWe have tried couple of different approaches, like Topic Modelling and BERT. Topic Modelling didn't give out the expected results and in the case of BERT the accuracy were not at the desired level.\n\nWhat are the other methodology which I can look into for this particular task?\n\nAre there any ways in which we can improve the accuracy with on these models?", "upvote_ratio": 1.0, "id": "t3_rx9rmp", "created_utc": 1641458299.0}
{"sub": "LanguageTechnology", "title": "Language, Mind and Infinite Use of Finite Means | Noam Chomsky", "selftext": "nan", "upvote_ratio": 0.77, "id": "t3_rx3svh", "created_utc": 1641438510.0}
{"sub": "LanguageTechnology", "title": "Is there a tokenizer that is able to classify contractions as one word?", "selftext": "As compared to multiple. My tokenizer currently splits \u201ccan\u2019t\u201d to \u201ccan\u201d, \u201c\u2018\u201c, and \u201ct\u201d. I\u2019m using Python", "upvote_ratio": 1.0, "id": "t3_rx30rj", "created_utc": 1641434473.0}
{"sub": "LanguageTechnology", "title": "John Snow Labs Spark-NLP 3.4.0: New OpenAI GPT-2, new ALBERT, XLNet, RoBERTa, XLM-RoBERTa, and Longformer for Sequence Classification, support for Spark 3.2, new distributed Word2Vec, extend support to more Databricks &amp; EMR runtimes, new state-of-the-art transformer models, bug fixes, and lots more!", "selftext": "nan", "upvote_ratio": 0.79, "id": "t3_rwpstk", "created_utc": 1641397541.0}
{"sub": "LanguageTechnology", "title": "A 5 million source code file dataset", "selftext": "nan", "upvote_ratio": 0.67, "id": "t3_rwjtna", "created_utc": 1641378480.0}
{"sub": "LanguageTechnology", "title": "Advice for stemming historical text", "selftext": "So I'm working on some early English text. For example, sometimes \"up\" is spelled \"vp\", or \"himself\" might be \"himselfe\"... or it might not. Is there any advice or good practice for how to handle stemming/lemmata etc.? Has anyone got experience doing word embeddings with this kind of data?", "upvote_ratio": 0.93, "id": "t3_rwihy9", "created_utc": 1641373228.0}
{"sub": "LanguageTechnology", "title": "Augmented SBERT for domain transfer", "selftext": "Hi all, I put together [an article on applying AugSBERT](https://www.pinecone.io/learn/domain-transfer/) from Thakur, Reimers, etc for domain transfer tasks. Great for improving sentence transformer performance in a domain where we don't have data, but we *do have* data in another similar domain. I hope it's useful, let me know if you have any questions, ideas, etc - thanks!", "upvote_ratio": 1.0, "id": "t3_rvy09b", "created_utc": 1641312793.0}
{"sub": "LanguageTechnology", "title": "NLP to Process Academic Citations", "selftext": "I have to process undergraduate and postgraduate student essays using spaCy. One of my first step is to remove citations, both narrative and parenthetical ones. And I am using regex to do this. My regex is getting longer and longer and becoming very unwieldy. Moreover, I am assuming students are using APA 7th and not earlier versions or other styles entirely.\n\nI am unable to get good results using NER or POS so have to rely on regex.\n\nAre there any python NLP packages that will recognise academic citations, both narrative and parenthetical ones? E.g. \"Lee (1990) said ...\", \"... in the study conducted (Lee, 1990)\".", "upvote_ratio": 1.0, "id": "t3_rvp049", "created_utc": 1641282563.0}
{"sub": "LanguageTechnology", "title": "Researchers Propose A Novel Parameter Differentiation-Based Method That Can Automatically Determine Which Parameters Should Be Shared And Which Ones Should Be Language-Specific", "selftext": "In recent years, neural machine translation (NMT) has attracted a lot of attention and has had a lot of success. While traditional NMT is capable of translating a single language pair, training a separate model for each language pair is time-consuming, especially given the world\u2019s thousands of languages. As a result, multilingual NMT is designed to handle many language pairs in a single model, lowering the cost of offline training and online deployment significantly. Furthermore, parameter sharing in multilingual neural machine translation promotes positive knowledge transfer between languages and is advantageous for low-resource translation.\n\nDespite the advantages of cooperative training with a completely shared model, the MNMT approach has a model capacity problem. The shared parameters are more likely to preserve broad knowledge while ignoring language-specific knowledge. To improve the model capacity, researchers use heuristic design to create extra language-specific components and build a Multilingual neural machine translation (MNMT) model with a mix of shared and language-specific characteristics, such as the language-specific attention, lightweight language adaptor, or language-specific routing layer. [Continue Reading](https://www.marktechpost.com/2022/01/03/researchers-propose-a-novel-parameter-differentiation-based-method-that-can-automatically-determine-which-parameters-should-be-shared-and-which-ones-should-be-language-specific/)\n\nPaper: https://arxiv.org/pdf/2112.13619v1.pdf\n\nGithub: https://github.com/voidmagic/parameter-differentiation", "upvote_ratio": 0.92, "id": "t3_rvkz91", "created_utc": 1641268721.0}
{"sub": "LanguageTechnology", "title": "Doubt about a point in BERT paper", "selftext": "In the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf) it says that during training it mask a fraction of the words and replaces them with random words:\n\n&gt; The training data generator chooses 15% of the token positions at random for prediction. If the i-th token is chosen, we replace the i-th token with (1) the \\[MASK\\] token 80% of the time (2) a random token 10% of the time (3) the unchanged i-th token 10% of the time.\n\nI can't wrap my head about the explaination it gives, can somebody point me somewhere about this part?\n\nEDIT: What I don't understand is the justification to do the random word thing.", "upvote_ratio": 1.0, "id": "t3_rvfi1w", "created_utc": 1641253131.0}
{"sub": "LanguageTechnology", "title": "Open Source Chinese Language Thesaurus", "selftext": "Are there any open source Chinese language thesauruses? Akin to CEDICT, but with synonyms?\n\nI have an application that could really make use of something like that, and without one existing, we'll essentially have to do it by hand, which is fairly laborious.", "upvote_ratio": 1.0, "id": "t3_rvc54a", "created_utc": 1641244245.0}
{"sub": "LanguageTechnology", "title": "Voice cloning + Language transfer == Clone yourself and speak a new language", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_rva3bz", "created_utc": 1641238911.0}
{"sub": "LanguageTechnology", "title": "Hi, guys I just upload a video to YouTube about Romance languages compared to Latin Fruits. I would like you guys to check it out and leave a like. Thank you.. link in the comment https://youtu.be/H-Z3L9kGGjk", "selftext": "nan", "upvote_ratio": 0.5, "id": "t3_rv6ty2", "created_utc": 1641230488.0}
{"sub": "LanguageTechnology", "title": "NLP: Hybridization of statistical approach and expert system ?", "selftext": "Hi everyone!\n\nI have a question for you. For context, we aggregate on a platform the various AI APIs on the market (GCP, Azure, etc.) and including NLP APIs (keyword extraction, sentiment analysis, NER, etc.). The idea is that a developer doesn't have to create accounts with different providers and can have them all on one API to test, compare and change whenever he wants.\n\nHowever, many customers ask us how to mix the \"statistical\" approach behind these APIs with expert systems and how to achieve hybridization.\n\nDo you have any idea how to do this?\n\nThanks,", "upvote_ratio": 1.0, "id": "t3_rv4mf8", "created_utc": 1641224832.0}
{"sub": "LanguageTechnology", "title": "Inserting documents into Postgres?", "selftext": "I have a postgres database that I want to use to store raw documents. These documents may contain lots of special characters.\n\nI'm trying to insert the documents into a postgres db and I keep getting syntax errors. Not sure what the best approach to this is.   \n\n\nHere is the code I'm using with psycopg2\n\nsql\\_statement = \"\"\" \n\nPREPARE fooplan(text, text) AS  \n\nINSERT INTO ocr (id, text) VALUES ($1, $2); \n\nEXECUTE fooplan({0}, {1});\"\"\".format(id ,text)  \n\n&amp;#x200B;\n\ncur.execute(sql\\_statement)", "upvote_ratio": 0.5, "id": "t3_rv4c4r", "created_utc": 1641224065.0}
{"sub": "LanguageTechnology", "title": "[R] The Illustrated Retrieval Transformer (GPT3 performance at 4% the size)", "selftext": "nan", "upvote_ratio": 0.87, "id": "t3_rv2maj", "created_utc": 1641219239.0}
{"sub": "LanguageTechnology", "title": "Faster keyword extraction", "selftext": "I\u2019m using KeyBERT to extract 1000 keywords from a file.\n\nIt was pretty slow when I did it for only 4 keywords.\n\nFor 1000 it ran for almost 15 minutes before I terminated it, I believe it was processing the entire time but it\u2019s just a massive computation.\n\nCan anyone advise me on speeding this up? I\u2019m using a Digital Ocean Droplet.\n\nWhat specs do I need to do something like this in hopefully a few seconds? Are we talking 64-core CPU or a certain GPU or something?\n\nOr is there any advice on how I can be certain it\u2019s still running, even after like 20 minutes?\n\nHow long would you expect an execution like this to take and why? What is it about BERT that is so computation-intensive?\n\nThank you", "upvote_ratio": 1.0, "id": "t3_ruzq2s", "created_utc": 1641209634.0}
{"sub": "LanguageTechnology", "title": "Open Domain Question Answering Part-1 [BlenderBot 2.0]", "selftext": "nan", "upvote_ratio": 0.67, "id": "t3_ruy7kl", "created_utc": 1641203851.0}
{"sub": "LanguageTechnology", "title": "NLP tool for simple sentence correction in English (i.e. grammer)?", "selftext": "Hi all. A little background: my mother is a Chinese immigrant who is always lacking self-esteem in her ability to speak \"correct\" English. Whenever she sends a text over to someone who is a native English speaker, she always bugs me to correct her sentences so it sounds more \"natural.\" Her English is honestly fine at a conversational level, but could definitely use some editing.\n\nI am wondering if there are NLP tools out there that can help my mom with this? Like if someone types a sentence like \"Hi, I almost done\" we can change it to something like \"Hi, I *am* almost done\"?\n\nThanks in advance.", "upvote_ratio": 0.92, "id": "t3_ruwav3", "created_utc": 1641195986.0}
{"sub": "LanguageTechnology", "title": "MediaRecorder based smartphone recording vs dedicated app", "selftext": "Hi all! I'm proving out an idea for emotion detection using smartphone recordings. Ideally I would like to gather recordings using a web-based application and the MediaRecorder API with smartphones (targeting iOS primarily). Does anyone have experience with doing so? Are the results good enough to work with, or am I better off working on a dedicated app with more control over recording?", "upvote_ratio": 0.9, "id": "t3_rtngyl", "created_utc": 1641056778.0}
{"sub": "LanguageTechnology", "title": "Next steps for after classification", "selftext": "Hello everyone!\n\nAfter lots of research and failure, I finally was able to use BERT for classifying text in my dataset.\n\nHowever, I feel like a dog that finally caught the car he has been chasing, because I am not sure what to do next.\n\nI had a series of questions that I want to pursue but was hoping for a professional opinion.\n\nFirst, I want to be able to look at some metrics for seeing how well my model performed. What are good metrics for a multiclass classification task? I know for a fact my classes are imbalanced, so what would be the best way to move forward with this?\n\nIn short, what do you ask yourselves once the model is done training and what do you do to evaluate it? How can I improve?\n\nI am a nuclear engineer by trade and NLP/DL is still a very new concept and I was hoping to get insight from the masters out there.\n\nThanks in advance and happy new year!", "upvote_ratio": 0.88, "id": "t3_rt6pvi", "created_utc": 1640995295.0}
{"sub": "LanguageTechnology", "title": "Having trouble with stemming (NLTK library)", "selftext": "I tried using different packages but they all still just return a \"None\" value whenever i try to stem a word. Is it because of my python version ?", "upvote_ratio": 0.6, "id": "t3_rsx4cz", "created_utc": 1640966945.0}
{"sub": "LanguageTechnology", "title": "High-leveled APIs ruined NLP?", "selftext": "It seems like things like HuggingFace and Spacy and whatever have done some harm to NLP as a whole.\n\nfor instance, I've heard NLP engineers have less pay potential compared to computer vision folk due to most models just being run through their pipelines.\n\nalso, it seems difficult to find tutorials post 2018 on topics like NER and such from scratch.\n\nEverything is getting abstracted to API's and fewer people are learning things from the ground up.\n\nWhat do you think?", "upvote_ratio": 0.42, "id": "t3_rsrwqa", "created_utc": 1640949886.0}
{"sub": "LanguageTechnology", "title": "Custom NER with spaCy v3 Tutorial | Free Web-based NER Data Annotation Tool", "selftext": "nan", "upvote_ratio": 0.71, "id": "t3_rsbokc", "created_utc": 1640897029.0}
{"sub": "LanguageTechnology", "title": "Healthsea: an end-to-end spaCy pipeline for exploring health supplement effects", "selftext": "nan", "upvote_ratio": 0.9, "id": "t3_rs6ger", "created_utc": 1640883670.0}
{"sub": "LanguageTechnology", "title": "[Project] Figuring the \"sophistication\" level of a text, similar to Grammarly.", "selftext": "Hi, I have a project in mind and the first \"mini-project\" within it is to assign a Score to a text depending on the depth of the vocabulary. Similar to what Grammarly does. I know I have to use a dictionary, but beyond that I don't have much.\n\n A bonus would be to also assign a \"Class\" to the text depending on the vocabulary used; ex: While a Scientist and a Writer might have very similar \"depth\" Scores, their vocabularies are not the same, the program should assign to which \"Class\" does the text belong. But this might be a bit hard.", "upvote_ratio": 1.0, "id": "t3_rs5rqj", "created_utc": 1640881864.0}
{"sub": "LanguageTechnology", "title": "Teaching transformer \"sentence\" orders", "selftext": "Hi there, I'm trying to tackle quite a difficult problem with the help of sentence-transformer-models.\n\nIve got a bunch of JSON (alternatively YAML) files from different domains, which contain basically entities as JSON schemas consisting of data fields and descriptions. The entities can be ordered in kind of a hierarchical structure, which is not really strict though and may differ from file to file.\n\nI assume that there exist common patterns between those files, precisely how the entities can be ordered in a semantically \"meaningful\" way (a human can understand the structures based on the descriptions). I would like to either\n\n**a) Cluster the schemas to identify similarities between those entities**\n\nWhat I tried: clustering the descriptions with KMEANS and SentenceTransformers. Problems here:\n\n\\- If I use only the descriptions they get clustered mostly by domain\n\n\\- If I try to cluster the \"raw\" JSON, most models don't find any similarity (tried also CodeBerta etc)\n\n=&gt; My idea here would be to fine-tune a model which encodes always two JSON parts as sentence input and I use the description similarity to generate either a classification score or even NLI scores, to train the model on this data, would this be a valid approach or what could be better ideas?\n\n**b) More of a crazy but interesting idea: If I assume that the \"structure\" can be modeled as a \"sentence\" which consists of \"words\" (embedded entities) than probably some sort of model could learn those \"sentences\".**\n\n=&gt; How to create \"words\" from sentences? I thought about creating sentence embeddings for all entities, and then building \"entity-sentences\" from the CLS-tokens? How to build a classifier for such \"sentences\"? Are there any good approaches or is there any previous work done?\n\n=&gt; Does it make sense to create the model from scratch or would it be helpful to fine-tune an existing model with this approach?\n\n=&gt; Would it make sense to look at a completely different sort of ML technology?", "upvote_ratio": 1.0, "id": "t3_rs2c2c", "created_utc": 1640872483.0}
{"sub": "LanguageTechnology", "title": "Clause segmentation", "selftext": "I\u2019m trying to learn how to segment text into significant clauses.\n\nHere\u2019s a promising approach:\n\nhttps://stackoverflow.com/questions/65227103/clause-extraction-long-sentence-segmentation-in-python\n\n\nchunks = []\n\nfor sent in doc.sents:\n\n    heads = [cc for cc in sent.root.children if cc.dep_ == 'conj']\n\n\nWhat are the children of a sentence\u2019s root? Does that mean every possible lowest level syntactic element like \u201cD\u201d, \u201cQuantifier\u201d, \u201cN\u201d, etc.?\n\nSo the author decided to find conjunctions?\n\nWhat about just looking at the syntax tree and breaking it on a lateral level - like the three elements on one level down from the root, make those the segments?\n\nOr what about just pure machine learning for this? Just train a custom segmenter by showing it where you would break sentences, and don\u2019t do any explicit syntax parsing?\n\n\n    for head in heads:\n\n        words = [ww for ww in head.subtree]\n\n        for word in words:\n\n            seen.add(word)\n\n        chunk = (' '.join([ww.text for ww in words]))\n\n        chunks.append( (head.i, chunk) )\n\n    unseen = [ww for ww in sent if ww not in seen]\n\n    chunk = ' '.join([ww.text for ww in unseen])\n\n    chunks.append( (sent.root.i, chunk) )\n\nchunks = sorted(chunks, key=lambda x: x[0])\n\nfor ii, chunk in chunks:\n\n    print(chunk)\n\n\nIs this just going to the break-points in the sentence the program identified and pulling out all the words consecutively? Or, what is this doing?\n\nThank you", "upvote_ratio": 0.84, "id": "t3_rs22ru", "created_utc": 1640871715.0}
{"sub": "LanguageTechnology", "title": "[P] Ecco - Language model analysis and visualization toolkit", "selftext": "nan", "upvote_ratio": 0.9, "id": "t3_rrydxi", "created_utc": 1640859007.0}
{"sub": "LanguageTechnology", "title": "[Explainer] Inter-Annotator Agreement: An Introduction to Krippendorff\u2019s Alpha", "selftext": "nan", "upvote_ratio": 0.75, "id": "t3_rrpbty", "created_utc": 1640829402.0}
{"sub": "LanguageTechnology", "title": "Any tools to help with labeling your own data set?", "selftext": "So, say I'm attempting to label a training data set for a sentence classification model. What would the best tool to load  a bunch of documents, have each document be split into sentences, and then show me each sentence so I can label it myself. Any ideas on what I should use?", "upvote_ratio": 0.72, "id": "t3_rrn0jg", "created_utc": 1640822982.0}
{"sub": "LanguageTechnology", "title": "Q: Transformers - Query, Key and Value Vectors in \"Attention is all you need\"", "selftext": "Hi everyone!\n\nCan someone explain to me how query, key and value vectors are received from the input word embeddings to an encoder or decoder layer? I see how they (the q, k, v vectors or matrices respectively) are used in the multihead attention layer, but I dont understand where they come from. They have to depend on the input word embedding, but how?\n\nIn the original transformer paper [(Attention is all you need)](https://arxiv.org/pdf/1706.03762.pdf) I only found those vectors mentioned in chapter 3.2:\n\n*An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key* \n\n&amp;#x200B;\n\nIf anyone could help me answering this question, it would be great!\n\nEDIT:\n\nMy thanks to /u/Brudaks, /u/boodleboodle and /u/mehtajineshs for clarification on this by providing explanations and resources.\n\nI do understand now, that the vectors depend on the output from previous layers and are received by multiplying previous layer output (or word embeddings, in case of first layer) with randomly initialized matrices, like other weights in an FF network for example are initialized randomly as well.\n\nAnd like weights in a feed forward network, the matrices for receiving QKV vectors are learned by backprop.", "upvote_ratio": 0.76, "id": "t3_rri6dm", "created_utc": 1640810570.0}
{"sub": "LanguageTechnology", "title": "Baidu And PCL Team Introduce ERNIE 3.0 Titan: A Pre-Training Language Model With 260 Billion Parameters", "selftext": "With recent breakthroughs in AI, humans have become more reliant on AI to address real-world problems. This makes humans\u2019 ability to learn and act on knowledge just as essential as a computer\u2019s. Humans learn and gather information through learning and experience to understand everything from their immediate surroundings. The ability to comprehend and solve issues, and separate facts from absurdities, increases as the knowledge base grows. However, such knowledge is lacking in AI systems, restricting their ability to adapt to atypical problem data.\n\nPrevious studies show that pre-trained language models improve performance on various natural language interpretation and generating tasks.\n\nA recent work of researchers at Baidu, in collaboration with Peng Cheng Laboratory (PCL), release PCL-BAIDU Wenxin (or \u201cERNIE 3.0 Titan\u201d), a pre-training language model with 260 billion parameters. It is the world\u2019s first knowledge-enhanced multi-hundred billion parameter model and its largest Chinese singleton model.\u00a0\n\nYou can read the short summary here: [https://www.marktechpost.com/2021/12/29/baidu-and-pcl-team-introduce-ernie-3-0-titan-a-pre-training-language-model-with-260-billion-parameters/](https://www.marktechpost.com/2021/12/29/baidu-and-pcl-team-introduce-ernie-3-0-titan-a-pre-training-language-model-with-260-billion-parameters/) \n\nPaper: https://arxiv.org/pdf/2112.12731.pdf", "upvote_ratio": 1.0, "id": "t3_rrgrnf", "created_utc": 1640807045.0}
{"sub": "LanguageTechnology", "title": "Extractive summarization", "selftext": "What model would you recommend for extractive summarization?\n\nI have a dataset of restaurant menus and I want to extract the dishes with their prices.\n\nSo the input will be the entire menu and the output will csv like text with the dishes as the first column and their prices as the second.\n\nI was thinking of T5 but I just dabble in NLP maybe you have a better idea?\n\nThanks", "upvote_ratio": 0.5, "id": "t3_rrehwy", "created_utc": 1640801420.0}
{"sub": "LanguageTechnology", "title": "Suggestions for newbie trying to dabble in NLP", "selftext": "Hi everyone!\n\nI'm an experimental social science researcher who is trying to get into some very basic NLP as a supplementary skillset.\n\nI learned how to use LIWC (in a very short 4-week workshop) during my doctoral program, but haven't done anything related to NLP for at least 5 years. I've skimmed through some posts here and someone said \"NLP has progressed a lot more from LIWC since the past couple years\" so I'm trying to get reacclimated with NLP.\n\nDo you guys have any suggestions (youtube videos / websites / books) on where to start? My goal is first to learn how to do the most basic sentiment analysis and/or any other elementary analyses using R, and then once comfortable, gradually move on to more advanced topics (I consider myself a good self-learner :) ).\n\nAnother question I had was whether there was anything similar to LIWC in R, but again others seem to have commented on this subreddit that there are better tools than LIWC these days..?\n\nSorry for the really vague / general question - I would appreciate any comments or pointers!", "upvote_ratio": 0.92, "id": "t3_rr9wgp", "created_utc": 1640789547.0}
{"sub": "LanguageTechnology", "title": "What is a Graph Neural Network?", "selftext": "nan", "upvote_ratio": 0.82, "id": "t3_rqu6xf", "created_utc": 1640737643.0}
{"sub": "LanguageTechnology", "title": "Open Discussion: ways to prevent Voice Synthesis misuse", "selftext": "nan", "upvote_ratio": 0.63, "id": "t3_rqoz8d", "created_utc": 1640723635.0}
{"sub": "LanguageTechnology", "title": "Rundown of Transformer Tokenizers and how to build them", "selftext": "Hi all, I'm working on a project to build a set of language models for the Maldivian language of Dhivehi. It's a lot of fun and super interesting, the first step (for me) has been building a tokenizer that handles the language and its unique Thaana script. I just published a [video](https://youtu.be/mjKqP3kRxbQ) and [article](https://towardsdatascience.com/designing-tokenizers-for-low-resource-languages-7faa4ab30ef4) ([link if you hit paywall](https://towardsdatascience.com/designing-tokenizers-for-low-resource-languages-7faa4ab30ef4?sk=c0c16de9eea7dbe1d2a9c106abf38e1a)) explaining the steps and each of the components in a tokenizer (eg normalization, pretokenization, decoding, etc).\n\nI hope some of you find it useful, lmk what you think - thanks!", "upvote_ratio": 0.72, "id": "t3_rqitqe", "created_utc": 1640706739.0}
{"sub": "LanguageTechnology", "title": "Searching for irregular forms automatically", "selftext": "Hello all,\n\nI would like to ask you which kind of tools do you use to search for something that is not in the training model when you do a computational analysis.\n\nLet's say that I want to search for errors in a corpus (e.g. mispelled words), while the lemmatization procedure fails because these elements are wrong - how can you deal automatically with such events?\n\nI hope that the question is clear enough :)", "upvote_ratio": 1.0, "id": "t3_rqegda", "created_utc": 1640693425.0}
{"sub": "LanguageTechnology", "title": "looking for help on approaching problem", "selftext": "I currently have a list of sentence fragments that loosely describe listing for sales for houses/apt/mansions etc.  \n\nThey might look something like this:\n\n*\\[apartment, 4 glazed windows, wood floors and well insulated, with large pool\\]*\n\n*\\[large apartment, 4 bedrooms, 1 master bathroom,  carpet everywhere but not in bathrooms\\]*\n\n*\\[baby room, 2 bed, half a washroom,  crawlspace attic for storage, garden with swim area\\]*\n\nI want to apply labels (keywords) to these fragments to \"standardized\" the language which I can then use to process later.  Knowing to group the following is important:\n\n\"large pool\" --&gt; has swimming pool\"\n\n\"garden with swim area\" --&gt; \"has swimming pool\"\n\nThe \"keywords\" I might want to use for the examples:\n\n1. \\[apartment, 4 glazed windows, wood floors and well insulated, with large pool\\]  ---&gt; \\[apt, has\\_floor, has\\_pool\\]\n2. \\[large apartment, 4 bedrooms, 1 master bathroom,  carpet everywhere but not in bathrooms\\]---&gt; \\[apt, has\\_floor, has\\_bedrooms\\]\n3. \\[baby room, 2 bed, half a washroom,  crawlspace attic for storage, garden with swim area\\]---&gt; \\[has\\_bedrooms, has\\_attic, has\\_pool\\]\n\nI do not need to \"capture\" all the descriptions from the sentence fragments.  And at least, I want to be able to grab the lowest hanging fruit first (right now I have nothing!)\n\nI see that I have some  issues:\n\n1. How do I break down these \"sentence fragments\"?  So that analysis can be done?\n2. How can I \"group\" text that shows up so that I know what categories I want to create?  Even better, if groupings can be automatically created/suggested\n3. Even if I have \"labels\" that I want to assign a set of fragments how do I train a model to actually do this?  (Like if I spent 5 hours (which i have) labeling some very basic categories.... how do I use this?)\n\nOne possible wrinkle I have,  is that I do not care which \"sentence fragment\" correspond to which label.  (when I labeled the dataset, I just said, does this sentence fragment correspond to these labels/keywords) - therefore it is difficult for me to map a \"sentence fragment DIRECTLY to a group with heuristics\" .   In the end, I do not necessarily care (or know) which of the sentence fragments actually correspond to the label, just that this example should have the given labels.\n\nI hope my problem description makes sense, and looking for any type of directed help/ approaches.  I have looked at \"tokenization\", \"word count\", \"bag of words\" etc but I am unable to understand it enough to see the full picture of how to use it.\n\nAny comments appreciated!\n\n\\[language of choice python\\]", "upvote_ratio": 1.0, "id": "t3_rqe7js", "created_utc": 1640692561.0}
{"sub": "LanguageTechnology", "title": "Huggingface for glossary creation", "selftext": "\nDoes anybody know of a leading AI model for glossary creation?\n\nI\u2019m considering using Spacy for this but so far I found their entity recognition and even their segmentation to be good but not necessarily flawless.\n\nI could stick it custom trained models for sure, it honestly might not be that hard.\n\nI\u2019m wondering if anybody has gone before me here, though.\n\nAn auto-glossary creation tool at minimum should:\n\n1. Recognise terms, not necessarily entities. Entities appear to be more trivial, like even just years and numbers come up sometimes. Terms are important keywords.\n\n2. Retrieve context/example sentences from the source documents for each word.. AI is not strictly necessary for this, but it could be leveraged in deciding which sentence containing a term is most \u201crepresentative\u201d. Plus, AI would come in handy for lemma-matching - it should be able to search for any grammatical form of a word in source text, and not match \u201ccrudely\u201d as in maybe a homonym of a word.\n\n3. Ideally, it should auto-categorize terms (I\u2019m planning on trying BERT to generate a \u201csimilarity score\u201d, grouping terms with nearness to each other and then generating a label for that group). \n\nSo: this is the project I\u2019m currently working on. Has anybody already done something like this, ready to go?\n\nThank you", "upvote_ratio": 1.0, "id": "t3_rqdq05", "created_utc": 1640690639.0}
{"sub": "LanguageTechnology", "title": "An awesome list about vector similarity search : find open source vector search libraries, service, platform service and research papers", "selftext": "nan", "upvote_ratio": 0.87, "id": "t3_rqaplb", "created_utc": 1640678986.0}
{"sub": "LanguageTechnology", "title": "I am looking for something like synthesia.io", "selftext": "I am searching for a natural text to speech or voice cloning program at least of the quality of [synthesia.io](https://synthesia.io) , Don't need the video part though.  Preferably open source or something cheap.", "upvote_ratio": 1.0, "id": "t3_rq2adm", "created_utc": 1640652886.0}
{"sub": "LanguageTechnology", "title": "Minimizing relation types in knowledge graphs", "selftext": "Has work been done on selecting a minimum subset of relation types?\n\nIdeally it could be reduced to just one. It would probably be one of the first words that that children learn. Something like \"is\" or \"has\".\n\nHaving just one type of relation would greatly simplify the representation.\n\n\"Is\" could represent categories \"cat is animal\"\n\n\"Has\" could represent parts \"cat has tongue\"\n\nSo what I'm thinking is that \"has\" would be a prime candidate as a single sufficient relation type, because categories and subcategories could be determined easily without any relation between entities: if one entity (animal) has a subset of relations that another entity (cat) has, then it means that \"cat is animal\".\n\nTo take the other route and use only \"is\" (categories) to infer parts from it -- I don't know how it could be easily done.\n\nAnyway, perhaps it is possible to use any 1 relation and infer all others based on that, the question is which is more natural for language as we commonly use it.", "upvote_ratio": 1.0, "id": "t3_rpuft7", "created_utc": 1640631395.0}
{"sub": "LanguageTechnology", "title": "Looking for people to learn Python Coding With", "selftext": "Hi there, \n\nI've recently graduated with a BA in Linguistics and I'm currently pursuing a career in Computational Linguistics. I plan on applying to an Msc in a CompLing related degree in a year or two, but I'm currently taking some time off to relax and also learn Python Coding and polish my math skills. \n\nHowever, learning Python from scratch and also learning it independently has been really difficult as I find myself stuck often with nobody that I could talk to about Python, and also I find myself lacking the motivation to keep going. \n\nIt would be really nice and helpful if I had a few people I can go to regarding Python-related things. We could motivate each / help each other out etc. \n\nPlease let me know if you're interested!", "upvote_ratio": 0.86, "id": "t3_rpmr79", "created_utc": 1640609029.0}
{"sub": "LanguageTechnology", "title": "Microsoft Introduces the Next Generation of the Conversational Language Understanding Client Library", "selftext": "The demand for intelligent technologies that can interpret brief text has increased. As increasingly sophisticated solutions are produced, there is a greater need to improve and facilitate the creation of these complex situations. These scenarios are intelligent customer assistance bots to independent computers that interpret human input. The Language Cognitive Service has opted to employ a multilingual transformer-based paradigm to deal with such problems. When using this model, customers will notice a considerable increase in performance over the old Language Understanding Service (LUIS).\n\nMicrosoft has released the next generation Conversational Language Understanding client library, allowing developers to use the Azure Cloud Conversational Language Understanding service to train models and use them in applications to provide related language services. Developers can use .NET or Python, and these libraries are currently under beta development.\n\nQuick Read: [https://www.marktechpost.com/2021/12/27/microsoft-introduces-the-next-generation-of-the-conversational-language-understanding-client-library/](https://www.marktechpost.com/2021/12/27/microsoft-introduces-the-next-generation-of-the-conversational-language-understanding-client-library/) \n\n* [CLU documentation](https://docs.microsoft.com/azure/cognitive-services/language-service/conversational-language-understanding/overview)\n* [.NET reference documentation](https://docs.microsoft.com/dotnet/api/Azure.AI.Language.QuestionAnswering?view=azure-dotnet)\n* [Python reference documentation](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/cognitivelanguage/azure-ai-language-conversations)\n\nMicrosoft Blog: https://devblogs.microsoft.com/azure-sdk/introducing-the-next-generation-of-the-conversational-language-understanding-client-library/", "upvote_ratio": 0.86, "id": "t3_rpjdav", "created_utc": 1640595999.0}
{"sub": "LanguageTechnology", "title": "Romanian word embeddings", "selftext": " Hello everyone!\u00a0  \nDid you hear about word embeddings?  \n\n\nNooo? Then you need to learn about it.  \n\n\nBut if seriously, I'm in process of create some corpus for word embedding for romanian language (and I know they exist a lot, but they are not mine).  \nThat why I decide to start this project.  \n\n\nWhat are the goals I am pursuing?  \n1. The text must be clean;  \n2. To be learned on a lot of text;  \n3. And the corpus must be accurate.  \n\n\nHere you can see some of them:\n\n[https://github.com/BlackKakapo/Romanian-Word-Embeddings](https://github.com/BlackKakapo/Romanian-Word-Embeddings)  \n\n\nThe rest will appear in the near future, and I will try to do them as soon as possible. Maybe some changes and fixes will appear, but I'll keep you posted.  \n\n\nAnd of course you can leave a comment, what you like or dislike. I will be very grateful.  \n\n\nRespectfully", "upvote_ratio": 0.67, "id": "t3_rphw8k", "created_utc": 1640590319.0}
{"sub": "LanguageTechnology", "title": "How did you advance your NLP career?", "selftext": "Dear all, \n\nIf there are any NLP/ML engineers, DS, or researchers out there, I could really use some advice. \n\nI am graduating from my MS in Economics with a full-time job lined job as a DS at a well-known fintech company. However, it is driving me crazy to find a clear path forward to pursue a more NLP-involved job down the line. \n\nHere is what I currently have that can be classified as NLP \"experiences\": \n\n1. Past Internships! I have done anything from Product management intern for data products powered by NLP to Management Consultant doing research on the data collection strategies that a client could take to improve their NLP classification outcome \n2. Research! I am writing a paper with researchers from NLP for applying NLP techniques to public policy related documents and is due to publish in the next couple of months \n3. Current job! The team that I am currently on and hired into (that I have been interning on) uses a lot of NLP for insights discovery. We also plan on launching a large scale NLP product down the line which I will be very involved in given our very lean corporate structure \n\nWhy I think I will have a hard time advancing in the field: \n\n1. I do not have a CS undergrad or MS in CS \n2. My background in economics dictated that I am good at math but not at linguistics \n3. I do not come from a hyper prestigious school like Stanford or MIT but a mid-tier school in the East Coast (US)\n\nI feel everyone in the field is so overqualified for what they are doing (granted people may just be very good imposters)! I have no clue what to do ??? \n\nShould I go get an MSCS to compete down the line? How does moving up in NLP careers work? Can any folks shine some light on a very confused young person! \n\nI will literally take any suggestions or advice haha. thank u y'all!", "upvote_ratio": 0.91, "id": "t3_rp96xk", "created_utc": 1640562579.0}
{"sub": "LanguageTechnology", "title": "January 5, 2022 online: \"Compositional Natural Language Processing on Quantum Computers\"", "selftext": "January 5, 2022 online: \"Compositional Natural Language Processing on Quantum Computers\" with Konstantinos Meichanetzidis, [Cambridge Quantum](https://cambridgequantum.com/) &amp; [Quantinuum](https://www.quantinuum.com/). Info &amp; RSVP: [https://www.meetup.com/NY-NLP/events/282107959/](https://www.meetup.com/NY-NLP/events/282107959/)\n\n\\#NLProc #QuantumComputing ", "upvote_ratio": 0.67, "id": "t3_royiix", "created_utc": 1640530776.0}
{"sub": "LanguageTechnology", "title": "creature_feature: Composable N-Gram Combinators that are Ergonomic and Bare-Metal Fast", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_rovnon", "created_utc": 1640520116.0}
{"sub": "LanguageTechnology", "title": "What are some good stories from the history of NLP?", "selftext": "I've been Wikipedia-diving some of the history of NLP recently, and I'd like to know if anyone has any interesting stories about researchers/experiments in the field. You know, like when your high school history teacher goes on a random tangent about all the different torture methods throughout the ages. Thanks!", "upvote_ratio": 0.78, "id": "t3_ro6v4v", "created_utc": 1640425622.0}
{"sub": "LanguageTechnology", "title": "How to measure accuracy of a generative chatbot model", "selftext": "Hi,\nHow to measure the accuracy of a generative chat bot or any generative model?\nIs it possible?", "upvote_ratio": 0.92, "id": "t3_rnnw4s", "created_utc": 1640357742.0}
{"sub": "LanguageTechnology", "title": "People with degrees in Language Technology, what are you doing now?", "selftext": "I'm (hopefully) starting a master's in Computational Linguistics in the fall, and I'm curious to know what people who have done an undergrad/master/phd in cl, nlp, or even just compsci with an nlp focus end up actually doing after their degrees.", "upvote_ratio": 0.93, "id": "t3_rn9kbo", "created_utc": 1640305798.0}
{"sub": "LanguageTechnology", "title": "Computational grammar checking", "selftext": "I\u2019ve been interested in this for a long time, hoping to finally crack the code here.\n\nIn Swedish an adjective can be determined or not. Like in English the article \u201cthe\u201d vs \u201ca\u201d.\n\nThis is a more obscure case, a phrase in a product catalog \u201cOn hard floor\u201d.\n\nNot sure what grammarians would say is going on here. It\u2019s a general case so I would say it\u2019s non-determined and they dropped the \u201ca\u201d just for brevity.\n\nOn the other hand it is more comparable to the general case which commonly uses the plural in English: \u201con hard floors\u201d.\n\nI would like to have a convenient system to check what is done in Swedish without just leafing through grammar websites and so on.\n\nI want to access a most convenient Swedish corpus - not a database requiring a sign up but just an easily downloadable dataset, maybe Kaggle?, or as part of some software package like Spacy.\n\nThen I want to execute a formula like \u201cshow me matches of sentences of the form \u201cpreposition determined adjective noun\u201d\u201d. I can develop it from here but this would be a good start.\n\nDoes anyone have a suggestion for an accessible corpus with syntax parsing and searching?\n\nThank you very much", "upvote_ratio": 1.0, "id": "t3_rmrmk1", "created_utc": 1640250370.0}
{"sub": "LanguageTechnology", "title": "OpenAI Researchers Find Ways To More Accurately Answer Open-Ended Questions Using A Text-Based Web Browser", "selftext": "Long-form question-answering (LFQA), a paragraph-length answer created in response to an open-ended question, is a growing difficulty in NLP. LFQA systems hold the potential to become one of the most important ways for people to learn about the world, yet their performance currently lags behind that of humans. Existing research has tended to concentrate on two key aspects of the task: information retrieval and synthesis.\n\nResearchers at OpenAI have recently developed WebGPT. They outsource document retrieval to the Microsoft Bing Web Search API and use unsupervised pre-training to produce high-quality synthesis by fine-tuning GPT-3. Rather than striving to improve these factors, they concentrate on integrating them with more consistent training goals. The team leverages human feedback to directly enhance the quality of answers, allowing them to compete with humans in terms of performance.\n\nIn this paper, the team offers two significant contributions. They create a text-based web-browsing environment that can be interacted with by a fine-tuned language model. This enables the use of general approaches like imitation learning and reinforcement learning to improve both retrieval and synthesis in an end-to-end manner. The team also creates replies with references, sections collected by the model when exploring web pages. This is critical because it allows labelers to assess the factual accuracy of answers without having to engage in a time-consuming and subjective independent research procedure.\n\nQuick Read: [https://www.marktechpost.com/2021/12/22/openai-researchers-find-ways-to-more-accurately-answer-open-ended-questions-using-a-text-based-web-browser/](https://www.marktechpost.com/2021/12/22/openai-researchers-find-ways-to-more-accurately-answer-open-ended-questions-using-a-text-based-web-browser/) \n\nPaper: [https://arxiv.org/pdf/2112.09332.pdf](https://arxiv.org/pdf/2112.09332.pdf)\n\nOpen AI Blog: https://openai.com/blog/improving-factual-accuracy/", "upvote_ratio": 0.94, "id": "t3_rmmn9e", "created_utc": 1640232389.0}
{"sub": "LanguageTechnology", "title": "How to make DialoGPT output random responses given the same query", "selftext": "Hi, \n\nI am making a chatbot with the use of dialoGPT, but I want it to give different responses even if the same question is asked. \n\neg: \n\n* How are you doing -&gt; I am good\n* How are you doing -&gt; I am doing good\n* How are you doing -&gt; I am fine, how are you\n\nsomething like that, so it doesn't seem repetitive.\n\nThis question might be stupid tho \ud83d\ude05\ud83d\ude05", "upvote_ratio": 1.0, "id": "t3_rmbz8l", "created_utc": 1640199166.0}
{"sub": "LanguageTechnology", "title": "Do you think that Large Language Models could be used to generate Knowledge Graphs?", "selftext": "Do you know of any such experiments?\n\nI keep reading about LLMs using external memory resources, but could they also be used to generate resources such as Knowledge Graphs on a huge scale?\n\nEdit: preliminary results from a little experimentation with GPT-3 (davinci-instruct)\n\n**Prompt**\n\nknowledge graph described by a list of relations:\n\nfinger -&gt; part of -&gt; hand\n\nfinger -&gt; subclass of -&gt; digit\n\nmusic -&gt; subclass of -&gt; sound\n\nEarth -&gt; instance of -&gt; terrestrial planet\n\ngreen -&gt; subclass of -&gt; color\n\npathogen -&gt; opposite of -&gt; nonpathogenic organism\n\ncolor -&gt; subclass of -&gt; property\n\nmusic -&gt; part of -&gt; culture\n\nculture -&gt; opposite of -&gt; nature", "upvote_ratio": 1.0, "id": "t3_rm5pzm", "created_utc": 1640181331.0}
{"sub": "LanguageTechnology", "title": "Custom Named Entity Recognition (NER) for identifying CVs.", "selftext": "I am thinking of creating model for extracting entities in a cv such as \n\n1. Name\n2. Address\n3. Institute\n4. Degree\n5. Skill\n6. Company\n7. School\n8. Designation\n9. Society - Ex: sport clubs, school societies\u2026 . In spaCy there are a very limited no of entities. What about training a model with these data ?", "upvote_ratio": 0.87, "id": "t3_rm1jdc", "created_utc": 1640165071.0}
{"sub": "LanguageTechnology", "title": "Anyone has experience with Dataiku?", "selftext": "Trying to choose between Dataiku and Databricks, wanna know if anyone have used both and have any preference?", "upvote_ratio": 1.0, "id": "t3_rlxxxv", "created_utc": 1640150772.0}
{"sub": "LanguageTechnology", "title": "Automatically categorised keyword extraction", "selftext": "Standard tools I know for keyword extraction are KeyBERT, PyTextRank, and Spacy\u2019s language object which automatically recognised \u201centities\u201d.\n\nI would like to automatically categorise keywords.\n\nI am considering making my own algorithm or adding on a step after the above keyword extraction.\n\nI believe it needs to cluster terms in some way - general semantic relatedness like WordNet, or a graph algorithm like textrank, or a similar statistical relationship to its lexical environment, maybe by comparing the BERT-generated vectors for each term and then grouping anything with a similarity above a certain score.\n\nThen it needs to guess a category name. Maybe BERT could scan through a list of words (from the text or in general) to see which one scores highest, in terms of relevance?\n\nI can think of two ideal scenarios:\n\n\n- tokenize the text\n\n- extract the key terms by mathematically noticing how terms cluster together in terms of cooccurrence. Put them in those groups and pull a term from the source text that is \u201crepresentative\u201d, it correlates highest with all of them.\n\n\nOr:\n\n- get keywords with Spacy (I still don\u2019t know how their method works)\n\n- cluster their similarity using a BERT score (as mentioned above)\n\n- name each cluster with GPT-3\n\n\nCould anyone please let me know what they think of these ideas?\n\nThank you very much", "upvote_ratio": 0.91, "id": "t3_rlmqgh", "created_utc": 1640116609.0}
{"sub": "LanguageTechnology", "title": "Summarize the \"idea\" of the text and estimate the relevance of the specific expression to it - is TF-IDF a winner here?", "selftext": "Hi guys,\n\nlet's assume I have a set of documents 1-3 page(s) each, and I need to solve the following 2 problems (these are *independent* tasks):\n\n1. **summarize (i.e. generalize) the meaning (plot)** of the text in each document\n2. estimate which particular document (among all others) **fits best** for the given specific expression (not just a single word!). \"Fits\" here means that the lexical/logical meaning of the expression is close to that of the document. Like the \"treating short-sightedness\" expression is a good match to the *medical* document, but a bad one to a *business* document which describes \"short-sighted decisions\"\n\nThough these 2 tasks are independent in *my* case, they are interconnected as decribed in this great [article](https://towardsdatascience.com/the-best-document-similarity-algorithm-in-2020-a-beginners-guide-a01b9ef8cf05), which also tests efficiency of several algorithms in execution of what I described above as Task #2.\n\n**TLDR:**\n\n* to fulfill Task #2, firstly do Task #1 to vectorize the text\n* the winner is an almost 50yr (!) old TF-IDF algorithm: precise and fast as a gunshot\n* BERT can handle mostly non-complicated plots and is waaaaay too slow\n* \"TF-IDF + **plenty** of data &amp; re-learning\" should be enough in most cases, but if you want *more* go for BERT and get ready to [build](https://cloud.google.com/architecture/building-real-time-embeddings-similarity-matching-system) a more serious infrastructure if you want it work fast\n\nWill be great if you guys comment on the following:\n\n1. is splitting the text into sentences and vectorizing - the best approach to handle Task #1, or are there other, posssibly more efficient technologies?\n2. have you ever done smth. similar to Task #1 and Task #2 and which technologies did you use?\n3. what are the weaknesses (if you're aware of them) of TF-IDF and/or BERT to keep in mind?\n4. any relevantly new and potentially perspective tehcnologies you can recommend, which may help me with either of those 2 tasks? Never late to learn, right? :)\n\nThank you!", "upvote_ratio": 1.0, "id": "t3_rli1dj", "created_utc": 1640103396.0}
{"sub": "LanguageTechnology", "title": "Spacy for keyword extraction", "selftext": "Does anybody know a best Spacy method for pulling out keywords and also context sentences for those keywords, from a text?\n\nThank you", "upvote_ratio": 0.72, "id": "t3_rleye8", "created_utc": 1640094166.0}
{"sub": "LanguageTechnology", "title": "Help with pattern matching", "selftext": "Hello everyone,\n\nIs it possible to match regex patterns with spacy?\n\nI need to find this structures in my texts: \"Sentence1. Sentence2.\" and then work with this specific part. I need to tell spacy not to split sentences in quotes (in structures \"...\"). I wrote a regex (and this regex works) to find this patterns but know I need this regex in the pipeline. \n\n&amp;#x200B;\n\nMy idea was:\n\n`If patternMatch == True:`\n\n `Go thru the hole matched pattern and set from every token is_sent_start = False`\n\n&amp;#x200B;\n\nOr does someone has another idea to tell spacy not to split inside quotes (\"...\")? Is it possible with spacy patterns (dictionary style {\"ORTH\": ' \" '}, ..., {\"ORTH\": ' \" '})?", "upvote_ratio": 1.0, "id": "t3_rlcu7k", "created_utc": 1640086566.0}
{"sub": "LanguageTechnology", "title": "Looking for a NLP which can rate a text for key features like innovation and disruption", "selftext": "I am looking for a NLP that evaluates characterization from a text on a scale of 0-100% and gives me this. I have 1000 records with rating and wanted to ask how complicated it is to build a model for this. I have basic knowledge in Python to start. It is for a paper at my university. \n\nIs there any library that is more suitable for this and where is the best place to start. \n\nWith best wishes \n\nPuzzlehead", "upvote_ratio": 0.7, "id": "t3_rlb7nk", "created_utc": 1640079794.0}
{"sub": "LanguageTechnology", "title": "\"Creative\" Videos on NLP : How Computers \"Learn\" Languages", "selftext": "I was telling my friend about NLP (Natural Language Processing) earlier today and thought that maybe a Youtube video might be able to do a better job explaining what NLP is.\n\nCan anyone recommend any \"creative\" videos (or blogs, websites, etc.) that illustrate how computers can \"learn\" languages (e.g. translation, text generation, understand language) using NLP algorithms? Maybe there are some good Ted Talks, or academic university lectures that do a good at introducing/explaining NLP? \n\nI tried to look on Youtube - but I was wondering if anyone had any reccomendations?\n\nHere is what I found:\n\n\\- [https://www.youtube.com/watch?v=CMrHM8a3hqw](https://www.youtube.com/watch?v=CMrHM8a3hqw)\n\n\\- [https://www.youtube.com/watch?v=fOvTtapxa9c](https://www.youtube.com/watch?v=fOvTtapxa9c)\n\n\\- [https://www.youtube.com/watch?v=8S3qHHUKqYk](https://www.youtube.com/watch?v=8S3qHHUKqYk)\n\n&amp;#x200B;\n\nIs there something that shows \"semantic segmentation\"? E.g. how computers learn language in \"layers\"? E.g. general idea of language followed by more complex forms of expressing thought?\n\n&amp;#x200B;\n\nThanks!", "upvote_ratio": 0.6, "id": "t3_rl86qr", "created_utc": 1640067877.0}
{"sub": "LanguageTechnology", "title": "The Spacy NER model for Spanish is terrible", "selftext": "Has anybody tried to use Spacy for NER in Spanish? I downloaded the biggest pipeline, but when implemented on some text it tends to extract full bits of sentences and label them as MISC (miscellaneous). \n\nIt does correctly extract people and locations, too, but it seems weird to me that the NER model of one of the 'main' languages would be so bad. Has anybody experienced this?", "upvote_ratio": 1.0, "id": "t3_rkx0k3", "created_utc": 1640033966.0}
{"sub": "LanguageTechnology", "title": "Working in the New York City area; looking for societies or groups dealing with linguistics/ML/NLP", "selftext": "Hi,\n\nI just became an NLP data scientist and would like to find people in real life.  Would you know where I can find groups to network with NLP data scientists/engineers in real life?\n\nThanks,\n\nDaniel", "upvote_ratio": 1.0, "id": "t3_rkuavb", "created_utc": 1640026445.0}
{"sub": "LanguageTechnology", "title": "Best ML to Identify Descriptors of List of Terms?", "selftext": "Hello fellow NLP fanatics, \n\nI'm back with another inquiry I know some of you geniuses can help answer. \n\nIn short, what's the best method, NLP, library, ML to identify the actually descriptors for a list of terms? \n\nLet's say that we want to know how people talk about a disease, the simple thing would be to start with bigrams (hate cancer, cancer sucks), but of course nothing is that simple. Take this example: \n\n\"It's horrible, diagnosed at 25 I had to deal with cancer...\"  \n\nNow we know they refer to the cancer as horrible but it's far removed so bigrams won't work. We could leverage POS and suggest that the first adjective before or after cancer should be observed. However this could also pull in several terms not relevant to the description of the disease. \n\nRunning a LDA or topic model might inform more high level discussion categories than specific descriptors. \n\nAny suggestions to optimize for this kind of research? \n\nMuch appreciated, \n\nN", "upvote_ratio": 1.0, "id": "t3_rkqf3a", "created_utc": 1640015858.0}
{"sub": "LanguageTechnology", "title": "Response evaluation in Virtual Assistants", "selftext": "Hi.\n\nDoes anyone know how virtual assistant responses are monitored, either manually or through automated ways? \nI'm curious how inappropriate or biased responses are monitored/ prevented.", "upvote_ratio": 0.83, "id": "t3_rkoh19", "created_utc": 1640010422.0}
{"sub": "LanguageTechnology", "title": "Meta AI Introduces A New AI Technology Called \u2018Few-Shot Learner (FSL)\u2019 To Tackle Harmful Content", "selftext": "For the training of AI models, a massive number of labeled data points or examples are required. Typically, the number of samples needed is tens of thousands to millions. Collection and labeling of these data can take several months. This manual collection and labeling delay the deployment of AI systems that can detect new types of harmful content over different social media platforms. To handle this issue, Meta has deployed a relatively new AI model called \u201cFew-Shot Learner\u201d (FSL) such that harmful contents can be detected even if enough labeled data is not available.\u00a0 \u00a0 \u00a0\u00a0\n\nMeta\u2019s new FSL deployment is a step towards developing more generalized AI models that will require very few to almost no labeled data for training. FSL falls under the category of an emerging field in AI called meta-learning, where the aim is \u201clearning to learn\u201d rather than \u201clearning patterns\u201d as done in traditional AI models. The FSL is first trained over generic natural language examples, acting as the training set. Next, the model is trained with new policy texts explaining the harmful target contents and policy-violating content that has been labeled in the past, which acts as a support set. Meta has reported that their FSL outperforms several existing state-of-the-art FSL methods by 12% on an average over various systematic evaluation schemes. For further details, one can consult Meta\u2019s\u00a0[research paper](https://arxiv.org/pdf/2104.14690.pdf?fbclid=IwAR1PrQI3y71EM5HyTHrdj5ti2hOosvIyRMKQvrRDNqk2ACgxaQnGjtYHnY4).\u00a0 \n\nQuick Read: https://www.marktechpost.com/2021/12/18/meta-ai-introduces-a-new-ai-technology-called-few-shot-learner-fsl-to-tackle-harmful-content/\n\nPaper: [https://arxiv.org/pdf/2104.14690.pdf](https://arxiv.org/pdf/2104.14690.pdf)\n\nMeta Blog: https://ai.facebook.com/blog/harmful-content-can-evolve-quickly-our-new-ai-system-adapts-to-tackle-it", "upvote_ratio": 0.36, "id": "t3_rjo620", "created_utc": 1639885867.0}
{"sub": "LanguageTechnology", "title": "Newbie Q: why bother using anything but gpt3?", "selftext": "Why would u use a model offered from HuggingFace?", "upvote_ratio": 0.59, "id": "t3_rjbvw3", "created_utc": 1639847618.0}
{"sub": "LanguageTechnology", "title": "BERT model is not learning!", "selftext": "I\u2019m working on a project where I\u2019m using Bert to classify binary stock movements using tweets.\nInput - tweets of last 5 days of a company\nTarget variable - 0 (if closing - opening price &lt;0 ) and 1 (if closing - opening price &gt;0)\n\nWhat ever I seem to do the model is not learning (training, validation and test accuracy and MCC) intact the MCC is so random every time.\n\nI tried many fine tuning methods - changing learning rates, epoch, dropouts , layer wise learning rate decay, reinitialising last few layers of BERT. But nothing seems to work.\n\nAny suggestion as to why this is happening and how to improve it? \n\nI\u2019m currently stuck at 50 percent accuracy and my target is 57 percent accuracy.\n Your help is greatly appreciated.", "upvote_ratio": 0.4, "id": "t3_rj21d1", "created_utc": 1639811691.0}
{"sub": "LanguageTechnology", "title": "MLCommons Releases Both A Multilingual Speech Dataset And A Large 30,000 Hour Diverse English Dataset To Drive Democratization of Machine Learning", "selftext": "The [MLCommons Association](https://mlcommons.org/en/), an open engineering community, dedicated to making machine learning more accessible to everyone, has [released free datasets and technologies to help democratize machine learning](https://www.globenewswire.com/news-release/2021/12/14/2352036/0/en/MLCommons-Association-Unveils-Open-Datasets-and-Tools-to-Drive-Democratization-of-Machine-Learning.html). The People\u2019s Speech Dataset and the Multilingual Spoken Words Corpus are the two significant new datasets (MSWC). Organizations can use these ground-breaking and openly licensed datasets to construct improved artificial intelligence models.\n\nQuick Read: https://www.marktechpost.com/2021/12/17/mlcommons-releases-both-a-multilingual-speech-dataset-and-a-large-30000-hour-diverse-english-dataset-to-drive-democratization-of-machine-learning/\n\nPeople\u2019s Speech Dataset Research: https://openreview.net/forum?id=R8CwidgJ0yT\n\nDownload: https://mlcommons.org/en/peoples-speech/\n\nMultilingual Spoken Words Corpus Research: https://openreview.net/forum?id=c20jiJ5K2H\n\nDownload: https://mlcommons.org/en/multilingual-spoken-words/", "upvote_ratio": 0.94, "id": "t3_riyxh4", "created_utc": 1639800278.0}
{"sub": "LanguageTechnology", "title": "Cosine similarity Vs Jaccard index vs TFDIF", "selftext": "Hello, for my Masters thesis I am researching boilerplate in corporate disclosures. Specifically I want to 1. show that similarity in annual reports has been increasing over time and 2. find the cross sectional characteristics that predict the amount of boilerplate. I will be using annual reports of 1630 Nasdaq listed firms from the years 2010-2018. I purchased the textbook \" Text Mining with R A Tidy Approach \" by Silge and Robinson but it did not provide an answer to which method to use. Specifically, to measure similarity I'm not sure whether it would be best to use Cosine similarity or Jaccard index. A friend of mine suggested TF-DIF but I do not see how that fits within this context. Any insights are appreciated. Also if you know of any book which would be helpful for my research please let me know.\n\nThanks!", "upvote_ratio": 1.0, "id": "t3_riq55z", "created_utc": 1639773002.0}
{"sub": "LanguageTechnology", "title": "Named entity recognition extraction from website", "selftext": "Can anyone recommend a most standard technique for extracting all keywords of a specific kind (i.e. of a certain category, like \u201cspecies of trees\u201d) from a whole website?\n\nBonus points if the crawler can identify a good context sentence for that term, as well as judge if a context sentence provides/acts as a definition. Ideally, it would grab a context sentence and a definition for each term.\n\nMy first attempt is going to be using Spacy for Named Entity Recognition, maybe their Prodigy software, or maybe GPT-3 for zero-shot classification.\n\nDoes anyone know any pre-existing \u201csmart\u201d web crawling libraries which, sort of like Google Search, crawl a website for terms and find a good context sentence for that term?\n\nThanks so much to anyone who can send me in the right direction here.\n\nThanks very much", "upvote_ratio": 0.93, "id": "t3_rie2pm", "created_utc": 1639734800.0}
{"sub": "LanguageTechnology", "title": "Scientific Literature Review generation v0.2", "selftext": "Hello everyone,\n\nI've developed recently an algorithm to automatically generate a literature review : [https://www.naimai.fr](https://www.naimai.fr/)\n\nHopefully that could be useful for the PhDs (and the non PhDs) !\n\nMore details on the algorithm [here](https://yaassinekaddi.medium.com/scientific-literature-review-generation-386f36b05eae).\n\nI'll be thankful if you have any remarks about it :)\n\nCheers,", "upvote_ratio": 1.0, "id": "t3_rie1wv", "created_utc": 1639734700.0}
{"sub": "LanguageTechnology", "title": "OpenAI Releases A New Feature That Allows Developers To Customize GPT-3, Its Powerful Natural Language Processing (NLP) Model", "selftext": "GPT-3 is the advanced natural language processing model developed by OpenAI. It returns a natural language text completion in response to any text request, such as a phrase or a sentence. Developers use GPT-3 (through on-demand charging via application programming (API)) in their applications to do tasks such as text translation and software code development.\u00a0\n\nOpenAI has recently released new functionality that will allow developers to create their own versions of GPT-3. The new customization option is now available in the API.\n\nGPT-3 can execute a wide range of natural language tasks with just a few instances, a notion known as few-shot learning or prompt design. GPT-3 can be customized to produce much better results because it allows users to provide far more instances than prompt design allows.\n\nGet Access: https://beta.openai.com/docs/guides/fine-tuning/preparing-your-dataset\n\nQuick Read: [https://www.marktechpost.com/2021/12/16/openai-releases-a-new-feature-that-allows-developers-to-customize-gpt-3-its-powerful-natural-language-processing-nlp-model/](https://www.marktechpost.com/2021/12/16/openai-releases-a-new-feature-that-allows-developers-to-customize-gpt-3-its-powerful-natural-language-processing-nlp-model/)\n\nOpen AI Blog: https://openai.com/blog/customized-gpt3/", "upvote_ratio": 0.89, "id": "t3_rhw24g", "created_utc": 1639677130.0}
{"sub": "LanguageTechnology", "title": "phrase similarity", "selftext": "I have a bunch of phrases - not full sentences - for which I want to calculate similarities. (Typically two to six words long, for questions like whether (made up example) \"senior data scientist\" is more similar to \"machine learning engineer\" than to \"project manager\"). I'm looking for an off-the-shelf sort of solution - no one in my company including myself has any NLP experience, and this isn't so important to us that I want to spend weeks developing a whole new skillset for it.\n\nMy impression from google is that the way to do this is to turn the phrases into vectors and take the cosine similarity of them. It looks like I could use Sentence Transformers (SBERT) with a pre-trained model to get a vector for each phrase. Or I could get a vector for each individual word from some pre-trained model (which one?) and add them up to make a phrase vector. \n\nIs there any better approach that I'm missing? Is the SBERT method the way to go for this problem?", "upvote_ratio": 0.86, "id": "t3_rhtvw0", "created_utc": 1639670937.0}
{"sub": "LanguageTechnology", "title": "Genetic algorithm", "selftext": "Hello, I would like to apply genetic algorithm with natural language processing of TSP.?", "upvote_ratio": 0.5, "id": "t3_rhr87f", "created_utc": 1639662916.0}
{"sub": "LanguageTechnology", "title": "Genetic algorithm", "selftext": "I wuld to apply radom generation on Genetic algorithm with TSP. can anyone suggest me your Ideas. How can I approach.", "upvote_ratio": 0.25, "id": "t3_rhr7k1", "created_utc": 1639662855.0}
{"sub": "LanguageTechnology", "title": "What are some available tools for multilingual emotion analysis (also question about LIWC)?", "selftext": "As the title says. I've heard of LIWC (which you have to pay for) and NRC Emotion Lexicon. \n\nI haven't used either of them yet, but I'm mainly interested in the multilingual aspect. Are there any other tools for emotion analysis out there that are available also for languages other than English?\n\nAlso, if anybody has paid to use LIWC (for academic purposes), do you automatically have access to all the languages available? Thank you!", "upvote_ratio": 1.0, "id": "t3_rhog6u", "created_utc": 1639652461.0}
{"sub": "LanguageTechnology", "title": "Designing a Framework for Conversational Interfaces using PL design, API Design, and Constraint Programming", "selftext": "nan", "upvote_ratio": 0.67, "id": "t3_rhjfib", "created_utc": 1639632050.0}
{"sub": "LanguageTechnology", "title": "Conferences without APC in NLP", "selftext": "Are there more conferences or workshops like SemEval which target NLP that self funded students can use to publish?", "upvote_ratio": 0.9, "id": "t3_rgy5kp", "created_utc": 1639571792.0}
{"sub": "LanguageTechnology", "title": "Help with Sentence Splitting", "selftext": "Does anyone knows a way to add to the spacy pipeline a custom strategy to split the text into sentences? \n\nBecause the splits aren't allowed to be done between citations tags.\n\nFor example:\n\nText: *\"He went into the store. The store was closed.\"*\n\nAfter splitting this must be one sentence! It's not allowed to *split* between *store* and The because these two sentences are written between citation tags.", "upvote_ratio": 0.85, "id": "t3_rgxvdc", "created_utc": 1639570850.0}
{"sub": "LanguageTechnology", "title": "A new dataset for text classification and domain adaptation in social media", "selftext": "A dataset of \\~22,500 labeled documents across four different domains. You can find it here:\n\n[https://github.com/p-karisani/illness-dataset](https://github.com/p-karisani/illness-dataset)", "upvote_ratio": 1.0, "id": "t3_rgef94", "created_utc": 1639508083.0}
{"sub": "LanguageTechnology", "title": "Free course: NLP for Semantic Search", "selftext": "Hi all, the first seven chapters of the course [NLP for Semantic Search](https://www.pinecone.io/learn/nlp) that I've been working on have been published today. It's all completely free and covers everything you need to get started with building SotA language models for semantic similarity, from machine translation to question-answering, and more!\n\nSemantic search allows us to search language-based data based on the semantics or 'meaning' of a text. It enables machine translation and question-answering, it's how Google understands \"what time is it in NYC?\", and even allows us to search for images using text-based queries.\n\nIt is in essence, a way for us to interact with machines in a more human way. NLP fits in as the 'semantic' in semantic search.\n\nCurrent chapters are:\n1. Dense Vectors\n2. Sentence Embeddings and Transformers\n3. Training Sentence Transformers with Softmax Loss\n4. Training Sentence Transformers with MNR Loss\n5. Multilingual Sentence Transformers\n6. Question Answering\n7. Unsupervised Training for Sentence Transformers\n\nLet me know what you think, I hope you enjoy it!", "upvote_ratio": 1.0, "id": "t3_rg8kll", "created_utc": 1639491950.0}
{"sub": "LanguageTechnology", "title": "Text Data Augmentation using GPT-2 Language Model", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_rg4yrg", "created_utc": 1639479012.0}
{"sub": "LanguageTechnology", "title": "Library that takes a pool of words and spits out sentences with only those words?", "selftext": "Hi,\n\nI was wondering if there exists a library that can take an array or list of words, say 500, and using either an API or a model, can generate sentences only out of those words? Open source is preferable. It would be nice if the sentences are sensible or real sentences that someone has written, but they don\u2019t have to be, as long as theyre grammatically correct.\n\nIt would be nice if there exists for other languages too - I\u2019m trying to experiment with taking French words I know and generating sentences to test myself, without having to deal with new vocabulary words.\n\nThanks.", "upvote_ratio": 0.77, "id": "t3_rg4xk3", "created_utc": 1639478868.0}
{"sub": "LanguageTechnology", "title": "Has anybody tried to update the Spacy NER model?", "selftext": "As the title says, have you ever tried to update the Spacy NER model on your own data (as described [here](https://spacy.io/usage/training))? It seems to me that the NER feature just gets worse after retraining, and I don't understand why.", "upvote_ratio": 1.0, "id": "t3_rg325c", "created_utc": 1639470775.0}
{"sub": "LanguageTechnology", "title": "Prefer volume or quality for BERT-based Text classification model", "selftext": "Ill train a binary classifier. Yes samples make up about 5 percent of all samples. There are multiple persons doing the labelling. They have a pairwise alpha of 0.65\n\nScenario A:\n\nLabel each sentence once, and have every 10 sentence for all workers to check reliability. Resulting in 52000 single vote samples, plus 6000 multiple vote samples by all. Together about 3000 positive labels\n\nScenario 2\n\nTripple label everything, resulting in 20000 samples, where i can majority vote, but only have 1000 positive labels.\n\nIn your experience, is the better quality of samples worth the volume?\n\nEdit: Added first paragraph, which got lost by copy pasting before.", "upvote_ratio": 0.86, "id": "t3_rfr175", "created_utc": 1639431899.0}
{"sub": "LanguageTechnology", "title": "Universal Grammar (2) | Noam Chomsky", "selftext": "nan", "upvote_ratio": 0.5, "id": "t3_rfljcc", "created_utc": 1639417809.0}
{"sub": "LanguageTechnology", "title": "Are spaces generally used as tokens?", "selftext": "I've recently started looking into different language modeling methods and once I got to positional embeddings a whole series of questions sprung up for me.\n\nOne of these is: **Do language models generally use spaces?**\n\nDuring courses at uni I've heard about using subwords both subwords and character level encodings with many types of language models (rnns to tree-lstms to transformers to seeding input order through probablistic (frequentist) parsers).\n\nHowevermuch I might have heard about models, I have heard much less about model **inputs**. As such I figured I could ask people who are already more preoccupid with nlp in general: is there any consensus on whether this should be done when working with subwords (either morphemes or something different like bytepair encoding)?", "upvote_ratio": 1.0, "id": "t3_rfkiun", "created_utc": 1639415246.0}
{"sub": "LanguageTechnology", "title": "Looking for sentiment analysis datasets in the news domain", "selftext": "I am searching for multiple multi-lingual datasets for news sentiment analysis.\n\nIt could be a headline or the body.\n\nEven non-English datasets would be great to look at.", "upvote_ratio": 1.0, "id": "t3_rfhh5u", "created_utc": 1639407243.0}
{"sub": "LanguageTechnology", "title": "BERT vs. XLNet for texts shorter than 512 tokens.", "selftext": "Hi everbody,\n\nI made a binary text classification on IMDB review dataset, where average token count is around 400, I used BERT and BERT based models as well as XLNet. XLNet outperformed others (except RoBerta) and I believe the main reason is that Transformer XL's ability to capture longer dependencies than BERT. However, I am not confident about this reason because BERT can take 512 tokens at once and I wonder if this ability of Transformer XL and XLNet is valid only for texts longer than 512 tokens or can I say that independent from token count, XLNet performs better for longer texts?\n\n&amp;#x200B;\n\n(P.S. : I recently asked a similar comparison for twitter sentiment classification where BERT outperformed XLNet , however this question is not relevant)", "upvote_ratio": 1.0, "id": "t3_rfgl6z", "created_utc": 1639404708.0}
{"sub": "LanguageTechnology", "title": "Topic modelling for labelled documents", "selftext": "Hi Language Tech Community,\n\nI have 2000 documents already classified into 60 categories of Topics (i.e I have **labelled data**). These topics are found using a string search in the doc and then these strings are mapped into topics. All of this is done using Excel and Alteryx..\n\n\nThe project I am trying is to **automatically** classify newly encountered docs into one of the 60 categories of Topics ( referenced  above) AND I am trying to use Topic Modelling for this.. Because tfidf,word2vec and wordnet based approaches all have bad results..\n\n\nAs I understand Topic Modelling is **unsupervised** (i.e should be run on unlabellef data)\n\n\n**QUESTION**: Is topic modelling helpful in my project?", "upvote_ratio": 1.0, "id": "t3_rfexfv", "created_utc": 1639399565.0}
{"sub": "LanguageTechnology", "title": "Apply OpenNMT-py on T-Rex Dataset", "selftext": " Hi,  \nI\u2019m a data science student and i\u2019m learning the basics of NLP.  \nI find OpenNMT a very intresting tool and i\u2019m trying to understand it, after i\u2019ve completed a couples of tutorial i\u2019m trying to understand how to use OpenNMT for Data to text tasks.  \nI completed webNLG challenge 2017 ([WebNLG Challenge 2017 - WebNLG Challenges](https://webnlg-challenge.loria.fr/challenge_2017/)) using OpenNMT-py and now i\u2019m trying to follow the same pipline using the t-rex dataset([T-REx : A Large Scale Alignment of Natural Language with Knowledge Base Triples](https://hadyelsahar.github.io/t-rex/)).  \nHowever i don\u2019t understand how can i obtain the src and tgt files from the NIF or JSON file of t-rex dataset.  \nIs there a documentation about this step? How can i use T-rex dataset with OpenNMT-py?  \nThanks for your attention.", "upvote_ratio": 0.88, "id": "t3_rfcqzk", "created_utc": 1639391256.0}
{"sub": "LanguageTechnology", "title": "DeepMind's new big language model beats GPT-3 and has 280 BILLION parameters", "selftext": "nan", "upvote_ratio": 0.9, "id": "t3_rew1j6", "created_utc": 1639336684.0}
{"sub": "LanguageTechnology", "title": "Is there a professional role in NLP for people who are good at foreign languages and writing?", "selftext": "As the title says, is there such a role, for example someone responsible for collecting, editing, or quality assuring texts, either as input data or generated content?", "upvote_ratio": 0.78, "id": "t3_repzdq", "created_utc": 1639318944.0}
{"sub": "LanguageTechnology", "title": "Is there a simple way to split Chinese symbols into words?", "selftext": "Seems like `list()` will (in python) split the symbols, but AFAIK the symbols are basically subwords. Is there a way to split the symbols based on word boundaries?", "upvote_ratio": 0.73, "id": "t3_reo6e8", "created_utc": 1639312540.0}
{"sub": "LanguageTechnology", "title": "What makes you interested in NLP?", "selftext": "I'm curious to know how other people have gotten into NLP. I'm a new grad, so I haven't gotten the chance to talk to too many industry veterans other than my professors and a few engineers at work.\n\nPersonally, I really loved the first linguistics class I took in college, and I ended up taking many more linguistics classes after that. I was a CS major with a strong linguistics background, so NLP was the natural career path for me to take, even though I wasn't super passionate about ML more generally.\n\nAt my company, almost all of the engineers actually working on NLP that I have talked to are ML and deep learning experts without a particular interest in language. One person who I had talked to said he had completed a PhD in computer vision. When I asked why he would not continue working in that domain, he said that NLP and computer vision are really all the same - just applications of deep learning.\n\nI'm curious to know other people's thoughts on this. Which sentiment is more common in your experience?", "upvote_ratio": 0.92, "id": "t3_red7bm", "created_utc": 1639270534.0}
{"sub": "LanguageTechnology", "title": "Haystack - an open source NLP framework that leverages Transformer models. It enables developers to implement production-ready neural search, question answering, semantic document search and summarization for a wide range of applications.", "selftext": "nan", "upvote_ratio": 0.8, "id": "t3_rea7cf", "created_utc": 1639260927.0}
{"sub": "LanguageTechnology", "title": "DataQA: the new Python app to do rules-based text annotation", "selftext": "nan", "upvote_ratio": 0.83, "id": "t3_re1whz", "created_utc": 1639236032.0}
{"sub": "LanguageTechnology", "title": "How to get Job in NLP?", "selftext": "Hi All, I am currently working in the Embedded field mostly in drivers, Linux kernel, and RTOS. Somehow I got an interest in NLP for the past 6 months. I was going through some courses in Coursera, udemy, and some youtube tutorials. These are really helpful and I did some side projects to test my skills. I want to work in NLP as an actual paying job not only during weekends. This is a completely new field and I really don't know how to get jobs in this field. Please give some tips.", "upvote_ratio": 0.93, "id": "t3_rdym6v", "created_utc": 1639225005.0}
{"sub": "LanguageTechnology", "title": "Extracting topics from blog titles", "selftext": "Hey. I have a pet project which is a blog aggregator. What I want to do, it to extract topics for each blog title, for example:\n\n\n\"Using Documentation-Driven Design to Guide API Decisions\" - API, Documentation\n\n\"Measuring Web Performance at Airbnb\" - Web-development, Software Engineering\n\n\"How Spotify Uses ML to Create the Future of Personalization\" - Machine Learning, Personalization\n\n\nI have zero background in ML and NLP. Can someone please suggest, where should I look? I have tried gensim, but it was hard for me. Maybe I need some background reading to start with it? \n\nMy initial thoughts were that I can train a model from Reddit titles from different subreddits, and then use it.", "upvote_ratio": 1.0, "id": "t3_rdy9o1", "created_utc": 1639223654.0}
{"sub": "LanguageTechnology", "title": "Training text-generating models locally", "selftext": "Hi all. I am teasing with the idea of buying some hardware to train language models that can generate text. Would the RTX 3090, for example, be an appropriate GPU for this use case given that it has 24GB of VRAM? Assuming a modest dataset with \\~100k samples for fine-tuning, would I be able to train and test models reasonably well? Obviously GPT-3 is out of the question as I'd probably need like 20 3090's so my question is more geared towards the \"2nd tier\" models like GPT-2, T5, or BART.\n\nI have quite a bit of experience with fine-tuning encoder transformer models like BERT and have ran into memory issues with those in the past and I know that the aforementioned models listed are even bulkier so I'm a little cautious. Does anyone have any experience with fine-tuning these kinds of models?\n\nThanks.", "upvote_ratio": 0.5, "id": "t3_rdq5rp", "created_utc": 1639192340.0}
{"sub": "LanguageTechnology", "title": "Universal Grammar (1) | Noam Chomsky", "selftext": "nan", "upvote_ratio": 0.88, "id": "t3_rdpsw4", "created_utc": 1639191202.0}
{"sub": "LanguageTechnology", "title": "Can someone suggest some good NLP resources?", "selftext": "I'm just starting out in NLP. I find it really intriguing and want to learn more about it and get some hands on. I'd be glad if y'all can suggest some videos, books, courses, etc!", "upvote_ratio": 0.67, "id": "t3_rdks94", "created_utc": 1639176172.0}
{"sub": "LanguageTechnology", "title": "How Good is Your Chatbot? An Introduction to Perplexity in NLP", "selftext": "nan", "upvote_ratio": 0.9, "id": "t3_rdjtp8", "created_utc": 1639173586.0}
{"sub": "LanguageTechnology", "title": "A New DeepMind Research Studies Language Modeling At Scale", "selftext": "Language is an essential human component because of its role in demonstrating and promoting comprehension \u2013 or intellect. It allows people to express ideas, create memories, and foster mutual understanding by allowing them to share their thoughts and concepts.\u00a0\n\nThe research and study of more sophisticated language models \u2014 systems that predict and generate text \u2013 has enormous potential for developing advanced AI systems. This includes systems that can securely and efficiently summarise information, provide expert advice, and follow directions using natural language. Research on the possible impacts of language models and the risks they entail are required before they can be developed. This includes working together with experts from many fields to foresee and fix the problems that training algorithms on current datasets can cause.\n\nQuick Read: https://www.marktechpost.com/2021/12/10/a-new-deepmind-research-studies-language-modeling-at-scale/\n\nPaper 1: https://storage.googleapis.com/deepmind-media/research/language-research/Training%20Gopher.pdf\n\nPaper 2: https://arxiv.org/abs/2112.04359\n\nPaper 3: https://arxiv.org/abs/2112.04426", "upvote_ratio": 0.75, "id": "t3_rdedez", "created_utc": 1639158974.0}
{"sub": "LanguageTechnology", "title": "What is the task(s) called if I want to generate questions automatically based on a document?", "selftext": "**Input:** I have a bunch of documents which contains the introduction of a company and some text about information requests like:  investors must provide the details of their income sources.\n\n**Desired output:** generate questions automatically like: What is your income sources?\n\nI have found some potential open sources solutions like [https://github.com/ramsrigouthamg/Questgen.ai](https://github.com/ramsrigouthamg/Questgen.ai)\n\nJust want to ask if guys have any idea on this problem. Or what keyword should I try to research for?\n\nThe problem seems to consist of two tasks:\n\n1. identify and extract the desired text from the input document that can be used  to generate questions. **I am not sure what is this task called in NLP so don't know where to start my search in the literature.**\n2. generate questions with the extracted text. **It is certainly a text generation task, but it's not a common task like text summarization or translation.  I am not sure if there is a more specific name/keyword for this task**", "upvote_ratio": 0.76, "id": "t3_rdd1d0", "created_utc": 1639155445.0}
{"sub": "LanguageTechnology", "title": "Determining subject company (listed stocks) referred to in many short text samples", "selftext": "I have a large amount of unlabeled data samples (under 100 words in each sample) where mostly each sample refers to a specific company - which I need to determine; The company can be referred to by stock symbol or name.\n\nMy best attempt so far involves parsing out stock symbols using regular expressions, searching a database, and calculating relative Levenshtein distances of the search results vs sample text to make a best guess; this is a bit over 60% accurate - in ideal cases.\n\nI have two main issues that I can see:\n\n1. I am getting some false positives in cases where the symbol actually matches, but it's the wrong company (maybe on a different exchange).\n2. In cases where no stock symbol is specified (just a company name), I am getting no results, as I don't currently handle just company names.\n\n**For issue 1 - The False Positives**\n\nThe database I am searching against also contains company descriptions or titles for each search result. What would be the best way of comparing the company descriptions of each search result with the sample text to get a more accurate guess? I am thinking some sort of keyword comparison here would help - I know that factoring in the context of the sample text is critical here.\n\n**For issue 2 - No Stock Symbol**\n\nI think the best candidate for this case would be to leverage token classification to find \"ORG\" entities; I have tried this with a few pre-trained models from HuggingFace, but haven't had great results. Can anyone recommend a model that is pre-trained on financial data - or would even just work well for recognizing company names? **In addition to this**, would anyone know of a good dataset or strategy for further training the model for this purpose?\n\nIf anyone has an alternate suggestion for issue 2, I would also be open to that. Note that I am relatively new to machine learning, but I do understand the basics of how transformer models work, how to use them, and the different types of classification problems.", "upvote_ratio": 1.0, "id": "t3_rdbpio", "created_utc": 1639151835.0}
{"sub": "LanguageTechnology", "title": "Increasing the Accuracy of Textual Data Analysis on a Corpus of 2 Billion Words", "selftext": "[https://engineering.soroco.com/increasing-the-accuracy-of-textual-data-analysis-on-a-corpus-of-2000000000-words-part-1/](https://engineering.soroco.com/increasing-the-accuracy-of-textual-data-analysis-on-a-corpus-of-2000000000-words-part-1/)\n\nAt Soroco, we ingest between 200 million and 2 billion words over the course of model training and analysis for a single team of workers using our Scout product. In this blog post, I talk about some tips and tricks that we might use to increase the accuracy of our models, including appropriate processing of text for the purpose of leveraging standard techniques from machine learning. I then demonstrate this by showing how to represent text in a high-dimensional vector space with applications to a toy regression problem.", "upvote_ratio": 1.0, "id": "t3_rd21rx", "created_utc": 1639116778.0}
{"sub": "LanguageTechnology", "title": "5 Text Decoding Techniques that every \u201cNLP Enthusiast\u201d Must Know", "selftext": "nan", "upvote_ratio": 0.78, "id": "t3_rd3geu", "created_utc": 1639122112.0}
{"sub": "LanguageTechnology", "title": "The Toxicity Dataset \u2014 building the world's largest free dataset of online toxicity", "selftext": "nan", "upvote_ratio": 0.87, "id": "t3_rcov8z", "created_utc": 1639077388.0}
{"sub": "LanguageTechnology", "title": "Tips about building a chatbot with GPT-3 or GPT-J", "selftext": "Hello!\n\nI realize I have more and more questions from people trying to leverage GPT-3 or GPT-J for their next chatbot. And usually questions are always about 2 things:\n\n* How to format my requests so the model understands that I am in conversational mode?\n* How can the model keep an history of my conversation?\n\nI'm answering these 2 points in this quick article: [https://nlpcloud.io/how-to-build-chatbot-gpt-3-gpt-j.html](https://nlpcloud.io/how-to-build-chatbot-gpt-3-gpt-j.html?utm_source=reddit&amp;utm_campaign=k431103c-ed8e-11eb-ba80-5242ac130007)\n\nI hope it will help!\n\nI any question please don't hesitate to ask.", "upvote_ratio": 0.86, "id": "t3_rckzpj", "created_utc": 1639066590.0}
{"sub": "LanguageTechnology", "title": "Best way to vectorize names of medical conditions/diseases?", "selftext": "Let's suppose the aim is to predict, let's say, hospital charges incurred (there are other predictor parameters too). I have thought of the following ways of vectorization so far-\n\n1. I don't think using word2vec makes a lot of sense because the similarity of words is meaningless here?\n2. Find a huge medical corpus online and make a count vectorizer matrix for each row of medical condition. But that would mean the matrix is too sparse.\n3. Use only the medical conditions in the dataset as corpus and make a count vectorizer matrix from them?\n4. Pick only the top few select hundred words and use them as a corpus\n\n&amp;#x200B;\n\nIf there's any other way you can think of, do let me know. I accept I don't know much about NLP.", "upvote_ratio": 0.79, "id": "t3_rchqz7", "created_utc": 1639056746.0}
{"sub": "LanguageTechnology", "title": "open source sentence rephrasing", "selftext": "I have numerical data. I can come up with basic sentence (eg you credit score is good). i want to make this response seem natural and  not bot like. ie the response varies everytime, doesnt change the meaning and sounds human. what is the best technology available ?  is NLP cloud's paraphrasing a good fit or are there similar/better services ?", "upvote_ratio": 1.0, "id": "t3_rcd6cs", "created_utc": 1639037740.0}
{"sub": "LanguageTechnology", "title": "CtrlGen Workshop at NeurIPS 2021 (Controllable Generative Modeling in Language and Vision)", "selftext": "Excited by generation, control, and disentanglement? Come to our CtrlGen controllable generation workshop ([https://ctrlgenworkshop.github.io](https://ctrlgenworkshop.github.io/?fbclid=IwAR2lx-sDgf_snUoI16g79geBeAHJ__6i9Wd6duQQbJRlrg4xI76jDutg9iA)) at NeurIPS next Monday, December 13th! We feature a mix of 7 talks on the latest in controllable generation, a live QA + panel discussion, poster presentations of several interesting works, creative demos of controllable generation systems, and networking opportunities.\n\nThis is an effort organized with researchers from Stanford, CMU, Microsoft, Dataminr, and the University of Minnesota. Our invited speakers and panelists include researchers from Facebook, Google, DeepMind, University of Washington, New York University, Stanford, and Tel-Aviv University.", "upvote_ratio": 0.88, "id": "t3_rc5z8r", "created_utc": 1639012383.0}
{"sub": "LanguageTechnology", "title": "Numerizer - Spacy powered Streamlit deployed on Hugging Face for Free - Applied NLP Tutorial", "selftext": "nan", "upvote_ratio": 0.86, "id": "t3_rbyxw5", "created_utc": 1638991739.0}
{"sub": "LanguageTechnology", "title": "Meta AI Develops A Conversational Parser For On-Device Voice Assistants", "selftext": "A variety of devices such as computers, smart speakers, cellphones, etc., utilize conversational assistants for helping users with tasks ranging from calendar management to weather forecasting. These assistants employ semantic parsing to turn a user\u2019s request into a structured form with intents and slots that may be executed later. However, to access larger models operating in the cloud, the request frequently needs to go off-device.\n\nComplex semantic parsers use seq2seq modeling. Auto-regressive generation (token by token) has a latency that makes such models impractical for on-device modeling.\u00a0\n\nFacebook/Meta AI introduces a new model for on-device assistants and illustrates how to make larger server-side models less computationally expensive.\n\nQuick Read: [https://www.marktechpost.com/2021/12/08/meta-ai-develops-a-conversational-parser-for-on-device-voice-assistants/](https://www.marktechpost.com/2021/12/08/meta-ai-develops-a-conversational-parser-for-on-device-voice-assistants/) \n\nPaper 1: https://arxiv.org/pdf/2104.04923.pdf\n\nPaper 2: [https://arxiv.org/pdf/2104.07275.pdf](https://arxiv.org/pdf/2104.07275.pdf)\n\nFacebook Blog:  https://ai.facebook.com/blog/building-a-conversational-parser-for-on-device-voice-assistants", "upvote_ratio": 0.86, "id": "t3_rbv9hq", "created_utc": 1638981582.0}
{"sub": "LanguageTechnology", "title": "A Visual Guide to Prompt Engineering [With GPT language models]", "selftext": "nan", "upvote_ratio": 0.75, "id": "t3_rbtfut", "created_utc": 1638976321.0}
{"sub": "LanguageTechnology", "title": "AI, DL, NLP,.. resources", "selftext": " \n\nHi,  \nHave a look at a great resource on [https://www.techontheedge.com/mobile](https://www.techontheedge.com/mobile?fbclid=IwAR3re_Ej70FRcwJJziNWjwcoHuHpSyrGtYf8CCgn4QiYYRiZfryR5vaAXWw). \n\n  \nYou will find the very latest news, articles, research in AI, ML, DL, NLP, web, mobile,... and the wider computing space.", "upvote_ratio": 0.44, "id": "t3_rbqhya", "created_utc": 1638967287.0}
{"sub": "LanguageTechnology", "title": "Zero-Shot Event Classification for Newsfeeds (including Notebooks and Code examples)", "selftext": "nan", "upvote_ratio": 0.88, "id": "t3_rbp2ox", "created_utc": 1638962041.0}
{"sub": "LanguageTechnology", "title": "General character string outlier detection", "selftext": "Hi, I'd like to preface this question by saying that I'm not looking for a solution to a problem in a specific dataset, as I already have these. I'm curious to know if there's a concise method to solve this problem in general.\n\nThe problem: Detect abnormal character strings from a set of non-language character strings. Effectively, to model normal behaviour as a combination of string and pattern frequency without prior information.\n\nData: Normal string behaviour may be due to high frequency, matching a set of patterns, or some combination of those. Typically they'll either be high frequency with no useful patterns or low frequency with common patterns. They could be as short as two characters and length may vary. They'll be a combination of alphanumeric and punctuation within which there may be a set of specific characters with either absolute or pattern relative location. Both high frequency and common pattern strings may occur with different orders of magnitude. Datasets may be 10-100k in size, with specific string occuring as often as 50% of the time, or as little as once while matching a pattern.\n\nSolution: This should be unsupervised, ideally model the frequency/pattern tradeoff itself, and generalise to any dataset described above with minimal intervention. \n\nAppreciate any contributions in advance. Even if it's just a set of keywords to describe the problem.\n\nRealise this is quite a vague problem statement, but I can detect them myself by eye with a combination of string and regular expression counts, so wondering what the world of NLP (I guess not technically NLP) can do.", "upvote_ratio": 1.0, "id": "t3_rbomdu", "created_utc": 1638960214.0}
{"sub": "LanguageTechnology", "title": "Multiple SEP tokens for keyword searches.", "selftext": "I am trying to train a siamese-net style BERT based retrieval model that supports both semantic queries and keyword queries for my domain. The keywords can be of different categories (for example, some are related to product specifications, some others to usability features, manufacturers etc.)\n\nTo index a document, I want to use SEP tokens to separate the different categories of keywords and the product description. Eg:\n[CLS] &lt;Product description&gt; [SEP] &lt;keywords type_1&gt; [SEP] &lt;Keywords type_2&gt; [SEP]...\n\nThe query can be a product description or a keywords or a combination of the two.\n\nMy training dataset size is 100K samples. \n\nHas anyone used an approach like this? Has it worked?", "upvote_ratio": 1.0, "id": "t3_rb9lph", "created_utc": 1638911423.0}
{"sub": "LanguageTechnology", "title": "List of filler words?", "selftext": "I would like to detect filler words in a spoken English text. I couldn\u2019t find a package:function for it (stop words don\u2019t work). I was wondering if anyone has a compiled list to share or refer to.", "upvote_ratio": 0.88, "id": "t3_rb6vfo", "created_utc": 1638904114.0}
{"sub": "LanguageTechnology", "title": "Has anybody tried to retrain Stanza NER on new data?", "selftext": "I have been trying to follow  the instructions in this page [https://stanfordnlp.github.io/stanza/training.html#ner-data](https://stanfordnlp.github.io/stanza/training.html#ner-data) to retrain Stanza on a new dataset for NER. \n\nI have managed to convert my .iob files (training, development and test datasets) into the .json files required by the model. However, I don't understand where I should put my data to have \"run\\_ner.py\" run successfully.\n\nThis command is mentioned in the page:\n\n    python -m stanza.utils.training.run_ner fi_turku\n\nBut I don't understand what \"fi\\_turku\" is supposed to be. I know it's a sample corpus I can download, but what is it exactly? A directory containing the three .json files? What is the path to it? \n\nIt seems like the only problem is the path to the new dataset I want to train the model on, but I'm failing to undestand where exactly I should put it.", "upvote_ratio": 0.67, "id": "t3_rb3tvm", "created_utc": 1638898053.0}
{"sub": "LanguageTechnology", "title": "How to implement a weighted string classifier that results in an exportable model and also gives a confidence score?", "selftext": "Hi there! I've been recently working on a side project that works vaguely like this:\n\n[https://imgur.com/a/g1FvKCM](https://imgur.com/a/g1FvKCM) &lt;-- link to flowchart cause apparently you guys hate images\n\nI already built the labeler, as you can see from the image it produces an already cleaned CSV file with the structure\n\n&gt;lowercase stopwords removed string i want to classify , \\[1/-1\\]\n\nWith 1 or -1 as the value I want to label to that string with.\n\n*BTW: the project is all on Python3.*\n\nNow, I need to build the classifier with these needs:\n\n* Creates an exportable model I can then use in a discriminator, that will process future inputs based on this model\n   * I absolutely need the discriminator to give me a confidence score when evaluating the inputs because I will pick only output with a certain confidence score or higher\n* Applies a weight based on data recency, last data in the file gets a higher weight\n   * In a scale from 0 \"I don't consider this at all\" to 1 \"this is the most important piece of information I will ever handle\" I would like to apply a soft growing weight, something like [this](https://imgur.com/a/mIoaaD8)\n   * I don't actually know if this is possible or not but if possible I would definitely do this even if makes things way more complicated\n\nNow, all the tutorials and GitHub repos and videos always went with the Bayes or Linear Regression approach, which I also tried and resulted in a not-that-bad result, with the KPI of AUC around 0.7, but it didn't solve either of the two problems before presented in the bullet list so I'm quite stuck.\n\nI did some image processing in the past so I thought it would have been easier to handle strings but until now it's giving me some troubles.\n\nI really appreciate any support comment, any indication, guide or study material to look after. Thank you all.\n\n**TLDR: Just read the title**", "upvote_ratio": 1.0, "id": "t3_rb0d5n", "created_utc": 1638887257.0}
{"sub": "LanguageTechnology", "title": "[Research paper] Hierarchical Topic Modelling Over Time", "selftext": "Hello Reddit,\n\nI am proud to present you HTMOT for Hierarchical Topic Modelling Over Time. This paper proposes a novel topic model able to extract topic hierarchies while also modelling their temporality. Modelling time provide more precise topics by separating lexically close but temporally distinct topics while modelling hierarchy provides a more detailed view of the content of a document corpus.\n\n[https://arxiv.org/abs/2112.03104](https://arxiv.org/abs/2112.03104)\n\nThe code is easily accessible on GitHub and a working interface provides the ability to navigate through the resulting topic tree with ease: [https://github.com/JudicaelPoumay/HTMOT](https://github.com/JudicaelPoumay/HTMOT)", "upvote_ratio": 0.92, "id": "t3_rav3g5", "created_utc": 1638868399.0}
{"sub": "LanguageTechnology", "title": "(Okapi) BM25 with using hierarchically clusterized keywords", "selftext": "Hey, all! Hope you are doing well!\n\nDo you know any work which tries to do Okapi BM25 matching using hierarchically clusterized words?\n\nRelabeling all tokens of a subtree to the same value would combine similar words into the same token_id. Lower subtrees imply in closer words This would be a query and document enrichment. And now, with robust word embeddings and clustering algorithms, this approach seems feasible.\n\nAlso this is a quite immediate idea so someone must have already done it. Do you know any work on this?\n\nCheersss", "upvote_ratio": 1.0, "id": "t3_rakwq0", "created_utc": 1638833431.0}
{"sub": "LanguageTechnology", "title": "Language education architecture", "selftext": "Hi all,\n\nI'm relatively new to the language domain - designing services that support language education.  What's the best practice for associating metadata to words and sentences?  This would include audio, video, pronunciation, and other words/sentences considered related.\n\nI've been reading up on various NLP corpus functionality - which seems lower level (i.e. either just the text, or some structure that is pretty specific)  Even multi-modal corpus doesn't seem to cover everything.  Am I getting the correct sense here?\n\nI've seen references to Lexical Resources - which seems like the right direction, but I don't see any dominant libraries for that (I'm a python guy.)\n\nIt seems somewhat straight forward to have a persistent lookup, especially if I assign an index key to all the words and sentences that I can then base the metadata on.  But I don't want to reinvent a wheel unnecessarily.", "upvote_ratio": 1.0, "id": "t3_rai9z2", "created_utc": 1638826172.0}
{"sub": "LanguageTechnology", "title": "Is there an open-source way to replicate entity-level sentiment from Google's Cloud Natural Language API?", "selftext": "I'm learning about NLP and was really impressed with Google's Natural Language API ([demo](https://cloud.google.com/natural-language#section-2)). It seems that entity-level sentiment analysis is the future of NLP. Has anyone in the community come across open-source libraries that replicate the API for learning purposes? I found an excellent [repo](https://github.com/songyouwei/ABSA-PyTorch) called ABSA-PyTorch but it seems that all the implementations are classification-based; that is, they return \"positive/negative\" rather than a spectrum between positive and negative. Is there a sub field of Aspect-Based Sentiment Analysis (ABSA) that isn't classification based? I wasn't able to find any keywords despite hours of Google searching.", "upvote_ratio": 1.0, "id": "t3_rahmej", "created_utc": 1638824472.0}
{"sub": "LanguageTechnology", "title": "Need help with clustering keywords", "selftext": "I have a set of keywords, and can extract similar keywords using word2vec model (with cosine similarity scores) or can calculate similarity scores from BERT model.. I need to cluster the keywords which would be semantically similar. Any help with the type of cluster would be appreciated. Just need a discussion before I try to implement.", "upvote_ratio": 1.0, "id": "t3_rae2xx", "created_utc": 1638815337.0}
{"sub": "LanguageTechnology", "title": "Fine tuning BERT for token classification.", "selftext": "Hello guys, I have a question regarding my work. I am pretty new at NLP.\n\nI want to try self supervision and semi supervised learning for my task in hand. The task relates to token wise classification for the 2 sequence of sentences (source and translated text). The labels would be just 0 and 1 determining if the word level translation is good or bad on both source and target side.\n\nTo begin, i used XLMRoberta as ai thought it would be best suited for my problem. First, I just trained normally using nothing fancy but model overfits after just one or two epochs, as i have very less data to fine tune on (approx 7k). I decided to freeze the bert layers and just train the classifier weights but it performed worse. I thought of adding more dense network on top of BERT but i am not sure if it would work good or not. One more thought that occurred to be was data augmentation where i increased the size of my data by multiple factors but that performed bad as well. (Also i am not sure what should be the proper number to increase the datasize with augmented data)  \n\n\nCan you please suggest which approach could be suitable here and if i am doing something wrong. Shall i just use all the layers for my data or freezing is actually a good option? Or you suspect i am ruining somewhere in the code and this is not what is expected.\n\nI know i have many questions but you are free to help as much as you can :) \n\nThanks a ton in advance.", "upvote_ratio": 0.84, "id": "t3_ra9k6s", "created_utc": 1638803824.0}
{"sub": "LanguageTechnology", "title": "How to capture words order in a sentence?", "selftext": "Hi guys, i'm data science student and i'm trying to capture the words order in a sentence for checking if in n triples (subject, predicate and object) this order is respected.\n\nFor example, given the phrase \"Rougue is a comedy movie\" and given these 3 triples:\n\n1. \\[Rougue, is, movie\\]\n2. \\[Rougue, movie, is\\]\n3. \\[Movie, is, Rogue\\]\n\nIn this example, only the first triple is correct (for my task).\n\nI guess that in order to achieve my goal I have to vectorize the reference sentence, but I don't understand how to capture the correct order so that only the first triple turns out to be right.\n\nHow can i do this? Thanks all.\n\nEDIT\n\nMy dataset, for more than 90%, is composed of simple sentences where the order S + V + O is respected, so even the triples that are extracted, as a second check, should follow this order. As a first check, I thought about using the Bag of Words or TF-IDF to check the presence of the words contained in the extracted triple within the reference sentence.\n\nHowever, it is still not clear to me how to check if the word order within the triple is respected. I know it's a coarse job, however it serves as a basic control for skimming.", "upvote_ratio": 0.91, "id": "t3_ra4f16", "created_utc": 1638787147.0}
{"sub": "LanguageTechnology", "title": "How to use Textblob for semantic analysis?", "selftext": "I'm using Textblob to identify if a paragraph text is positive or negative. I'm new to Textblob, for my data I cleaned the data (remove stop word , extend word , punctuations..etc) tokenized the text into sentences then into words then performed lemmatization then applied Textblob to lemmatize data.\n\nI read that Textblob do all of these as well as pos tag when calling TextBlob() I was wondering do I need all the steps that I performed before or will calling Textblob be enough?", "upvote_ratio": 0.78, "id": "t3_r9r2xq", "created_utc": 1638742801.0}
{"sub": "LanguageTechnology", "title": "sort 150k facebook posts in hebrew to 3 defined topics", "selftext": "I have an existing list with 150k public posts that were extracted from Facebook for academic research purposes, they are all in Hebrew.\n\nI need to tag each post to one of 3 categories: General news, Political Related, Other.\n\nI know these categories are a bit vague. Is there a tool/method I can use to train a model to sort these posts to the categories? \n\nI'm not an expert in ML or NLP so I will just clarify what I mean:\n\nA tool where I can tag a few thousand posts according to the categories and then let the model tag the rest of the posts automatically.\n\n\\*The posts cannot be translated to English\n\n&amp;#x200B;\n\nThanks!", "upvote_ratio": 0.87, "id": "t3_r9hldc", "created_utc": 1638716807.0}
{"sub": "LanguageTechnology", "title": "Reproducing WebNLG Challenge 2017 on OpenNMT-py", "selftext": "Hi guys, I'm Data Science student and i'm learning to use OpenNMT-py for my master degree thesis. \n\nI reproduced the challenge with the old deprecated repository, now I would like to replicate it with the updated repository (as I will need it for a similar task within my thesis).\n\nI am now approaching the NLP field, but I am not very clear about some things:\n\n* since it is not a translation task, is it necessary to build a vocabulary like in the machine translation OpenNMT-py tutorial?\n* The epochs command I noticed has been deprecated, now it works with train\\_steps, however I am not clear about the \"conversion\", so to speak. With the old repository the number of epochs to train the model with was 13. I tried this by  looking at old problems from these repositories: default train\\_steps (100000) / deault batch\\_size (64) \\* 13 (epochs number of the old repository) = 20313.\n\nis this reasoning correct? Thanks everyone for your attention.", "upvote_ratio": 1.0, "id": "t3_r9cyld", "created_utc": 1638700005.0}
{"sub": "LanguageTechnology", "title": "What is the difference between Rule-Based &amp; Feature-Based methods in sentiment analysis?", "selftext": "I use Textblob to get lable value of texts (positive text or negative) and then used logistics regression for training and prediction , Is this feature methods or rule based method?", "upvote_ratio": 0.8, "id": "t3_r9bb4p", "created_utc": 1638693079.0}
{"sub": "LanguageTechnology", "title": "What are the leading knowledge evaluation models?", "selftext": "I'm new to NLP and ML. I've been playing with GPTJ and other stuff provided by huggingface. I'm also playing with compromise and nltk. I have some ideas I want to try with regards to knowledge extraction from multiple sources.\n\nOne problem I imagine is, what are the preferred ways to evaluate the truthiness of a statement? I see that T0PP can extract information from within a contained context, but what about from the unbounded context of reality?\n\nIf anyone can help me out with clues or ideas that would be awesome! Thanks gang.", "upvote_ratio": 0.93, "id": "t3_r8dtkk", "created_utc": 1638579956.0}
{"sub": "LanguageTechnology", "title": "What is topic modeling and how can it help with sentiment analysis?", "selftext": "If I apply it to my data will it change the outcome of my sentiment analysis?", "upvote_ratio": 0.71, "id": "t3_r80t23", "created_utc": 1638543164.0}
{"sub": "LanguageTechnology", "title": "Need ideas for a story generator", "selftext": "Hi, I am working on a story generator and rather than fine tuning a pre-trained model, I need more ideas to make it interesting, like maybe how could i make it so it generate the beginning, body and end of the story. Share your thoughts pls thx", "upvote_ratio": 0.78, "id": "t3_r7vmvs", "created_utc": 1638525036.0}
{"sub": "LanguageTechnology", "title": "Doubt about the originality of a submisssion in ICLR2022", "selftext": " *Hindsight: Posterior-guided training\u00a0 of retrievers for improved open-ended generation.* is a manuscript\u00a0submitted to ICLR2022 conference and received relatively very high scores (8 8 6 6). But I doubt the originality of this work since it is very similar to \\[1\\] and \\[2\\]. What do you think\uff1fIs this a article spining? \n\n\\[1\\] Lian, Rongzhong, et al. \"Learning to select knowledge for response generation in dialog systems.\"\u00a0*arXiv preprint arXiv:1902.04911*\u00a0(2019). \n\n\\[2\\] Kim, Byeongchang, Jaewoo Ahn, and Gunhee Kim. \"Sequential latent knowledge selection for knowledge-grounded dialogue.\"\u00a0*arXiv preprint arXiv:2002.07510*\u00a0(2020).", "upvote_ratio": 1.0, "id": "t3_r7sy6b", "created_utc": 1638514332.0}
{"sub": "LanguageTechnology", "title": "Help with Sentence splitting", "selftext": "Hey!\nI\u2019m using Python with spacy library. \n\nCan anyone help me with sentence splitting. \nI have some court decisions to analyze. How can I write a sentence splitting expansion that it don\u2019t split inside of quotes?", "upvote_ratio": 1.0, "id": "t3_r7sapp", "created_utc": 1638511978.0}
{"sub": "LanguageTechnology", "title": "Low rent automated essay scoring", "selftext": "I am building an online elementary history course and I\u2019d like to ask students to write a paragraph on an inquiry question.  E.g. How did the seven years war help cause the revolutionary war?  Unfortunately I don\u2019t have human graders, or a dataset of graded responses or an NLP/ML programmer for that matter.  I\u2019m thinking I could just count the number of sentences and number of key phrases the student mentions for low rent automated essay scoring.  It might be labor intensive to come up with variation of the keywords.  Does anyone know of open source or commercial solutions like this that work well?  Goal is to give the student enough feedback/scaffolding so that they feel like it is worth writing down their thoughts.", "upvote_ratio": 1.0, "id": "t3_r7moad", "created_utc": 1638494517.0}
{"sub": "LanguageTechnology", "title": "Topic Model - Generating more Context", "selftext": "I am new to Python and just started playing around with LDA - using pyLDAvis to visualize the keywords from a few documents. I\u2019m a novice at best.\n\nThe problem: I find it\u2019s difficult to determine accurate topics (for the LDA model) that explain the list of keywords because there is not enough context to frame the topic. Maybe my model sucks.\n\nDoes anyone know how to do the following?\n\n1.) Extract all of the sentences from a [file](https://www.mckinsey.com/business-functions/people-and-organizational-performance/our-insights/building-workforce-skills-at-scale-to-thrive-during-and-after-the-covid-19-crisis) (URL for the sake of this query) that specifically contain the most salient terms from my LDA model e.g., Skills, Digital, Pandemic, etc. prior to any pre-processing.\n\nThe way I see this looking (in a df) is that in Column A there would be a salient term e.g., Skill, and column B would contain the sentences i.e., context to the topics in the LDA model.\n\nI would appreciate any guidance on this.\n\nCheers", "upvote_ratio": 1.0, "id": "t3_r7djy6", "created_utc": 1638468999.0}
{"sub": "LanguageTechnology", "title": "Literature on plain language generation?", "selftext": "Hello. \n\nDoes anybody know of any literature on [plain language](https://en.wikipedia.org/wiki/Plain_language) generation? \n\nThis might be considered a kind of summarisation or paraphrasing task \u2013 or maybe even a type of machine translation task \u2013 but I'm wondering if there are any recent papers specifically on plain language. \n\nE.g., {It is incumbent on the buyer to furnish all requisite documents.}=&gt;{The buyer must provide the necessary paperwork.}\n\nThanks in advance.", "upvote_ratio": 0.5, "id": "t3_r79793", "created_utc": 1638456914.0}
{"sub": "LanguageTechnology", "title": "Formal grammar parser for English", "selftext": "Hello, I am looking for a parser for English, not a dependency parser, but a formal grammar parser (i.e. One that makes tree with rules such as S -&gt; NP VP, VP -&gt; V NP, and so on...)\n\nI thought that finding one would be easy but when I searched I couldn't find any good ones. I only found one library that gives wrong parsing for some very simple sentences. Any suggestions out there?\n\nPlease help I need this for a class project and the deadline is close.", "upvote_ratio": 1.0, "id": "t3_r6w34h", "created_utc": 1638411338.0}
{"sub": "LanguageTechnology", "title": "Cohen's kappa \u2014 worth the hype?", "selftext": "I often see subtle misuses of interrater reliability metrics.\n\nFor example, imagine you're running a Search Relevance task, where search raters label query/result pairs on a 5-point scale: Very Relevant (+2), Slightly Relevant (+1), Okay (0), Slightly Irrelevant (-1), Very Irrelevant (-2).\n\nMarking \"Very Relevant\" vs. \"Slightly Relevant\" isn't a big difference, but \"Very Relevant\" vs. \"Very Irrelevant\" is. However, most IRR calculations don't take this kind of ordering into account, so it gets ignored.\n\nI wrote [an introduction to Cohen's kappa](https://www.surgehq.ai/blog/inter-rater-reliability-metrics-understanding-cohens-kappa) (a rather simplistic and flawed metric, but a good starting point to understanding IRR). Hope it helps. I welcome feedback and am curious to hear the IRR metrics you find yourself relying on most.", "upvote_ratio": 0.96, "id": "t3_r6pmce", "created_utc": 1638393868.0}
{"sub": "LanguageTechnology", "title": "Custom training issue: best_model_ranking not outputting for certain ConLL files", "selftext": "Hi, I have successfully trained a custom model using the neuralcoref for a  set of ConLL files. However, when I add more from another set I get this  error: \n\n    FileNotFoundError: [Errno 2] No such file or directory: '...best_modelallpairs'\n\nbest\\_model\\_ranking (the custom model I used for coreference resolution with neuralcoref) is not present in the checkpoints folder.\n\nHave you encountered this error before? I think it might be because the  token distance between coreferences is too long in some of the new ConLL  files. Do you have any ideas? Thank you very much.", "upvote_ratio": 1.0, "id": "t3_r6l91f", "created_utc": 1638382684.0}
{"sub": "LanguageTechnology", "title": "Getting aligned vector representations in two languages", "selftext": "Any model (links or references) that can provide me with vector representations of semantically similar sentences from two languages. For ex English and Croatian. \n\n*Ex* \n\n**Ja volim Kavu** and **I love coffee** having similar vectors", "upvote_ratio": 1.0, "id": "t3_r6bzq2", "created_utc": 1638355316.0}
{"sub": "LanguageTechnology", "title": "How to highlight intonation, word stress in a text?", "selftext": "I'm very new to NLP.\n\nWhen I give a text file, I want the output to be highlighted with colors to indicate the amount of stress each word should have.\n\nUsing spacy it's possible to highlight Parts of speech. I also searched on this sub but couldn't find anything related to word stress.\n\n  \n\n\nGoogle search provided this result. [https://stackoverflow.com/questions/58251398/how-to-detect-sentence-stress-by-python-nlp-packages-spacy-or-nltk](https://stackoverflow.com/questions/58251398/how-to-detect-sentence-stress-by-python-nlp-packages-spacy-or-nltk)\n\nI have the same question too and the answers on SO are focused on speech but not on text.", "upvote_ratio": 0.86, "id": "t3_r6byqn", "created_utc": 1638355207.0}
{"sub": "LanguageTechnology", "title": "What is the difference between text classification and semantic analysis?", "selftext": "nan", "upvote_ratio": 0.8, "id": "t3_r69kbo", "created_utc": 1638345207.0}
{"sub": "LanguageTechnology", "title": "Question about Good Turing Smoothing", "selftext": "As in [https://youtu.be/1vUVNdDkIJI?t=485](https://youtu.be/1vUVNdDkIJI?t=485) , I do not understand how the `c*` formula is being obtained ?\n\nIt seems different from equation (2) of [https://www.cs.cornell.edu/courses/cs6740/2010sp/guides/lec11.pdf#page=2](https://www.cs.cornell.edu/courses/cs6740/2010sp/guides/lec11.pdf#page=2)", "upvote_ratio": 1.0, "id": "t3_r5azop", "created_utc": 1638235151.0}
{"sub": "LanguageTechnology", "title": "NLP with Arabic language", "selftext": "Hi, I am new to arabic related nlp and don't have the sense of the actual language. But as par my understanding, there are vowels named 'tashkeel' and 'harakat'. I just don't understand why we would need to remove the vowels ( strip\\_tashkeel and strip\\_harakat functions in pyarabic) from Arabic text before processing it further? also I am not getting any good answer regarding it.\n\nTIA for your help.", "upvote_ratio": 0.67, "id": "t3_r55w1g", "created_utc": 1638220569.0}
{"sub": "LanguageTechnology", "title": "Best available pronoun coreference resolution systems?", "selftext": "I wanna study singular they and what its status is in current NLP research. Not looking for answers on here, just for pointers.\n\n1. What are the best systems for pronoun coreference resolution?\n\n2. Have you come across something related to singular they in NLP? For example in pronounc coreference resolution?\n\nCurious to hear what you know!", "upvote_ratio": 0.94, "id": "t3_r52lzh", "created_utc": 1638211697.0}
{"sub": "LanguageTechnology", "title": "Evaluating performance of BERT fine tuning for classification", "selftext": "Hello all!\n\nI am working on fine tuning BERT for classification using a custom dataset.\n\nNLP is not my area of expertise but BERT seems like a great tool for conducting classification problems.\n\nI was wondering if anyone can give me some metrics I can use to evaluate the performance of my classification model? \n\nThanks in advance!", "upvote_ratio": 0.63, "id": "t3_r4tla1", "created_utc": 1638184252.0}
{"sub": "LanguageTechnology", "title": "Looking for NLP cloud-based technologies expert/consultant", "selftext": "Hello everyone, \n\nMy team is currently looking for experts/consultants in cloud-based NLP/Text Mining technologies.\n\nWe are developing a platform that aggregates the best AI engines on the market but some of our prospects want to be supported by specialists in their projects. We are therefore looking for experts for this step of personalized audit (paid) before using our platform. \n\nIf you are interested, please send me a message or an email: [contact@edenai.co](mailto:contact@edenai.co)! \n\nThank you, \n\nTaha", "upvote_ratio": 0.75, "id": "t3_r4s9tz", "created_utc": 1638178693.0}
{"sub": "LanguageTechnology", "title": "Sentiment Analysis Questions", "selftext": "Hi all,\n\nI'm working on a final project for one of my classes involving sentiment analysis on a data set of IMDB movie reviews (data set courtesy of keras). It's a fairly straightforward binary classification (classify the review as positive or negative). The thing is, I'm a little short on ideas with regards to how to accomplish this. I've already utilized a few models but I feel like they're a little simple, and in the interest of getting into the spirit of things I was wondering if there were more advanced techniques that a novice like me could still use.\n\nFor what I've already done (all inputs are word embeddings of the dataset imported from keras):\n\n1. CNN\n2. LSTM\n3. Transformer\n4. CNN-LSTM\n5. LSTM-SVM\n\nThus far, I've achieved an accuracy of 87-89% for all but the last one, which performed significantly more poorly. I'd say I'm pretty much done with the project (nothing too complex) but I'm interested in seeing what else I can apply for the sake of it.\n\nA few things I've been looking at but not sure about implementing:\n\n1. Generative-discriminative models (the generator would perform feature extraction on the data)\n2. Data augmentation techniques to use on the data and then feed into the aforementioned models (thus far they've been fairly simple, like swapping in synonyms or antonyms, randomly deleting or adding words, etc.)\n\nAny recommended course of action/source for the two ideas I've been looking at? And do you have any other suggestions that I could feasibly implement? I'm using Google Colab (got a premium membership) and the data set isn't very large (25,000 training samples) so computational expense should not be a concern. I'd appreciate any suggestions you guys have!", "upvote_ratio": 0.72, "id": "t3_r4hcd2", "created_utc": 1638141733.0}
{"sub": "LanguageTechnology", "title": "Query Intent Classification in chatbots using distilled transformers", "selftext": "Hi\n\nI am writing a paper about Query Intent Classification in chatbots and would like to also have a section about distilled Transformers (fx distilBERT) , but have been unsuccessful in finding papers or   \n chatbot companies that use such models.\n\nAre  distilled transformers simply not used for chatbots? - it seems like a good trade off in terms of use of resources and general performance, like precision.\n\nAre there any good papers or company blogs about deploying Distilled Transformer Models in Chabot settings?", "upvote_ratio": 0.85, "id": "t3_r46zry", "created_utc": 1638113914.0}
{"sub": "LanguageTechnology", "title": "nlp project", "selftext": "Hello everyone,I want to know how to learn nlp. could you please give me some advices ? in fact, these days I learned some basic model like cnn, rnn, lstm, transformer, belt. but I don't know how to imply these knowledge into project, in other words, could you please recommend me some interesting projects for me? thank you so much!", "upvote_ratio": 0.54, "id": "t3_r3ib94", "created_utc": 1638034181.0}
{"sub": "LanguageTechnology", "title": "MetaICL: A New Few-Shot Learning Method Where A Language Model Is Meta-Trained To Learn To In-Context Learn", "selftext": "Large language models (LMs) are capable of in-context learning, which involves conditioning on a few training examples and predicting which tokens will best complete a test input. This type of learning shows promising results because the model learns a new task solely by inference, with no parameter modifications. However, the model\u2019s performance significantly lags behind supervised fine-tuning. In addition, the results show high variance, which can make it difficult to engineer the templates required to convert existing tasks to this format.\n\nResearchers from Facebook AI, the University of Washington, and the Allen Institute for AI have developed Meta-training for In-Context Learning (MetaICL), a new few-shot learning meta-training paradigm. In this approach, LM is meta-trained to learn in context, conditioning on training instances to recover the task and generate predictions.\n\n**You can read** [**a short summary-based article**](https://www.marktechpost.com/2021/11/26/metaicl-a-new-few-shot-learning-method-where-a-language-model-is-meta-trained-to-learn-to-in-context-learn/) [**here**](https://www.marktechpost.com/2021/11/26/metaicl-a-new-few-shot-learning-method-where-a-language-model-is-meta-trained-to-learn-to-in-context-learn/)**. The Github can be** [**accessed here**](https://github.com/facebookresearch/metaicl)**. If you are looking to read the full paper, then you can** [**read it here**](https://arxiv.org/pdf/2110.15943.pdf)**. The demo** [**project is here**](http://qa.cs.washington.edu:2021/)**.**", "upvote_ratio": 1.0, "id": "t3_r2sfyk", "created_utc": 1637947982.0}
{"sub": "LanguageTechnology", "title": "What should I visualize for humor detection model to gain some useful insight?", "selftext": "I was going through bunch ([1][1],[2][2],[3][3]) of humor detection paper. But most papers don't include any visualizations, say some graph related to model being trained. I was thinking to train some language models like BERT, GPT, XLNet. But was guessing what kind of some interesting visualization should I aim for in order to gather the data during training and gain some sort of insight.  \n\nOr is it like that these fine-tuning or zero/one/few shot learning based models don't have to train for long and does not involve significant learning \"from scratch\" or they are somewhat black boxes, that's why there is nothing much to visualize? \n\n\n  [1]: https://cs224d.stanford.edu/reports/OliveiraLuke.pdf\n  [2]: https://arxiv.org/pdf/1909.00252.pdf\n  [3]: https://arxiv.org/pdf/2004.12765v5.pdf", "upvote_ratio": 1.0, "id": "t3_r2rvzw", "created_utc": 1637946368.0}
{"sub": "LanguageTechnology", "title": "(Need opinion) Sentiment Analysis for long text", "selftext": "I was planning  to perform sentiment analysis for news articles but after reading this post it seem that it will not be easy, I have an assessment and I was asked to perform 2 NLP tasks so I chose sentiment analysis and summarisation \nNow I'll perform sentiment for article title (positive or negative) and then summarise the article\n\nhttps://datascience.stackexchange.com/questions/82313/sentiment-analysis-on-long-and-structured-texts\n\n\n\nI have 2 questions:\n\n1- Is there away to perform sentiment to long text?\n\n2- From bussiness perspective is my idea is good (examples xx company don't have time to read every article about their company from different news outlets,  this project will help in analysing articles and determine if they are positive or negative based in the title and the team can choose to read rhe summary of each negative article)", "upvote_ratio": 1.0, "id": "t3_r2qkk6", "created_utc": 1637942560.0}
{"sub": "LanguageTechnology", "title": "Learning how to read papers", "selftext": "Hello everyone! I need some advice on learning how to read academic papers in NLP. I really struggled to understand some of the papers I had to read for my research in undergrad... I could take several days just to get through one paper. At first, I attributed this struggle to my lack of ML/DL experience. This was definitely a factor, but even after I filled some of those gaps in my knowledge, I still have trouble understanding new papers. For example, I can conceptually understand Word2Vec because there are several great video lectures or tutorials that explain it, but I probably wouldn't be able to understand it just from reading the paper by itself.\n\nI'm really interested in academia, but I definitely need to get better at reading papers. I would greatly appreciate any advice or recommendations of papers to start with.", "upvote_ratio": 0.93, "id": "t3_r2imrd", "created_utc": 1637915458.0}
{"sub": "LanguageTechnology", "title": "Dependency graph", "selftext": "Dear all,\n\nCan you suggest me a free, good, and easy tool to obtain dependency graphs (as arcs, lines, and so on between nodes), starting from a CONNLU file?\n\nMany thanks", "upvote_ratio": 1.0, "id": "t3_r2iiji", "created_utc": 1637914970.0}
{"sub": "LanguageTechnology", "title": "Creating numeric word representation of input sentences resulting in MemoryError", "selftext": "I am trying to use [`CountVectorizer`](https://scikit-learn.org/stable/modules/feature_extraction.html#common-vectorizer-usage) to obtain word numerical word representation of data which is essentialy list of 160000 English sentences:\n\n    import pandas as pd\n    import numpy as np\n    from sklearn.feature_extraction.text import CountVectorizer\n    \n    df_train = pd.read_csv('data/train.csv')\n    \n    vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=1)\n    X = vectorizer.fit_transform(list(df_train.text))\n\nThen printing `X`:\n\n    &gt;&gt;&gt; X\n    &lt;160000x693699 sparse matrix of type '&lt;class 'numpy.int64'&gt;'\n    with 3721191 stored elements in Compressed Sparse Row format&gt;\n\nBut converting the whole to array to get the numerical word representation of all data gives:\n\n    &gt;&gt;&gt; X.toarray()\n    ---------------------------------------------------------------------------\n    MemoryError                               Traceback (most recent call last)\n    ~\\AppData\\Local\\Temp/ipykernel_11636/854451212.py in &lt;module&gt;\n    ----&gt; 1 X.toarray()\n    \n    c:\\users\\crrma\\.virtualenvs\\humor-detection-2-8vpiokuk\\lib\\site-packages\\scipy\\sparse\\compressed.py in toarray(self, order, out)\n       1037         if out is None and order is None:\n       1038             order = self._swap('cf')[0]\n    -&gt; 1039         out = self._process_toarray_args(order, out)\n       1040         if not (out.flags.c_contiguous or out.flags.f_contiguous):\n       1041             raise ValueError('Output array must be C or F contiguous')\n    \n    c:\\users\\crrma\\.virtualenvs\\humor-detection-2-8vpiokuk\\lib\\site-packages\\scipy\\sparse\\base.py in _process_toarray_args(self, order, out)\n       1200             return out\n       1201         else:\n    -&gt; 1202             return np.zeros(self.shape, dtype=self.dtype, order=order)\n       1203 \n       1204 \n    \n    MemoryError: Unable to allocate 827. GiB for an array with shape (160000, 693699) and data type int64\n\nFor the example in the linked schikit learn [doc page](https://scikit-learn.org/stable/modules/feature_extraction.html#common-vectorizer-usage), they have used only five sentences. Thus, for them `X.toarray()` seem to have returned the array of numerical word representation. But since my dataset contains 160000 sentences, (in error message) it seems that it is resulting in vocabulary of size 693699 (which contains both unique unigrams and bigrams, due to `ngram_range` parameter passed to `CountVectorizer`) and hence facing insufficient memory issue.\n\n**Q1.** How can I fix this? I am thinking to simply reject `X` and separately transform in mini batches as shown below. Is this correct?\n\n    &gt;&gt;&gt; X_batch = list(df_train[:10].text)  # do this for 160000 / batch_size batches\n    &gt;&gt;&gt; X_batch_encoding = vectorizer.transform(X_batch).toarray()\n    &gt;&gt;&gt; X_batch_encoding\n    array([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)\n    \n    &gt;&gt;&gt; X_batch_encoding[0].shape\n    (693699,)\n\n**Q2.** I am thinking to train neural network and decision tree on this encoding for humor detection. But I guess it wont be great idea to have 693699 length vector to represent single sentence. Right? If yes, what should I do instead? Should I opt to use only unigrams while fitting `CountVectorizer` (but it will not capture even minimal context of words, unlike bigrams) ?\n\nPS: I am creating baseline for humor detection, I am required to use `CountVectorizer`.", "upvote_ratio": 1.0, "id": "t3_r24ytp", "created_utc": 1637870179.0}
{"sub": "LanguageTechnology", "title": "Is there a corpus of English words + their language of origin?", "selftext": "Specifically, I want to be able to determine if a word is of Native origin... I've been searching around but I just can't believe there's no corpus for words plus their etymologies... apologies if dumb question", "upvote_ratio": 1.0, "id": "t3_r2483t", "created_utc": 1637868158.0}
{"sub": "LanguageTechnology", "title": "Text summarization", "selftext": "Hello,\nIs there any real world text summarization project example ?\nTy", "upvote_ratio": 0.67, "id": "t3_r22kqe", "created_utc": 1637863668.0}
{"sub": "LanguageTechnology", "title": "Any Historical Newspaper Headline Datasets? Like from WW2 to present?", "selftext": "I'm working on a social science project where we want to grab newspaper headlines from one or more mainstream (preferably US but UK is fine) media outlets. \n\nSo far I haven't been able to find any. Can anyone point to one that might be available? It does have to be free.", "upvote_ratio": 1.0, "id": "t3_r220on", "created_utc": 1637862202.0}
{"sub": "LanguageTechnology", "title": "No DOI for Published Paper", "selftext": "I recently published a paper to Findings in EMNLP: [https://aclanthology.org/2021.findings-emnlp.143/](https://aclanthology.org/2021.findings-emnlp.143/)\n\nI am trying to update my Arxiv submission, but I don't see a DOI here. Where can I find this information? Thanks!", "upvote_ratio": 0.67, "id": "t3_r21jqo", "created_utc": 1637860952.0}
{"sub": "LanguageTechnology", "title": "How important it is to give sentences to BERT tokenizer rather than the whole text?", "selftext": "I'm currently working on document classification, where every document has many sentences within. I intend to use BERT sequence classifier for the task, however as I check out the tokenization results of BERT, I saw that the special token \\[SEP\\] is only added at the end of the document, rather than replacing every period in the text - as they are my end of sentence marks. However, I saw that Bert gave \".\" punctuations a specific ID, which means it has some meaning to BERT already.\n\nMy question is, should I go ahead and have only \\[SEP\\] at the end of the document and hope that the ID corresponding to the punctuation marks can distinguish the sentence-level information, or should I re-do my tokenization while I give the texts sentence by sentence, and then merge the id's into a single vector later? There must be a better way though. I believe knowing where a sentence begins and ends is important for the classification task, so I'm open to suggestions.", "upvote_ratio": 0.92, "id": "t3_r1yw1r", "created_utc": 1637853765.0}
{"sub": "LanguageTechnology", "title": "Companion Texts to Jurafsky and Martin's Speech and Language Processing?", "selftext": "The text is really dense and it's a bit hard to understand at times. Anyone know good companion texts that explain the content more? Maybe with some python examples?", "upvote_ratio": 0.92, "id": "t3_r1f71s", "created_utc": 1637787906.0}
{"sub": "LanguageTechnology", "title": "Tutorial to build Deep Learning Punctuation Corrector in Python", "selftext": "nan", "upvote_ratio": 0.78, "id": "t3_r1cp3z", "created_utc": 1637781046.0}
{"sub": "LanguageTechnology", "title": "Neural edit-tree lemmatization for spaCy", "selftext": "nan", "upvote_ratio": 0.81, "id": "t3_r1c536", "created_utc": 1637779600.0}
{"sub": "LanguageTechnology", "title": "No data no problem, unsupervised learning and sentence transformers", "selftext": "Hi all, I put together [an article and video covering TSDAE fine-tuning](https://www.pinecone.io/learn/unsupervised-training-sentence-transformers/) for sentence transformer models. Basically, how we can use plain unstructured text data to fine-tune a sentence transformer (not quite *no* data, but close!). From the TSDAE paper, you actually only need something like 10-100K sentences to fine-tune a pretrained transformer for producing pretty good sentence embeddings.\n\nI was achieving same STSb evaluation with a TSDAE train BERT as I was getting with my own NLI (labeled) dataset trained BERT (using softmax loss). So pretty cool imo - although in reality supervised methods produce better performing models, if you have no labeled data, unsupervised is the way to go.\n\nIt was really cool learning about this, planning to do more on unsupervised sentence transformers in the future - let me know what you think!", "upvote_ratio": 0.57, "id": "t3_r195uq", "created_utc": 1637771670.0}
{"sub": "LanguageTechnology", "title": "Evaluating quality of synthetically generated questions dataset", "selftext": "Hi  all, I have an NLP related question for you! I have synthetically  generated questions in a SQuAD like format (context, question, answer triplets). The data consists of domain specific questions in Dutch as the questions are generated from 1500+ Dutch technical manuals.\n\nHow can I evaluate the quality of this dataset and therefore the quality of my question generator?\n\nMany, many thanks in advance!", "upvote_ratio": 0.81, "id": "t3_r1166j", "created_utc": 1637745604.0}
{"sub": "LanguageTechnology", "title": "Best Way to Identify if a Social Post is Written by a Doctor vs Patient?", "selftext": "Hello fellow NLP nerds :) \n\nI had a tricky question that I'd love to crowdsource some solutions for.\n\nProblem: I'm trying to clean out all the social posts written by doctors vs patients. I've already started to separate based on typical identifiers such as \"as a patient\" vs \"being a doctor\" or \"my patient\" vs \"my doctor\" and \"patient here\" vs \"I treated patient\". The issue is that this process of coming up with ways a patient self identifies compared to a doctor is extremely manual on the upfront. I wanted to check if the community knew of any libraries, previous code or other research that could help speed things up? \n\nAny and all ideas, thoughts and suggestions are more than welcomed. \n\nAlways all the best, \n\nNE", "upvote_ratio": 0.84, "id": "t3_r0qv0d", "created_utc": 1637711575.0}
{"sub": "LanguageTechnology", "title": "Summaries readability improvement", "selftext": "I'm doing my research with multi-document summarization for domain-specific texts. We want to show summaries that we generate using our approach (and state-of-the-arts) to domain experts for readability evaluation. Summaries that we generate are pretty good, but hard to read for real people.\n\nCould you recommend some python libraries for automatic improvement of readability (capitalization, punctuation, finding orthographic mistakes, etc.)", "upvote_ratio": 1.0, "id": "t3_r0j4ro", "created_utc": 1637690572.0}
{"sub": "LanguageTechnology", "title": "NLP thesis ideas?", "selftext": "I am currently doing a postgraduate Computer Science conversion course in the UK and did English Language and Linguistics for my undergrad. I know that I want to combine both fields for my postgrad thesis but I don\u2019t know anything about NLP. I know that there are tons of material out there but I don\u2019t know where to begin. Posting here in the hopes that someone could guide me to some places for NLP or even just give me some ideas for possible avenues to follow for my thesis. Any suggestions would be greatly appreciated", "upvote_ratio": 0.93, "id": "t3_r0djf0", "created_utc": 1637675035.0}
{"sub": "LanguageTechnology", "title": "Classifying documents in categories using keyword sets, without ML", "selftext": "Hi\n\nI am trying to classify documents in categories for which I have lists of keywords.  \nIdeally the solution should not use machine learning.\n\nI was thinking of creating vectors of both the document and the keywords for each category, and consequently calculating cosine similarity in order to see which category has the highest match.\n\nHowever, as cosine similarity is aimed at comparing 2 documents rather than 1 document and a list of keywords, I was wondering if this was the ideal solution.\n\nAny feedback on either oh this would be highly appreciated. Whether it is optimising the cosine sim, a different approach or proposing ML anyway,... all feedback is welcomed :).\n\nThanks in advance.", "upvote_ratio": 1.0, "id": "t3_r0d1gd", "created_utc": 1637673454.0}
{"sub": "LanguageTechnology", "title": "Speech Emotion Classifcation", "selftext": "We all know that in ASR problem (Audio Speech Recognotion) is all about extracting features so called spectograms and waveforms and making a prediction based on it. So the text or the actual meaning of the text is not the main thing. So the question is, if i have a have AUDIO SPEECH RECOGNITION model that is trained lets say in english, can I use it to predict things on lets on Russian? Would it show similar accuracy on Russian as on predicting english sentences?", "upvote_ratio": 0.81, "id": "t3_r05vxl", "created_utc": 1637645269.0}
{"sub": "LanguageTechnology", "title": "Identifying sections from corpus of documents.", "selftext": "Hello everyone,\n\nRecently I have been looking for ways to identify sections in pdf documents where sections are not separated from one another. I was wondering if anyone has any good paper suggestions regarding this. I already read this paper http://ceur-ws.org/Vol-710/paper23.pdf but just wondering if anyone has additional suggestions for papers that tackle this problem. \n\n\nThanks in advance for all your suggestions.", "upvote_ratio": 1.0, "id": "t3_r01mwc", "created_utc": 1637631482.0}
{"sub": "LanguageTechnology", "title": "Which weights do we use to get embedding matrix for CBOW?", "selftext": "I originally asked this question at StackOverflow. However I couldn't get any answers there as usual. [https://datascience.stackexchange.com/questions/104332/how-to-get-word-embedding-in-cbow](https://datascience.stackexchange.com/questions/104332/how-to-get-word-embedding-in-cbow)\n\n&amp;#x200B;\n\nSo the problem is for skip-gram we take the weights of the input to multiply and get the embedding matrix as a result. However in case of cbow we take the weights of the input, but there are multiple inputs! Which one do we take? I couldn't find any answers about this. Can someone explain? (For diagrams refer to the link pls)", "upvote_ratio": 1.0, "id": "t3_qzpgkc", "created_utc": 1637598918.0}
{"sub": "LanguageTechnology", "title": "Meta/Facebook AI Releases XLS-R: A Self-Supervised Multilingual Model Trained On 128 Languages For A Variety Of Speech Tasks", "selftext": "Talking to one another is a natural way for people to engage. With advancing speech technology, people are now interacting with devices in day to day lives.\n\nDespite this, speech technology is only available for a small percentage of the world\u2019s languages. Few-shot learning and even unsupervised speech recognition can be helpful, but the effectiveness of these methods is dependent on the quality of the self-supervised model.\n\nA recent Facebook study presents [XLS-R](https://arxiv.org/pdf/2111.09296.pdf), a new self-supervised model for a range of speech tasks. By training on approximately ten times more public data in more than twice as many languages, XLS-R significantly outperforms previous multilingual models.\n\nQuick Read: [https://www.marktechpost.com/2021/11/22/meta-facebook-ai-releases-xls-r-a-new-self-supervised-model-for-a-variety-of-speech-tasks/](https://www.marktechpost.com/2021/11/22/meta-facebook-ai-releases-xls-r-a-new-self-supervised-model-for-a-variety-of-speech-tasks/) \n\nPaper: https://arxiv.org/abs/2111.09296?\n\nGithub: [https://github.com/pytorch/fairseq/tree/main/examples/wav2vec/xlsr](https://github.com/pytorch/fairseq/tree/main/examples/wav2vec/xlsr)\n\nFacebook Blog: [https://ai.facebook.com/blog/xls-r-self-supervised-speech-processing-for-128-languages](https://ai.facebook.com/blog/xls-r-self-supervised-speech-processing-for-128-languages)", "upvote_ratio": 1.0, "id": "t3_qzpdhc", "created_utc": 1637598689.0}
{"sub": "LanguageTechnology", "title": "Replay webnlg challenge 2017 using T-rex dataset", "selftext": "Hi everybody, i'm data science student and i'm very newbie in NLP task. i'm learning how to use openNMT for my master degree thesis. I completed the webnlg challenge 2017 tutorial ([https://webnlg-challenge.loria.fr/challenge\\_2017/](https://webnlg-challenge.loria.fr/challenge_2017/)), now i would to apply this tutorial to T-rex sample dataset ([https://hadyelsahar.github.io/t-rex/downloads/](https://hadyelsahar.github.io/t-rex/downloads/)).\n\n&amp;#x200B;\n\nMy question is: how can i prepare the T-rex sample dataset like webnlg challenge 2017 dataset ([https://gitlab.com/shimorina/webnlg-dataset/-/tree/master/webnlg\\_challenge\\_2017](https://gitlab.com/shimorina/webnlg-dataset/-/tree/master/webnlg_challenge_2017)). \n\n&amp;#x200B;\n\nThanks all.", "upvote_ratio": 0.84, "id": "t3_qzp0x0", "created_utc": 1637597745.0}
{"sub": "LanguageTechnology", "title": "Does anyone know of a list of NLP/SpeechTech non-profits?", "selftext": "How's my favourite community?\n\nI'm currently doing my Master's at the University of Groningen's new Voice Technology programme. So far the only groups who have reached out offering thesis project proposals have been your typical, big data tech companies, or companies with government(/policing)-adjacent projects.\n\nI'm looking to solicit some non-profits, NGOs, and maybe the healthcare sector for thesis project proposals. Does anyone know of a resource where I could get a list of these sorts of organizations?\n\nMany thanks!", "upvote_ratio": 0.75, "id": "t3_qzow1b", "created_utc": 1637597373.0}
{"sub": "LanguageTechnology", "title": "Add-K smoothing", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_qznetk", "created_utc": 1637593461.0}
{"sub": "LanguageTechnology", "title": "Mentoring SemEval2022 projects", "selftext": "Hey everyone, I have seen some people here working on/interested in the SemEval projects. I was wondering if some folks would like me to mentor their projects. Having published at venues like ACL, EACL, and SemEval2021, I believe I have some knowledge about academic paper writing and NLP literature in general. Also, I believe it would allow everyone to network as well, which is always a great thing. :)", "upvote_ratio": 0.81, "id": "t3_qzma02", "created_utc": 1637590307.0}
{"sub": "LanguageTechnology", "title": "[project-showcase] - zeroshot_topics: Label your text data automatically!", "selftext": "zeroshot\\_topics: [Github link](https://github.com/AnjanaRita/zeroshot_topics)\n\nHand-labelled training sets are expensive and time-consuming to create usually. Some datasets call for domain expertise (eg: medical/finance datasets etc). Given these factors around costs and inflexibility of hand-labelling, it would be nice if there are tools that can help us get started quickly with a minimal labelled dataset - enter weak supervision.\n\n**But what if you do not have any labelled data at all? is there a way to still label your data automatically in some way?** That's where **zeroshot\\_topics** might be useful! to help you to be up and running quickly.\n\n*zeroshot\\_topics* lets you do exactly that! it leverages the power of zero-shot-classifiers, transformers &amp;  knowledge graphs to automatically suggest labels/topics from your text data. all you need to do is point it towards your data.\n\n&amp;#x200B;\n\nplease check this out and share your feedback.", "upvote_ratio": 0.94, "id": "t3_qzidz5", "created_utc": 1637576422.0}
{"sub": "LanguageTechnology", "title": "What next now?", "selftext": "Hey guys, I just wanted to get to know about the future career possibilities in the field of NLP. I am from India, so a brief background about me:\n\n1. Graduated from STEM field (Mechanical Engineering to be exact) in 2021\n2. I have a brief amount of research experience: published 3 papers as a first author at EACL, ACL, SemEval, and 2 more papers currently submitted in ACL. \n3. I have worked in UCLA NLP lab and currently working as a machine learning intern in a startup. \n\nI am applying for Fall 2022 PhD this year but I am mostly applying to ambitious places. In case I don't get an admit that I am might not like, I will apply for Fall 2023 admissions. Now my questions are about what can I do for the next year that might benefit my profile:\n\n1. Research based positions are already scarce in the industry and generally go for graduate students. Does my research experience compensate for that when apply to such places? \n\n2. I have heard of programs like google pre doctoral, Microsoft Research fellow and Allen young investigator. But these are all super competitive I think. What experience would make me be a competitive candidate at such places? Are there any other programmes like that where I might stand a chance with my current profile? \n\n3. How do I hunt for research opportunities in the industry at my current level?\n\n4.If not research opportunities, which type of roles should I prefer? (Like data scientist, data engineer, software engineer etc.)", "upvote_ratio": 0.9, "id": "t3_qzcg5a", "created_utc": 1637553560.0}
{"sub": "LanguageTechnology", "title": "Looking for examples of conversational chatbot companies with recorded demos.", "selftext": "I'm doing some research into currently successful chatbot companies (both voice and text) and am looking for chatbot companies that have their chatbots in recorded demos working fully.\n\nOne I found is [Brooke.ai](https://Brooke.ai) which works in the car dealership appointment setting industry as a customer service bot for inbound calls and their demo can be found [here](https://www.brooke.ai/?utm_source=google&amp;utm_medium=cpc&amp;utm_campaign=chatbot&amp;creative=538797336365&amp;keyword=chatbot%20service%20providers&amp;matchtype=b&amp;network=g&amp;device=c&amp;gclid=Cj0KCQiA-eeMBhCpARIsAAZfxZA5yf_BC4bJ46ILeeVlQXpXjdSLVm5fg-kAej4oHTUCJMg27vRI8KoaAn5nEALw_wcB#) and also on [youtube](https://www.youtube.com/watch?v=OARPZvWlUnk).\n\nI tried looking for other companies that have demos as well but couldn't find many that have pre-recorded demos of their ai chatbot product working fully, or are mostly sequence-based chatbot building software such as ManyChat or ChatFuel.\n\nCould you guys recommend chatbot companies that have ai with intent recognition software like Google Duplex or Amazon Lex? If they have demo recordings I can listen to or view that would be great. Looking for companies that have been successful in implementing AI in chatbots.", "upvote_ratio": 1.0, "id": "t3_qza06w", "created_utc": 1637545544.0}
{"sub": "LanguageTechnology", "title": "Lojban, constructed languages and NLP", "selftext": "Lojban is a constructed language that aims at clarity. As a language it is less syntactically ambiguous, contains no homophones and has many other features intended to reduce both semantic and grammatical ambiguity.\n\nThe big problem with trying to train an NLP on Lojban is, of course, is corpus size and scale. Although many side by side translations texts into Lojban exist, they have nothing like the scope that would be necessary to teach a neural net a language.\n\nI think it's entirely possible that, if we did have a large enough corpus, a computer trained on Lojban might be able to achieve things a standard machine learning setup can't. Still we run into that fundamental barrier, corpus size.\n\nI can't help but think though that there is *something here*\\- an opportunity for a skilled research team in this area, if only they could locate it. Perhaps some intermediate case, like Esperanto, might be more possible?", "upvote_ratio": 1.0, "id": "t3_qz6niv", "created_utc": 1637535205.0}
{"sub": "LanguageTechnology", "title": "[Advice] What are some ways to engage with Academia without a Phd?", "selftext": "Hi - a big fan of this subreddit!\n\nI am an applied NLP researcher in the industry with a masters. I have a PhD offer but I am in double minds about doing a PhD - because of publishing culture/duration of a phd in the US etc. However, I very much enjoy keeping up with the recently published work and seeing how they can be tweaked and applied to real-world scenarios. \n\nWhat are some ways in which I can continue engaging with the academic community without doing a phd? I some how feel that one is not valued as much without a phd (even with equivalent industry experience) so wanted to get opinion from either sides. \n\nThanks!", "upvote_ratio": 1.0, "id": "t3_qz3zw0", "created_utc": 1637527504.0}
{"sub": "LanguageTechnology", "title": "Resource list for NLP beginners from a Meta AI ML researcher", "selftext": "During my morning scroll of tech twitter, I came across this [round-up of resources for anyone new to NLP](https://elvissaravia.substack.com/p/my-recommendations-for-getting-started), written and shared by an ML research at Meta AI. I thought this community might be interested since I've seen quite a few posts by people looking for advice on where to start.\n\nI haven't personally used all of the books and other resources that he recommends, but the ones on his list that I have used -- the Bender book, the Jurafsky book, and the Manning lectures -- were all excellent. Moreover, I strongly agree with the approach of \"studying the fundamentals\", including linguistics concepts, before jumping straight to the ML.\n\nAnyway, I hope it's useful to someone! \"Elvis\" (the author) is also very worth following on Twitter. Maybe all of you already do this, but I learn so much from following NLP academics and researchers/engineers in  industry.", "upvote_ratio": 0.88, "id": "t3_qyyb0m", "created_utc": 1637511479.0}
{"sub": "LanguageTechnology", "title": "GUI app for text processing?", "selftext": "I\u2019m picturing a desktop application where you can highlight some text and say a command like, \u201ctokenize these words\u201d, and the list containing various text elements makes each of the highlighted words a unique element of the list. Or you can highlight a region and say \u201csegment this text\u201d, and it does the same but for sentences.\n\nIs there any way to do this?", "upvote_ratio": 0.66, "id": "t3_qywnxp", "created_utc": 1637506728.0}
{"sub": "LanguageTechnology", "title": "Better segmentation than NLTK.sent_tokenize()", "selftext": "I am segmenting text in Juno, a Jupyter notebook iOS app. \n\nThey don\u2019t support Spacy at the moment.\n\nNLTK.sent_tokenize does not segment sentences perfectly, for me.\n\nI am thinking my only choice is to write a custom segmentation rule, unless anybody knows a different library with a high quality, AI-intelligent segmenter that can comprehend where the boundaries of sentences are, even if the text is not perfectly formatted.\n\nThanks!", "upvote_ratio": 0.83, "id": "t3_qyw34c", "created_utc": 1637504877.0}
{"sub": "LanguageTechnology", "title": "[P] Pyconverse - Conversational Text transcript analysis library", "selftext": "Github project link: [pyconverse](https://github.com/AnjanaRita/converse)\n\nConversation analytics plays an increasingly important role in shaping great customer experiences across various industries like finance/contact centres etc.. primarily to gain a deeper understanding of the customers and to better serve their needs. This library, *PyConverse* is an attempt to provide tools &amp; methods which can be used to gain an understanding of the conversations from multiple perspectives using various NLP techniques.\n\nI have been doing what can be called conversational text NLP with primarily contact centre data from various domains like  Financial services, Banking, Insurance etc for the past year or so, and I  have not come across any interesting open-source tools that can help in understanding conversational texts as such I decided to create this library that can provide various tools and methods to analyse calls and help answer important questions/compute important metrics that usually people want to find from conversations, in contact centre data analysis settings.\n\n&amp;#x200B;\n\nThings that can be done with this library:\n\n1. Emotion identification\n2. Empathetic statement identification\n3. Call Segmentation\n4. Topic identification from call segments\n5. Compute various types of Speaker attributes: (word counts/number of words per utterance/negations etc., Identify periods of silence &amp; interruptions, Question identification,  Backchannel identification, Assess the overall nature of the speaker via linguistic attributes and tell if the Speaker is: Talkative, verbally fluent, Informal/Personal/social, Goal-oriented or Forward/future-looking/focused on past, Identify inhibition.)\n\nPlease give it a try and share your feedback.", "upvote_ratio": 1.0, "id": "t3_qyt2p8", "created_utc": 1637493972.0}
{"sub": "LanguageTechnology", "title": "How to extract prepositions from parallel texts?", "selftext": "Hello.\n\nI have parallel texts in English-German and English French.\n\n    EN.txt = \"The hat is on the table.\\n The picture is on the wall.\\n The bottle is under the sink.\"\n    DE.txt = \"Der Hut liegt auf dem Tisch.\\n Das Bild h\u00e4ngt an der Wand.\\n Die Flasche ist unter dem Waschbecken.\"\n    FR.txt = \"Le chapeau est sur la table.\\n La photo est sur le mur.\\n La bouteille est sous l'\u00e9vier.\"\n\nI would like to extract the prepositions from the EN-DE, EN-FR sentence pairs and create some kind of frequency counter in a dictionary of the pairs. Something like this, I guess:\n\n    EN_DE = {\"on\":{\"auf\":1, \"an\":1}, \"under\":{\"unter\":1}}    \n\nEventually, I'd like to create an alignment matrix or a heatmap with the frequencies.\n\n&amp;#x200B;\n\nSome questions:\n\n1. Is this feasible?\n2. How should I go about doing this? Algorithmically, I think I need to tokenise the sentences, POS tag them using Stanza, **figure out what prepositions in English Sentence X align with what prepositions in German Sentence X**, and then update the counter dictionary.\n3. The bold part is what I am particularly having trouble with. Any idea how I can best do that?\n4. I am thinking of doing this in a pandas DataFrame. Is that a good idea?\n5. Any other approaches to this problem?\n\nPlease, any advice or suggestions would be much appreciated.\n\nI am very much a beginner programmer. I'm a linguist trying to use computational tools to analyse my data, but I feel like I might be out of my league.\n\nThank you in advance for your suggestions.", "upvote_ratio": 0.9, "id": "t3_qysddr", "created_utc": 1637490937.0}
{"sub": "LanguageTechnology", "title": "Attributing dialogue to specific characters", "selftext": "Hi all, \n\nI'm currently working on a side project trying to analyse the sentiment of book characters in The Stormlight Archive, and as part of it need to determine who's saying what in a given dialogue. \n\nE.g. in the following\n\n&gt;\u201cI heard the guards talking,\u201d the slave continued, shuffling a little closer. He had a twitch that made him blink too frequently. \u201cYou've tried to escape before, they said. You have escaped before.\u201d  \n&gt;  \n&gt;Kaladin made no reply.  \n&gt;  \n&gt;\u201cLook,\u201d the slave said, moving his hand out from behind his rags and revealing his bowl of slop. It was half full. \u201cTake me with you next time,\u201d he whispered. \u201cI'll give you this. Half my food from now until we get away. Please.\u201d As he spoke, he attracted a few hungerspren. They looked like brown flies that flitted around the man's head, almost too small to see.  \n&gt;  \n&gt;Kaladin turned away, looking out at the endless hills and their shifting, moving grasses. He rested one arm across the bars and placed his head against it, legs still hanging out.  \n&gt;  \n&gt;\u201cWell?\u201d the slave asked.  \n&gt;  \n&gt;\u201cYou're an idiot. If you gave me half your food, you'd be too weak to escape if I were to flee. Which I won't. It doesn't work.\u201d  \n&gt;  \n&gt;\u201cBut\u2014\u201d  \n&gt;  \n&gt;\u201cTen times,\u201d Kaladin whispered. \u201cTen escape attempts in eight months, fleeing from five different masters. And how many of them worked?\u201d\n\nI'd want to get an output something like \n\n    {\n        'Kaladin': [\n            \u201cYou're an idiot. If you gave me half your food, you'd be too weak to escape if I were to flee. Which I won't. It doesn't work.\u201d, \n            \"Ten times\", \n            \u201cTen escape attempts in eight months, fleeing from five different masters. And how many of them worked?\u201d\n        ],\n    \n        \"Slave\": [\n            \u201cLook,\u201d,\n            \u201cTake me with you next time,\u201d,\n            [...]\n        ]\n    }\n\nAssuming that identifying the characters themselves isn't an issue, and I have a ton of training data (it's a long book), what are some good methods to do this? \n\nThe literature seems really sparse, except for this one paper [http://www.cs.columbia.edu/\\~delson/pubs/AAAI10-ElsonMcKeown.pdf](http://www.cs.columbia.edu/~delson/pubs/AAAI10-ElsonMcKeown.pdf) which doesn't seem very easily transferable and is 11 years old.\n\nMy current thought is that I can treat it as a classification task:\n\n&gt;label = attribution\\_pipeline(quote=''' \"You're an idiot. If you gave me half your food, you'd be too weak to escape if I were to flee. Which I won't. It doesn't work.\", context=\\[full paragraph\\])\n\nbut this seems a bit... overly-general as a solution, and I'm not sure if I'm just missing papers that discuss this. \n\nAny thoughts much appreciated!", "upvote_ratio": 1.0, "id": "t3_qyi237", "created_utc": 1637452220.0}
{"sub": "LanguageTechnology", "title": "Auto-Translator for Preserving a Semitic Language", "selftext": "Long story short, there's a dying Semitic Language with native speakers still alive, Assyrian Neo-Aramaic, and I'm looking to increase the amount of data out there so I could hopefully train an Assyrian-English translation model.\n\nContext: Assyrian is a modern dialect of Aramaic. There is virtually no data out there I could process into translated sentence pairs to train any sort of deep learning model. Since I have access to native speakers (my family and friends), I want to develop a software that selects/generates English sentences then has volunteers provide a translation.\n\n&amp;#x200B;\n\nFEW QUESTIONS ABOUT THIS!\n\n1. The language is written in it's own script [https://en.wikipedia.org/wiki/Syriac\\_alphabet](https://en.wikipedia.org/wiki/Syriac_alphabet). Writing in the Syriac script is FAR from standardized as there are sooo many dialects and there's no standard system of spelling. Also, I'm not sure how well autoML stuff works on non-Latin characters ([https://cloud.google.com/translate/automl/docs/prepare](https://cloud.google.com/translate/automl/docs/prepare)). Should I ask volunteers to give translations in an English phonetic spelling?\n2. How much sentences would I need to train an effective translation model? Let's say I have a team of 10 native speakers who devote 30 minutes a day for translating sentences, would this produce enough training data even? And given that there is no standard spelling, translations are going to be super noisy, as in the same words in Assyrian are going to be transliterated in many different ways.\n3. How should I pick which English sentences to ask speakers to translate? Should this be randomly generated? Should they be randomly selected from English books? Would it be more useful to have translations of collections of sentences within a same context rather than stand-alone sentences?\n\nThank you so much, this project means a lot.", "upvote_ratio": 1.0, "id": "t3_qyfyez", "created_utc": 1637445808.0}
{"sub": "LanguageTechnology", "title": "Which method/model to opt for while identifying semantic similarity?", "selftext": "I have a text classification dataset of registered issues. Now within each category of issues there are specific issues that show similar pattern. How can I identify those sub categories within the categories. I dont have any means to manually categories each sub category (rather its impractical). All I understand is that this problem falls under unsupervised learning.\n\nI have already performed the text classification using BERT and it works well enough.", "upvote_ratio": 1.0, "id": "t3_qyba90", "created_utc": 1637432041.0}
{"sub": "LanguageTechnology", "title": "WEBNLG challenge 2017 on Google Colab error", "selftext": "Hi guys, i'm data science student and i'm newbie in NLP field. For my master degree thesis, i need to learn the basic of NLP problem so i'm trying to follow the webnlg challenge 2017 tutorial ([https://webnlg-challenge.loria.fr/challenge\\_2017/](https://webnlg-challenge.loria.fr/challenge_2017/)).However, i am not familiar with torch and unix and i can't understand how i can run this line of code:\n\n    th preprocess.lua \\\n    -train_src &lt;data-directory&gt;/train-webnlg-all-delex.triple \\\n    -train_tgt &lt;data-directory&gt;/train-webnlg-all-delex.lex \\\n    -valid_src &lt;data-directory&gt;/dev-webnlg-all-delex.triple \\\n    -valid_tgt &lt;data-directory&gt;/dev-webnlg-all-delex.lex \\\n    -src_seq_length 70 \\\n    -tgt_seq_length 70 \\\n    -save_data baseline\n\nRight here i put my Google Colab Notebook with all my steps:\n\n[https://github.com/dariodellamura/WebNLG-Challenge-2017-test/blob/main/nlg\\_pipeline.ipynb](https://github.com/dariodellamura/WebNLG-Challenge-2017-test/blob/main/nlg_pipeline.ipynb).\n\ni get this error:\n\n    bash: cannot set terminal process group (72): Inappropriate ioctl for device\n    bash: no job control in this shell\n    th 70 \\ -tgt_seq_length 70 \\ -save_data baseline\n    /content/torch/install/bin/luajit: cannot open preprocess.lua: No such file or directory\n    stack traceback:\n    \t[C]: in function 'dofile'\n    \t...tent/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:150: in main chunk\n    \t[C]: at 0x55f8357a6570\n\nHow can i solve this? Thanks all.", "upvote_ratio": 0.67, "id": "t3_qy3alk", "created_utc": 1637405859.0}
{"sub": "LanguageTechnology", "title": "Job offer advice for new grad interested in NLP", "selftext": "Hi everyone.\n\nI am a new grad trying to pick my first software engineering job.\u00a0As an undergrad, I had a strong interest in NLP and did research in that area, and I would like to continue working in NLP.\u00a0I am deciding between two job offers.\n\nAt the first company, I would be working in NLU/NLG teams for the company's voice assistant technologies. They also publish often, which sounds nice to me since I might consider grad school in the long term. However, they are offering me a systems role, so I will\u00a0mostly be working on ML infrastructure (C++) without manipulating their models or doing core ML engineering.\n\nAt the second company, I am hired as an ML engineer, but I will be working on ranking. The tech stack is mostly Python. The downside is that I won\u2019t be doing any NLP work.\n\nIf I want to have a career in NLP, would accepting the first company be better, even if I am not working directly on the models as an ML engineer? Or would it be most important to have the title of \"ML engineer\" even if I am working on a different problem area?", "upvote_ratio": 1.0, "id": "t3_qxwv30", "created_utc": 1637379354.0}
{"sub": "LanguageTechnology", "title": "Make a bot based on social media chat data", "selftext": "I have very long chats with my friends like one with 4 years of constant msging and then two years of medium level messaging. I also have some groups in which i have heavily participated for the last 6 years. Can i make a chatbot type thing which chats like me? Is there anyone who has already worked on it?", "upvote_ratio": 0.72, "id": "t3_qxu7y2", "created_utc": 1637370523.0}
{"sub": "LanguageTechnology", "title": "I want to spy on myself. My digital and physical stuff. I want to index and map the clutter comprehensively. Topic collection bags? In NLP what do you call this? Anyone done this before? There should be some generic recipe in some Python cookbook out there.", "selftext": "Rather than keep rummaging through the clutter I create while working, I want to look for labels. So let's say I have a detailed inventory of my shit, and whatever is on my file system, online accounts, email, etc.\n\n1. What NLP recipes should I follow to build metadata and generate labels for me? **What should I learn?**\n2. How do you visualize all of this? **What should I learn?**", "upvote_ratio": 0.78, "id": "t3_qxjkma", "created_utc": 1637338569.0}
{"sub": "LanguageTechnology", "title": "SemEval-2022 Task 09: R2VQ - Competence-based Multimodal Question Answering", "selftext": "FIRST CALL FOR PARTICIPATION\n\nWe invite you to participate in the SemEval-2022 Task 9: Competence-based Multimodal Question Answering (R2VQ).\n\nThe task is being held as part of SemEval-2022, and all participating team will be able to publish their system description paper in the proceedings published by ACL.\n\nCodalab (Data download): [https://competitions.codalab.org/competitions/34056](https://competitions.codalab.org/competitions/34056)\n\n&amp;#x200B;\n\nMotivation \n\n================================================\n\nWhen we apply our existing knowledge to new situations, we demonstrate a kind\n\nof understanding of how the knowledge (through tasks) is applied. When viewed\n\nover a conceptual domain, this constitutes a competence. Competence-based\n\nevaluations  can be  seen as a new approach for designing NLP challenges, in\n\norder to better characterize the underlying operational knowledge that a\n\nsystem has for a  conceptual domain, rather than focusing on individual tasks.\n\nIn this shared task, we present a challenge that is reflective of linguistic\n\nand cognitive competencies that humans have when speaking and reasoning.\n\n&amp;#x200B;\n\nTask Overview \n\n================================================\n\nGiven the intuition that textual and visual information mutually inform each\n\nother for semantic reasoning, we formulate the  challenge as a competence-\n\nbased question answering (QA) task, designed to involve rich semantic\n\nannotation and aligned text-video objects. The task is structured as question\n\nanswering pairs, querying how well a system understands the semantics of\n\nrecipes.\n\nWe adopt the concept of \"question families\" as outlined in the CLEVR dataset\n\n(Johnson et al., 2017). While some question families naturally transfer over\n\nfrom the VQA domain (e.g., integer comparison, counting), other concepts such\n\nas ellipsis and object lifespan must be employed to cover the full extent of\n\ncompetency within procedural texts. \n\n&amp;#x200B;\n\nData Content\n\n================================================ \n\nWe have built the R2VQ (Recipe Reading and Video Question Answering) dataset, a dataset consisting of a collection of recipes sourced from [https://recipes.fandom.com/wiki/Recipes\\_Wiki](https://recipes.fandom.com/wiki/Recipes_Wiki) and [foodista.com](https://foodista.com), and labeled according to three distinct annotation layers: (i) Cooking Role Labeling (CRL), (ii) Semantic Role Labeling (SRL), and (iii) aligned image frames taken from creative commons cooking videos downloaded from YouTube. It consists of 1,000 recipes, with 800 to be used as training, and 100 recipes each for validation and testing. Participating systems will be exposed to the aforementioned multimodal training set, and will be asked to provide answers to unseen queries exploiting (i) visual and textual information jointly, or (ii) textual information only. \n\n&amp;#x200B;\n\nTask Website and Codalab Submission site: [https://competitions.codalab.org/competitions/34056](https://competitions.codalab.org/competitions/34056)\n\nMailing List: [semeval-2022-task9@googlegroups.com](mailto:semeval-2022-task9@googlegroups.com)\n\n&amp;#x200B;\n\nImportant Dates\n\n================================================ \n\nTraining data available: October 15, 2021\n\nValidation data available: December 3, 2021\n\nEvaluation data ready: December 3, 2021\n\nEvaluation start: January 10, 2021\n\nEvaluation end: January 31, 2022\n\nSystem Description Paper submissions due: February 23, 2022\n\nNotification to authors: March 31, 2022\n\n&amp;#x200B;\n\nOrganization \n\n================================================ \n\nJames Pustejovsky, Brandeis University, jamesp@brandeis.edu\n\nJingxuan Tu, Brandeis University, jxtu@brandeis.edu\n\nMarco Maru,\u00a0Sapienza University of Rome, maru@di.uniroma1.it\n\nSimone Conia,\u00a0Sapienza University of Rome, conia@di.uniroma1.it\n\nRoberto Navigli,\u00a0Sapienza University of Rome, navigli@diag.uniroma1.it\n\nKyeongmin Rim, Brandeis University, [krim@brandeis.edu](mailto:krim@brandeis.edu)\n\nKelley Lynch, Brandeis University, kmlynch@brandeis.edu\n\nRichard Brutti, Brandeis University,\u00a0richardbrutti@brandeis.edu\n\nEben Holderness, Brandeis University, [egh@brandeis.edu](mailto:egh@brandeis.edu)", "upvote_ratio": 1.0, "id": "t3_qxj7hi", "created_utc": 1637337539.0}
{"sub": "LanguageTechnology", "title": "AI-Based Generative Writing Models Frequently \u2018Copy and Paste\u2019 Source Data", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_qxepnh", "created_utc": 1637323099.0}
{"sub": "LanguageTechnology", "title": "Text Mining Project on eating disorders &amp; social networks: help us!", "selftext": "We are a team of academic researchers interested in psychology and natural language use. We are currently interested in gathering some data from people in Social Media.\n\nWe would greatly appreciate it **if you could fill the questionnaire attached.** **It only takes 2 minutes :)**\n\nIt is a standard inventory of questions used by psychologists. Note that the questionnaire contains a field in which the respondent has to provide his/her Reddit username. This would help us to link word use (as extracted from your Reddit's public submissions) with your responses to the questionnaire.\n\nOf course, we will treat the information you provide with the utmost confidentiality and privacy. All information we will extract from Reddit will be anonymised and we will be the only one capable of connecting your username with your postings and your questionnaire. Such information will be kept in an encrypted file and will not be disclosed to anybody.\n\nLink to the questionnaire: [https://forms.gle/PkWyB64aAu6BQTqi6](https://forms.gle/PkWyB64aAu6BQTqi6)\n\nDavid E. Losada, Univ. Santiago de Compostela, Spain ([david.losada@usc.es](mailto:david.losada@usc.es))\n\nFabio Crestani, Univ. della Svizzera Italiana, Switzerland ([fabio.crestani@usi.ch](mailto:fabio.crestani@usi.ch))\n\nJavier Parapar, Univ. A Coru\u00f1a, Spain ([javierparapar@udc.es](mailto:javierparapar@udc.es))\n\nPatricia Martin-Rodilla, Univ. A Coru\u00f1a, Spain ([patricia.martin.rodilla@udc.es](mailto:patricia.martin.rodilla@udc.es) )", "upvote_ratio": 0.86, "id": "t3_qxd9tp", "created_utc": 1637317009.0}
{"sub": "LanguageTechnology", "title": "Any RASA users out there? Setting variables based on response text chosen", "selftext": "I am creating a chatbot but the use case is a little bizarre. More specifically, the chatbot will be playing the role of someone asking questions, and I will be acting as the customer service representative. So for example, I will type \u201cHow can I help you?\u201d, and the chatbot will respond with something like \"What are the meal options on flight ABC?\"I already have a massive list of potential questions the chatbot could ask.\n\nSo as you might expect, there will be many text options for when I type \u201cHow can I help you\u201d\u2026from can I get a flight from A to B on day X to what airports are available in state Y? From what I understand, if there are many text options under something like utter\\_help\\_me under responses in domain, a text will be chosen randomly. But I want variables to be set based on what text is randomly chosen. Is there a way to do that? I know this is usually done based on what is typed and not what is returned in a response, so this is the strange part", "upvote_ratio": 0.67, "id": "t3_qx7h9s", "created_utc": 1637293637.0}
{"sub": "LanguageTechnology", "title": "Deploying Serverless spaCy Transformer Model with AWS Lambda", "selftext": "In this article, we show you how to push an NER spacy transformer model to Huggingface and deploy the model on [AWS Lambda](https://docs.aws.amazon.com/lambda/latest/dg/welcome.html) to run predictions.  **Deploying models without the need to manage backend servers** will enable developers and small startups who do not have devops resources to start deploying models ready to use in production.\n\nFull Article =&gt; [https://towardsdatascience.com/deploying-serverless-spacy-transformer-model-with-aws-lambda-364b51c42999](https://towardsdatascience.com/deploying-serverless-spacy-transformer-model-with-aws-lambda-364b51c42999)", "upvote_ratio": 1.0, "id": "t3_qwxrhc", "created_utc": 1637264418.0}
{"sub": "LanguageTechnology", "title": "Collaboration Request for SemEval-2022", "selftext": "Hello,\nI\u2019ve been working to compete in Task 4: \u2018Patronizing and condescending Language Detection\u2019 (SemEval 2022). I implemented few baselines and did some experiments. Also got several ideas for further improvement. I\u2019m looking for a team member to speed up the work process. Anyone interested can ping me.\nLooking forward for responses!", "upvote_ratio": 1.0, "id": "t3_qw42wb", "created_utc": 1637169467.0}
{"sub": "LanguageTechnology", "title": "spaCy's config and project systems", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_qw311q", "created_utc": 1637166570.0}
{"sub": "LanguageTechnology", "title": "Recommendations for pre-trained word embedding models.", "selftext": "Does anyone have any (preferably) free dataset recommendations outside of Google\u2019s news vector model (too specific/scientific at times) and the Common Crawl\u2019s model (too many typos) for generating similarity scores?\n\nGensim, KeyedVectors, usually .bin or .vec files as input I believe.", "upvote_ratio": 0.86, "id": "t3_qvwzuw", "created_utc": 1637147451.0}
{"sub": "LanguageTechnology", "title": "Hardware used for dev work at enterprise company", "selftext": "I'm curious to know what people are using for dev hardware at large companies - with particular interest in hearing from those who work with large language models. is anyone surviving without a GPU? OS info helpful as well.", "upvote_ratio": 0.5, "id": "t3_qvn6tf", "created_utc": 1637111220.0}
{"sub": "LanguageTechnology", "title": "Work regarding probing language models for semantics?", "selftext": "Hi. I'm taking a look at some literature regarding language model probing and have noticed that there's a lot of work that's been done focusing on whether language models have _syntactic_ properties, but I haven't really seen a lot of work probing the semantic capabilities of these models and what information is being used during inference.\n\nI think the closest I've found is a paper titled [_Sorting Through the Noise: Testing Robustness of Information Processing in Pre-trained Language Models (Pandia and Ettinger, 2021)_](https://arxiv.org/abs/2109.12393) but I'm not entirely sure if this paper dives into analyzing what kind of semantic information is being used by LMs and rather focuses on fail cases of LMs.\n\nJust wondering if anyone here can give me some pointers or recommendations. Thanks!", "upvote_ratio": 1.0, "id": "t3_qvmfrv", "created_utc": 1637108881.0}
{"sub": "LanguageTechnology", "title": "Using Python to extract highest occurring and most unique keywords in text?", "selftext": "Suppose I have a column where each cell has a description of a product. What packages/ algorithms can I use to tag each description with the keywords that are both highest occurring while also being relatively unique to the text? For example, if half the products are toys, words like \"child\", \"toy\", \"learning\", etc, which are not typical stop words, become less important to analysis.", "upvote_ratio": 1.0, "id": "t3_qvctdw", "created_utc": 1637083059.0}
{"sub": "LanguageTechnology", "title": "Do you think NLP will be able to comprehend linguistic typology?", "selftext": "The idea behind linguistic typology is that there are patterns common to all languages. These patterns repeat themselves at different levels. They are also specific to individual languages.\n\nLinguistic classification organizes languages based on structural features, patterns, and linguistic units. It offers a systematic way of grouping languages to discover linguistic properties shared by these languages.\n\nSince linguistic classification involves collecting and analyzing data from various sources (fieldwork, literature, language documentation, linguistic atlas, etc.), could something like GPT-3 be able to comprehend it?\n\nI\u2019m referring mainly to the translation and localization field.\n\nAlgorithms are currently unable to grasp the context and nuances of a text. This means that we still need human translation to interpret cultural references and preserve the style and intention of the original text.\n\nHow long do you think it will take for AI to surpass a human translator?\n\nMy question is based on [this article](https://www.oneskyapp.com/blog/how-linguistic-typology-helps-us-understand-languages/?utm_source=reddit&amp;utm_medium=language-technology) that goes over linguistic typology and why it makes human translators indispensable in localization processes.", "upvote_ratio": 0.86, "id": "t3_qv8rw9", "created_utc": 1637072276.0}
{"sub": "LanguageTechnology", "title": "Webinar with creator of sentence transformers later today", "selftext": "I figured a few of you will be interested in this, we have a webinar later today (11AM ET) where the creator of SBERT and the sentence transformers library - Nils Reimers - will be talking about semantic search and fine-tuning sentence transformers. It'll cover how to build sentence vectors with SBERT, I assume a little on Hugging Face, and how to index and perform a semantic search using Pinecone.\n\n[Registration link is here!](https://pinecone-io.zoom.us/webinar/register/1416360828695/WN_FNyqH2EsTnesF3Rh9-QSHA?utm_source=sendgrid.com&amp;utm_medium=email&amp;utm_campaign=email)\n\nHope it's useful, thanks :)", "upvote_ratio": 1.0, "id": "t3_qv5p8e", "created_utc": 1637061798.0}
{"sub": "LanguageTechnology", "title": "Advices on my bachelor's thesis topic", "selftext": "Hello everyone.\n\nThis is my last year of uni, I'm taking computer science major. Now I'm thinking about my thesis topic.\n\nMy hobby is also learning foreign languages so I have to use a machine translators like Google translator, Papago, etc. As you might know, those are kind of... not really good, especially with certain languages. \n\nSo I thought, \"Hm, what if I build my own translator?\"\n\nBut is it, you know, possible to build a translator that would work better than those that so many people work on for years? \n\nI'm interested in Russian-Korean or Russian-Japanese and backwards translator because those are the languages I learn, and when I translate text in those languages Google Translate it makes no sense most of the time.\n\nI also think that it might be a bit too much for just an undergraduate thesis? If you maybe have some ideas related to this problem that are not so complex, I would be glad to read them.", "upvote_ratio": 0.67, "id": "t3_qv3m1r", "created_utc": 1637053146.0}
{"sub": "LanguageTechnology", "title": "Talk with AI (NLP) Model", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_qv0f2i", "created_utc": 1637040313.0}
{"sub": "LanguageTechnology", "title": "GPT-J through API + training on custom datasets", "selftext": "Anybody checked out Eleuter\u2019s new GPT-J yet?  \nI feel like it\u2019s on par with OpenAI\u2019s Curie. It pretty good overall for inference, but I thought it would be cool to fine-tune it on a custom dataset.\n\nI personally found it hard to do because of the lack of resources out there, so I ended up putting together this project to simplify custom training of GPT-J and deployment to production after the training. Both can be done through a web interface I built. Also, I added a default pre-trained GPT-J to use through an interface or API too.  \nPlease, check it out and give me feedback if you can! [https://www.tensorbox.ai/](https://www.tensorbox.ai/)", "upvote_ratio": 1.0, "id": "t3_quskm5", "created_utc": 1637016221.0}
{"sub": "LanguageTechnology", "title": "Daily digest of new NLP Research Papers", "selftext": "Hi Everyone, is there any website or subscription where we can get daily digest of top new Research Papers submitted in NLP or any of it's subfields", "upvote_ratio": 0.97, "id": "t3_quox74", "created_utc": 1637006266.0}
{"sub": "LanguageTechnology", "title": "quality dimension for alignment dataset", "selftext": "Hi guys, i'm data science student and i'm working on NLG data-to-text task. I'm seeking How to evaluate my alignment dataset with adequate quality dimensions. I'm newie in this field, for now i'm reading these excellent papers:\n\n\\-  A Survey of Evaluation Metrics Used for NLG Ststems: [https://arxiv.org/abs/2008.12009](https://arxiv.org/abs/2008.12009)\n\n\\- Survey of the State of the Art in Natural Language Generation: Core tasks, applications: [https://arxiv.org/abs/1703.09902](https://arxiv.org/abs/1703.09902)\n\nAs evaluation metrics, I was planning to use the following:\n\nBLEU, NIST, GTM, METEOR, ROUGE, Vector Extrema, MoveScore, BLEURT, PARENT.\n\nHowever, it's still not clear to me what quality dimensions mean and which ones I should use for. Could someone direct me to some specific paper? Thank you very much and sorry for my inexperience.\n\n&amp;#x200B;\n\nThanks all.\n\n&amp;#x200B;\n\n#", "upvote_ratio": 1.0, "id": "t3_quhwck", "created_utc": 1636987171.0}
{"sub": "LanguageTechnology", "title": "Why language pars are used the most in the evaluation of machine translation models and why?", "selftext": "The title should be \"what language pairs\" instead of \"why language pars\".\n\nIn my experience I see English-German and English-Romanian very frequently. Not sure why that is the case.", "upvote_ratio": 1.0, "id": "t3_quaw5s", "created_utc": 1636961223.0}
{"sub": "LanguageTechnology", "title": "Normalizing Named Entities", "selftext": "[Machine-Guided Polymer Knowledge Extraction Using Natural Language Processing: The Example of Named Entity Normalization](https://pubs.acs.org/doi/abs/10.1021/acs.jcim.1c00554)\n\nThis paper talks about Supervised Clustering methods for Named Entity Normalization (NEN), a sometimes overlooked but very important area of Information Extraction. We cluster the variations of name with which chemical entities are referred to in literature. We establish the advantage of fastText embeddings over Word2Vec embeddings and show that parameterized cosine distance as well as ensembling of models lead to performance gains for NEN. This is also one of the few works to study normalization for named entities for a niche domain, i.e., polymers. This dataset is one of the biggest out there for normalization and presents unique challenges not present in general English text as the cluster sizes are much larger and cluster size variance is greater than typical synonym clusters.\n\nThe code and data for this paper are available [here](https://github.com/Ramprasad-Group/polymerNEN). Consider using this data set for bench marking and evaluation purposes if you are working in this area.", "upvote_ratio": 0.9, "id": "t3_qu4zpo", "created_utc": 1636940571.0}
{"sub": "LanguageTechnology", "title": "Question about statistics and algebra for NLP", "selftext": "I'm a journalist and freelance translator and I worked in the banking system in my country for many years. A couple of years ago I decided I wanted to get into data science, took a few practical courses and got a job for a consulting company, building simple models for businesses. Nothing too technical.\n\nFor about six weeks now I've been getting into NLP to tie my past experiences with my present ones. But I want to dive deeper into the inner workings of NLP to professionalize my profile a little bit.\n\nWhat topics do you think I should focus on? I'm particularly interested in learning the basics of statistics and algebra oriented for NLP but I don't know where to start. Thanks in advance!", "upvote_ratio": 0.81, "id": "t3_qtwga4", "created_utc": 1636915696.0}
{"sub": "LanguageTechnology", "title": "CMU Researchers Develop A Unified Framework For Evaluating Natural Language Generation (NLG)", "selftext": "Natural language generation (NLG) is a broad term that encompasses a variety of tasks that generate fluent text from input data and other contextual information. In actuality, the goals of these jobs are frequently very different. Some well-known instances of NLG include compressing a source article into a brief paragraph conveying the most significant information, converting content presented in one language into another, and creating unique responses to drive the discourse.\n\nNatural language processing has advanced at a breakneck pace in terms of enhancing and developing new models for various jobs. However, assessing NLG remains difficult: human judgment is considered the gold standard, but it is typically costly and time-consuming to get. Automatic evaluation, on the other hand, is scalable, but it\u2019s also time-consuming and challenging. This problem originates because each work has varied quality requirements, making it difficult to establish what to assess and how to measure it.\n\nResearchers from Carnegie Mellon University, Petuum Inc., MBZUAI and UC San Diego recently took a step in this direction by developing [a single framework for NLG evaluation](https://arxiv.org/pdf/2109.06379.pdf) that makes it easier to create metrics for various language generation tasks and characteristics.\n\n# [Quick Read](https://www.marktechpost.com/2021/11/13/researchers-develop-a-unified-framework-for-evaluating-natural-language-generation-nlg/) |  [Paper](https://arxiv.org/pdf/2109.06379.pdf)| [Code](https://github.com/tanyuqian/ctc-gen-eval) | [CMU Blog](https://blog.ml.cmu.edu/2021/10/29/compression-transduction-and-creation-a-unified-framework-for-evaluating-natural-language-generation/)", "upvote_ratio": 1.0, "id": "t3_qtic2e", "created_utc": 1636865436.0}
{"sub": "LanguageTechnology", "title": "Meta AI Open-Sourced It\u2019s First-Ever Multilingual Model (Won The WMT Competition): A Step Towards Future Of Machine Translation", "selftext": "Machine translation (MT) is the process of employing artificial intelligence to automatically translate text from one language (the source) to another (the destination) (AI). The ultimate goal is to create a universal translation system that will allow everyone to access information and communicate more effectively. It is a long road ahead for this vision to turn into reality.\n\nMost currently used MT systems are bilingual models, which require labeled examples for each language pair and job. Such models are, however, unsuitable for languages with insufficient training data. Its enormous complexity makes it impossible to scale to practical applications such as Facebook, where billions of users post in hundreds of languages every day.\n\nTo address this problem and develop [a universal translator,](https://ai.facebook.com/blog/the-first-ever-multilingual-model-to-win-wmt-beating-out-bilingual-models/) the MT field must witness a transition from bilingual to multilingual models. A single translation model is used to process numerous languages in multilingual machine translation. The research would attain its peak if it were possible to build a single model for translation across as many languages as possible by effectively using the available linguistic resources.\n\n# [Quick Read](https://www.marktechpost.com/2021/11/13/meta-ai-open-sourced-its-first-ever-multilingual-model-won-the-wmt-competition-a-step-towards-future-of-machine-translation/) | [Paper](https://arxiv.org/pdf/2108.03265.pdf) | [Github](https://github.com/pytorch/fairseq/tree/main/examples/wmt21?) | [Meta Blog](https://ai.facebook.com/blog/the-first-ever-multilingual-model-to-win-wmt-beating-out-bilingual-models/)", "upvote_ratio": 0.91, "id": "t3_qt5436", "created_utc": 1636822195.0}
{"sub": "LanguageTechnology", "title": "Text Classification Master Thesis in NLP", "selftext": "Hello! I've made some research online about some ideas for a text classification project but most likely i have some doubts about them.\n\nSo, my teacher offered me an idea \\*to make only text classification with comparison between some methods.\\*.\n\n So i was thinking, between this and maybe doing a popular topic like \"Fake News Detection\" or \"Sentiment analysis\" is it a good idea? \n\nI'm asking that more because as far this master goes, i want an easier project thesis because of some related personal problems.\n\nI'm also opened for more ideas if you have some.", "upvote_ratio": 1.0, "id": "t3_qszw9g", "created_utc": 1636804965.0}
{"sub": "LanguageTechnology", "title": "NLP switch advice for bio", "selftext": "Hi. Could anyone working in NLP shoot me some advice?\n\nI'm trying to switch to NLP based work. I'm a biologist/bioinformatician (M.S.) and I've done ML with computer vision in industry. I've even turned down a pretty nice job offer with computer vision, but it had restrictions and not NLP focused so I turned it down. \n\nMy goals are to get a jump start on NLP for bioinformatics with protein and gene language models. To that effect, I've been studying pytorch and NLP from scratch. I expect to have a working understand of transformer/BERT based langauge models and a decent example or two to start applying for biology based NLP.\n\nHowever, I'm afraid I'm a bit too early for gainful employment strictly working with proteins and genes given the sporadic appearence of job postings.\n\nTo summarize, I've got a bio background, I've done ML in industry with computer vision and I am prioritizing a research career using NLP and biology. Today, I *think* I would like a job where I can work with NLP in some context, with enough of a salary I can live comfortably in Spain as a remote worker (am American). I'd like to do this until more opportunity appears.\n\nA few questions;\n\n1) Are there places other than LinkedIn you seek NLP jobs?\n\n2) What skiills can get me a remote NLP job?\n\n- I've learned the basics of Flask, I would continue figuring this out to serve models on the cloud and make them accessible via REST APIs if it would greatly increase my chances at a paid remote gig.\n- I could do something with huggingface, but I don't know what general project would be good to get non-bio jobs\n-Coding models with JAX?\n\n3) Would you recommend a different path on the short-term?\n\n- focus on finding a computer vision job (And use transformer models to gain more transferable knowledge to future career)\n- focus on learning the mininmal backend/webdev to get a job to get a paycheck on the short-term?\n\nAny pertinent advice would be appreciated. This is my dead-set goal, but I don't really want to get side-tracked or accept an offer that would require me to establish myself somewhere physically, or do a PhD.\n\nThanks", "upvote_ratio": 0.87, "id": "t3_qszvx8", "created_utc": 1636804928.0}
{"sub": "LanguageTechnology", "title": "Are there anyone studying CS224n from Stanford?", "selftext": "Hello guys, if there're anyone else who's studying this course and want to collaborate on homeworks and in general toss me a message!", "upvote_ratio": 0.81, "id": "t3_qsymqv", "created_utc": 1636799696.0}
{"sub": "LanguageTechnology", "title": "University of Waterloo AI Researchers Introduce A New NLP Model \u2018AfriBERTa\u2019 For African Languages Using Deep Learning Techniques", "selftext": "A technology that has been around for years but most often taken for granted is Natural Language Processing(NLP). It is the employment of computational methods to analyze and synthesize natural language and speech. Pre-trained multilingual language models have proven efficient in performing various downstream NLP tasks like sequence labeling and document classification.\u00a0\n\nThe notion behind designing pre-trained models is to build a black box that comprehends the language that can then be instructed to perform any task in that language. The goal is to construct a machine that can replace a \u2018well-read\u2019 human. However, these models require large chunks of training data to build them. As a result, the world\u2019s under-resourced languages are left out from being explored.\n\nResearchers from the David R. Cheriton School of Computer Science at the University of Waterloo dispute this assumption and introduce [AfriBERTa](https://aclanthology.org/2021.mrl-1.11.pdf). This new neural network model leverages deep-learning approaches to generate state-of-the-art outcomes for under-resourced languages. The researchers show that they can build competitive multilingual language models with less than 1 GB of text. Their AfriBERTa model covers 11 African languages, four of which have never had a language model before.\n\n# [Quick Read](https://www.marktechpost.com/2021/11/12/university-of-waterloo-ai-researchers-introduce-a-new-nlp-model-afriberta-for-african-languages-using-deep-learning-techniques/)|  [Paper](https://aclanthology.org/2021.mrl-1.11.pdf) | [Code](https://github.com/keleog/afriberta)", "upvote_ratio": 0.9, "id": "t3_qsqal7", "created_utc": 1636767570.0}
{"sub": "LanguageTechnology", "title": "Could you give examples of types of NLP projects you worked on at work in real business scenarios?", "selftext": "I get the impression that Kaggle competitions aren't reflective of real-world applications of data science in NLP, and common NLP examples like chatbots, search engines, and grammar checking are not necessarily the majority of real-world projects either? Am I wrong? Or are real-world business applications of NLP really quite different and unique compared to the examples I just mentioned?\n\nCould some of you in the field give me examples of what real-world business projects look like? I want to get a feel of what working in NLP as a data scientist would be like.\n\nSide question, is there normally not enough work to go around to just focus on NLP alone as a career, and do you have to do computer vision or other subfields of data science in a typical work setting?", "upvote_ratio": 0.87, "id": "t3_qsofis", "created_utc": 1636761671.0}
{"sub": "LanguageTechnology", "title": "Spacy vs NLTK for Spanish Language Statistical Tasks", "selftext": "Hey all,\n\nI have some experience using both NLTK and Spacy for different NLP tasks. I find myself wanting to gravitate towards Spacy becuase of their community and documentation, but I can't help feeling that for my specific use case NLTK may be the better route.\n\nMy idea is to scrape the entirety of a Spanish news site and analyze the content of all their news articles. I want to answer questions such as:\n\n1. What are the top 100 most frequent words used among all their articles.\n2. What is the lexical diversity across the entire site (and perhaps per article so that I can try to predict which articles are easier to read for non native learners)\n3. What are the most common n-grams across the entire site to help learners know what vocabulary to study.\n\nBetween NLTK and Spacy, which framework is better for completing tasks such as the above? My guess is both can do it, but I wonder if one is better suited for it than another. \n\nThanks!", "upvote_ratio": 1.0, "id": "t3_qsld7i", "created_utc": 1636752608.0}
{"sub": "LanguageTechnology", "title": "Create semantic search applications with machine-learning workflows", "selftext": "&amp;#x200B;\n\nhttps://reddit.com/link/qs9ajk/video/slkwnfkqh5z71/player\n\nCreate semantic search applications with machine-learning workflows. The demo above shows how various NLP pipelines can be connected together to build a semantic search application.\n\ntxtai executes machine-learning workflows to transform data and build AI-powered semantic search applications. txtai has support for processing both unstructured and structured data. Structured or tabular data is grouped into rows and columns. This can be a spreadsheet, an API call that returns JSON or XML or even list of key-value pairs.\n\nSome example workflows:\n\n* Summarize news articles\n* Summarize and translate research papers\n* Load and index data via a CSV\n* Schedule a recurring job to query an API and index results for semantic search\n\nReferences:\n\n[Live Demo](https://huggingface.co/spaces/NeuML/txtai)  \n[GitHub](https://github.com/neuml/txtai)  \n[Article](https://towardsdatascience.com/run-machine-learning-workflows-to-transform-data-and-build-ai-powered-text-indices-with-txtai-43d769b566a7)  \n[Notebook](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/14_Run_pipeline_workflows.ipynb)  \n[Notebook](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/22_Transform_tabular_data_with_composable_workflows.ipynb)", "upvote_ratio": 0.89, "id": "t3_qs9ajk", "created_utc": 1636716891.0}
{"sub": "LanguageTechnology", "title": "Speech recognition hackathon (ends Nov. 17)", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_qrya6s", "created_utc": 1636674232.0}
{"sub": "LanguageTechnology", "title": "Macaw - Question Answering NLP Model - Applied NLP Tutorial with Python Code", "selftext": "nan", "upvote_ratio": 0.75, "id": "t3_qrtokz", "created_utc": 1636660964.0}
{"sub": "LanguageTechnology", "title": "Explicit content detector python", "selftext": "Hello !\n\nI want to build a project thats aimed to detect explicit content in texts, it's just going to flag if the text has explicit content. I already made something that detects explicit words in a text, but I want to do something more complex. As you know, you don't have to use bad words to make explicit sentences, I thought about creating a list of possible explicit sentences, but that would be infinite.\n\nWhat are my chances here ? Do I have any other options ?\n\nThanks in advance.", "upvote_ratio": 0.75, "id": "t3_qr6qht", "created_utc": 1636585384.0}
{"sub": "LanguageTechnology", "title": "Experience with Context-Based Sentiment Analysis?", "selftext": "Sentiment analysis is a pretty standard problem. It's generally done with short input texts and is not seen as a super difficult problem. However I haven't thought about is adding context to the task\u2014say like trying to predict the sentiment of one comment given the above comments on a Facebook post. I imagine that adding context could be as simple as concatenating the entire context (e.g. other comments and the original post) to the input given the capabilities of Transformer models. But I've never actually tried to solve a problem like this. Does anyone have experience or insights to share for a problem like this?", "upvote_ratio": 1.0, "id": "t3_qr5zpw", "created_utc": 1636583335.0}
{"sub": "LanguageTechnology", "title": "Cedille, the largest French language model, released in open source", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_qr4udb", "created_utc": 1636580166.0}
{"sub": "LanguageTechnology", "title": "An Introduction to Language Models in NLP (Part 1: Intuition)", "selftext": "nan", "upvote_ratio": 0.94, "id": "t3_qr4juj", "created_utc": 1636579347.0}
{"sub": "LanguageTechnology", "title": "Same Document, two OCRs, super classifier?", "selftext": "I have 150k documents, 10 pages each document, and two outputs, the Original OCR and my Tessearact OCR. \n\nI've built classifiers with the Tesseract output, but am seeking ways to strengthen my model. Model in question: [https://scikit-learn.org/stable/modules/naive\\_bayes.html](https://scikit-learn.org/stable/modules/naive_bayes.html) along with \\`CountVectorizer(ngram\\_range(2,2))\\` \n\nIt dawned on me that it might be possible to somehow sandwich the Original OCR with the Tessearact OCR. Would such a thing be possible? Or even useful? \n\nI plan to try it out, but what do you say internet? \n\n(open to all thoughts and considerations)", "upvote_ratio": 1.0, "id": "t3_qr4c0c", "created_utc": 1636578743.0}
{"sub": "LanguageTechnology", "title": "I need an NLP model which can be trained with tabular data like biblical corpus and be able To make direct predictions.", "selftext": "Tapas is the closest I've come across so far but it only accept the data as input and you can't train with the data.  Prediction are slow and it can't handle the size of data I have", "upvote_ratio": 1.0, "id": "t3_qr2ln2", "created_utc": 1636573920.0}
{"sub": "LanguageTechnology", "title": "Word Sense Disambiguation. Recommendations", "selftext": "Hello everyone!\n\nI'm taking a class on NLP, and I have to give a presentation on Word Sense Disambiguation, so I'm asking for any valuable resources that anyone could recommend, both theory and algorithms stuff, so that I can do my research.\n\nThanks in advance!", "upvote_ratio": 0.5, "id": "t3_qr13xy", "created_utc": 1636569765.0}
{"sub": "LanguageTechnology", "title": "Advice - University of Paris, Master", "selftext": "Hello everyone, \n\nI was wondering if anyone is familiar with the Linguistics Master's program at the University of Paris? \n\nAfter all, the university has only existed since 2019, having been formed from two universities that emerged from the Sorbonne, so it's probably hard to find someone who has had a lot of experience there. \n\nBut the program looks very interesting and the master program has new different specializations. In addition, the *Laboratoire De Linguistique Formelle* seems to be part of the university, which also looks very big!\n\nIs there anyone here who has already had experience with the program?", "upvote_ratio": 1.0, "id": "t3_qqy0ep", "created_utc": 1636561275.0}
{"sub": "LanguageTechnology", "title": "Resources and Books About NLP", "selftext": "I'm looking to read more books and resources about NLP. Kindly share the title and resources of the book with me. \n\nThank you.", "upvote_ratio": 0.5, "id": "t3_qqurh9", "created_utc": 1636551885.0}
{"sub": "LanguageTechnology", "title": "Autoregressive meaning", "selftext": "What does the word \u201cautoregressive\u201d mean when describing NLP models?", "upvote_ratio": 1.0, "id": "t3_qqr55f", "created_utc": 1636538943.0}
{"sub": "LanguageTechnology", "title": "SEP token", "selftext": "Question 1: In LMs that pretrain without NSP like objectives, the SEP tokens appear only once in the sentence at the end. So effectively it will only be trained to mark the end of the sentence, rather than something that separates sentences. If our task involves sentence pairs, we have to rely on fine-tuning for the model to learn that it is actually a separator token. Is my thinking here correct?\nQuestion 2: Can we use multiple SEP tokens to separate more than two sentences in models like BERT?", "upvote_ratio": 1.0, "id": "t3_qqqu2m", "created_utc": 1636537692.0}
{"sub": "LanguageTechnology", "title": "How AI is transforming MarTech (featuring NLP)", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_qqqjje", "created_utc": 1636536424.0}
{"sub": "LanguageTechnology", "title": "MIT AI Researchers Introduce \u2018PARP\u2019: A Method To Improve The Efficiency And Performance Of A Neural Network", "selftext": "Recent developments in machine learning have enabled automated speech-recognition technologies, such as Siri, to learn the world\u2019s uncommon languages, which lack the enormous volume of transcribed speech required to train algorithms. However, these methods are frequently too complicated and costly to be broadly used.\n\nResearchers from MIT, National Taiwan University, and the University of California, Santa Barbara, have developed a simple technique that minimizes the complexity of a sophisticated speech-learning model, allowing it to run more efficiently and achieve higher performance.\n\nTheir method entails deleting unneeded components from a standard but complex speech recognition model and then making slight tweaks to recognize a given language. Teaching this model an unusual language is a low-cost and time-efficient process because only minor adjustments are required once the larger model is trimmed down to size.\n\n# [Read The](https://arxiv.org/pdf/2106.05933.pdf) [Paper](https://arxiv.org/pdf/2106.05933.pdf) | [Checkout The](https://people.csail.mit.edu/clai24/parp/) [Project](https://people.csail.mit.edu/clai24/parp/) | [5 Min Read](https://www.marktechpost.com/2021/11/09/mit-ai-researchers-introduce-parp-a-method-to-improve-the-efficiency-and-performance-of-a-neural-network/) | [MIT Blog](https://news.mit.edu/2021/speech-recognition-uncommon-languages-1104)", "upvote_ratio": 0.92, "id": "t3_qqmhp1", "created_utc": 1636519942.0}
{"sub": "LanguageTechnology", "title": "Intel Optimizes Facebook DLRM with 8x speedup (Deep Learning Recommendation Model)", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_qqgjei", "created_utc": 1636501227.0}
{"sub": "LanguageTechnology", "title": "Please help", "selftext": "Hi i am a complete beginner dumbo to NLP and want to try learning topic modeling. Is it okay to use LDA on just 16 documents. They are business reports and I would like to extract topics to assess the trends. \n\nOmg please help !!", "upvote_ratio": 0.5, "id": "t3_qq5yxi", "created_utc": 1636471346.0}
{"sub": "LanguageTechnology", "title": "Language model built on LSTM?", "selftext": "Hey everyone! Could I get an example (or multiple, if possible) of a language model which as been built on LSTM rather than Transformers? \n\nThank you\n\nEdit: preferably one that can be used with the Hugging Face API", "upvote_ratio": 0.67, "id": "t3_qq55ds", "created_utc": 1636469008.0}
{"sub": "LanguageTechnology", "title": "Improving Chatbot technology with NLP", "selftext": "Hi\n\nI am currently studying Computer Science and am interested in writing my thesis in the field of NLP about improving chatbots. This is of course too broad of a topic itself. Where do you think  the biggest obstacle in developing better chatbots currently lies?  \nSome topics I have been suggested so far are:\n\n1.dialogue success/fluency \n\n2.query intent classification\n\nDo you agree with any of these or is there another bigger obstacle?\n\nPS I know better is sort of vague, but what I mean is better in general human terms, so that an average users experience would be better. The current chatbots are often algorithmic, cannot answer many questions and are more like category selection/specification tools and less like AI customer service they are branded as.", "upvote_ratio": 1.0, "id": "t3_qq4tsg", "created_utc": 1636468055.0}
{"sub": "LanguageTechnology", "title": "NLPAug: what proportion of augmented sentences do you usually add to the dataset?", "selftext": "Hi,\n\nWe are working on an NLP problem that is near to hate speech detection.\n\nWe use a BERT neural network that has 2 outputs:\n\n\\- Rating the sentence\n\n\\- Classifying it among 16 types of speech classes\n\nWe have 12K sentences tagged in a dataset.\n\nSince the dataset is relatively tiny, we are working on augmenting it with [NLPAug](https://github.com/makcedward/nlpaug). We use 2 strategies. Synonymisation and back translation.\n\nI was wondering if you have experience with that, what is the usual ratio of augmented sentences in your dataset? 1/3, 1/2, 3/4...\n\nThanks", "upvote_ratio": 0.94, "id": "t3_qq1ok9", "created_utc": 1636457302.0}
{"sub": "LanguageTechnology", "title": "Hugging Face Introduces \u2018Datasets\u2019: A Lightweight Community Library For Natural Language Processing (NLP)", "selftext": "***Datasets***\u00a0is a modern NLP community library created to assist the NLP environment**.**\u00a0[***Datasets***\u00a0aims to standardize end-user interfaces, versioning, and documentation while providing a lightweight front-end tha](https://arxiv.org/pdf/2109.02846.pdf)t works for tiny datasets as well as large corpora on the internet. The library\u2019s design involves a distributed, community-driven approach to dataset addition and usage documentation. After a year of work, the library now features over 650 unique datasets, over 250 contributors and has supported many original cross-dataset research initiatives and shared tasks\n\n[Quick Read](https://www.marktechpost.com/2021/11/08/hugging-face-introduces-datasets-a-lightweight-community-library-for-natural-language-processing-nlp/) | [Paper](https://arxiv.org/pdf/2109.02846.pdf) | [Github](https://github.com/huggingface/datasets)", "upvote_ratio": 0.67, "id": "t3_qpt6cl", "created_utc": 1636423419.0}
{"sub": "LanguageTechnology", "title": "SemEval 2022 Task 11: MultiCoNER Multilingual Complex Named Entity Recognition (Call for Submission)", "selftext": "We invite you to participate in SemEval-2022 Task 11: **Multi**lingual **Co**mplex **N**amed **E**ntity **R**ecognition (MultiCoNER).  \n\n\n**Task Website:** [https://multiconer.github.io/](https://multiconer.github.io/)  \n**Codalab (Data download + Submission):**  [https://competitions.codalab.org/competitions/36044](https://competitions.codalab.org/competitions/36044)  \n\n\nThis task focuses on the detection of complex entities, such as movie, book, music and product titles, in low context settings (short and uncased text).  \n\n\nThe task covers 3 domains (sentences, search queries, and questions) and provides data in 11 languages: **English, Spanish, Dutch, Russian, Turkish, Korean, Farsi, German, Chinese, Hindi**, and **Bangla**. Here are some examples in English, Chinese, Bangla, Hindi, Russian, Korean, and Farsi, where entities are enclosed inside brackets with their type:  \n\n\n* the original **\\[ferrari daytona |** **PRODUCT\\]** replica driven by **\\[don johnson |** **PERSON\\]** in **\\[miami vice |** **CreativeWork\\]**\n* \u5b83 \u7684 \u5ea7 \u4f4d \u5728 \\[**\u5723 \u5e03 \u91cc \u5384** | **LOCATION\\]** .\n* \u09b8\u09cd\u099f\u09c7\u09b6\u09a8\u099f\u09bf\u09b0 \u09ae\u09be\u09b2\u09bf\u0995 \\[**\u099f\u09be\u0989\u09a8\u09b8\u09cd\u0995\u09c7\u09af\u09bc\u09be\u09b0 \u09ae\u09bf\u09a1\u09bf\u09af\u09bc\u09be** | **CORPORATION\\]** \u0964\n* \u092f\u0939 \\[**\u0915\u0928\u0947\u0932 \u0935\u093f\u092d\u093e\u0917** | **LOCATION**\\] \u0915\u0940 \u0930\u093e\u091c\u0927\u093e\u0928\u0940 \u0939\u0948\u0964\n* \u0432 \u043e\u0441\u043d\u043e\u0432\u0435 \u0444\u0438\u043b\u044c\u043c\u0430 \u2014 \u0441\u0442\u0438\u0445\u043e\u0442\u0432\u043e\u0440\u0435\u043d\u0438\u0435 \\[**\u0433. \u0441\u0430\u043f\u0433\u0438\u0440\u0430** | **PERSON\\]** .\n* \\[**\ube14\ub8e8\ub808\uc774 \ub514\uc2a4\ud06c** | **PRODUCT\\]** : \uad11 \uae30\ub85d \ubc29\uc2dd \uc800\uc7a5\ub9e4\uccb4\uc758 \ud558\ub098\n* \\[**\u0646\u06cc\u0646\u062a\u0646\u062f\u0648** | **CORPORATION\\]** / \\[**\u0628\u0627\u0646\u062f\u0627\u06cc \u0646\u0627\u0645\u06a9\u0648 \u0627\u0646\u062a\u0631\u062a\u06cc\u0646\u0645\u0646\u062a** | **CORPORATION\\]** \u2013 \\[**\u0628\u0631\u0627\u062f\u0631\u0627\u0646 \u0633\u0648\u067e\u0631 \u0645\u0627\u0631\u06cc\u0648 \u0646\u0647\u0627\u06cc\u06cc** | **CreativeWork\\]**\n\nAdditionally, a **multilingual NER track** is also offered for multilingual systems that can process all languages. A **code-mixed track** allows participants to build systems that process inputs with tokens coming from two languages. For example, the following are some code-mixed examples from Turkish, Spanish, Dutch, German, and English.  \n\n\n* it was produced at the \\[**soyuzmultfilm** | **GROUP\\]** studio in \\[**moskova** | **LOCATION\\]** .\n* \\[**arturo vidal** | **PERSON\\]** ( born 1987 ) , professional footballer playing for \\[**f\u00fatbol club barcelona** | **GROUP\\]**\n* daarmee promoveerde hij toen naar de \\[**premier league** | **CORPORATION\\]** .\n* piracy has been a part of the \\[**sultanat von sulu** | **LOCATION\\]** culture .\n\nThe task focuses on detecting semantically ambiguous and complex entities in short and low-context settings. Participants are welcome to build NER systems for any number of languages. And we encourage to aim for a bigger challenge of building NER systems for multiple languages. The task also aims at testing the domain adaption capability of the systems by testing on additional test sets on questions and short search queries.    \n\n\nWe have released training data for 11 languages along with a baseline system to start with. Participants can submit their system for one language but are encouraged to aim for a bigger challenge and build multi-lingual NER systems.  \n\n\n**Task Website:** [https://multiconer.github.io/](https://multiconer.github.io/)  \n**Codalab Submission site:**  [https://competitions.codalab.org/competitions/36044](https://competitions.codalab.org/competitions/36044)  \n**Mailing List:** [multiconer-semeval@googlegroups.com](mailto:multiconer-semeval@googlegroups.com)  \n**Slack Workspace:** [https://join.slack.com/t/multiconer/shared\\_invite/zt-vi3g97cx-MpqTvS07XX22S78nRC2s0Q](https://join.slack.com/t/multiconer/shared_invite/zt-vi3g97cx-MpqTvS07XX22S78nRC2s0Q)  \n**Training Data:** [https://multiconer.github.io/dataset](https://multiconer.github.io/dataset)  \n**Baseline System:** [https://multiconer.github.io/baseline](https://multiconer.github.io/baseline)  \n\n\n**Shared task schedule:**  \n\n\n* Training data ready: September 3, 2021\n* Evaluation data ready: December 3, 2021\n* Evaluation start: January 10, 2022\n* Evaluation end: by January 31, 2022 (latest date; task organizers may choose an earlier date)\n* System description paper submissions due: February 23, 2022\n* Notification to authors: March 31, 2022\n\n**Task organizers**  \n\n\n* Shervin Malmasi (Amazon)\n* Besnik Fetahu (Amazon)\n* Anjie Fang (Amazon) \n* Sudipta Kar (Amazon) \n* Oleg Rokhlenko (Amazon)\n\nPlease reach out to the organizers at [multiconer-semeval-organizers@googlegroups.com](mailto:multiconer-semeval-organizers@googlegroups.com), or join the Slack workspace to connect with the other participants and organizers.", "upvote_ratio": 0.92, "id": "t3_qprljs", "created_utc": 1636418394.0}
{"sub": "LanguageTechnology", "title": "Is it possible to do an Aspect Based Sentiment Analysis using XLNet?", "selftext": "Hi everyone,\n\nI am doing an Aspect Based Sentiment Analysis using BERT Model, however, I noticed that the state of art XLNet model over performed the BERT model in most of NLP applications. I couldn't see any implementation for Aspect Based Sentiment Analysis on Internet , so I am curious if it is possible to do it?", "upvote_ratio": 1.0, "id": "t3_qpdk9p", "created_utc": 1636377870.0}
{"sub": "LanguageTechnology", "title": "Get all the topics from a given text.", "selftext": "I am a complete newbie to NLP. I have a situation in front of me:\n\nSuppose there is a (finite) set (**A**) of topics, for example- environment, space technology, tribal development, economics, politics, etc.\n\nI have **another set (B)** of a large number of texts, each containing about 100 to 500 words each. \n\nI have to classify every piece of text against the **given** set (A) of topics only, for example:\n\n**Text 1 -&gt;**  \n\n\"Deforestation of the Amazon rainforest in Brazil has surged to its highest level since 2008, the country's space agency (Inpe) reports. A total of 11,088 sq km (4,281 sq miles) of rainforest were destroyed from August 2019 to July 2020. This is a 9.5% increase from the previous year. The Amazon is a vital carbon store that slows down the pace of global warming. Scientists say it has suffered losses at an accelerated rate since Jair Bolsonaro took office in January 2019. The Brazilian president has encouraged agriculture and mining activities in the world's largest rainforest.\" (credits: BBC)\n\nOutput 1 should be - **environment**\n\nOutput 2 (can be more liberal and can contain topics other than those present in the given set (A) of topics) - environment, global warming, rainforests, Brazil, etc.\n\n**Text 2 -&gt;**\n\n\"Elon Musk is developing a vehicle that could be a game-changer for space travel. Starship, as it's known, will be a fully reusable transport system capable of carrying up to 100 people to the Red Planet. The founding ethos of Elon Musk's private spaceflight company SpaceX was to make life multi-planetary. This is partly motivated by existential threats such as an asteroid strike big enough to wipe out humanity.\" (credits: BBC)\n\nOutput 1 should be - **space technology**\n\nOutput 2 (can be more liberal and can contain topics other than those present in the given set (A) of topics) - space technology, science, technology, Elon Musk, space, etc.\n\nWhat can be the different approaches to deal with the above problem, get both output 1 and 2, and costs associated with them.\n\nPS. I'm new to this area of learning, so please be liberal with your advice and forgive any mistakes that I could have made while asking the question.", "upvote_ratio": 0.86, "id": "t3_qpby7i", "created_utc": 1636372105.0}
{"sub": "LanguageTechnology", "title": "About to apply for a Master's degree in Computational Linguistics; in want of information from current or former students (especially from Saarland, Tubingen and Stuttgart)", "selftext": "Hi everyone,\n\nI'm about to complete my bachelor's degree in English studies (I'm in third year, Western Europe), and I have to apply for a Master's degree this year. Alongside my studies, it's now been four years since I've started working as a translator, specialized in localization, and I've had the opportunity to work regularly with famous video games companies and translate a variety of content.\n\nI first had in mind to apply for a translation Master's degree, but as I already have had a peek at the translation industry by working, I'd like to broaden my skills so as to get better opportunities in the future as well as career development prospects, since I don't see myself having the same job during all my life.\n\nOne of the classes that I appreciate the most where I study, aside from translation, is linguistics. Moreover, I've always had a genuine interest in computing, and even though I'm only doing web development stuff (HTML/CSS/JS), I'm willing to learn other languages and develop my skills in this field.\n\nNow, with those two variables in the equation, I think computational linguistics could be a great opportunity for me, as it mixes two of my biggest interests and is still a relevant field with regard to the translation industry.\n\nOne of my biggest flaws is maths: it's been more than five years now that I've stopped doing maths, because I didn't need it during my studies. I've seen that some universities in Western Europe accepted students coming from a linguistics background and offered optional courses for such students. From what I've seen, these universities are generally located in Germany, namely Saarland, Tubingen and Stuttgart.\n\nAs far as I'm concerned, Germany would be the best choice as, even though I do not speak German, the country is contiguous to where I live and has extremely low fees compared to other universities, such as the University of Edinburgh, or University of Washington in Seattle. Now, here are some specific questions I'd like to ask to current or former students of these German universities:\n\n\u2014 as someone who has few programming experience but is willing to learn, which university would be the best choice?\n\n\u2014 how much math knowledge is required? Just enough for programming or more?\n\n\u2014 how many hours of classes are there on average per week, and does the general schedule allows one to have a job alongside one's studies? To take my own example, where I am, I have about 20 hours of classes per week, about 10 hours of work at home for the university, and 10 to 15 hours of real work (translation).\n\nObviously, I'd also love to hear the answers of people not coming from these universities \u2014 I've taken those as examples because I've heard of them the most on the Internet, but feel free to talk about your own path, it may give me ideas!\n\nThank you much for reading!", "upvote_ratio": 0.94, "id": "t3_qoomq5", "created_utc": 1636292054.0}
{"sub": "LanguageTechnology", "title": "FLAN: Fine-tuned LAnguage Nets", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_qoh2dj", "created_utc": 1636259079.0}
{"sub": "LanguageTechnology", "title": "One sentence highlight for every EMNLP-2021 Paper", "selftext": "Here is the list of all EMNLP 2021 (Empirical Methods in Natural Language Processing) papers, and a one sentence highlight for each of them. EMNLP 2021 is to be held both online and in Punta Cana, Dominican Republic from Nov 07 2021. \n\n[https://www.paperdigest.org/2021/11/emnlp-2021-highlights](https://www.paperdigest.org/2021/11/emnlp-2021-highlights)", "upvote_ratio": 1.0, "id": "t3_qodet2", "created_utc": 1636245970.0}
{"sub": "LanguageTechnology", "title": "Quoting in pandas", "selftext": "Can anyone please explain what is quoting=n, when reading a pandas data frame\nI got this solution on stack overflow when trying to solve eof error but I don't understand why", "upvote_ratio": 0.67, "id": "t3_qo82t8", "created_utc": 1636229038.0}
{"sub": "LanguageTechnology", "title": "finding all ngrams given specific (n-1)gram in nltk", "selftext": "Im struggling to find a efficient method for what seems, conceptually, to be a fairly simple task. I want to take a given trigram and look for all 4-grams in my text that contain that specific trigram. Eventually i want to do this recursively, which I feel like shouldnt be computationally intensive, but I'm struggling to find options using the tokenized vocabulary and corpus rather than having to constantly go back to strings.", "upvote_ratio": 1.0, "id": "t3_qo3spl", "created_utc": 1636216064.0}
{"sub": "LanguageTechnology", "title": "Word senses clustering with state-of-the-art models?", "selftext": "Hi everyone\n\nI'm a CS student trying to study and research on a specific  topic for my AI class. I'm literally new to this field but done some searches about the topic.\n\nAs the Header says, I'm trying to semantically cluster polysemous words or word with different meanings in a corpus.\n\nmy input is: a corpus\n\nthe output I want is: clustering of different meanings of K frequent words  with their semantical synonyms; e.g.: suppose word \"cell\" is 1000 time frequent in corpus but with different meanings, like the sentence \" *There are many organelles in a biological* ***cell*** \" the cell here is related semantically to biological stuff or the sentence \" *He went to prison* ***cell*** \" cell here means prison or we mean mobile for cell in \"cell phone\", so we have some clusters of cell with their synonyms.\n\nFinding the K frequent words is kind of preprocessing and can be done easily.\n\nFor the clustering part I searched for related papers, there was a wordnet that seems to be similar!\n\nAlso there are some literature word embeddings like Glove, FastText, Word2vec, Bert, Elmo (which is contextualized and seems to be helpful) that can propose similar vectors, The vectors with the highest percentage of similarity will be selected.\n\nThe thing is most words have multiple senses and as I said explained above each meaning of word is contextualized to the correspondent sentence. I thought that would be cool if we make a BERT vector (e.g. cell as in cell phone) of one of the K frequent words and compare it with other sentences in our corpus. (that's actually my first intuition but not sure about under the hood) so we would have clusters of polysemous words with their semantically similar meanings in a cluster, plus keeping their correspondent sentences as an example for later use.\n\nI'm not sure If this is the right way to do it or not! but I'm asking my question here, to get another intuitions if possible or to know other more accurate and popular techniques in the field.\n\nthanks for your time.\n\nany information would be helpful.", "upvote_ratio": 0.67, "id": "t3_qnxzlt", "created_utc": 1636196536.0}
{"sub": "LanguageTechnology", "title": "Google AI Introduces \u2018GoEmotions\u2019: An NLP Dataset for Fine-Grained Emotion Classification", "selftext": "The emotions one experiences daily can motivate them to act and influence the significant and minor decisions they make in their lives. Therefore, they greatly influence how people socialize and form connections.\u00a0\n\nCommunication helps us to express a vast range of delicate and complicated emotions with only a few words. With recent advancements in NLP, several datasets for language-based emotion categorization have been made accessible. The majority of them focus on specific genres (news headlines, movie subtitles, and even fairy tales) and the six primary emotions (anger, surprise, disgust, joy, fear, and sadness). There is, therefore, a need for a larger-scale dataset covering a greater range of emotions to allow for a broader range of possible future applications.\n\nA recent Google study introduces [GoEmotions](https://arxiv.org/pdf/2005.00547.pdf): a human-annotated dataset of fine-grained emotions with 58k Reddit comments taken from major English-language subreddits and 27 emotion categories identified. It has 12 positive, 11 negatives, 4 ambiguous emotion categories, and 1 \u201cneutral\u201d emotion category, making it broadly useful for conversation interpretation tasks that demand delicate discrimination between emotion displays. They also demonstrate a full tutorial that shows how to use GoEmotions to train a neural model architecture and apply it to recommending emojis based on conversational text.\n\n# [Quick Read](https://www.marktechpost.com/2021/11/05/google-ai-introduces-goemotions-an-nlp-dataset-for-fine-grained-emotion-classification/) | [Paper](https://arxiv.org/pdf/2005.00547.pdf)| [Google Blog](https://ai.googleblog.com/2021/10/goemotions-dataset-for-fine-grained.html)", "upvote_ratio": 0.94, "id": "t3_qnlhbn", "created_utc": 1636148816.0}
{"sub": "LanguageTechnology", "title": "Identify Scenarios/Topics from dataset", "selftext": "Hi Guys!\n\nI have the following use case: I have a Dataset containing roughly 100 sentences which are describing certain components of a  multi component system.\n\nI am interested to identify which sentence is describing which component in this system. I know that I can use a Topic modeling algortihm like LDA to find topics for each sentence in the dataset.\n\nThe problem is, from what I know LDA does not regard context. The difficulty for my specific case, is that there are certain sentences in my dataset that only have semantic value when the context is known. \n\nI think its better to showcase an example to illustrate what I mean lol \n\nLets assume hypothetically that my Dataset contains 100 sentences describing the various components of a Computer, Like CPU, GPU, Motherboard, ect.\n\nand these two sentences are part of the Dataset:\n\n* The GPU is manufactured by ASUS\n* it has 12GB Memory \n\nSo we can see that the first sentence is talking about the component GPU and the algorithm should identify this sentence as GPU Topic, the second sentence is obviously also talking about the GPU if we look at the context (not a problem for us humans), but if we look at the sentence on its own, it would be impossible to say that the algorithm should also classifiy this as GPU topic. So the algorithm should somehow understand that this sentence in its own row in the dataset belongs together with the sentence is the row before inside the dataset and classify it into the same topic GPU.\n\n&amp;#x200B;\n\nSo my question is, what is the best way to solve this issue, apart from manually letting a human look over the dataset and join rows together ?", "upvote_ratio": 0.72, "id": "t3_qnfrw7", "created_utc": 1636132294.0}
{"sub": "LanguageTechnology", "title": "Using NLP way to identify controversial topics?", "selftext": "Hi all,\n\nI\u2019m a psychology researcher, and am interested in the prospect of using nlp and topic modelling to find potential controversial topics in online forums (such as here on Reddit). Would there be any particular techniques in nlp (sentiment analysis etc) that could be used to do this?\n\nThank you in advance.", "upvote_ratio": 0.9, "id": "t3_qn9es3", "created_utc": 1636113037.0}
{"sub": "LanguageTechnology", "title": "Flattening / neutralizing emotion in text", "selftext": "Hi all! I'm working on a research process looking to do entailment for fact checking, and one of the things I want to experiment with is modifying emotional words from text. The models seem to rely too much on emotion to make their classifications, so I want to take that away from them and see how they perform. Some examples might be:\n\n\"I hated the movie, it was terrible. And I loathe the actor.\" --&gt; \"I disliked the movie, it was bad. And I dislike the actor.\"\n\nI imagine it would be a lexicon-based approach, and not as simple at all as that example, but I'm curious if anyone has head of anything along these lines.\n\nThanks!", "upvote_ratio": 1.0, "id": "t3_qmqqsm", "created_utc": 1636049427.0}
{"sub": "LanguageTechnology", "title": "Context and Resources to apply NLP to source code", "selftext": "Hello I am new here and I am a 3rd year data science major looking to work on a personal project regarding applying NLP to identify/classify vulnerabilities in source code (c++, c). Given that I am new to this game, I would be much obliged if more experienced folk could refer me to some resources using code as texts for NLP. I am having trouble finding resources for this myself aside from the odd research paper w/o code :( .", "upvote_ratio": 1.0, "id": "t3_qmne6s", "created_utc": 1636040546.0}
{"sub": "LanguageTechnology", "title": "Multilingual sentence vectors for 50+ languages", "selftext": "Hey everyone, I wrote a pretty long article covering [multilingual sentence transformers](https://www.pinecone.io/learn/multilingual-transformers/), including how to build our own. It's super interesting imo and I focused on something called 'multilingual knowledge distillation' by Nils Reimers and Iryna Gurevych, which has been used to build sentence transformers that work with 50+ languages - which is incredible to me!\n\nIt's really useful for low-resource languages as it just requires translation pairs (like English-to-&lt;insert language here&gt;) and doesn't need that much data either.\n\nAnyway, I hope it's useful - let me know what you think, thanks!", "upvote_ratio": 0.95, "id": "t3_qmlw1a", "created_utc": 1636036352.0}
{"sub": "LanguageTechnology", "title": "Have any one used sentence Bert embedding for sentiment analysis ?", "selftext": "Not sure if it is feasible to use sentence embedding to do few shot sentiment analysis?", "upvote_ratio": 0.75, "id": "t3_qmllgs", "created_utc": 1636035493.0}
{"sub": "LanguageTechnology", "title": "Speech Emotion Recognition", "selftext": "What are some of the state of the art speech emotion recognition architectures/alghorithms?", "upvote_ratio": 1.0, "id": "t3_qmihzw", "created_utc": 1636025296.0}
{"sub": "LanguageTechnology", "title": "Real life needs for NLP", "selftext": "Hello,\n\nI have a question, what are real life example and motivation to make use of NLP services (text summarization, sentiment analysis,etc.)??\n\nIs there a need to NLP tasks in real life financial domain, like for banks, insurance\u2026 and healthcare domain too\n\nThank you", "upvote_ratio": 1.0, "id": "t3_qmgu8s", "created_utc": 1636018172.0}
{"sub": "LanguageTechnology", "title": "What method to use for Out-of-distrubution detection?", "selftext": "I have a stream of log data from users. There are some comments from users that I would like to classify as distinct from others (only 1 class). This seems to me like an OOD problem where 99% percent of the data could be whatever (i.e normal language) and 1% of them belong to a certain class. Has anyone worked on a similar problem or has any good ideas/papers that I should try implementing?", "upvote_ratio": 1.0, "id": "t3_qmgjlz", "created_utc": 1636016771.0}
{"sub": "LanguageTechnology", "title": "HuBERT: How to Apply BERT to Speech, Visually Explained", "selftext": "nan", "upvote_ratio": 0.98, "id": "t3_qmgm0v", "created_utc": 1636017081.0}
{"sub": "LanguageTechnology", "title": "Why one of the features is dominating all rest of the features in my trained SVM?", "selftext": "I have been given a task to train the SVM model on [conll2003 dataset](https://huggingface.co/datasets/conll2003) for Named Entity \"Identification\" (That is I have to tag all tokens in \"Statue of Liberty\" as named entities not as a place, which is the case in named entity recognition.)\n\nInitially, I built a feature `first_letter_caps` which returned `1` if the first letter of the token was capital else `0`. This resulted in the first token of every sentence always getting identified as a named entity. So, I changed it to do that only for non-sentence-start-token and always return `0` for sentence-start-token (that is, for the first word in the sentence). This resulted in the first token of every sentence always getting identified as a NON-named entity. So I realized that somehow I have to \"turn off\" this feature for sentence-start-token and not return a fixed value. So, I made this feature return logical OR of other features (explained in next paragraph) for sentence-start-token, thinking that this will have the effect of turning off this feature and this turned out to be true. And this was quite successful. It stopped \"always\" identifying sentence-start-token as either named entity or non-named-entity.\n\nBut now, I have a few issues. But let me explain other features first. To avoid \"The\" in \"The Federal Bank\" getting identified as a named entity, I built feature `is_stopword` which returns `1` when the token is stopword else return `0`. Also, I have a third feature `contains_number` which returns `1` if the token contained number in it, else returns `0`.\n\nI have trained `sklearn.svm.SVC` with linear kernel. But it never identified tokens containing numbers as named entities. And if stop words had first letter capital (like in \"The\"), it will classify it as named entity. After outputting [`SVC.coef_`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC.coef_) I realized that the issue is that it assigns positive coefficient only to feature `first_letter_caps` and negative or zero coefficient to all other features. When I plotted feature comparison, I realized that it is using only `first_letter_caps` feature for decision making:\n\n [https://i.stack.imgur.com/HVzvrm.png](https://i.stack.imgur.com/HVzvrm.png)\n\nSomehow the feature `first_letter_caps` is dominating all other features and SVM decision boundary. How do I fix this? What I am missing?", "upvote_ratio": 1.0, "id": "t3_qm7n5r", "created_utc": 1635983205.0}
{"sub": "LanguageTechnology", "title": "Stanza not tokenising sentences as expected", "selftext": "Hello.\n\nI am trying to pre-process my text data for a word alignment task.\n\nI have a text file of sentences. Each sentence is on a new line:\n\n    a man in an orange hat starring at something . \n    a boston terrier is running on lush green grass in front of a white fence . \n    a girl in karate uniform breaking a stick with a front kick . \n    five people wearing winter jackets and helmets stand in the snow , with snowmobiles in the background . \n    people are fixing the roof of a house . \n    a man in light colored clothing photographs a group of men wearing dark suits and hats standing around a woman dressed in a strapless gown . \n\nI am using [Stanza](https://stanfordnlp.github.io/stanza/) to tokenise the sentences:\n\n    en_token = []\n    for i, sentence in enumerate(doc_en.sentences):\n        list_of_tokens = [sent.text for sent in sentence.tokens]\n        en_token.append(list_of_tokens)\n\nMy expected output is:\n\n    [[\"a\", \"man\", \"in\", \"an\", \"orange\", \"hat\", \"starring\", \"at\", \"something\", \".\"],  [\"a\", \"boston\", \"terrier\", \"is\", \"running\", \"on\", \"lush\", \"green\", \"grass\", \"in\", \"front\", \"of\", \"a\", \"white\", \"fence\", \".\"],  \n    [\"a\", \"girl\", \"in\", \"karate\", \"uniform\", \"breaking\", \"a\", \"stick\", \"with\", \"a\", \"front\", \"kick\", \".\"], \n    [\"five\", \"people\", \"wearing\", \"winter\", \"jackets\", \"and\", \"helmets\", \"stand\", \"in\", \"the\", \"snow\", \",\", \"with\", \"snowmobiles\", \"in\", \"the\", \"background\", \".\"], \n    [\"people\", \"are\", \"fixing\", \"the\", \"roof\", \"of\", \"a\", \"house\", \".\"], \n    [\"a\", \"man\", \"in\", \"light\", \"colored\", \"clothing\", \"photographs\", \"a\", \"group\", \"of\", \"men\", \"wearing\", \"dark\", \"suits\", \"and\", \"hats\", \"standing\", \"around\", \"a\", \"woman\", \"dressed\", \"in\", \"a\", \"strapless\", \"gown\", \".\"]] \n\nEssentially, a list of lists, with each sentence in its own list and its words tokenised.\n\nHowever, the output that I get is this:\n\n    [[\"a\", \"man\", \"in\", \"an\", \"orange\", \"hat\", \"starring\", \"at\", \"something\", \".\"],  [\"a\", \"boston\", \"terrier\", \"is\", \"running\", \"on\", \"lush\", \"green\", \"grass\", \"in\", \"front\", \"of\", \"a\", \"white\", \"fence\", \".\"],  \n    [\"a\", \"girl\", \"in\", \"karate\", \"uniform\", \"breaking\", \"a\", \"stick\", \"with\", \"a\", \"front\", \"kick\", \".\", \"five\", \"people\", \"wearing\", \"winter\", \"jackets\", \"and\", \"helmets\", \"stand\", \"in\", \"the\", \"snow\", \",\", \"with\", \"snowmobiles\", \"in\", \"the\", \"background\", \".\", \"people\", \"are\", \"fixing\", \"the\", \"roof\", \"of\", \"a\", \"house\", \".\"], \n    [\"a\", \"man\", \"in\", \"light\", \"colored\", \"clothing\", \"photographs\", \"a\", \"group\", \"of\", \"men\", \"wearing\", \"dark\", \"suits\", \"and\", \"hats\", \"standing\", \"around\", \"a\", \"woman\", \"dressed\", \"in\", \"a\", \"strapless\", \"gown\", \".\"]] \n\nStanza appears to be ignoring sentence boundaries in certain instances.\n\nWould anyone know how to remedy this?\n\nSince each sentence begins with a newline character, would it be possible to simply force a new list at every newline character and then perform word tokenisation? If yes, how would I do that?\n\nThank you in advance for any help and advice.", "upvote_ratio": 0.84, "id": "t3_qm6j8c", "created_utc": 1635979968.0}
{"sub": "LanguageTechnology", "title": "Can open-domain QA models handle yes-no questions?", "selftext": "My understanding of open-domain QA is that it receives a question and must retrieve the evidence passage and the appropriate answer within that passage.\n\nCan such models handle yes-no questions? I'm just curious because \"yes\" and \"no\" aren't really things you find in, for example, Wikipedia passages.", "upvote_ratio": 0.95, "id": "t3_qlums5", "created_utc": 1635946629.0}
{"sub": "LanguageTechnology", "title": "Wav2CLIP: Connecting Text, Images, and Audio", "selftext": "nan", "upvote_ratio": 0.83, "id": "t3_qlkgdw", "created_utc": 1635905835.0}
{"sub": "LanguageTechnology", "title": "Tool for normalizing abbreviations?", "selftext": "Hello all, \n\nI need to process a text and I'm looking for a Python tool able to transform abbreviations into their standard forms - for example, from \"I'm\" to \"I am\". I could do it by using regex, but I need to save time. \n\nDoes anyone know if there exist something like this, or at least a list of abbreviations that could be of use? Thank you in advance!", "upvote_ratio": 1.0, "id": "t3_ql97x7", "created_utc": 1635873586.0}
{"sub": "LanguageTechnology", "title": "Scientific Literature Review generation", "selftext": "Hello everyone,\n\nI've developed an algorithm to automatically generate a literature review : [https://www.naimai.fr](https://www.naimai.fr/)  \nHopefully that could be useful for the PhDs (and the non PhDs) !  \n\n\nFor those curious to understand how it works : [https://yaassinekaddi.medium.com/scientific-literature-review-generation-386f36b05eae](https://yaassinekaddi.medium.com/scientific-literature-review-generation-386f36b05eae)\n\nI'll be thankful if you have any remarks about that :)  \n\n\nCheers,", "upvote_ratio": 1.0, "id": "t3_ql2ofp", "created_utc": 1635854601.0}
{"sub": "LanguageTechnology", "title": "Good stopwords list for sentiment analysis", "selftext": "Does anyone know of a good stop words list for sentiment analysis pre processing? \n\n&amp;#x200B;\n\nI'm trying to avoid removing words like 'can't', 'won't', 'no' etc.", "upvote_ratio": 1.0, "id": "t3_ql1m42", "created_utc": 1635850481.0}
{"sub": "LanguageTechnology", "title": "Top 10 Named Entity Recognition (NER) API", "selftext": "nan", "upvote_ratio": 0.8, "id": "t3_ql1813", "created_utc": 1635848797.0}
{"sub": "LanguageTechnology", "title": "WinkNLP, a developer friendly NLP", "selftext": "See how winkNLP processes text as you type it! POS tagging, entity recognition and sentiment analysis all rolled into one simple package!\n\n[https://winkjs.org/showcase-wiz/](https://winkjs.org/showcase-wiz/)", "upvote_ratio": 0.93, "id": "t3_qkxdfz", "created_utc": 1635831261.0}
{"sub": "LanguageTechnology", "title": "Any movie dataset with movie summaries?", "selftext": "Do you know of a dataset that contains movie summaries?\n\nDo you know if researchers are legally allowed to download IMDB movie summaries for research purposes?", "upvote_ratio": 0.76, "id": "t3_qkvojb", "created_utc": 1635824901.0}
{"sub": "LanguageTechnology", "title": "Why do various balancing techniques yield no improvement in NLP tasks?", "selftext": "I have been given a task to train the SVM model on conll2003 dataset for Named Entity \"Identification\" (That is I have to tag all tokens in \"Statue of Liberty\" as named entities not as a place, which is the case in named entity recognition.)\n\nI have built several features and was able to improve the performance. Now the task asks to deal with imbalanced data. I have tried several techniques, oversampling, undersampling, SMOTE and undersampling using nearmiss. But surprisingly, I got exactly the same F1 score as without doing anything to deal with unbalanced data. I felt that I am doing something fishy and had done some stupid mistake. But now I feel that is not the case and I miss some subtle understanding. \n\nCan you please share insight exactly why such balancing techniques have no effect? Also, is it text data or SVM, in the context of which such techniques don't have any effect? Any details / links? \n\nPS: The task specifically asks to use SVM and not any other model.", "upvote_ratio": 0.83, "id": "t3_qkosr8", "created_utc": 1635803561.0}
{"sub": "LanguageTechnology", "title": "How to format input for NLTK IBM alignment models?", "selftext": "Hello.\n\nI have a bunch of parallel data (&gt;2.000.000 characters per language) in English-German and English-French that I need to word-align.\n\nI intend to use NLTK's implementation of the [IBM alignment models](https://www.nltk.org/api/nltk.translate.ibm_model.html).\n\nBased on the [documentation](https://www.nltk.org/api/nltk.translate.ibm1.html), it appears that the module input needs to be two lists of tokenised data. E.g., `([\"I\", \"am\", \"going\", \"to\", \"the\", \"cinema\", \".\"], [\"Je\", \"viens\", \"au\", \"cin\u00e9ma\", \".\"])`\n\nI have text files with about 34,000 lines of parallel sentences for English-German and English-French.\n\nHow can I process them to be able to input them into the module?\n\nIt is easy enough to tokenise the data in one language and place it into a list, but I am not sure how to create two separate lists for the input data.\n\nEssentially what I have is:\n\n    EN.txt = '''The dog jumps.\n    The cow eats.\n    The fish swims.'''  \n    \n    FR.txt = '''Le chien saut.\n    La vache mange.\n    Le poisson nage.''' \n\nAnd what I need to get is:\n\n    ([\"The\", \"dog\", \"jumps\", \".\"], [\"Le\", \"chien\", \"saut\", \".\"]) \n    ([\"The\", \"cow\", \"eats\", \".\"], [\"La\", \"vache\", \"mange\", \".\"]) \n    ([\"The\", \"fish\", \"swims\", \".\"], [\"Le\", \"poisson\", \"nage\", \".\"]) \n\nIf I have not explained myself well enough, please let me know.\n\nThank you in advance for your help.", "upvote_ratio": 1.0, "id": "t3_qkib1j", "created_utc": 1635785783.0}
{"sub": "LanguageTechnology", "title": "Time Complexity of Transformers (and RNNs and ConvNets)", "selftext": "I was watching the guest lecture by the authors of the original Transformers for Stanford's CS224n course on NLP \\[[Link-YouTube](https://youtu.be/5vcj8kSwBCY)\\] in which they talk about how Transformers perform much faster than the traditional RNN and ConvNet models *if the sequence length is orders of magnitude smaller than the model dimension which is usually the case*. They also had this slide on the time complexities of different models \\[[Link-Image](https://ibb.co/2gC4Rzw)\\]. My question is that shouldn't the compute time be independent of sequence length for ConvNets and Transformers since they can be parallelized (while training). And even while testing, can you explain from where did the length^(2) term come for the Transformers? Thanks!", "upvote_ratio": 1.0, "id": "t3_qk7hgj", "created_utc": 1635745483.0}
{"sub": "LanguageTechnology", "title": "How can I balance sentence data for NLP tasks", "selftext": " I have been given a task to train the SVM model on [conll2003 dataset](https://huggingface.co/datasets/conll2003) for Named Entity \"Identification\" (That is I have to tag all tokens in \"Statue of Liberty\" as named entities not as a place, which is the case in named entity recognition.)  \n\nI am building features which involve multiple tokens in sequence to determine whether token at particular position in that sequence is named entity or not. That is, I am building features that use surrounding tokens to determine whether a token is named entity or not. So as you have guessed there is relation between these tokens.\n\nNow the data is very imbalanced. That is there are far more non-named entities than named entities and I wish to fix this. But I cannot simply oversample / undersample tokens randomly as it may result non-sensical sentences due loss of relation between tokens. \n\nI am unable to guess how I can use [other balancing techniques](https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets) like tomek links, SMOTE for such sentence data (that is without making sentences sound meaningless).\n\nSo what are best / preferred techniques to balance such data?", "upvote_ratio": 0.88, "id": "t3_qk0t52", "created_utc": 1635720995.0}
{"sub": "LanguageTechnology", "title": "Best way to store BERT embeddings on AWS?", "selftext": "I'm using sentence-transformers to generate 768 vector embeddings. I was previously saving these in Postgres on RDS as DOUBLE[], which works great. I'm looking to scale MLOps, and Sagemaker tooling seems pretty S3 heavy. I'm also looking to move towards Serverless Aurora, which has a 1mb read limit - so my current psql setup wont do. And also, I'd love all that data pipeline / feature store / step caching functionality built around S3.\n\nLet's say one user has any number of embeddings, realtime read/writing those. I don't think saving each embedding as a single CSV is the way. If I want to read all of a user's embeddings, multiple single CVSs seems wasteful. A big CSV doesn't seem real-time write safe. I'm new to Parquet, and keep seeing it mentioned. Is it pretty real-time friendly? Are there other solutions?", "upvote_ratio": 1.0, "id": "t3_qjyy3n", "created_utc": 1635714986.0}
{"sub": "LanguageTechnology", "title": "Bert embedding NLP", "selftext": "We are working on an NLP project using a Universal Dependencies Tamil Tree Bank. The following is the preprocessed data frame where the column Form is to be word embedded using BERT. Since the column is already tokenized only word embedding is left and all examples we came across are taking raw text data and tokenizing using Bert.\n\nSo I just wanted to know whether a way to word embed the column using Bert was possible.\n\nI have attached a snippet of the preprocessed data in the chat.", "upvote_ratio": 0.92, "id": "t3_qjsaxj", "created_utc": 1635695451.0}
{"sub": "LanguageTechnology", "title": "How can I use features POS tags and chunk ids to train model when the input test sentence wont have them", "selftext": "I have been given a task to train the SVM model on [conll2003 dataset](https://huggingface.co/datasets/conll2003) for Named Entity \"Identification\" (That is I have to tag all tokens in \"Statue of Liberty\" as named entities not as a place, which is the case in named entity recognition.) Conll2003 dataset contains part of speech tags and chunk IDs for each token. We used them to train the SVM model. We can also find models' performance against test and validation datasets as both of them also contain part of speech tags and chunk IDs for each token. But what if someone simply inputs some random sentence (without pos tags and chunk IDs) for predicting (as it is out of test dataset)? How should we handle this? Should we altogether avoid these features while training? Or \"somehow\" generate these features for input sentence before feeding it to the model for prediction? If yes, then how this generation is usually done? Also what is the standard approach?", "upvote_ratio": 1.0, "id": "t3_qjppb9", "created_utc": 1635687569.0}
{"sub": "LanguageTechnology", "title": "[P] \u201cAbstractified Multi-instance Learning (AMIL) for Biomedical Relation Extraction\u201d", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_qjbmll", "created_utc": 1635631220.0}
{"sub": "LanguageTechnology", "title": "Per-sentence readability metrics", "selftext": "Wondering if anyone has come across any text readability metrics that work on a per-sentence basis?  I\u2019ve come across - and used- several that work on a full-text basis, as in telling me the readability of corpus X, but none that will tell me the readability ofeach sentence x in X.", "upvote_ratio": 1.0, "id": "t3_qj81b3", "created_utc": 1635620244.0}
{"sub": "LanguageTechnology", "title": "Suggestions on how to classify paragraphs in fiction books to a set of genres", "selftext": "Hello everyone! I am new to NLP, and am working on a project where we have to classify fiction books either at paragraph or chapter level into a set of genres (we're keeping a set of 5 main labels like 'romance', 'suspense', 'adventure', 'tragedy', 'comedy') and sub-labels within each main label. \n\nWe are using books available from Project Gutenberg, and have some paragraph/chapter breaks ready. However, there are no genre annotations, so based on my background study, I have the following ideas/conclusions:\n\nThis seems like a task between text classification and sentiment analysis. I found that text classification seemed to rely a lot on some special seed/key words which may not be the best approach when trying to understand context in a fiction book. Hence I am leaning towards sentiment analysis methods which take into account context, but we do lack labeled data here. \n\n- For an unsupervised technique, I am thinking to start with LDA, and then try to manually match the outputted topics to our main set of genres. I fear this approach would lack capturing context from the text.\n\n- For an unsupervised technique, I have found a paper 'Contextualized Weak Supervision for Text Classification'. I have to try and see how this will fare.\n\n- I will try to annotate some books in order to try some supervised methods, but want to keep this as a backup option since it would be a monumental task.\n\nDo you think I am headed in the right direction? I would appreciate any and all suggestions! Thank you so much.", "upvote_ratio": 1.0, "id": "t3_qj4kjx", "created_utc": 1635610008.0}
{"sub": "LanguageTechnology", "title": "Building a Grammar Model", "selftext": "I'm learning an inflected language, and I would like to build a grammar model to check self with.\n\nI have a corpus of sentences with grammatical tags (POS, case, conjugation, etc.). I'm specifically looking for something that will check if nouns and verbs are correctly cased/conjugated.\n\nIs there an automated tool that could build syntax trees from the corpus, and then check my sentences against them?", "upvote_ratio": 0.76, "id": "t3_qj0pya", "created_utc": 1635597795.0}
{"sub": "LanguageTechnology", "title": "The Obscenity List - Free Dataset of Profanities", "selftext": "nan", "upvote_ratio": 0.81, "id": "t3_qipnx7", "created_utc": 1635552196.0}
{"sub": "LanguageTechnology", "title": "How to Approach [NLP]: Classification of partial sentences (or words)", "selftext": "How to approach this problem:\n\nSuppose we have partially completed sentences (or words), and their corresponding labels. How to do classification of them.\n\nExample: Suppose we have to predict the category of sentence in App Store or Play Store.\n\n`Text Label`\n\n`\"instagram\" -&gt; social`\n\n`\"inst\" -&gt; social`\n\n`\"whatsapp\" -&gt; communication`\n\n`\"wha\" -&gt; communication`\n\n\"instagram\" is a full word but \"inst\" is a partial word. \"whatsapp\" is a full word but \"wha\" is a partial word.", "upvote_ratio": 1.0, "id": "t3_qiuw40", "created_utc": 1635572153.0}
{"sub": "LanguageTechnology", "title": "How can we assign sentiment score to preprocessed words?", "selftext": "I'm currently implementing a domain based sentiment dictionary. And couldn't find a way to assign sentiment score to the preprocessed words. If anyone could give an advice that would be great.\n\nThank you for your kind replies.", "upvote_ratio": 0.67, "id": "t3_qij7y9", "created_utc": 1635532228.0}
{"sub": "LanguageTechnology", "title": "Apple AI Researchers Propose \u2018Plan-then-Generate\u2019 (PlanGen) Framework To Improve The Controllability Of Neural Data-To-Text Models", "selftext": "In recent years, developments in neural networks have led to the advance of data-to-text generation. However, their inability to control structure can be limiting when applied to real-world applications requiring more specific formatting.\n\nResearchers from Apple and the University of Cambridge propose a novel [Plan-then-Generate (PlanGen)](https://arxiv.org/pdf/2108.13740.pdf) framework to improve the controllability of neural data-to-text models. PlanGen consists of two components: a content planner and a sequence generator. The content planner starts by first predicting the most likely plan that their output will follow. Thereafter, the sequence generator generates results using the data and content plan as input.\n\n# [Quick Read](https://www.marktechpost.com/2021/10/28/apple-ai-researchers-propose-plan-then-generate-plangen-framework-to-improve-the-controllability-of-neural-data-to-text-models/) | [Paper](https://arxiv.org/pdf/2108.13740.pdf) | [Github](https://github.com/yxuansu/plangen) | [Dataset](https://github.com/google-research-datasets/ToTTo)", "upvote_ratio": 0.88, "id": "t3_qhxg5f", "created_utc": 1635457677.0}
{"sub": "LanguageTechnology", "title": "Using Blenderbot w/o ParlAI", "selftext": "Hi all, \n\nI'm really new to the field of NLP and deep learning in general (have never used torch before haha). I wanted to know how one would go about getting Blenderbot to run independent of ParlAI, or at the very least, create a script to run the bot using ParlAI's library. I have managed to download the model files (in .tgz), but am not sure exactly how to go about that task. \n\nI've looked at ParlAI's scripting section but it was not clear to me how to go about incorporating one of their models. Ideally, I'd want to be able to write a method that takes in a string input and produces a string output through blenderbot. If you have any experience in this or advice, it'd be greatly appreciated! Thanks!", "upvote_ratio": 0.75, "id": "t3_qht3vz", "created_utc": 1635445075.0}
{"sub": "LanguageTechnology", "title": "improving seq2seq model", "selftext": "i'm using an encoder decoder seq2seq model for my chatbot and turns out it's not performing very well. (answering 15 questions right out of 20 questions) are there any ways i can improve the performance or accuracy of it?\n\nwhat i can think of right now is the dataset which i have only about 400 questions, but how much data is really enough though ? i read somewhere that increasing the amount of data for those with longer target sequence lengths may help\n\nit may also be because of the number of epochs or the word embedding used, will using glove/word2vec be better than keras' embedding layer?\n\nwhat else could be affecting the performance of the chatbot?", "upvote_ratio": 0.82, "id": "t3_qhp49v", "created_utc": 1635433663.0}
{"sub": "LanguageTechnology", "title": "Deploy TFBert Model with SageMaker for word embeddings inference?", "selftext": "So I have trained a TFBert Model and made a script for getting the word embeddings from the trained model. I conducted the whole process on Google Colab, but now I am trying to move all the things to AWS- that is I can train and deploy the model to an endpoint for further backend functions.\n\nDid anyone use sagemaker before? I did not want to adapt to their training steps (I was using huggingface for the original training), but I just got stuck at wrapping a trained model and deploy that to an endpoint. Would be grateful if anyone can give me some tips on this. Thanks!", "upvote_ratio": 1.0, "id": "t3_qhovkg", "created_utc": 1635432993.0}
{"sub": "LanguageTechnology", "title": "Glove Number of Parameters to Train", "selftext": "Hi guys pretty much in the title, but I want to figure out the number of parameters required to train my GloVE model. I have a vocab size of 95k and an embedding dimension of 100. Any help would be really appreciated :)", "upvote_ratio": 1.0, "id": "t3_qhle7v", "created_utc": 1635422340.0}
{"sub": "LanguageTechnology", "title": "How does dictionary based sentimental analysis work?", "selftext": "How can we combine both machine learning approaches with the sentiment dictionaries for predict the severity level in a text? \n\nIf anyone simplyfy the general workflow it would be really helpful for me.\n\nThank you for your kind replies.", "upvote_ratio": 0.86, "id": "t3_qhhzmh", "created_utc": 1635408120.0}
{"sub": "LanguageTechnology", "title": "Spanish course: Listening Comprehension + SPANISH ONLINE QUIZ", "selftext": "nan", "upvote_ratio": 0.5, "id": "t3_qhhqiu", "created_utc": 1635406987.0}
{"sub": "LanguageTechnology", "title": "NER on non-sentence data", "selftext": "I have data being read from pdf's that is english text, more or less, like equipment details such as model numbers, manufacturer names and a ton of technical descriptive info for electrical installations. I am trying to extract specifically model numbers, manufacturers, etc and have attempted to naively do so through an NER model from spaCy. Prior to the NER model, we have some rule based thing, that does not work very well due to the many formats that this data can come in. Is there some better way of doing NER on non-sentence data - note that I do need labels on a word by word basis, not whole piece of text - than using some pretrained english model? I have tried using 'blank' english spacy models which performs even worse. Are there any ideal architectures in tensorflow or some other frameworks that would work better?", "upvote_ratio": 0.86, "id": "t3_qh9z7r", "created_utc": 1635378866.0}
{"sub": "LanguageTechnology", "title": "What a Cognitive Linguist means by meaning and why it could impact research in #NLProc (an unpretentious unfinished reading list)", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_qgazie", "created_utc": 1635267846.0}
{"sub": "LanguageTechnology", "title": "Looking for partners on a project related to AI and Gender Bias (from a developing country)", "selftext": " Hello everyone,  \nI'm looking for an NLP researcher from a research project related to Bias and Artificial intelligence by The Feminist AI Research Network. The researcher has to be from a developing country. The term in the document is \"Global South\", which is confusing because it does NOT mean the southern hemisphere. It basically means developing countries.  \nMy email is [hashem.elassad@hotmail.com](mailto:hashem.elassad@hotmail.com), I can send you the document from there and my LinkedIn is hashem [https://www.linkedin.com/in/hashemelassad/](https://www.linkedin.com/in/hashemelassad/)", "upvote_ratio": 0.5, "id": "t3_qg8zh9", "created_utc": 1635262315.0}
{"sub": "LanguageTechnology", "title": "Custom sentence embeddings by fine-tuning transformers", "selftext": "Hi all, I put together some videos and articles covering the fine-tuning methods used when creating sentence transformer models, which can be used to create dense vector representations of sentences/paragraphs. It starts with [fine-tuning on NLI data with softmax loss](https://www.pinecone.io/learn/train-sentence-transformers-softmax/), then the more recent, and effective [fine-tuning with multiple negatives ranking loss](https://www.pinecone.io/learn/fine-tune-sentence-transformers-mnr/).\n\nBoth articles and videos look at the PyTorch implementation, then using `sentence-transformers`. It's surprisingly easy to fine-tune, and the results (particularly with the latter approach) are really good. I hope you find it useful!\n\nLet me know if you have any questions etc :)", "upvote_ratio": 0.94, "id": "t3_qg675e", "created_utc": 1635254236.0}
{"sub": "LanguageTechnology", "title": "Recommendation on embedding method", "selftext": "Working on a text classification project, I\u2019ve explored TFIDF and word2vec before for converting text to vectors. Need recommendations on best approach that has worked for you!", "upvote_ratio": 0.67, "id": "t3_qg5s32", "created_utc": 1635252889.0}
{"sub": "LanguageTechnology", "title": "A Comprehensive Comparison of Word Embeddings in Event &amp; Entity Coreference Resolution, (Accepted in Findings of EMNLP 2021)", "selftext": "Hello reddit, this is my first paper which has been accepted at Findings of EMNLP 2021.\n\nWords are made letters that cannot be understood by AI as is. Thus, word embeddings are tools used to encode a vocabulary of words into a mathematical space which allows deep learning models to ingest textual data. To date, many word embeddings methods exist with various characteristics.\n\nHence, this paper studies how the various kind and various combinations of these embeddings perform. Additionally, I found that while there exist various kind of embeddings which have been trained differently, combining them does not greatly improve performance. This has a few consequence such as the fact that word embeddings are better compared when used alone instead of alongside others otherwise their difference in performance is overshadowed by the performance already provided by other embeddings in the system.\n\n[https://arxiv.org/abs/2110.05115](https://arxiv.org/abs/2110.05115)", "upvote_ratio": 0.9, "id": "t3_qg5gul", "created_utc": 1635251823.0}
{"sub": "LanguageTechnology", "title": "How to rate quotes and sentences", "selftext": "Hi,\n\n&amp;#x200B;\n\nI am building a project where we want to return quotes for a user input like \"today was a funny day\".\n\n&amp;#x200B;\n\nHow would you approach to give a rating for the quotes and sentences to actually propose a quote which fits?\n\nRight now we only work with standard sentiment which is not accurate at all.", "upvote_ratio": 1.0, "id": "t3_qfp1xd", "created_utc": 1635192138.0}
{"sub": "LanguageTechnology", "title": "Issues encoding label column for deep learning", "selftext": " Hi, I was wondering if anyone could provide any help? I am carrying out a comparison binary classification of Twitter sentiment model using various models; some sci kit learn ones and a few deep learning / transformer models.\n\nMy models run fine for my transformer models and sci kit learn models, however, my LSTM was producing terrible results.\n\nWhen using get\\_dummies() to encode my label column, it was producing a single dimension array of shape (5825, ). When I changed get dummies to produce a two dimensional (5825,2) so the output is more like \\[0,1\\] , my model began to run well (with a two neuron output instead of one).\n\nIdeally, I'd like to have a single neuron. I've looked online for solutions but can see anyone having a similar issue, could anyone advise at all?", "upvote_ratio": 1.0, "id": "t3_qfmb9t", "created_utc": 1635184292.0}
{"sub": "LanguageTechnology", "title": "How to get a sentiment analysis 'overall score'", "selftext": "Hi!\n\nI'm currently working on an application that essentially runs sentiment analysis on tweets by users, using Microsoft Azure text analytics.\n\nWhenever I send a tweet to the API, the following is returned.\n\nsentiment: (positive, negative or neutral)\n\nand the confidence scores, so e.g.\n\nnegative: 0.03\n\nneutral: 0.01\n\npositive: 0.96\n\nI'm looking to calculate an overall sentiment score, which is essentially an average of all sentiment messages by that user, from 0-100% - 100% being very positive and 0% being negative. \n\nWhat I was thinking of is potentially just having a ranking, so e.g. each message will be ranked by:\n\nPositive = 1\n\nNeutral = 0\n\nNegative = -1\n\nand then just calculating the average, multiple by 100 and then receive a percentage?\n\nAppreciate any advice, Thanks!", "upvote_ratio": 1.0, "id": "t3_qffszl", "created_utc": 1635165542.0}
{"sub": "LanguageTechnology", "title": "Linguistics for the Age of AI (open access)", "selftext": "nan", "upvote_ratio": 0.97, "id": "t3_qfc89u", "created_utc": 1635150903.0}
{"sub": "LanguageTechnology", "title": "Fully funded PhD position in speech tech in the Netherlands", "selftext": "nan", "upvote_ratio": 0.8, "id": "t3_qfbwz8", "created_utc": 1635149319.0}
{"sub": "LanguageTechnology", "title": "Using Huggingface Transformers with ML.NET", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_qfbqpt", "created_utc": 1635148511.0}
{"sub": "LanguageTechnology", "title": "Why is there not much research into flow models for text?", "selftext": "Hello. We've seen a lot of work on text VAEs and text GANs. \n\nI've yet to see a comprehensive exploration into the only remaining one of \"big-three\" generative models: flow-based models. \n\nCould you provide some insight into why flow-based text models are not explored much?", "upvote_ratio": 0.5, "id": "t3_qfaec1", "created_utc": 1635142224.0}
{"sub": "LanguageTechnology", "title": "NLP for Semantic Similarities", "selftext": "Need some guidance and directions. I'm very new to NLP - have used spaCy previously to perform sentiment analysis but nothing more. \n\nMy work recently requires me to build a proof-of-concept model to extract the 10 most occurring concepts in a written essay of an academic nature, and the 10 most related concepts for each of the initial 10. \n\nTo update my knowledge, I've familiarised myself further with spaCy. In doing so, I also came across Hugging Face and transformers. I realised that using contextual word embeddings might be more worthwhile since I am interested in meanings. So, I would like to be able to differentiate between \"river bank\" and \"investment bank\".\n\n1) I would like to ask if Hugging Face will allow me to analyse a document and extract the most occurring concepts in the document, as well as most related concepts in the document given a specified concept. I would prefer to use an appropriate pre-trained model if possible as I don't have sufficient data currently.\n\n2) My approach would be to get the most occurring noun phrases in a document, and then get noun phrases with the most similarities. Is this approach correct or is there something more appropriate?\n\n3) spaCy does not seem to allow you to get words most similar to a specified word unlike Gensim's `word2vec.wv.most_similar`. Is there an equivalent or something in Hugging Face I can use?\n\nWould really appreciate some guidance and directions here for someone new to NLP. Thank you.", "upvote_ratio": 1.0, "id": "t3_qf8paf", "created_utc": 1635135026.0}
{"sub": "LanguageTechnology", "title": "NLP + documenting endangered +/ extinct languages?", "selftext": "I'm really sorry if this is vague, but I wanted to write about NLP used for documenting endangered and/or extinct languages... for anyone experienced in NLP, what would that look like?", "upvote_ratio": 1.0, "id": "t3_qf3uyp", "created_utc": 1635117909.0}
{"sub": "LanguageTechnology", "title": "Multitask Prompted Training Enables Zero-shot Task Generalization (Explained)", "selftext": "nan", "upvote_ratio": 0.67, "id": "t3_qf3j4d", "created_utc": 1635116782.0}
{"sub": "LanguageTechnology", "title": "How does a Chatbot use NLP?", "selftext": "Hey friends, I need some help. So for my final year project for college, I have been tasked with creating a chatbot, that is able to answer a series of questions with relative accuracy. While I do have some average knowledge in python; I wasn't too sure where to begin.\n\n I looked it up on the internet, and most sources tell me I need to implement 'Natural Language Processing'. I was hoping I would be able to get answers regarding what purposes NLP would serve in a chatbot, and how exactly should I go around the implementation.", "upvote_ratio": 1.0, "id": "t3_qeomnf", "created_utc": 1635066905.0}
{"sub": "LanguageTechnology", "title": "How to extract information from documents with structures", "selftext": "So let\u2019s say you have 500 different companies that are your suppliers, and each one of those companies sends you 200 invoices.\n\nNow company A always uses its same invoice structure, and company B also uses its same invoice structure, etc. So each company has their different way of designing their invoices. \n\nBut all of them have common features: list of products, total price, total VAT, etc. \n\nMy objective is to develop on Python (sort of beginner with NLP!) a model that standardises all the information into a structured XML.\n\nAny guidance would really be appreciated :)", "upvote_ratio": 1.0, "id": "t3_qehfvm", "created_utc": 1635036389.0}
{"sub": "LanguageTechnology", "title": "Leveraging Out-of-domain Data to Improve Punctuation Restoration via Text Similarity", "selftext": "nan", "upvote_ratio": 0.83, "id": "t3_qe8ori", "created_utc": 1635008224.0}
{"sub": "LanguageTechnology", "title": "I increased the iterations in gensim LDA and the topics came out worse", "selftext": "I ran an LDA with 100 passes and 1 iteration and the results were pretty much ok. I increased the iterations to 100 thinking it would improve but the coherence decreased and also the topics were more similar to each other (also there was very little difference in computing time between the two runs, 1h44 Vs 1h53). What could be the reason behind this?", "upvote_ratio": 1.0, "id": "t3_qe00hr", "created_utc": 1634973262.0}
{"sub": "LanguageTechnology", "title": "Tool for simple sentence rewriting", "selftext": " Hello!\n\nI am looking to either create or find a tool that can do some simple sentence rewriting. In particular, I'd like to take a handful of 1-sentence descriptions of services (e.g. \"Understanding existing layouts and diagnosing layout issues.\") and make them more consistently phrased / follow a consistent tone(?) - namely I want them all to be action-phrases instead of descriptions.\n\nI'm a Python dev and have done a little bit of NLP a long time ago - I feel like there have to be a handful of either simple NLP libraries that can identify parts of speech which are being used (to help humans do the rewriting) or even better, some ML model like GPT-3 which can just rewrite the sentences following a consistent style.\n\nAny recommendations for libraries, services, or apps which could help would be appreciated! I have a Grammarly Pro subscription - not sure if they have an API or interface which could help with this?", "upvote_ratio": 1.0, "id": "t3_qds2tv", "created_utc": 1634942210.0}
{"sub": "LanguageTechnology", "title": "[Python] Best Python NLP library to segment run-on and list-like sentences", "selftext": "Hi everyone!\n\nI am completely new to NLP and new to Python so I'm feeling a bit overwhelmed by the number of choices at the moment.\n\nI need a library that will allow me to take product titles such as these:\n\n1. 50/100pcs Kraft Paper Bag Gift Bags Packaging Biscuit Candy Food Cookie Bread Seen Snacks Baking Takeaway Bags\n2. Wholesale 2019 New Fashion 3D Mitsubishi Hat Cap Car logo MOTO GP Racing F1 Baseball Cap Hat Adjustable Casual Trucket Hat\n\nand run them through some function that will spit out something like this with added commas:\n\n1. 50/100pcs Kraft Paper Bag, Gift Bags, Packaging, Biscuit, Candy, Food, Cookie, Bread, Seen Snacks, Baking, Takeaway Bags\n2. Wholesale 2019, New Fashion, 3D Mitsubishi Hat, Cap, Car logo, MOTO GP Racing, F1, Baseball Cap, Hat, Adjustable, Casual, Trucket Hat\n\nSo it's very close to segmenting a paragraph into sentences but not quite.\n\nI need something that, ideally, already has a good dictionary and, mandatorily, provides support for both English and Portuguese. The more languages, the better.\n\nWhat do you recommend? What specific functions in the recommended libraries should I look into? I have already checked out spacy and it's dictionary was pretty good. Is it the best option? What specific functions would I use for this? Would I need to create one of my own based on grammar?\n\nThanks a lot!\n\n**EDIT:** Another thing I'd like is a way to detect sections in product titles containing brand and model names. For example:\n\n**Vgate Icar2 Obd2 Scanner ELM327 BT ELM 327 V2.1 Obd 2 Wifi Icar 2** Auto Diagnostic Tool For Android/Pc/Ios Code Reader\n\nThe first part of this sentence, which I bolded for emphasis is basically just the brand and model numbers. Is there a ready-made solution I could use to I automatically detect and segment these, perhaps based on the presence of numbers, abbreviations and unknown words?", "upvote_ratio": 1.0, "id": "t3_qdxcac", "created_utc": 1634961537.0}
{"sub": "LanguageTechnology", "title": "Google Pixel 6 Tensor SoC for developing NLP applications?", "selftext": "The new Google Pixel 6 and Pixel 6 Pro use a new \"Tensor\" SoC that has support for ML. I'm getting one, and since I'm interested in NLP, I was wondering.. would the hardware configuration make a significant difference for using this with NLP applications ? Or possibly even for developing NLP applications on the phone?\n\nThe Pixel 6 has 8GB RAM and 128 or 256GB storage. The Pixel 6 Pro has 12 GB RAM 128/256/512 GB storage. Just wondering if spending the extra $$  for better specs might possibly pay off with ML/NLP, or if it wouldn't make much of a difference.", "upvote_ratio": 1.0, "id": "t3_qdwefu", "created_utc": 1634957845.0}
{"sub": "LanguageTechnology", "title": "more questions on text preprocessing for seq2seq models", "selftext": "i am doing a chatbot and i have a few more questions on text preprocessing for seq2seq models, hoping some of y'all know an answer to these\n\nso i need a large dataset but i'm doing a closed domain one and my current dataset is too small (about 300 questions), how many more questions should i add to make my dataset bigger?\n\nsecondly, what should the threshold value (word occurrences) be? if i were to put it as 5, does that mean i have to add more questions for questions with words that did not appear more than 5 times as they will be removed and a wrong response may be given? \n\nlastly, questions and answers that are too long or too short have to be removed under preprocessing but most of the questions and answers i have in my dataset are very long. \nshould i shorten them or give a bigger max length value in the codes?", "upvote_ratio": 0.76, "id": "t3_qdtp75", "created_utc": 1634947761.0}
{"sub": "LanguageTechnology", "title": "How do I fine-tune zero shot models?", "selftext": "I want to fine tune a deberta model from huggingface .my objective is zero shot text classification as I donot know the classes.\n\nHow do I go about doing this? Would really appreciate some sample code as well.", "upvote_ratio": 0.75, "id": "t3_qdp67x", "created_utc": 1634933404.0}
{"sub": "LanguageTechnology", "title": "NLP Unsupervised", "selftext": "Hi,\nI am working on NLP unsupervised problem.\nProblem statement is to identify the emotions behind each review.\nBut Data is not labelled , I have tried to label it using TextBlob but I am not sure on what should be the Threshold to label the data into Worry,Sad, frustrated,anger etc.\nCan you suggest me any different ways to label it?", "upvote_ratio": 0.75, "id": "t3_qdjlls", "created_utc": 1634917225.0}
{"sub": "LanguageTechnology", "title": "Aprender Aleman: Declinaciones de AKKUSATIV Test Pr\u00e1ctico", "selftext": "nan", "upvote_ratio": 0.29, "id": "t3_qdjdlq", "created_utc": 1634916577.0}
{"sub": "LanguageTechnology", "title": "text preprocessing for seq2seq", "selftext": "i noticed that text cleaning is needed for preprocessing in seq2seq. since i'm doing a chatbot, is it possible to clean ONLY the questions and not the answers? because if the answera are cleaned, it will affect the response given to tbe user (e.g. lost punctuations, no whitespaces etc.)", "upvote_ratio": 0.91, "id": "t3_qdeuqe", "created_utc": 1634901987.0}
{"sub": "LanguageTechnology", "title": "How to improve LDA topics convergence through passes and iterations", "selftext": "Hello! I have a corpus of 33535 documents (SAP community posts about sap cloud platform) made up of  9120 unique tokens. I am trying to extract 15 topics as after running a model with 1 iteration and 25 passes for every number of topics between 10 and 25, 15 had the highest coherence. Until now I have tried various combinations of iterations and passes (up to 100 iterations and 100 passes) but I am still not happy with the quality of the topics as they are still pretty similar and I cannot really understand how topics differ between each other. How could I improve my results?", "upvote_ratio": 1.0, "id": "t3_qcysgm", "created_utc": 1634842730.0}
{"sub": "LanguageTechnology", "title": "Updating / Editing vocab.txt for BERT finetuning", "selftext": "I am using Huggingface transformers for finetuning a simple classification task.\n\n However, I want to update the vocab.txt that comes with standard BERT checkpoint files, with some of the words that are frequent in my training corpus. When I added these words in place of the 'Unusedx' tokens in the 'vocab.txt' , still it was not tokenising the added words.  Can anyone guide me as to with the steps to do it ?", "upvote_ratio": 1.0, "id": "t3_qcuamx", "created_utc": 1634829943.0}
{"sub": "LanguageTechnology", "title": "T5 text-classification on colab", "selftext": "Hi Reddit,\n\nI wrote a [blog](https://pedrormarques.wordpress.com/2021/10/21/fine-tuning-a-t5-text-classification-model-on-colab/) post and tutorial on how to fine tune a T5 model on colab using free tier resources. Hope someone finds it useful.", "upvote_ratio": 1.0, "id": "t3_qcu9l4", "created_utc": 1634829863.0}
{"sub": "LanguageTechnology", "title": "The power of constrained language models.", "selftext": "nan", "upvote_ratio": 0.96, "id": "t3_qcrnq1", "created_utc": 1634822406.0}
{"sub": "LanguageTechnology", "title": "I need help designing a text-to-pictogram system", "selftext": "Hello community.\n\nI got my first job at NLP and I need your help. My task is to design an algorithm that receives text as input in the form of medical indications and outputs a series of pictograms that represent the text (text-to-pictogram system).\n\nI would appreciate any kind of indications, like what kind of task should I solve. Or some route to follow so as not to waste time.\n\nThank you", "upvote_ratio": 0.81, "id": "t3_qcqpd3", "created_utc": 1634819433.0}
{"sub": "LanguageTechnology", "title": "Learn Spanish Online: Spanish Pronouns + SPANISH ONLINE TEST", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_qcfgck", "created_utc": 1634777177.0}
{"sub": "LanguageTechnology", "title": "Illustrated intro to sentence transformers", "selftext": "Another [illustrated guide](https://www.pinecone.io/learn/sentence-embeddings/), this time introducing sentence embeddings with transformers (aka sentence transformers) - an awesome topic I'm excited to write more about, but for now introducing sentence embeddings and transformers, which we can use in cool applications like semantic search or topic modeling.\n\nHope you enjoy, feel free to ask me any questions, give feedback etc - thanks all!", "upvote_ratio": 0.99, "id": "t3_qc6b0c", "created_utc": 1634751341.0}
{"sub": "LanguageTechnology", "title": "[D]Need some perspective on data tagging for NER.", "selftext": "Hello reddit peeps. I am using the common BIO tagging method to tag words in a sentence.   \nI have structured my data in two lists list a contains the sentence that needs to be tagged listA --&gt; \\[text\\] and listB is a list of words contained within the sentence that needs to be tagged listB---&gt;\\[worda, wordb, wordc,....etc\\].   \nNow i have looked for open source solutions but none seem to quite work, so i wrote my own and it works fine for English language but not for Spanish or other languages. (DM will send the gist link)  \nDoes anyone know how to solve this????", "upvote_ratio": 1.0, "id": "t3_qbxiqm", "created_utc": 1634724663.0}
{"sub": "LanguageTechnology", "title": "New to programming - would like to make an android app that counts syllables from natural speech", "selftext": "Hi all, \n\nI am a speech and language pathology student and I would like to make an app that counts a client's syllables. \n\nI am new to programming and I've realized that there is a lot to know in terms of the front end, back end, and natural language processing as well. So my simple app idea is not so simple anymore ( I was naive, maybe still am). \n\nIf you were in my position, how would you tackle this challenge? which coding languages would you learn? \n\nI am not looking to become an employed programmer but more so as a hobby. \n\nThank you and any words of advice would be highly appreciated", "upvote_ratio": 0.78, "id": "t3_qbxcn2", "created_utc": 1634723964.0}
{"sub": "LanguageTechnology", "title": "Help webscraping ACM Library (pull infomration that's not initially on the site)", "selftext": "nan", "upvote_ratio": 0.8, "id": "t3_qbjukd", "created_utc": 1634674342.0}
{"sub": "LanguageTechnology", "title": "Looking for the right framework to implement an enterprise document management and analysis system", "selftext": "I have already spent quite some time with literature reviews and Google searches, but I didn't find anything suitable, yet.\n\nThe task is to implement a flexible and scalable enterprise document management and analysis system. I guess that represents a prototypical use case for many businesses.\n\nThe perfect framework would allow on premises operation (only Azure would be an option) and provide a low-code platform that allows to receive, tag and register documents (PDFs, Word and Excel files, other text files), indexing and smart search within and across documents and document collections, plus an interface to implement NLP tasks with Python.\n\nMoreover, it would be benefitial, if this framework also would allow to model meta data about documents and about the business processes they are embedded in (for example, to check and verify completeness of a set of necessary documents, before further processing gets triggered).\n\nI thought about a combination of Elastic Search and a NoSql Database like Cassandra, but that would not fit the low-code requirement.\n\nYou might call me naive, but I supposed that there ought to be trillions of such frameworks, as this is such a typical use case in terms of business automation. But I did not find the right framework, yet. I hope someone can provide hints.\n\nSummary:\n\nA document management and analysis framework that features:\n\n* Enterprise-ready (on premises or compatible with Microsoft Azure)\n\n* Low-code framework\n\n* Large-scale document management and analysis\n\n* Modular and extensible via Python and NLP models\n\n* Connectable to business logics (i.e. checks for completeness of document collections)\n\n* Allowing for meta data and smart search within and across documents", "upvote_ratio": 1.0, "id": "t3_qbdsje", "created_utc": 1634657047.0}
{"sub": "LanguageTechnology", "title": "Categorising into topics and sub-topics", "selftext": "Hello Reddit!  \n\n\nI am currently starting on my way into NLP topics and am trying to create the following application.  \n1.) I would like to employ python libraries in order to read a document and sort it into one of three PREDETERMINED groups.\n\n2.) I would then like to pull out parts of the text and check if they fall into one of the 6 different PREDETERMINED sub-groups\n\n3.) I would like to extract the section of the sub-text that falls into each group out of the original text\n\nFor example, the document that was uploaded was in the food industry and was talking about vegetables. 1.) Foods 2.)Vegetables 3.) \"these potatoes are good for frying since they have...\"  \n\n\nI was Looking into the LDA topic, but that creates its own groups...  \n\n\nAll ideas and tips are highly appreciated!!!", "upvote_ratio": 0.93, "id": "t3_qbbfdt", "created_utc": 1634649720.0}
{"sub": "LanguageTechnology", "title": "How to create a dataset for training NER models when you only have entity data", "selftext": "We have a list of entities in text files separated with a new line. We intend to train the [flair](https://github.com/flairNLP/flair) model to detect these entities in text, but NER models require the entity to be labeled in a paragraph with BOI format.\n\nOne thing that comes to my mind is to create a random paragraph and inject entities at a random position in the paragraph but I am not sure how will it perform.\n\nCan anyone share their thoughts on this ?", "upvote_ratio": 0.85, "id": "t3_qb5r8u", "created_utc": 1634625548.0}
{"sub": "LanguageTechnology", "title": "word2vec chatbot", "selftext": "should i use a pretrained word2vec model or train the word2vec model using my own corpus? if i were to train a custom word2vec model how do i do it?", "upvote_ratio": 0.5, "id": "t3_qb2dqv", "created_utc": 1634612157.0}
{"sub": "LanguageTechnology", "title": "A New, Cheap, and Accurate Transformer Model", "selftext": " Hey Reddit :)\n\nWe are currently two young entrepreneurs developing a customizable, in-house Transformer model that could reduce standard computation costs for models like GPT-3 by up to 50% without sacrificing quality or accuracy. As this is a largely untapped market, we want to get the community's feedback on how they would use this service or if people would even care.\n\nIt would be much appreciated we could have a discussion around what you'd hypothetically use it for (either by comment or DM). Who knows, we might even throw in some free product keys in the near future.\n\nJust for idea generation purposes: some possible use cases would be:\n\n* Document Classification (e.g. sort research papers by category)\n* Sequence Tagging (e.g. summarization of news articles)\n* Named Entity Recognition (e.g. sorting customer reviews)\n* Question Answering (e.g. chatbots)\n* Natural Language Generation (e.g. automatically creating advertising material)\n* Data Exploration (e.g. automatically analyzing all of a company's contracts)", "upvote_ratio": 0.66, "id": "t3_qapsuf", "created_utc": 1634573321.0}
{"sub": "LanguageTechnology", "title": "BigScience's first paper, T0: Multitask Prompted Training Enables Zero-Shot Task Generalization", "selftext": "The first modeling paper out of BigScience ([https://bigscience.huggingface.co/](https://bigscience.huggingface.co/)) is here!\n\nT0 shows zero-shot task generalization on English natural language prompts, outperforming GPT-3 on many tasks while being 16x smaller!\n\nA very big collection of prompts (\\~2'000 prompts for 170+ datasets) was released ([https://github.com/bigscience-workshop/promptsource](https://github.com/bigscience-workshop/promptsource)) along with the model and the paper.\n\nThis was an international collaborative effort, with over 40 people across more than 25 organizations.\n\nThe group included dedicated researchers and engineers from different universities, companies, and think tanks.\n\nModel: [https://huggingface.co/bigscience/T0pp](https://huggingface.co/bigscience/T0pp)\n\nRepo: [https://github.com/bigscience-workshop/promptsource](https://github.com/bigscience-workshop/promptsource)\n\nPaper: [https://arxiv.org/abs/2110.08207](https://arxiv.org/abs/2110.08207)\n\nAdditionally, the T0 models were released in the Hugging Face Model Hub and you can try it out in your browser here: [https://huggingface.co/bigscience/T0pp](https://t.co/QvEaqkfmgk?amp=1)", "upvote_ratio": 0.96, "id": "t3_qanhik", "created_utc": 1634566333.0}
{"sub": "LanguageTechnology", "title": "Need answer on approach for chatbot development", "selftext": "Working on a side project which requires development of chatbot that can have general conversation with employees and ask survey questions relatable at the point of conversation to get key metrics on employee mental health. Any idea how should I proceed here ?", "upvote_ratio": 1.0, "id": "t3_qak57t", "created_utc": 1634553926.0}
{"sub": "LanguageTechnology", "title": "General Questions about Higher Ed / Jobs in NLP", "selftext": "I have a lot of decisions to make about my career, and have been getting conflicting advice from people in my personal life. My friends and family are some very smart people, but they also do not know much about the specifics of NLP/Comp Ling industry, so I figured it would make sense to ask for some advice from people who know the industry well.  \n\nI graduated this June with a Bachelors in linguistics, and a minor in computer science. I think I have a pretty strong background in linguistic theory, and a decent background in the theory of computer science, but my practical programming skills are pretty rusty (I had to take a voluntary leave from school from 2019-2021 for family illness, and haven\u2019t programmed much in the past few years) and I haven\u2019t had much experience with the actual cutting edge implementations of NLP or Comp Ling. Also, for NLP, I have taken one statistics course, but I think my math/stats background is not so great for getting into the ML aspect of NLP.  \n\nRight now I have two main decisions I need to make:\n\n1. Do I apply for a Masters program in NLP/Comp Ling this cycle (for starting Fall 2022)?\n\nI have seen lots of job postings in the field which want a higher degree, which is not the norm for general programming positions. It seems like having a Masters would be a significant benefit for finding a good job in the field, is this true in your experience? I think it would also have the practical benefit of helping me brush up on my programming skills, and bringing together my theoretical linguistics and CS knowledge and helping me learn how to apply that to actual practical industry problems.  \n\n2. Do I put all my focus on programming practice and interview prep now to land a job in the field as soon as possible, or do I get a part time job in the meantime doing something like being a barista?\n\nGetting a part time job doing something unrelated would obviously reduce my available free time to work towards getting a job in the industry, and there are other considerations that don\u2019t factor into my main question here, such as paying for health insurance, etc. The real thrust of it is the question of whether I should put serious effort into getting a job in the industry during the year before I go to do a Masters, if I do in fact choose to pursue a masters.\n\nSorry for coming in here with such an open ended set of questions, I just don\u2019t know where else to ask people who know about the field. I really appreciate any response, or other information you think might be helpful!", "upvote_ratio": 0.72, "id": "t3_qa3ixg", "created_utc": 1634492321.0}
{"sub": "LanguageTechnology", "title": "I have a problem in Arabic that I have no idea how to start solving.", "selftext": "Hello, so currently I'm looking at a problem that ~~AFAIK has never been done before.~~(See Edit).\n\nIn Arabic we have something called \"Tashkeel\", when you see a word in usual text like: \"\u0639\u0644\u0645\" It's actually lacking vowels. When you add the vowels (Tashkeel) it becomes: \"\u0639\u064e\u0644\u064e\u0645\u064c\". As you might've expected, the problem I'm trying to solve is to add the Tashkeel to a text.\n\nThis however isn't a simple table lookup, and a framework that will learn Tashkeel will implicitly learn most of the Arabic grammar and all its morphemes!\n\nThe Tashkeel depends on two things: Position in the sentence and context of the text.\n\n**Position in the sentence** : If I say \"\u0627\u0644\u0639\u064e\u0644\u0645\u064f \u062c\u0645\u064a\u0644\" (The flag is beautiful). Notice how the Tashkeel of \"\u0627\u0644\u0639\u0644\u0645\" at the end is a small \"\u0648\" (Pronounced Alam**ou**), this is the default Tashkeel i.e: Default form of the word. However, if I say \"\u0648\u0642\u0642\u062a \u0644\u0644\u0639\u064e\u0644\u0645\u0650\" (I stood up for the Flag), notice how the Tashkeel of the same word is now that slant under it (Pronounced: Alam**i**). This is because the \"\u0644\u0640\" is an acting \"letter\" and it acts on the word by slanting it (\"\u062d\u0631\u0641 \u062c\u0631\"), a \"Slanting letter\" if you will. These Letters are the main reason why nouns and verbs change their form. However, \"\u0644\u0640\" here is a slanting letter, but if I say \"\u0644\u0627\u0644\u0634\u0645\u0633\u064f \u0623\u0643\u0628\u0631\" the same letter is now a \"Swearing Letter\" and it has no effect apart from meaning. Form also changes depending on the role in the Verbal Sentence: \"\u0623\u0643\u0644 \u0639\u0645\u0631\u064f \u0627\u0644\u062e\u0628\u0632\u064e\", (Omar has eaten the bread). In this sentence, \"\u0639\u0645\u0631\u064f\"(Omar) is the \"Actor\" (Or \"doer\", idk) of the (Eating), and \"\u0627\u0644\u062e\u0628\u0632\u064e\" is the what the action was done on. \"\u0639\u0645\u0631\u064f\" has that symbol on the end of it because he's the \"Actor\", while \"\u0627\u0644\u062e\u0628\u0632\u064e\" has the Slant because he's the one who the act was done on. And this is not just positioning, inverting Omar and The Bread in the sentence without changing the form DOES NOT change the meaning, and changing the Tashkeel makes the sentence \"Omar was eaten by The Bread\"! \n\n**Context**: If we go back to the original sentence \"\u0627\u0644\u0639\u064e\u0644\u0645\u064f \u062c\u0645\u064a\u0644\" this means (The flag is beautiful), but if I change not the Tashkeel at the end, but the Tashkeel of the \"Stem\" and make it \"\u0627\u0644\u0639\u0650\u0644\u0645\u064f \u062c\u0645\u064a\u0644\" it becomes (Knowledge/Science is beautiful)! The reason behind this is that the whole Stem of the word has changed, this isn't the same morpheme anymore.\n\nSo how am I to undertake this? Just collect data and bash it with a transformer? How to preprocess it?\n\nEDIT: Found an Egyptian AI company that did it. https://rdi-tashkeel.com/ ([English description](https://rdi-eg.ai/wp-content/uploads/2021/02/TASHKEEL-V4-En-final.pdf)) They say it \"uses the most advanced deep-learning neural network algorithms for Tashkeel\u2019s diacritization engine\". Have no idea what that means, could be using CNN's for all I know... It has some issues when I tried it with some serious grammatical exceptions. But overall quite good. They claim 99% Accuracy in modern Arabic and 98% in classical Arabic (text-wise).", "upvote_ratio": 1.0, "id": "t3_qa1ufi", "created_utc": 1634487238.0}
{"sub": "LanguageTechnology", "title": "Allen Institute for AI (AI2) Open-Sources \u2018Macaw\u2019, A Versatile, Generative Question-Answering (QA) System", "selftext": "OpenAI\u2019s GPT-3 system is the best at many tasks, including question answering (QA), but it costs money and can only be used by approved users. While there are other pretrained QA systems out on the market, none has matched its few-shot performance so far.\n\nAs a possible solution to the above problem, a team of researchers from AI2 has just released [**Macaw**](https://arxiv.org/pdf/2109.02593.pdf). This versatile and generative question answering system exhibits strong zero-shot performance on a wide range of questions. The best part of Macaw is that it is publicly available for free.\n\nAccording to a recent study, \u2018Challenge300\u2019 (300 challenge questions), Macaw outperformed GPT-3 by over 10%. This is despite the fact that it is an order of magnitude smaller (11 billion vs. 175 billion parameters). Macaw is an impressive (T5-based) language model with not quite as wide-ranging capabilities, but it\u2019s still better than many other systems.\n\n# [5 Min Quick Read](https://www.marktechpost.com/2021/10/16/allen-institute-for-ai-ai2-open-sources-macaw-a-versatile-generative-question-answering-qa-system/)| [Paper](https://arxiv.org/pdf/2109.02593.pdf) | [Code](https://github.com/allenai/macaw)| [AI2 Blog](https://medium.com/ai2-blog/general-purpose-question-answering-with-macaw-84cd7e3af0f7)", "upvote_ratio": 0.95, "id": "t3_q9qebq", "created_utc": 1634439551.0}
{"sub": "LanguageTechnology", "title": "Question about word to vector proccess", "selftext": "Hello\n\nI am very new to NLP technology, so please excuse my beginner question. So in order to use various techniques like word similarity or even transformers, it looks like the first step is to convert word to vector representation. And in order to do word to vector, it uses a huge text data like Wikipedia and run CBOW, Skipgram or others to get the features from the word. My question is that say that I want to predict a word somewhere in the sentence (like fill mask pipeline in hugginface) for very domain specific text that is not covered by Wikipedia. So my questions are:\n\n1.Since vector is coming from Wikipedia but this data is not really covered, will it give some off/random result?\n\n2. Say if word2vec used very domain specific dataset and I am doing word similarity from completely different domain specific, then would it give some random result as well?\n\n3. Where I am confused is that in order to run transformer mechanisms/others, I need feature representation so would both need to use same type of domain dataset? I know Wikipedia generally covers a lot of domains, but at the same time, bit scared if there is any blind spot and that's where my data mostly came from?\n\nThank you so much for reading and helping this!", "upvote_ratio": 0.67, "id": "t3_q9e55j", "created_utc": 1634397897.0}
{"sub": "LanguageTechnology", "title": "Picking the right tool", "selftext": "I am developing an application to analyze large pdf files that include government forms, and medical records.  I want to be able to identify key values in the forms, and I also want to identify key medical diagnoses, findings, symptoms, test findings, and doctor observations.\n\nI am trying to come up with a framework for determining which software tool would be suit my needs.  I have looked a bit at AWS Comprehend.  I just cranked an MRI through it, and none of the key findings I would want were identified (\"severe stenosis\", \"degenerative arthritis\" etc.).  I of course surely am not going to throw up my hands after one test, but it makes me wonder: is it the right tool? and most specifically, how to go about determining what would be the best tool.  In making this decision, I am not interested in spending buckets of money on a software product.", "upvote_ratio": 1.0, "id": "t3_q9dyqy", "created_utc": 1634397305.0}
{"sub": "LanguageTechnology", "title": "Integer embeddings (LSTM vs GloVE vs BERT) [screencast tutorial]", "selftext": "nan", "upvote_ratio": 0.83, "id": "t3_q9937f", "created_utc": 1634378446.0}
{"sub": "LanguageTechnology", "title": "AutoNLP - by HuggingFace was just announced", "selftext": "https://www.youtube.com/watch?v=y7xEDeK7KVk&amp;ab_channel=HuggingFace\nCheck it out! But also let me know your thoughts!", "upvote_ratio": 0.95, "id": "t3_q98erv", "created_utc": 1634375161.0}
{"sub": "LanguageTechnology", "title": "Offline text-labeling tool", "selftext": "Hi!\nI'm working on a project involving NER and a large amount of unlabeled data. I need to label some documents to create an evaluation set.\nThe problem is that I have to work without an internet connection. I have tried a few labeling-tools, which claimed to be available for offline use, such as doccano, but none worked.\nHas anyone had any experience with a  labeling tool that worked offline?\nThanks", "upvote_ratio": 1.0, "id": "t3_q8vxw9", "created_utc": 1634327143.0}
{"sub": "LanguageTechnology", "title": "Some questions when I read the paper", "selftext": "Has anybody read the paper [Multi-Granularity Interaction Network for Extractive and Abstractive Multi-Document Summarization](https://aclanthology.org/2020.acl-main.556.pdf)?\n\nI am confused on the input of the decoder. What is the g\\^0? (the paragraph is the upper of the equation 9) and Why the objective function use the lambda on the L\\_ext?", "upvote_ratio": 1.0, "id": "t3_q8rx95", "created_utc": 1634314995.0}
{"sub": "LanguageTechnology", "title": "Seeing Voices: 1 - Intro to Spectrograms [Video]", "selftext": "nan", "upvote_ratio": 0.92, "id": "t3_q8mb6d", "created_utc": 1634296682.0}
{"sub": "LanguageTechnology", "title": "Japanese search engine", "selftext": "I want to build a search engine for Japanese. Japanese is difficult because there are no spaces between words, verbs are conjugated to show tense, negation, and politeness. Japanese is also tricky because it uses multiple character systems: two phonetic systems (one for words from Japanese, hiragana, and one for foreign words, katakana) and a symbolic systems (borrowed from Chinese).\n\n&amp;#x200B;\n\nWhat would you need to do first before you could create a tf-idf index that is different from English?", "upvote_ratio": 1.0, "id": "t3_q8c7b6", "created_utc": 1634256646.0}
{"sub": "LanguageTechnology", "title": "Resume section segmentation", "selftext": "Hi, newbie to NLP here. I am trying to build a resume parser to extract structured data from resumes. But before doing extraction, I'm thinking of doing section segmentation. E.g. skills is its own section, then I will just extract from the section with NER then label them as skills. How do I go about this segmentation thingy? Does it involve OCR? TIA.", "upvote_ratio": 0.5, "id": "t3_q8evw7", "created_utc": 1634265823.0}
{"sub": "LanguageTechnology", "title": "Nywspaper: comparing news using transformers", "selftext": "Hello everyone,\n\nI have built [nywspaper](https://nywspaper.com), a news aggregator / reader / comparison tool for my bachelor's thesis, and I am very excited to share it with you here.\n\nThe goal of this tool is to make it easier for the readers to understand media bias in the news, by allowing paragraph by paragraph comparison between news articles covering the same story. When you're reading an article on nywspaper, if you click on a paragraph, you get paragraphs similar to the one you clicked from other publishers. This way you can see how a right wing news publisher delivers the same information differently than a left wing publisher.\n\nIn the main page you can see articles grouped by events, and you can just navigate to the article and begin comparing. There is also a feedback button in the similar paragraph boxes, if you particularly like or dislike a paragraph that was suggested.\n\nI am really looking forward to hearing your thoughts on this tool, and if it could be used to fight media bias. I would also hugely appreciate it if you could have a chance to fill out this [survey](https://www.questionpro.com/t/AT7qPZpHzt) after you use the tool (this would help for the thesis). \n\nThanks!", "upvote_ratio": 0.95, "id": "t3_q87sxl", "created_utc": 1634242441.0}
{"sub": "LanguageTechnology", "title": "We Need to Talk About Data: The Importance of Data Readiness in Natural Language Processing", "selftext": "Hey there,\n\nWe've collected our experiences on teasing out the data readiness of organizations in relation to ML/NLP projects. We describe a method comprised of 15 questions that help stakeholders gauge their data readiness, along with a way to visualize the outcome of applying the method.\n\narXiv: [https://arxiv.org/abs/2110.05464](https://arxiv.org/abs/2110.05464)\n\nAbstract: In this paper, we identify the state of data as being an important reason for failure in applied Natural Language Processing (NLP) projects. We argue that there is a gap between academic research in NLP and its application to problems outside academia, and that this gap is rooted in poor mutual understanding between academic researchers and their non-academic peers who seek to apply research results to their operations. To foster transfer of research results from academia to non-academic settings, and the corresponding influx of requirements back to academia, we propose a method for improving the communication between researchers and external stakeholders regarding the accessibility, validity, and utility of data based on Data Readiness Levels. While still in its infancy, the method has been iterated on and applied in multiple innovation and research projects carried out with stakeholders in both the private and public sectors. Finally, we invite researchers and practitioners to share their experiences, and thus contributing to a body of work aimed at raising awareness of the importance of data readiness for NLP.\n\nAnd the code for the visualizations is here:\n\nGitHub: [https://github.com/fredriko/draviz](https://github.com/fredriko/draviz)\n\nI'll be happy to hear any feedback! :)", "upvote_ratio": 0.9, "id": "t3_q86ouv", "created_utc": 1634239112.0}
{"sub": "LanguageTechnology", "title": "BERT models: how resilient are they to typos?", "selftext": "Hello,\n\nlet me introduce the context briefly: I'm fine tuning a generic BERT model for the context of food and beverage. The final goal is a classification task.\n\nTo train this model, I'm using a corpus of text gathered from blog posts, articles, magazines etc... that cover the topic.\n\nI am however facing a predicament that I don't know how to handle: specifically, there are sometimes words that either contain a typo, or maybe different accents, but that are semantically the same.\n\nLet me give you an example to briefly illustrate what I mean:\n\nThe wine `Gew\u00fcrztraminer` is correctly written with the `\u00fc`, however sometimes you also find it written with just a normal `u`, or some other times even just `Gewurtz`. There are several situations like this one.\n\nNow, a human being would obviously know that we're talking exactly about the same thing, but I have absolutely no idea about how BERT would handle these situations. Would it understand that they're the same thing? Would it consider them instead to be completely different words?\n\nI am currently in the process of cleaning my training data, fixing the typos and trying to even out all these inconsistencies, but at this point I'm not even sure if I should do that at all, considering that the text that will need to be classified can potentially contain typos and situations like the one described above.\n\nWhat would you guys suggest?", "upvote_ratio": 0.97, "id": "t3_q821td", "created_utc": 1634225344.0}
{"sub": "LanguageTechnology", "title": "How the context is stored in context vector in encoder decoder transformer model?", "selftext": "I mean I know that transformer for eg BERT can Understand the context of the paragraph but how the BERT model stored the context. \n\nI understand that word can be put into vector using on hot encoding or any other approach but storing context into vector I don't get it at all.\n\nPlease help.", "upvote_ratio": 1.0, "id": "t3_q7zvj7", "created_utc": 1634218607.0}
{"sub": "LanguageTechnology", "title": "On Self-Service, Data Democratization and (Natural) Language", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_q7yqws", "created_utc": 1634214779.0}
{"sub": "LanguageTechnology", "title": "Dense vectors for NLP (and some vision)", "selftext": "Hi all, I put together an [article and video](https://www.pinecone.io/learn/dense-vector-embeddings-nlp/) on a few of the coolest (and useful of course) embeddings for NLP, and also text-image with OpenAI's CLIP at the end. Planing on diving into each area in more depth in the future!\n\nLet me know what you think, if I'm missing anything or you have any questions!\n\nThanks!", "upvote_ratio": 1.0, "id": "t3_q7y7pn", "created_utc": 1634212770.0}
{"sub": "LanguageTechnology", "title": "Sentiment analysis on software engineering texts", "selftext": "What are the possible ways to improve sentiment dictionaries to analyse  SE texts? There are several SE specific sentiment dictionaries but  cannot expect much accuracy when analysing open-source projects. Thank you", "upvote_ratio": 1.0, "id": "t3_q7ueud", "created_utc": 1634194803.0}
{"sub": "LanguageTechnology", "title": "I have some problems with understading how LSTM can solve Sentiment Analysis.", "selftext": "I already got a grip of how to solve Sentiment Analysis problem ( pre-processing dataset, word embedding, feed word-vectors to LSTM structure and boom, I have a model that can predict a sentence is positive or negative), what I still don't understand is what LSTM layer do with word-vectors. Does it use them to understand the meaning of sentence, if so, how does it do it? Finally, when it understood the meaning, how can it know the sentence is positive or negative?", "upvote_ratio": 0.92, "id": "t3_q7r5mj", "created_utc": 1634181134.0}
{"sub": "LanguageTechnology", "title": "Ways to reduce memory consumption in Q&amp;A tasks without damage (or at least, not that much) the accuracy?", "selftext": "i\u2019m facing this problem: I\u2019m trying to spend less  memory in my Q&amp;A task using bert. I debugged my steps and saw that  the start\\_logits and end\\_logits\n\n&gt;start\\_logits, end\\_logits = model(\\*\\*inputs)  \n \n\ncosts more than 11gb of ram. Is there any ways to solve this? I mean,  use less memory to perform this task without harm my model accuracy? If  so, can someone share some of them? And some alternative ways in case  is not possible to do this?", "upvote_ratio": 1.0, "id": "t3_q7ppn0", "created_utc": 1634175966.0}
{"sub": "LanguageTechnology", "title": "Cambridge Quantum (CQ) Open-Sources \u2018lambeq\u2019: A Python Library For Experimental Quantum Natural Language Processing (QNLP)", "selftext": "[Cambridge Quantum (\u201cCQ\u201d)](https://cambridgequantum.com/) announced the release of the world\u2019s first toolkit and an [open-source library ](https://github.com/CQCL/lambeq)for Quantum Natural Language Processing (QNLP), called [\u2018lambeq\u2019](https://arxiv.org/abs/2110.04236).\n\nSpeaking in simple words, \u2018lambeq\u2019 is the toolkit for QNLP (Quantum Natural Language Processing) to convert sentences into a quantum circuit. It can be used to accelerate development in practical, real-world applications such as automated dialogue systems and text mining, among other things.\n\n\u2018lambeq\u2019 has been released on a fully [open-sourced basis](https://github.com/CQCL/lambeq) for the benefit of all quantum computing researchers and developers. Lambeq seamlessly integrates with CQ\u2019s (Cambridge Quantum) TKET, the world\u2019s leading and fastest-growing quantum software development platform that is also fully open-sourced. The open-sourcing of this technology provides QNLP developers with an even broader range for their work.\n\n# [Quick 3 Min Read](https://www.marktechpost.com/2021/10/13/cambridge-quantum-cq-open-sources-lambeq-a-python-library-for-experimental-quantum-natural-language-processing-qnlp/) | [Paper](https://arxiv.org/abs/2110.04236) | [Github](https://github.com/CQCL/lambeq) | [Documentation](https://cqcl.github.io/lambeq/) |[CQ Blog](https://medium.com/cambridge-quantum-computing/quantum-natural-language-processing-ii-6b6a44b319b2)", "upvote_ratio": 1.0, "id": "t3_q7lfap", "created_utc": 1634161498.0}
{"sub": "LanguageTechnology", "title": "Fresh Machine Translation benchmark study: 29 MT engines, 13 language pairs, 7 domains (Aug 2021)", "selftext": "Fresh Machine Translation benchmark study: 29 MT engines, 13 language pairs, 7 domains (Aug 2021)\n\nHi folks, we've just published our new State of the Machine Translation 2021 report we have prepared together with TAUS https://hubs.la/H0ZbJhN0\n\nEvery year we release an independent multi-domain evaluation of MT engines to help you choose the best-fit providers for your language pair and industry sector. In this year\u2019s edition, we analyzed 29 commercial and open-source MT engines across 13 language pairs and 7 key domains, including Healthcare, Education, Financial, Legal, Hospitality, and General. We also explain what scores to use for MT evaluation.\n\nHappy reading, and please share your questions and ideas afterward!", "upvote_ratio": 1.0, "id": "t3_q7jr8p", "created_utc": 1634156389.0}
{"sub": "LanguageTechnology", "title": "Job opportunities for a fellow linguist?", "selftext": "Hello folks, first time posting here, I bring a potentially different question.\n\nMy girlfriend is a newly graduated linguist applying for a Master's Degree in Linguistics. She doesn't have a computer science or mathematics background.\n\nBut we were looking online and some NLP job openings do seem to exist for linguists and natural language teachers/researchers.\n\nI am a computer scientist with a background in programming languages, and I have some (albeit not deep) knowledge of machine learning.\n\nWe are looking for a way to get her into a more company-oriented career, rather than an academic one as a lecturer. Always good to have options.\n\nWhat are your thoughts on this? Could she potentially land a NLP-related job?  \nHow much of a statistics/machine learning/computer science background would she have to develop?", "upvote_ratio": 1.0, "id": "t3_q7hq6t", "created_utc": 1634150607.0}
{"sub": "LanguageTechnology", "title": "Microsoft and NVIDIA AI Introduces MT-NLG: The Largest and Most Powerful Monolithic Transformer Language NLP Model", "selftext": "Transformer-based language models have made rapid progress in many natural language processing (NLP) applications, thanks to the availability of large datasets, large computation at scale, and advanced algorithms and software to train these models.\n\nThe high-performing language models need many parameters, a lot of data, and a lot of training time to develop a richer, more sophisticated understanding of language. As a result, they generalize well as effective zero\u2013 or few\u2013shot learners on various NLP tasks and datasets with high accuracy.\n\nHowever, training such models is problematic for two reasons:\n\n* The parameters of these models can no longer be fit into the memory of even the most powerful GPU.\n* Special attention is required for optimizing the algorithms, software, and hardware stack as a whole. If proper attention is not provided, the large number of computing operations required can result in unrealistically long training times.\n\nMicrosoft and NVIDIA present the Megatron-Turing Natural Language Generation model (MT-NLG), powered by [DeepSpeed ](https://github.com/microsoft/DeepSpeed)and [Megatron](https://github.com/NVIDIA/Megatron-LM), the largest and robust monolithic transformer language model trained with 530 billion parameters.\n\n# [5 Min-Quick Read](https://www.marktechpost.com/2021/10/13/microsoft-and-nvidia-ai-introduces-mt-nlg-the-largest-and-most-powerful-monolithic-transformer-language-nlp-model/) | [Microsoft Blog](https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/)\n\n&amp;#x200B;", "upvote_ratio": 0.94, "id": "t3_q7fx3x", "created_utc": 1634145444.0}
{"sub": "LanguageTechnology", "title": "Label unstructured data using Enterprise Knowledge Graphs", "selftext": "hi. I have published a new blogpost about entity linking with domain-specific enterprise KGs: [https://revenkoartem.medium.com/label-unstructured-data-using-enterprise-knowledge-graphs-3-ca3cd1b14a36](https://revenkoartem.medium.com/label-unstructured-data-using-enterprise-knowledge-graphs-3-ca3cd1b14a36) there is also a piece of code that allows to train the model and try it out.", "upvote_ratio": 1.0, "id": "t3_q7amqo", "created_utc": 1634130016.0}
{"sub": "LanguageTechnology", "title": "Hello, I'm getting into NLP and wandering if I should start with a project or normal courses.", "selftext": "I need to learn NLP for a position and need help on whether to take a project to learn with or start with a book/course (One that seems interesting is [this](https://web.stanford.edu/%7Ejurafsky/slp3/ed3book_sep212021.pdf).)\n\nBackground: I'm already familiar with DNN's and a bit familiar with CNN's and their architectures. Already know what LSTM is.\n\nA project that I want to do is an Arabic document (mostly books) summarizer.\n\nWhich should I do?", "upvote_ratio": 0.81, "id": "t3_q78mna", "created_utc": 1634122616.0}
{"sub": "LanguageTechnology", "title": "An illustrated tour of wav2vec 2.0", "selftext": "When Transformers started getting popular for NLP, we saw great visualizations to understand better the internals of these models like The Illustrated BERT, GPT...\n\nI haven't seen much like that for speech processing, so I wrote this quick post to illustrate the architecture and pre-training process of wav2vec 2.0 (now part of the HuggingFace library).\n\n[https://jonathanbgn.com/2021/09/30/illustrated-wav2vec-2.html](https://jonathanbgn.com/2021/09/30/illustrated-wav2vec-2.html)\n\nHope this is useful : )", "upvote_ratio": 0.9, "id": "t3_q76fa4", "created_utc": 1634112147.0}
{"sub": "LanguageTechnology", "title": "Machine Translation With Sequence To Sequence Models And Dot Attention Mechanism", "selftext": "nan", "upvote_ratio": 0.8, "id": "t3_q6xxox", "created_utc": 1634079766.0}
{"sub": "LanguageTechnology", "title": "A tutorial on how to create quick NLP Text Generation Using Gradient Workflows and GitHub", "selftext": "nan", "upvote_ratio": 0.75, "id": "t3_q6vlv4", "created_utc": 1634072491.0}
{"sub": "LanguageTechnology", "title": "Sentiment Analysis on Bug reports' description", "selftext": "Does the sentiment analysis on bug reports' description field important to severity prediction?\n\nIf it is, what things can be done to improve the process?\n\nThank you for your kind replies.", "upvote_ratio": 1.0, "id": "t3_q6ta1d", "created_utc": 1634065620.0}
{"sub": "LanguageTechnology", "title": "JAX/Flax speedup on HuggingFace", "selftext": "A friend of mine pointed out the faster compute times of JAX/Flax vs PyTorch testing by HuggingFace [over here](https://github.com/huggingface/transformers/tree/master/examples/flax), maybe I'm just late to the party but they're pretty significant, MLM training for example is 15h32m with JAX/Flax vs 23h46m with PyTorch/XLA\n\nThought it was cool, maybe a good idea to put some time into JAX", "upvote_ratio": 1.0, "id": "t3_q6s7xh", "created_utc": 1634062528.0}
{"sub": "LanguageTechnology", "title": "[Spacy and Yake] 107+ million journal articles, mined: the General Index (4.7 TiB)", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_q6rpa3", "created_utc": 1634061016.0}
{"sub": "LanguageTechnology", "title": "How do I specify a max character length per sentence for summarization using transformers (or something else!)?", "selftext": " Hi there,\n\nI am exploring different summarization models for news articles and am struggling to work out how to limit the number of characters per sentence using huggingface pipelines, or if this is even possible/a silly question to begin with!\n\nI have the following setup when being passed through the article text and model name of \u2018facebook/bart-large-cnn\u2019, \u2018google/pegasus-cnn\\_dailymail\u2019 and \u2018sshleifer/distilbart-cnn-6-6\u2019:\n\nsummarizer = pipeline(\u201csummarization\u201d, model=model\\_name)  \nsummarized = summarizer(article\\_text, max\\_length=118, clean\\_up\\_tokenization\\_spaces=True, truncation = True)\n\nThe articles range in length from 100 words to 1000 words.\n\nI am hoping to cap the number of characters per sentence to 118, a hard cap for my application. When I set max\\_length to 118 they usually are below this limit but can be, say, 220 characters or sometimes just truncate off at the end.\n\nAlternatively, if there are different summarization methods which allow limiting if it is not possible using transformers then would love to hear.\n\nWould be wonderful if someone could let me know what I\u2019m doing wrong!  \nThanks a lot", "upvote_ratio": 0.88, "id": "t3_q6jai3", "created_utc": 1634035133.0}
{"sub": "LanguageTechnology", "title": "GEC Master's Research Proposal: English or Japanese?", "selftext": "I am applying for a Japanese NLP master's program, and I have decided that I am interested in Grammatical Error Correction. My issue is choosing a research topic to list on the application, and particularly what language to work with. Let us assume that the jobs I would apply to in the future will be working with English.\n\nIf I choose to do something in English, it is clearly the largest market and has the most research being done. I could use the latest public resources immediately and there are huge and detailed corpora. However, coming up with research ideas is proving hard for me. It seems that every time I have an idea, I search and find that it has already been done by people far beyond my level, and matters are accelerating if anything. I also feel like anything I could do would be such a drop in the bucket.\n\nOn the other hand, I can do something with Japanese. However, it has a learner population of just a few million, and the native population is actually shrinking. In terms of research, there are much more gap to fill and unexplored paths, but there are fewer tools and corpora available. My Japanese level is N2/B2, so I can survive in the uni and approach text, but I probably won't be the best choice to write authoritative grammatical rules or annotations or anything.\n\nI'm really wavering trying to figure this out. To employers, does it look better to work on the language with fewer resources, since it implies that I can do at least as well in the richer environment of English?       I'm hearing that the majority of Japanese NLP researchers are choosing English, and they could surely do better Japanese work than I can, so that has been worrying me as well.\n\nMy core question is whether Japanese GEC is a reasonable choice for a native English speaker, but I am also open to any GEC research suggestions at all, since I am still just starting off on the proposal.", "upvote_ratio": 0.67, "id": "t3_q6fzfj", "created_utc": 1634020504.0}
{"sub": "LanguageTechnology", "title": "Do any of you know if there is an app that based on your typing can create a list with the words of your vocabulary(and any other useful stats like spelling mistakes,...) ?", "selftext": "nan", "upvote_ratio": 0.8, "id": "t3_q68xob", "created_utc": 1633995627.0}
{"sub": "LanguageTechnology", "title": "How should I engineer features for Named Entity Identification task?", "selftext": "I was working on Named Entity Identification (not recognition) task. In this NLP task, given a sentence, the model has to predict whether each word (aka token) is named entity or not. The dataset used was CONLL2003 dataset.\n\nInitially, I included a feature `first-letter-capital` which was `1` if a token has its first letter capitalized. The model learned to predict the first word of each sentence as a named entity.\n\nSo I removed this feature and added a feature `first-letter-capital-for-non-sentence-start-word`, which was `1` if a word is not the first word of the sentence and has the first letter a capital. This made the model classify the first word of each sentence as a non-named entity.\n\nWhen I kept neither, the model predicted no word as a named entity. Why this might have happened? Can someone share their insight?\n\n**PS:**\n\n* I am using SVM (and I have to solve this problem with SVM only as that's what the task given to me is).\n* I am not using any word embedding!!! Somehow it was taking a lot of execution time with SVM (may be due to 300 dimensions of embeddings). I simply formed some features by parsing sentences / surrounding tokens of the target token   (I know this simply reduces down this task to possibly non NLP simple classification task)\n   * `first-letter-capital-for-non-sentence-start-word` required to check if the target token was the first one in the sentence.\n   * Feature  `first-letter-capital`  does not need to consider surrounding tokens\n* There are other features too (like POS tags etc), but they are not much related here as they don't relate with a capitalization of any letter of the tokens", "upvote_ratio": 0.76, "id": "t3_q68ilm", "created_utc": 1633994320.0}
{"sub": "LanguageTechnology", "title": "Available Filipino / Tagalog Dictionary for LIWC", "selftext": "Hello!\n\nI am trying to extract features from texts using the Linguistic Inquiry and Word Counter. The texts has both English and Filipino / Tagalog as its language. After checking through their documentation and asking them, they mentioned that they only have English and other certain languages except Filipino / Tagalog. But they did mention that we can use custom-made dictionaries to apply it to the text with Filipino / Tagalog language in it. So I would just like to ask if there are any available Filipino / Tagalog dictionary files that we can use for LIWC?\n\nThanks!", "upvote_ratio": 0.91, "id": "t3_q5ymki", "created_utc": 1633966907.0}
{"sub": "LanguageTechnology", "title": "How to compare speed between NLP models", "selftext": "Hey everyone, \n\nHow do you compare the speed between, say, two NLP models? For example comparing one that uses Glove and one that uses Word2Vec?", "upvote_ratio": 1.0, "id": "t3_q5xp76", "created_utc": 1633964339.0}
{"sub": "LanguageTechnology", "title": "Need Help With LDA Topic Modelling", "selftext": "Hey There\n\nI've been playing with LDA for topic modelling recently and been wondering - how do you assess the results of this model not manually? I looked for ways to do it but didn't find many interesting leads. Also - any rule of thumb for setting the number of topics? and any other useful tips you would give to a newbie in this area?\n\nTIA", "upvote_ratio": 1.0, "id": "t3_q5yfhw", "created_utc": 1633966343.0}
{"sub": "LanguageTechnology", "title": "Video Series on How to Create a Virtual Assistant using Python", "selftext": "nan", "upvote_ratio": 0.67, "id": "t3_q5w7wt", "created_utc": 1633960021.0}
{"sub": "LanguageTechnology", "title": "Preparing data for training NER models", "selftext": "Training most of the Named Entity Recognition (NER) models for example [Flair](https://github.com/flairNLP/flair) usually needs to format data in [BOI tagging](https://en.wikipedia.org/wiki/Inside-outside-beginning_(tagging)) scheme as shown below where each sentence is separated by blank line\n\n    George N B-PER\n    Washington N I-PER\n    went V O\n    to P O\n    Washington N B-LOC\n    \n    Sam N B-PER\n    Houston N I-PER\n    stayed V O\n    home N O\n\nBut instead of labeled text data we have entity data separated by newline in text files, so if we process the data in above format it will look something like as below which only contains entity information\n\n    George N B-PER\n    Washington N I-PER\n    \n    Washington N B-LOC\n    \n    Sam N B-PER\n    Houston N I-PER\n\nIs it ok if processed data looks as above", "upvote_ratio": 0.67, "id": "t3_q5ryjv", "created_utc": 1633943898.0}
{"sub": "LanguageTechnology", "title": "Keyphrase extraction tools for non-english languages", "selftext": "Hey, people! Hope y'all are doing fine!\n\n**TLDR: Please share fine key phrase extraction tools for Portuguese, Spanish and English**\n\nI've been trying to find a nice key-phrase extraction tool for Portuguese, Spanish and English. However, it hasn't been a trivial task since most tools I've found require some effort for handling non-English languages and also because comparison between these tools isn't that feasible. Also some papers I've found provide no or ill maintained code, making its usage difficult. So it isn't that simple at all finding fine tools for this task.", "upvote_ratio": 1.0, "id": "t3_q5ibxw", "created_utc": 1633905038.0}
{"sub": "LanguageTechnology", "title": "Findings of EMNLP 2021 Poster Presentation?", "selftext": "Did anyone else accepted to Findings of EMNLP 2021 receive an email from PC EMNLP-2021 asking if you wanted to present a poster at EMNLP 2021? If you filled out the attached form, have you heard any details back? Hoping I'll hear back from them soon so I can plan travel.", "upvote_ratio": 1.0, "id": "t3_q5epba", "created_utc": 1633893522.0}
{"sub": "LanguageTechnology", "title": "using tf-idf vectorizer with JSON file", "selftext": "initially was using bow model instead of tf-idf, the input was a json file (dictionary). i found many online using just a corpus to do tf-idf but not a json file, does anyone know how to do it with json file? my json dataset is quite large", "upvote_ratio": 0.5, "id": "t3_q4xdvc", "created_utc": 1633825127.0}
{"sub": "LanguageTechnology", "title": "When should you train a custom tokenizer/language model?", "selftext": "I am trying to better understand when you should train a custom tokenizer and language model for your dataset. My go-to is spaCy and prodigy, but I realize there are limitations. Training a RoBERTa model or something similar with HuggingFace seems like the MLM could give you some advantages over what I would get with spaCy models plus prodigy Active Learning, just given the robustness of the model learning the domain context. My primary cases are NER &amp; text classification. Any suggestions or tips would be greatly appreciated.", "upvote_ratio": 1.0, "id": "t3_q4fau3", "created_utc": 1633759170.0}
{"sub": "LanguageTechnology", "title": "Training NER models for detecting custom entities", "selftext": "Hello everyone, we are working on a task to detect certain `custom entities` in the text, we tried training [sPacy](https://spacy.io/) but it's not converging\n\nCan anyone suggest some other `Named Entity Recognition (NER)` models which can be trained to detect custom entities", "upvote_ratio": 0.79, "id": "t3_q4f6qw", "created_utc": 1633758588.0}
{"sub": "LanguageTechnology", "title": "Google AI Introduces \u2018FLAN\u2019: An Instruction-Tuned Generalizable Language (NLP) Model To Perform Zero-Shot Tasks", "selftext": "To generate meaningful text, a machine learning model needs a lot of knowledge about the world and should have the ability to abstract them. While language models that have been trained to accomplish this are becoming increasingly capable of acquiring this knowledge automatically as they grow, it is unclear how to unlock this knowledge and apply it to specific real-world activities.\n\nFine-tuning is one well-established method for doing so. It involves training a pretrained model like BERT or T5 on a labeled dataset to adjust it to a downstream job. However, it has a large number of training instances and stored model weights for each downstream job, which is not always feasible, especially for large models.\n\nA recent Google study looks into a simple technique known as instruction fine-tuning, sometimes known as instruction tuning. This entails fine-tuning a model to make it more receptive to performing NLP (Natural language processing) tasks in general rather than a specific task.\u00a0\n\n# [Google AI Blog](https://ai.googleblog.com/2021/10/introducing-flan-more-generalizable.html) | [5 Min Read](https://www.marktechpost.com/2021/10/08/google-ai-introduces-flan-an-instruction-tuned-generalizable-language-nlp-model-to-perform-zero-shot-tasks/) | [Paper](https://arxiv.org/pdf/2109.01652.pdf) | [Github](https://github.com/google-research/flan)", "upvote_ratio": 0.94, "id": "t3_q4btiq", "created_utc": 1633744192.0}
{"sub": "LanguageTechnology", "title": "Comparative study of extractive summarization", "selftext": "Hello,\n\nI've been looking for a comparative study for a while that shows the characteristics of each model of State of art in a table and I didn\u2019t find, who can help me?\n\nExample: BERT is has bi-directional encoder \u2714\ufe0f multilingual \u2714\ufe0f used for summarization \u2714\ufe0f and other features that distinct it from other models ..\n\nThank you", "upvote_ratio": 0.57, "id": "t3_q4a9ak", "created_utc": 1633738365.0}
{"sub": "LanguageTechnology", "title": "Any allennlp users in this sub?", "selftext": "I have a whole host of questions that the official allennlp docs are unclear on - too many to post individually here - but no one to answer them.\n\nIf there are any allennlp users in this sub who wouldn't mind discussing them with me one-on-one, I would appreciate it tremendously. Apologies for the nebulous post, but thank you in advance!", "upvote_ratio": 1.0, "id": "t3_q43qjg", "created_utc": 1633717745.0}
{"sub": "LanguageTechnology", "title": "BART: Denoising Sequence-to-Sequence Pre-training for NLG &amp; Translation (Explained)", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_q40avj", "created_utc": 1633707625.0}
{"sub": "LanguageTechnology", "title": "Introduction to Natural Language Processing (blog)", "selftext": "Towards Data Science: [https://towardsdatascience.com/introduction-to-natural-language-processing-nlp-323cc007df3d](https://towardsdatascience.com/introduction-to-natural-language-processing-nlp-323cc007df3d)\n\nKDnuggets: [https://www.kdnuggets.com/2019/10/introduction-natural-language-processing.html](https://www.kdnuggets.com/2019/10/introduction-natural-language-processing.html)\n\n&amp;#x200B;\n\nFeedback is welcome!", "upvote_ratio": 0.86, "id": "t3_q3yrby", "created_utc": 1633702949.0}
{"sub": "LanguageTechnology", "title": "Objectives of NLP, NLU &amp; NLG", "selftext": "I read on a blog about NLP the following:\n\n\\- NLU: reads data and converts it to structured data.\n\n\\- NLP:  NLP converts unstructured data to structured data. \n\n\\- NLG:   NLG writes structured data. \n\nIsn't the NLG part false? Shouldn't it be: \"Converts structured data to natural language\"\n\nSource: [https://www.xenonstack.com/blog/difference-between-nlp-nlu-nlg](https://www.xenonstack.com/blog/difference-between-nlp-nlu-nlg)", "upvote_ratio": 0.5, "id": "t3_q3y7nm", "created_utc": 1633701196.0}
{"sub": "LanguageTechnology", "title": "NLP Conferences with a decent industry track?", "selftext": "I just got back from RecSys2021 and was surprised in a good way by the industry presentations. Being mostly a NLP guy - but one who hasn't attended a NLP conference for years, I couldn't stop wondering if any of 'ours' have a similar focus. Are there any good conferences that mix academia with industry?", "upvote_ratio": 1.0, "id": "t3_q3xo05", "created_utc": 1633699356.0}
{"sub": "LanguageTechnology", "title": "Using CLIP to get sentence/description from image", "selftext": "I want to use CLIP to generate a sentence by inputting an image. \n\nI've worked with a lot of implementations where the opposite is done. But I'm not very acquainted with modern text generation models. I'm guessing the principle is similar: optimise the latent vector that CLIP gives you and generate text using this latent vector, convert back into CLIP's latent space again and calculate the loss using the CLIP latent of the image. \n\nAny suggestions on which model I should use for this? Preferably one that I can run on a 3090.", "upvote_ratio": 1.0, "id": "t3_q3xmt6", "created_utc": 1633699244.0}
{"sub": "LanguageTechnology", "title": "Removing whitespace between characters", "selftext": "Any NLP algorithm that removes extra whitespaces in between characters in a word (not in between words)?\n\nExample: \"How m uc h is it?\" should be interpreted as \"How much is it?\" instead of \"Howmuchisit\"\n\ncodes:\n\n    tokens = [lemmatizer.lemmatize(word.lower()) for word in nltk.word_tokenize(text) if word not in ignore_words] \n\n\nappreciate anyone's help!", "upvote_ratio": 0.86, "id": "t3_q3v092", "created_utc": 1633688964.0}
{"sub": "LanguageTechnology", "title": "How to approach Jurafsky &amp; Martin for learning NLP?", "selftext": "I'm looking to get a good overview/review of NLP in preparation for grad school. I was looking at the PhD programs I'm interested in, and quite a few of them list the Jurafsky &amp; Martin textbook as requisite knowledge for their qualifying examinations. I've read portions of the book for classes in undergrad, but I'm not familiar with all of the topics covered, and I'd also like to review the topics I'm more familiar with. However, the book is quite long and seems tedious to read from cover to cover.\n\nIf I'm more of a visual learner, do [Jurafsky's NLP lectures from Stanford](https://www.youtube.com/playlist?list=PLLssT5z_DsK8HbD2sPcUIDfQ7zmBarMYv) cover the topics from the textbook well enough? Or is there another way to approach learning from the textbook (or a better way to learn core topics in NLP altogether)?", "upvote_ratio": 0.92, "id": "t3_q3t9hx", "created_utc": 1633680230.0}
{"sub": "LanguageTechnology", "title": "LDA model returns same words in all the topics", "selftext": "I'm running an LDA model with 14k unique tokens from 33k documents. The documents are questions and answers from a technical community and are rather short and focused on the same macro topic (SAP cloud Platform).\n\nI decided to extract 25 topics as I clustered the tags assigned to the original questions in groups and it seemed logical to divide them in 25 groups.\n\nI've run the model with 100 passes and 100 iterations for 7 hours but at the end I am still returned a model in which the topics are defined mostly by the same words and don't show significant differences. What could I do to improve my results?", "upvote_ratio": 1.0, "id": "t3_q3tf7k", "created_utc": 1633681031.0}
{"sub": "LanguageTechnology", "title": "Looking for a table to text codebase", "selftext": "Hi, I am trying to implement a table to text summarizer for pharma tables. I am looking for existing codebase which can help me jumpstart the project. Any suggestions? I tried looking for them (papers that use ToTTo, WebNLG etc) but most of them do not have complete code. Thanks!", "upvote_ratio": 1.0, "id": "t3_q3ihgi", "created_utc": 1633640261.0}
{"sub": "LanguageTechnology", "title": "Allennlp: What in the frig is a Predictor?", "selftext": "Title says it all. I know there is [a tutorial](https://guide.allennlp.org/training-and-prediction#4), and this description in [the docs](https://docs.allennlp.org/v2.7.0/api/predictors/predictor/):\n\n&gt; a `Predictor` is a thin wrapper around an AllenNLP model that handles JSON -&gt; JSON predictions that can be used for serving models through the web API or making predictions in bulk.\n\nBut I dunno, I just don't get it. I had initially thought a `Predictor` was, intuitively, the \"glue\" needed on the backend to link up a `Model` and a `DatasetReader` and have them share information, but I'm able to train a model using `allennlp train` + a config without so much as (knowingly) touching a `Predictor`. This finding only heightened my confusion about what a `Predictor` is and why I should care about it.\n\nIf there are any allennlp users here, can you help me understand the purpose of this component of the pipeline, and how I should use it? Thanks!", "upvote_ratio": 0.6, "id": "t3_q3dtez", "created_utc": 1633626910.0}
{"sub": "LanguageTechnology", "title": "Just finished my first proper NLP project", "selftext": "Today I launched my first ever twitter bot  [AAPLinsight](https://twitter.com/AAPLinsights) that focus on providing sentiment scores on $AAPL. I broke down my approaches in three categories: Apple Products, Company News and Social Media. These sentiment scores come from around 20 different sources in the web. The base model that I used was BERT and I added some additional layers to create a sentiment classiifer that specialises in financial news sentiment. Although it may be quite a simple project, I think it is quite cool and thank you for the subreddit for all the advices!", "upvote_ratio": 0.9, "id": "t3_q3c95t", "created_utc": 1633622522.0}
{"sub": "LanguageTechnology", "title": "T-V Distinction Classifier", "selftext": "Hi all,\n\n&amp;#x200B;\n\nA bit of a shot in the dark, but I was wondering if there were any available tools to detect if a sentence in Spanish (or any language with this distinction) is using the formal or informal form of \"you\" through the T-V distinction?\n\nWhile one can make a naive baseline by explicitly checking for \"tu\" or \"usted\" in Spanish, this wouldn't capture word conjugations or the likes.", "upvote_ratio": 1.0, "id": "t3_q3b4qv", "created_utc": 1633619288.0}
{"sub": "LanguageTechnology", "title": "Styleformer performance. Or anything that turns informal to formal.", "selftext": "Hi, everyone.\n\nI have been playing around with Styleformer today and am wondering about performance. I'm unsure if this is the right place to ask.\n\nhttps://github.com/PrithivirajDamodaran/Styleformer\n\nI set up a basic Flask server so it would be loaded into ram and each query takes around two seconds on my laptop. What sort of server would be required to make this decently fast? Is it something I'd use DigitalOcean for, or are there better options?\n\n\n\nSorry if this question is far too basic. It's my first day using Python and this sort of thing. I love the output of Styleformer and would rather use it than an API.\n\n\nCheers.", "upvote_ratio": 0.81, "id": "t3_q37xf3", "created_utc": 1633609432.0}
{"sub": "LanguageTechnology", "title": "Is Debatepedia website/dataset non-existent?", "selftext": "Hi all,\n\nThe other day, I was looking at a paper DDA (Diversity Driven Attention) Model.\n\nhttps://arxiv.org/abs/1704.08300\n\nThey scraped data from the Debatepedia website for the purpose of Query-Focused Abstractive Text summarization.\n\nHowever the links provided (in the bash script for scraping data from Debatepedia) are not accessible. I.e. I cannot access Debatepedia.\n\nhttps://github.com/PrekshaNema25/DiverstiyBasedAttentionMechanism\n\n\nDoes anyone know how I can access Debatepedia?\n\nThanks.", "upvote_ratio": 1.0, "id": "t3_q2tpel", "created_utc": 1633554929.0}
{"sub": "LanguageTechnology", "title": "Best Cleaning Models or Processes", "selftext": "Hello everyone, \n\nHappy wonderful Wednesday! I wanted to quickly ask the community about their favorite cleaning model or process. Prior to running analysis, as we all know very well the data gathering phase will always result in a ton of noise, how do you reduce this in the quickest and most accurate fashion?\n\n\\- Do you build a pipe of specific cleaning stages (dedup, irrelevant language, terms used, normalize, remove stop words, lemmatize etc.)\n\n\\- Have you built a model to remove posts and clean the data? How did you trained said model? How big was your training dataset? What steps did you take to validate or verify it's quality? \n\n\\- Other processes?\n\nI appreciate any and all comments, have an awesome day!\n\nAll the best, \n\nN", "upvote_ratio": 1.0, "id": "t3_q2oors", "created_utc": 1633539757.0}
{"sub": "LanguageTechnology", "title": "What really is perplexity, and why is it important for model evaluation?", "selftext": "I know that its uncertainty, but how is it any different from entropy? \n\nSuper confused and fairly new to NLP, so would love any easy-to-understand explanations! Thanks!", "upvote_ratio": 0.88, "id": "t3_q2bzev", "created_utc": 1633490159.0}
{"sub": "LanguageTechnology", "title": "Probing Language Model with WIKI-PSE: looking for implementation details", "selftext": "Hi all, \n\nI found this [https://github.com/yyaghoobzadeh/WIKI-PSE](https://github.com/yyaghoobzadeh/WIKI-PSE), related to the work [https://arxiv.org/pdf/1906.03608.pdf](https://arxiv.org/pdf/1906.03608.pdf), and I am looking for any implementation or details. \n\n  \nFor those who want to read or are familiar with the article:\n\nI would like to try to implement the 34 MLPs (one for each as described in section 3) but I can't figure out what the input is for each MLP. Also, wanting to probe BERT, I found this other work [https://arxiv.org/abs/2004.12198](https://arxiv.org/abs/2004.12198). But I can't figure out the implementation structure.  \n\n\nThanks to anyone who may be interested :D", "upvote_ratio": 1.0, "id": "t3_q29bdq", "created_utc": 1633480066.0}
{"sub": "LanguageTechnology", "title": "Locate handwriting in mixed text document", "selftext": "Hi all!\n\nI currently have a project to OCR mixed text documents. Tesseract is fine for machine text but struggles for handwriting.\n\n\nI am looking for a method to only recognise sections with handwriting so it can be shipped off to a Vision API. Does anyone know any low computational methods to do this?\n\nOne thought is to use the confidence output from tesseract to filter out bad segments to ship.\n\nThanks", "upvote_ratio": 0.67, "id": "t3_q28u5u", "created_utc": 1633478365.0}
{"sub": "LanguageTechnology", "title": "New to Python and NLP but have to work on a basic NLP project at work (classification of text into a topic)", "selftext": "I do know very basic python(syntax not programming concepts) but that's about it.\nCan someone please help me where to begin?\nShould I go about learning Python first , get myself a course? I really want to do good at work and hence thought I could ask here for advice.", "upvote_ratio": 1.0, "id": "t3_q23ife", "created_utc": 1633461734.0}
{"sub": "LanguageTechnology", "title": "Your experience with referrals in the industry.", "selftext": "I'm currently ending my PhD and looking into different options for a job and I'm trying to understand the role of referrals better. Some companies pay as much as 10k for successful referrals so I'm curious about the experience you have had with referrals in the past. \n\nHave you referred friends for positions? Why yes/no? Is it weird to ask out for a referral and vice versa, have you ever asked somebody if you can refer them for a position out of the blue?", "upvote_ratio": 1.0, "id": "t3_q2108w", "created_utc": 1633454118.0}
{"sub": "LanguageTechnology", "title": "Hot off the press! Exploring NLP Part 2: A New Way to Measure the Quality of Synthetic Text", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_q1yyld", "created_utc": 1633448177.0}
{"sub": "LanguageTechnology", "title": "Free 'course' on vector similarity search and Faiss!", "selftext": "Hi all, I've been working with [Pinecone](https://www.pinecone.io) for the last few months on putting together a big set of articles and videos covering many of the **essentials behind vector similarity search**, and how to apply what we learn using **Faiss** (and sometimes even plain Python). Today we released the final (for now) article on HNSW.\n\nWith that, I wanted to share a *'course guide'* with you all, every link below takes you to the article, and in each article, we included one or two videos too - you can read and watch in whichever order you like, but we think this makes the most sense!\n\n# Course Guide\n\n## Part 1: Introduction\n\n1. [Semantic Search: Measuring Meaning From Jaccard to Bert](https://www.pinecone.io/learn/semantic-search/)\n\n2. [Getting Started with Faiss](https://www.pinecone.io/learn/faiss-tutorial/)\n\n3. [Nearest Neighbor Indexes for Similarity Search](https://www.pinecone.io/learn/vector-indexes/)\n\n## Part 2: Algorithm Deep Dives\n\n4. [Traditional Locality Sensitive Hashing (LSH)](https://www.pinecone.io/learn/locality-sensitive-hashing/)\n\n5. [Random Projection for LSH](https://www.pinecone.io/learn/locality-sensitive-hashing-random-projection/)\n\n6. [Compression with Product Quantization](https://www.pinecone.io/learn/product-quantization/)\n\n7. [Hierarchical Navigable Small Worlds (HNSW) Graphs](https://www.pinecone.io/learn/hnsw/)\n\n## Part 3: More Advanced Index Concepts\n\n8. [Filtering: The Missing WHERE Clause in Vector Search](https://www.pinecone.io/learn/vector-search-filtering/)\n\n9. [Composite Indexes: Facebook AI and the Index Factory](https://www.pinecone.io/learn/composite-indexes/)\n\nWe've written and recorded *a lot* of content, hopefully, you'll find vector search as fascinating as I do :)", "upvote_ratio": 0.98, "id": "t3_q1xky0", "created_utc": 1633444267.0}
{"sub": "LanguageTechnology", "title": "Identifying medical information in text?", "selftext": "I have access to a large dataset of medical texts - notes from doctors about patients etc. - and was wondering if there is a way to take one such text and automatically create tags for it.\n\nLet's say the text describes the condition of a patient with Covid, then the algorithm would look over the text and filter out terms like \"cough\" \"covid-19\" \"high temperature\" etc. \n\nI guess what I am looking for is a dataset of medical terms I could use on the texts.\n\nIf I want to train an ML model for this, focusing on one disease for the beginning, what would be a good amount of training data? I could tag a bunch of texts myself and just provide this as training data.\n\nObviously, I'm pretty new to the whole field, so links to similar projects or papers would be great too.", "upvote_ratio": 0.83, "id": "t3_q1tmtb", "created_utc": 1633429845.0}
{"sub": "LanguageTechnology", "title": "German POS Corpus for Commercial use", "selftext": "I'm trying to find a German corpus with POS tags that can be used for commercial purposes. I know about the TIGER corpus for which you could get a commercial license at leat in theory... however they haven't responded in months. Is there any alternative?", "upvote_ratio": 1.0, "id": "t3_q1t4ur", "created_utc": 1633427630.0}
{"sub": "LanguageTechnology", "title": "Phone interview for Language Engineer job at Amazon", "selftext": "I have one coming up soon and have no idea how to prepare for it or what kind of questions I should expect. I tried to search reddit and only found posts about onsite interviews. If anyone could share their experience I'd be very grateful.\n\nNot sure if important, but the job is not language-specific afaik. I was told earlier that I will be interviewed by 2 people but in the most recent email, the recruiter says \"interviewer\" singular, so not sure anymore.", "upvote_ratio": 0.88, "id": "t3_q1kfk5", "created_utc": 1633397621.0}
{"sub": "LanguageTechnology", "title": "Groningen Master in Voice Technology", "selftext": "[https://www.rug.nl/masters/voice-technology/](https://www.rug.nl/masters/voice-technology/)\n\nHey guys, anyone doing the new MSc in Voice Technology at the University of Groningen?\n\nIt sounds quite interesting and they seem to accept students from a very diverse background, different to many CL Master's. It doesn't seem to be very NLP-focused though, which might be a bummer for many people on this sub.\n\nAnyway, apparently, the degree only started this fall for the first time ever, and there's little information on the actual contents So, if anyone's doing it, I would love to know what it's like!", "upvote_ratio": 1.0, "id": "t3_q1ide0", "created_utc": 1633390772.0}
{"sub": "LanguageTechnology", "title": "Small-Bench NLP: Benchmark for small single GPU trained models in Natural Language Processing", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_q1dain", "created_utc": 1633376557.0}
{"sub": "LanguageTechnology", "title": "Question-Answering Model", "selftext": "Hey guys! I am a bit new to NLP and Question-Answering in general. How would one create a Question-Answering model on a very specific domain? I know that there are ways to train a given model (SimpleTransformers for example) but I was wondering what you guys would suggest for such a task.", "upvote_ratio": 1.0, "id": "t3_q19000", "created_utc": 1633365024.0}
{"sub": "LanguageTechnology", "title": "Entity extraction from videos?", "selftext": "Hi all,\n\nI am working on a recommendation engine which suggests the most likely related video(s) for a given news article. There is little to no metadata outside a video title so the approach that I am considering is automatically transcribing the video and then performing entity extraction on the transcript, performing the same entity extraction on the article text and comparing the two. \n\nMy worry is entity extraction will be impacted negatively by noisy transcription. Does anyone have any recommendations on NER from messy data or as to whether my approach to the problem of linking relevant videos to articles is with relative merit? Thanks", "upvote_ratio": 1.0, "id": "t3_q16m99", "created_utc": 1633357993.0}
{"sub": "LanguageTechnology", "title": "I just released a \"Youtube name generator\" over the weekend by training a massive neural network", "selftext": "nan", "upvote_ratio": 0.67, "id": "t3_q157ds", "created_utc": 1633353287.0}
{"sub": "LanguageTechnology", "title": "creating a dataset for summerization", "selftext": "I'm creating a dataset for summerization and I have crawled 100k articles and summaries from 10 news sites.\n\nobviously there are some articles that are not good for the task. for example : article is  too short.\n\nwhat other requirements do you recommend so that I can filter out the bad ones.", "upvote_ratio": 1.0, "id": "t3_q13oyo", "created_utc": 1633347516.0}
{"sub": "LanguageTechnology", "title": "NLP applications using Statistical Methods", "selftext": "I am a novice in NLP. I have started reading HMM approach to Part of Speech Tagging and I am enjoying this ! I could really make use of some NLP techniques that invoke statistical methods to solve interesting problems.\n\nI consider that I have a pretty solid statistical and mathematical background, so I won't shy away from possibly very 'involved' approaches. Cheers !", "upvote_ratio": 0.65, "id": "t3_q0cetn", "created_utc": 1633243219.0}
{"sub": "LanguageTechnology", "title": "Teach Computers to Understand Videos and Text without Labeled Data - VideoClip", "selftext": "nan", "upvote_ratio": 0.78, "id": "t3_q059gv", "created_utc": 1633214980.0}
{"sub": "LanguageTechnology", "title": "Question about scraping unstructured texts using BERT", "selftext": "Hello,  \nFirst of all, I'm a data analyst with some data engineering background as well. I never really studied/worked with ML models...\n\nI am working on a project where I need to extract data from unstructured texts (PDF documents with multiple pages each). I assume it's possible to find the data I'm looking for in the texts. Since I know nothing about the text, and since it is unstructured, I looked into using BERT, pre-trained on the CoQA dataset to answer questions, based on:   \n[https://towardsdatascience.com/question-answering-with-a-fine-tuned-bert-bc4dafd45626](https://towardsdatascience.com/question-answering-with-a-fine-tuned-bert-bc4dafd45626).  \nI get good results from this pre-trained model if I manually locate the paragraph that contains the answer to the question, and let the model predict the answer with that paragraph as input. However, since I don't know in which paragraph the answer is hiding, this is clearly not helping me much. Some ideas I've tried:  \n\n\n* Splitting the text into paragraphs and asking the model to predict an answer for the same question for each paragraph. I assume I'll get the right answer, but I won't know which one it is.... So not really helpful. (I might be able to ask the model to predict again on the outputs from the\u00a0 previous step, seems a bit messy but I'll try).\n* Extracting a list of headers from the text (meaning the title of each paragraph), and asking the model to predict which header's paragraph might contain the answer my question. This method works in some cases, but certainly not good enough.\n\n  \nIs there an elegant method you are familiar with? I'm sure I'm not the first person to try scraping large documents with BERT. Any inputs or ideas are welcome.   \nThanks!", "upvote_ratio": 1.0, "id": "t3_q031pb", "created_utc": 1633207448.0}
{"sub": "LanguageTechnology", "title": "Suggestions on Cool NLP Projects!", "selftext": "Hi all, receiving suggestions on any NLP projects that you may find cool in 2021! Currently brainstorming for an upcoming group project for school. It's an open-ended project where we have to build NLP models.\n\nWhen browsing past student's project choices I realised many of the projects were repetitive (e.g. hate speech detection, sentiment analysis, predicting stock prices). Would love to see if the community has any fresh ideas!\n\nHere are some interesting topics that I've noted down but would love to have more for me to think about. It could be anything, with existing papers or not.\n\n* Detecting personality based on social media\n* Automated essay scoring\n* Resume scoring/analysis\n\n**EDIT:** Thank you everyone for your contributions! Know that I'm looking into each and every one of them. You guys are awesome.", "upvote_ratio": 0.9, "id": "t3_pzvtyh", "created_utc": 1633183685.0}
{"sub": "LanguageTechnology", "title": "Braifun-nlp: A free Natural Language Processing tool to help Researchers brain storm their ideas (Alpha release)", "selftext": "nan", "upvote_ratio": 0.75, "id": "t3_pzrlwl", "created_utc": 1633165073.0}
{"sub": "LanguageTechnology", "title": "Roberta Tokenizer Query", "selftext": "I use roberta-base tokenizer \n\n    tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base',add_prefix_space=True)\n\ntrained on english data to tokenize bengali just to see how it behaves . When I try to to encode a bengali character \n\n    tokenizer.encode('\u09ac\u09be')\n\n, I get \n\n    [0, 1437, 35861, 11582, 35861, 4726, 2]\n\nwhich means that it finds some tokens in it vocabulary which match bengali characters even though train on english. On further exploration I find these are all special characters \n\n    ['&lt;s&gt;', '\u0120', '\u00e0\u00a6', '\u00ac', '\u00e0\u00a6', '\u00be', '&lt;/s&gt;']\n\n. My question is why does it happen, isn't it supposed to output unknown tokens when applied on a new language ? Any help greatly appreciated", "upvote_ratio": 1.0, "id": "t3_pzr72v", "created_utc": 1633162850.0}
{"sub": "LanguageTechnology", "title": "Microsoft AI Unveils \u2018TrOCR\u2019, An End-To-End Transformer-Based OCR Model For Text Recognition With Pre-Trained Models", "selftext": "The problem of text recognition is a long-standing issue in document digitalization. Many current approaches for text recognition are usually built on top of existing convolutional neural network (CNN) models for image understanding and recurrent neural network (RNN) for char-level text generation. There are some latest progress records in text recognition by taking advantage of transformers, but this still needs the CNN as the backbone. Despite various successes by the current hybrid encoder/decoder methods, there is definitely some room to improve with pre-trained CV and NLP models.\n\nMicrosoft research team unveils \u2018[TrOCR](https://arxiv.org/pdf/2109.10282.pdf),\u2019 an end-to-end Transformer-based OCR model for text recognition with pre-trained computer vision (CV) and natural language processing (NLP) models. It is a simple and effective model which is that does not use CNN as the backbone. TrOCR starts with resizing the input text image into 384 \u00d7 384, and then the image is split into a sequence of 16 \u00d7 16 patches used as the input to image Transformers. The research team used standard transformer architecture with the self-attention mechanism on both encoder and decoder parts where word piece units are generated as recognized text from an input image.\n\n# [4 Min Read](https://www.marktechpost.com/2021/10/02/microsoft-ai-unveils-trocr-an-end-to-end-transformer-based-ocr-model-for-text-recognition-with-pre-trained-models/)| [Paper](https://arxiv.org/pdf/2109.10282.pdf) | [Github](https://github.com/microsoft/unilm/tree/master/trocr)", "upvote_ratio": 0.92, "id": "t3_pzqqq9", "created_utc": 1633160500.0}
{"sub": "LanguageTechnology", "title": "Text Classification - Sentiment Classifier without Training Data - Hugging Face NLP", "selftext": "nan", "upvote_ratio": 0.5, "id": "t3_pzgnu3", "created_utc": 1633120872.0}
{"sub": "LanguageTechnology", "title": "How to get access to Wu Dao?", "selftext": "Is there any way to get access to the Chinese language model from BAAI? Or is it proprietary?", "upvote_ratio": 0.86, "id": "t3_pzdb72", "created_utc": 1633110775.0}
{"sub": "LanguageTechnology", "title": "Get list of authors for topic in gensim atmodel", "selftext": "In gensim atmodel  **get\\_author\\_topics(***author\\_name)* returns the topic distribution for the selected author.\n\nIs there any method that given a topic, returns a list of the most probable authors?", "upvote_ratio": 1.0, "id": "t3_pza2yi", "created_utc": 1633101239.0}
{"sub": "LanguageTechnology", "title": "Training GPT-2 with HuggingFace Transformers to sound like a certain author", "selftext": "I'm training a GPT-2 model (transfer learning from a pre-trained model) on \"The Complete Works of HP Lovecraft\", and my goal is to fine tune it to look for certain relationships between words, eventually training it to use the same words and similar relationships to the original stories. The training goal would be this: let's say I break down the call of cthulhu, pt. 1: the horror in clay into what the primary subject is, who the characters are, what actions they performed, and what order they performed the actions in; I'd like for the trained model to match those milestones. what I'm *not* saying is that the story would match the original; rather, the syntax of the story would be the same. does this make sense? Is gpt-2 with huggingface transformers the best way to approach this, or is there some other library I could use? Thanks.", "upvote_ratio": 0.88, "id": "t3_pz9uy1", "created_utc": 1633100586.0}
{"sub": "LanguageTechnology", "title": "Please suggest some papers describing advantages of neural MT over statistical MT", "selftext": "I've seen people write about these in empirical manner - [https://www.tilde.com/about/news/316](https://www.tilde.com/about/news/316) as well as Philipp Koehn's textbooks on NLP. Are there some good research papers that summarize these findings and/or talk of this in a theoretical manner - as to what makes neural MT better than SMT?\n\nThanks!", "upvote_ratio": 0.5, "id": "t3_pz3iwe", "created_utc": 1633076063.0}
{"sub": "LanguageTechnology", "title": "Download Wikipedia Text Dump?", "selftext": "Does anyone know of any script which can be used to pull Wikipedia text data (preferrably the XML dump) for processing?", "upvote_ratio": 0.5, "id": "t3_pz1be3", "created_utc": 1633065706.0}
{"sub": "LanguageTechnology", "title": "[P]AI Biomedical Writer", "selftext": "nan", "upvote_ratio": 0.67, "id": "t3_pyxtes", "created_utc": 1633052418.0}
{"sub": "LanguageTechnology", "title": "Automated conversion of NL into formal logic.", "selftext": "Hi. I'm wondering if anyone is familiar with any work/code that deals with translating natural language into formal logic in particular modal logic/epistemic logic. Thank you!", "upvote_ratio": 1.0, "id": "t3_pyw8f3", "created_utc": 1633046881.0}
{"sub": "LanguageTechnology", "title": "Transformer NLP model similar to GPT-2 345M with nice up-to-date code base and multi-GPU training support?", "selftext": "\n\nI am working on an interactive poetry project and I am searching for a model that would be easy to work with.\n\nI have worked on a previous project that involved a pre-trained version of the 345M GPT-2 model. It delivered great results for our use case. Larger models also worked great, but we opted for this smaller version since we had very limited compute available for inference \u2014 this was a personally-funded web-based application, and server time got expensive very quickly.\n\nI am working on a new project that both gives us the resources to train and fine-tune that model with our chosen datasets (cloud GPUs got really good and inexpensive in recent years!). We need to train it both in French and English. The datasets we have aren\u2019t huge, they have respectively about 60,000 and 8,000 literary pieces, so using a gigantic model wouldn\u2019t really be beneficial. We don\u2019t have as much of a restriction on inference compute here, as long as it can run fine on a decent CPU at a few words per second.\n\nMy initial thought was to simply train the same model, but the code base is somewhat old (not compatible past TensorFlow 1.15), which seems to cause issues with newer Ampere GPUs. It also doesn\u2019t support multi-GPU training. I know there is a TensorFlow 2.0 fork, and I know I could spend a bit of time getting multi-GPU working by splitting batches, but time is short, and I figure there must have been a lot of NLP code written since then.\n\nSo my question is: is there a nice, roughly similarly sized NLP model with a modern codebase you\u2019d recommend for this?", "upvote_ratio": 1.0, "id": "t3_pyuhmk", "created_utc": 1633040945.0}
{"sub": "LanguageTechnology", "title": "How to customize UI", "selftext": "Hello, im planning on creating a naural language processing ui to help me with homework, find information on the web, make calculations, and more. My only question is, can how can I make the voice the UI responds in unique, and not the same as the first siri, or whatever default voice it uses.", "upvote_ratio": 1.0, "id": "t3_pysipu", "created_utc": 1633034798.0}
{"sub": "LanguageTechnology", "title": "Data Analyst seeking to learn Text Analytics", "selftext": "Hi Everyone, \n\nI used an off-the-shelf text and sentiment analysis tool in a previous job. I am an Analyst with SQL and Python (for Data Analysis) skills. I enjoyed text analysis and would like to apply it for use cases in my current job. \n\n\\--Can you please advise if there are any free tools I may use. It looks like there are none! \n\n\\--What should I learn in order to be able to use Python for text analysis. \n\nThanks so much for your time!", "upvote_ratio": 0.92, "id": "t3_pynvzm", "created_utc": 1633020868.0}
{"sub": "LanguageTechnology", "title": "A New NLP book for Transformers!", "selftext": "The Book Mastering Transformers is out!\n\nOur new book Mastering Transformers has been published. In this book, we discuss the transformers revolution:  Not only the introductory topics and the key aspects regarding Transformers, but also advanced topics.  You can build state-of-the-art models from scratch with advanced natural language processing techniques\n\n I'm the co-author :)\n\n[https://www.amazon.com/Mastering-Transformers-state-art-processing/dp/1801077657](https://www.amazon.com/Mastering-Transformers-state-art-processing/dp/1801077657)", "upvote_ratio": 0.87, "id": "t3_pygz96", "created_utc": 1632997606.0}
{"sub": "LanguageTechnology", "title": "Difference b/w Elasticsearch and Retriever", "selftext": "\n\nI'm in the process of documenting a build of an extractive QA pipeline using haystack and elasticsearch.  From my understanding, we first take the corpus and store the documents/contexts from the corpus into a sparse (ie. elasticsearchdocumentstore) or a dense documentstore (ie. FAISS).  Once encoded, the retriever (ie. sparse or dense passage retriever) will perform a similarity search to identity top-n of most relevant documents.  The reader will then predict where in each context the answer is located.  I'm confused where elasticsearch comes into the picture.  I read that elasticsearch is the back-end search engine but isn't the retriever doing the actual searching/similarity calculations.", "upvote_ratio": 1.0, "id": "t3_pychbp", "created_utc": 1632976530.0}
{"sub": "LanguageTechnology", "title": "Fine-tuning pre-trained word vectors to explore word \"meaning\"", "selftext": "Hi everyone!   \n\n\n!Disclaimer: I am a beginner in NLP who finds word embeddings very fascinating!\n\nSay I am interested in the \"meaning\" of some word *w* within a certain corpus. I'd like to explore that meaning by training word embeddings and looking at the nearest neighbors of *w* in the vector space.   \n(1) First of all, would it make sense to do that?  \n\n\nSay, then, that I would like to see the \"meaning\" of *w* in a more general context. (2) Would it also make sense to fine-tune pre-trained word vectors on my corpus? I am wondering if then the meaning of *w* would shift towards something else, which could be an indication that the usage of *w* was sort of biased/different in my corpus.   \n\n\n(3) If all of the above is valid to explore, and it makes sense, could anyone point me to readings/resources to fine-tune pre-trained word vectors? I find plenty of explanations, papers and courses on what word embeddings are and how to generate them, but I can't easily find stuff for fine-tuning.  \n\n\nThanks in advance :)", "upvote_ratio": 1.0, "id": "t3_py4zts", "created_utc": 1632950428.0}
{"sub": "LanguageTechnology", "title": "Baidu AI Research Releases PLATO-XL: World\u2019s First Dialogue Generation (NLP) Model Pre-Trained On 11 Billion Parameter", "selftext": "Artificial intelligence (AI) applications have a significant impact on our daily lives, making them easier. One of such applications is AI bots that are already proven effective in the automation of day-to-day tasks. These bots gather data and even imitate real-time human discussions, allowing humans to focus on more strategic activities.\n\nHowever, having clear, informative, and engaging conversations in the same manner that humans do is difficult for AI bots. Robots must build high-quality open-domain dialogue systems if they are to serve as emotional companions or intelligent assistants. As pre-training technology improves models\u2019 ability to learn from vast amounts of unannotated data, mainstream research concentrates on making better use of massive data to improve open-domain discussion systems.\n\n# [4 Min Read](https://www.marktechpost.com/2021/09/29/baidu-ai-research-releases-plato-xl-worlds-first-dialogue-generation-nlp-model-pre-trained-on-11-billion-parameter/) | [Paper](https://arxiv.org/abs/2109.09519) | [BAIDU Blog](http://research.baidu.com/Blog/index-view?id=163)", "upvote_ratio": 1.0, "id": "t3_py23t7", "created_utc": 1632941866.0}
{"sub": "LanguageTechnology", "title": "Release John Snow Labs Spark-NLP 3.3.0: New ALBERT, XLNet, RoBERTa, XLM-RoBERTa, and Longformer for Token Classification, 50x times faster to save models, new ways to discover pretrained models and pipelines, new state-of-the-art models, and lots more!", "selftext": "nan", "upvote_ratio": 1.0, "id": "t3_pxxkyd", "created_utc": 1632928784.0}
{"sub": "LanguageTechnology", "title": "Looking for best way to do embedding search in production", "selftext": "Hi all,\nI came across with one problem of finding similar documents in a set of huge corpus. Looking for your help to figure out best possible solution.\n\nWhat I am looking for is, given a new document I want to retrieve similar documents based on the semantic similarities from a collection of documents (millions, billions in number)\n\nCurrently I am looking at the pre-computation of all the documents in corpus and store it somehow(maybe elastic search). Now whenever a new document comes, calculate embedding and find similar documents (with some threshold). Now since documents are huge in number and for every new document I have to calculate similarity with all documents which is way too time taking. So looking for a way to reduce complexity and latency. (Results should be achieved in less than a second)\n\nHelp me, if you guys know anything similar or how should I proceed with such problem.", "upvote_ratio": 1.0, "id": "t3_pxro46", "created_utc": 1632907159.0}
{"sub": "LanguageTechnology", "title": "Google AI Introduces Translatotron 2 For Robust Direct Speech-To-Speech Translation", "selftext": "The Natural Language Processing (NLP) domain is experiencing remarkable growth in many areas, including search engines, machine translation, chatbots, home assistants and many more. One such application of S2ST (speech-to-speech translation) is breaking language barriers globally by allowing speakers of different languages to communicate. It is therefore extremely valuable to humanity in terms of science and cross-cultural exchange.\u00a0\n\nAutomatic S2ST systems are typically made up of a series of subsystems for speech recognition, machine translation, and speech synthesis. However, such cascade systems may experience longer latency, information loss (particularly paralinguistic and non-linguistic information), and compounding errors between subsystems.\n\nGoogle\u2019s recent study presents the improved version of Translatotron, which significantly enhances performance. [Translatotron 2](https://arxiv.org/abs/2107.08661) employs a new way for transferring the voices of the source speakers to the translated speech. Even when the input speech involves numerous speakers speaking in turn, the updated technique to voice transference is successful while also decreasing the potential for misuse and better complying with our AI Principles.\u00a0\n\n# [5 Min Read](https://www.marktechpost.com/2021/09/28/google-a-introduces-translatotron-2-for-robust-direct-speech-to-speech-translation/) | [Paper](https://arxiv.org/abs/2107.08661) | [Google AI Blog](https://ai.googleblog.com/2021/09/high-quality-robust-and-responsible.html)", "upvote_ratio": 1.0, "id": "t3_pxn8zo", "created_utc": 1632887418.0}
{"sub": "LanguageTechnology", "title": "Loss stuck. Model for speech-to-text system", "selftext": " \n\nI\u2019m trying to build a speech-to-text system my data is (4 - 10 seconds audio wave files) and their transcription (preprocessing steps are char-level encoding to transcription and extract mel-Spectrograms from audio files). this is my model architecture is ( a 3 conv1d layers with positional encoding to the audio file - embedding and positional encoding to encoded transcription and then use those as input to transformer model and lastly a dense layer) the loss function is cross entropy and optimizer is Adam.\n\nthe problem is that the loss is always stuck at some point it starts around 3.8 (I have 46 classes) and after some batches it decreases to (e.g. 2,8) and stuck their. it bounces around that value and never decrease again. I tried changing parameters of the model, I\u2019ve changed the optimizer and learning rate always result the same problem.\n\nI don\u2019t understand what I\u2019m doing wrong \n\n[Training Loss](https://i.stack.imgur.com/1q8Jc.png)", "upvote_ratio": 0.78, "id": "t3_pxbkw1", "created_utc": 1632850784.0}
{"sub": "LanguageTechnology", "title": "Using NLP to parse and analyse cooking recipes.", "selftext": "Hey everyone, I'm a intermediate programmer with an interest but no experience in Natural Language Processing and I was hoping to get some guidance.\n\nI'm trying to write a command-line program that takes plain text files of recipes and returns an analysis of potential typos in weight, volume, temperature, time, etc. For example, if a given recipe says to bake for 45 seconds instead of minutes.  \n\nI should also be able to query the recipe for things like \"well-cookedness\" where (given the previous example), the program would identify that the recipe produces 'uncooked' or 'undercooked' results. \n\nI was hoping to do all of the work in Python and I read that Python's default NLP library, the Natural Language Toolkit (NLTK) would be a good place to start.\n\nI am ready to learn everything as I go along but I'm hoping for guidance on the overall process of implementing such a project. Please forgive me if the following questions sound stupid \ud83d\ude05:\n\n* Is there an NLP library I should use instead of or in addition to Python's NLTK?\n* What recommended AI or NLP techniques should I research and implement for a program like this?\n* What would be the main stages of this program? From text analysis straight to querying data or are there some intermediate steps?\n\nThank you for reading up to this point and for any advice!", "upvote_ratio": 0.77, "id": "t3_px9ifn", "created_utc": 1632844877.0}
{"sub": "LanguageTechnology", "title": "OpenAI\u2019s New Machine Learning Model Can Summarize Any Size Book with Human Feedback", "selftext": "OpenAI has developed a[ new model to study the alignment problem of machine learning](https://arxiv.org/pdf/2109.10862.pdf). This model can summarize books of any length by creating summaries of each chapter. Yes, you heard it right; OpenAI\u2019s new machine learning model can summarize the entire book.\n\nThe proposed machine learning model summarizes a small part of the book and then summarizes these summaries to obtain a higher-level overview. This research has been done as an empirical study on scaling correspondence problems which is usually tricky for AI algorithms because they require complex input text or numbers that have not yet been trained.\n\n# [3 Min Read](https://www.marktechpost.com/2021/09/27/openais-new-machine-learning-model-can-summarize-any-size-book-with-human-feedback/) | [Paper](https://arxiv.org/pdf/2109.10862.pdf) | [OpenAI Blog](https://openai.com/blog/summarizing-books/)", "upvote_ratio": 0.92, "id": "t3_pwvj3s", "created_utc": 1632792589.0}
{"sub": "LanguageTechnology", "title": "PLATO-XL: Exploring the Large-scale Pre-training of Dialogue Generation", "selftext": "Abstract: To explore the limit of dialogue generation pre-training, we present the models of PLATO-XL with up to 11 billion parameters, trained on both Chinese and English social media conversations. To train such large models, we adopt the architecture of a unified transformer with high computation and parameter efficiency. In addition, we carry out multi-party aware pre-training to better distinguish the characteristic information in social media conversations. With such designs, PLATO-XL successfully achieves superior performances as compared to other approaches in both Chinese and English chitchat. We further explore the capacity of PLATO-XL on other conversational tasks, such as knowledge grounded dialogue and task-oriented conversation. The experimental results indicate that PLATO-XL obtains state-of-the-art results across multiple conversational tasks, verifying its potential as a foundation model of conversational AI.\n\nPaper link: [https://arxiv.org/abs/2109.09519](https://arxiv.org/abs/2109.09519)", "upvote_ratio": 1.0, "id": "t3_pwovgu", "created_utc": 1632771883.0}
{"sub": "LanguageTechnology", "title": "BERT fine-tuning techniques", "selftext": "Hello everyone,\n\nI am currently in the process of fine-tuning BERT for a classification problem using a small dataset.\n\nI came across this article stepping through a tutorial on how to do so.\n https://www.analyticsvidhya.com/blog/2020/07/transfer-learning-for-nlp-fine-tuning-bert-for-text-classification/\n\nOne area I was curious about in the article was the brief discussion in techniques. They discussed training the entire architecture, freeze some layers or freeze the entire architecture.\n\nCan anyone here help point me in a direction to learn more about each technique? \n\nMore specifically, what the pros and cons?\nWhen to apply them in practice?\nAnd are these the only ones?\n\nThank you!", "upvote_ratio": 1.0, "id": "t3_pwj8bi", "created_utc": 1632755742.0}
{"sub": "LanguageTechnology", "title": "STS-B Glue", "selftext": "Hi guys has anyone used STS-B before (it\u2019s one of the glue benchmark tests). I\u2019m not really sure how to evaluate my model. The gold labels are human scores between 0-5 and correspond to how similar two sentences are. I have a model which returns vector representations of two sentences. I then compute the cosine similarity and scale the result to be between 0 and 5 by doing ((res+1)/2)*5 but that just seem wrong. Does anyone have any experience with this? Any pointers would be greatly appreciated!", "upvote_ratio": 1.0, "id": "t3_pwijwb", "created_utc": 1632753798.0}
{"sub": "LanguageTechnology", "title": "Classify short sentences into 6 different classes using BERT pretrained model", "selftext": "How can I train the bert pretrained model with a custom dataset that I have in the .xlsx format? The training data has 2 columns, an input column and a class column.", "upvote_ratio": 0.78, "id": "t3_pwc0ao", "created_utc": 1632726856.0}
{"sub": "LanguageTechnology", "title": "[R] Compressing Large-Scale Transformer-Based Models: A Case Study on BERT", "selftext": "Hi all,\n\nWe have released a survey on current SOTA in BERT model compression. We do a thorough study of various components of BERT-like Transformer models, collect various compression methods in literature and finally provide our insights on future research directions. The paper was recently **published by TACL.**\n\nYou can find the paper at -&gt;\n\n[https://direct.mit.edu/tacl/article/doi/10.1162/tacl\\_a\\_00413/107387/Compressing-Large-Scale-Transformer-Based-Models-A](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00413/107387/Compressing-Large-Scale-Transformer-Based-Models-A)\n\nHopefully, this can help new NLP researchers get a better understanding of the field. We welcome your feedback.", "upvote_ratio": 0.94, "id": "t3_pw9c7a", "created_utc": 1632715497.0}
{"sub": "LanguageTechnology", "title": "Best open source solution to automatically shorten product titles to 60 characters or fewer", "selftext": "Hi everyone!\n\nI need an open source solution that could help me automatically shorten tens of thousands of product titles from 100-200 characters to 60 characters or fewer. Is such a miraculous solution available to the poor and uneducated like myself (or even to others more fortunate)?\n\nThanks a lot!", "upvote_ratio": 1.0, "id": "t3_pw6plw", "created_utc": 1632705548.0}
{"sub": "LanguageTechnology", "title": "Struggling with understanding pytorch model code. I need to train this model, but I literally don\u2019t understand how it works (haven\u2019t worked with pytorch previously). Any tips or resources I can get on where I could start from the basics?", "selftext": "Title. I\u2019ve just gotten a new position and my group threw a bunch of code (like, literally) and told me to train this model. Have no idea where to start. I\u2019d like to start from the basics and learn more on pytorch training. First time doing this sort of work, used TF and keras before. Any resources on where I should start?\n\nBtw: people I am working with are PhDs and adults who have a ton of experience in NLP, and I\u2019m a high schooler who has to learn all of this by myself in the next 3 days.", "upvote_ratio": 1.0, "id": "t3_pw5k9l", "created_utc": 1632701339.0}
{"sub": "LanguageTechnology", "title": "Gothenburg vs Uppsala Masters", "selftext": "What\u2019re the reputations like for these programs? I\u2019m currently doing my undergrad in Linguistics and a minor in CS. I\u2019m a junior so I\u2019m trying to figure out some good options. I know Edinburg is good, what about some other schools?", "upvote_ratio": 0.81, "id": "t3_pw2z4x", "created_utc": 1632692311.0}
{"sub": "LanguageTechnology", "title": "Need a mentor for his/her guidance in my NLP project", "selftext": "Hi community! I am in search for a mentor who can guide me on how to approach for a project I want to build. My project is aimed to build a NLP model which can take information about a certain topic/query from various sources and summarises the text in a more understandable manner. The key task is the model takes a query from user and uses Google's search results to extract text from the webpages, understand the semantics and provide a more summarised and understandable output for the searched topic. As I am new to this there might be some assumptions I am making wrong or arbitrary. I don't know how should I approach this problem neither I have worked upon an NLP project before but I can learn and work for it. If anyone can mentor me for this, it'll be great.\nThanks in advance!", "upvote_ratio": 0.67, "id": "t3_pvwdxw", "created_utc": 1632671176.0}
{"sub": "LanguageTechnology", "title": "Training or fine-tuning transformers on weighted sample data", "selftext": "Hi there, I am wondering if there is a way to use weights (e.g. upvotes/downvotes) into the fine-tuning of GPT-2 or a different NLP algorithm.  In other words, the higher the human rating given to a sample in the corpus, the more influence it should have on the fine-tuned model.  I apologize in advance if this is very basic functionality that I'm just not aware of!", "upvote_ratio": 0.81, "id": "t3_pvvyjc", "created_utc": 1632669816.0}
{"sub": "LanguageTechnology", "title": "Indox - text summarization engine", "selftext": "Hi all!  \nI\u2019ve developed a cutting-edge summarization engine and want to start a company that will provide AI services to customers. I dropped an article on medium [https://medium.com/@OlexanderKorenyuk/indox-summarizaton-engine-b2fc49864ddf](https://medium.com/@OlexanderKorenyuk/indox-summarizaton-engine-b2fc49864ddf) .   \n\n\nIf you like, please, look at it, demo area on a website will be very appreciated for a feedback  \nThanks!", "upvote_ratio": 0.78, "id": "t3_pvvor5", "created_utc": 1632668984.0}
{"sub": "LanguageTechnology", "title": "[Hiring] Looking for data scientists with NLP experience in USA", "selftext": "Hi all, \n\nMy team is currently looking for data scientists with NLP experience. The role could potentially be remote from anywhere in the USA. \n\nAlthough the role would involve the usual data science suspects like EDA and ad hoc analysis, there would be a heavy NLP element to the role including custom NER modeling. \n\nIdeal candidate - have industrial data science experience and comfort with messy data. \n\nIf anyone is interested, pls reach out to me.", "upvote_ratio": 0.76, "id": "t3_pvlf10", "created_utc": 1632625053.0}
{"sub": "LanguageTechnology", "title": "Would you say that creating data for relation extraction (RE) is \"harder\" than creating data for named entity recognition (NER)?", "selftext": "Title is the question. Creating labeled data is expensive for _any_ subtask of machine learning, but I'm focused particularly on the two information extraction subtasks of RE and NER.\n\nI'm wondering if it's legitimate to say that \"creating data for RE is harder than that for NER\" since, well, I don't really have any concrete way to prove the difficulty.\n\nI came to wonder this because NER is largely seen by many as a task that's achieved a lot of progress and SoTA NER tools can be used out of the box without any horrendous error cases. Therefore it seems that creating silver standard data for NER is fairly simple (i.e., just use these tools or a SoTA neural model on unlabeled text), but for RE we have to go an extra step. What I mean is that we have to perform NER and then additionally annotate the relation between two entities.\n\nCould you say that creating data for RE is more difficult in this regard? Also, is there any research work out there that touches upon this subject? Thanks!", "upvote_ratio": 1.0, "id": "t3_pveedg", "created_utc": 1632600125.0}
{"sub": "LanguageTechnology", "title": "How will machines understand people? That's how! The Folks\u2019Talks understanding test.", "selftext": "[https://youtube.com/watch?v=mlJakDX\\_93g&amp;feature=share](https://youtube.com/watch?v=mlJakDX_93g&amp;feature=share)", "upvote_ratio": 0.5, "id": "t3_puxtcr", "created_utc": 1632536910.0}
{"sub": "LanguageTechnology", "title": "Facebook AI Unveils Dynatask, A New Paradigm For Benchmarking AI, Enabling Custom NLP Tasks For AI Community", "selftext": "Last year, Facebook AI launched\u00a0[Dynabench\u00a0](https://ai.facebook.com/blog/dynabench-rethinking-ai-benchmarking/)as a first-of-its-kind platform that rethinks benchmarking in artificial intelligence. Now, they are introducing \u2018Dynatask\u2019, a new feature unlocking Dynabench\u2019s full capabilities for the AI community.\n\n[Dynatask](https://ai.facebook.com/blog/dynatask-a-new-paradigm-of-ai-benchmarking-is-now-available-for-the-ai-community/) helps researchers identify weaknesses in NLP models by having human annotators interact with them naturally. Dynatask has developed a new artificial intelligence model benchmarking system that is more accurate and fair than traditional methods. Researchers will be able to utilize the strong capabilities of the Dynatask platform and can compare models on the dynamic leaderboard. This is not limited to just accuracy but includes a measurement approach of fairness, robustness, compute, and memory.\n\nWhen Dynabench was launched, it had four tasks: natural language inference, question answering, sentiment analysis, and hate speech detection. The Facebook AI research team has powered the multilingual translation challenge at Workshop for Machine Translations with its latest advances. Cumulatively these dynamic data collection efforts resulted in eight published papers and over 400K raw examples.\n\n# [5 Min Read](https://www.marktechpost.com/2021/09/24/facebook-ai-unveils-dynatask-a-new-paradigm-for-benchmarking-ai-enabling-custom-nlp-tasks-for-ai-community/) | [Facebook Blog](https://ai.facebook.com/blog/dynatask-a-new-paradigm-of-ai-benchmarking-is-now-available-for-the-ai-community/)\n\n&amp;#x200B;\n\nhttps://reddit.com/link/puv5nd/video/m3lfbzn9ejp71/player", "upvote_ratio": 0.76, "id": "t3_puv5nd", "created_utc": 1632526646.0}
{"sub": "LanguageTechnology", "title": "We are now publishing some downloadable NLP datasets from reddit posts and comments. First subreddits covered are /r/wallstreetbets (25K posts and 1 million comments) and /r/NoNewNormal (120k posts 2.5 million comments) for Aug 2021", "selftext": "nan", "upvote_ratio": 0.98, "id": "t3_putyjx", "created_utc": 1632522237.0}
{"sub": "LanguageTechnology", "title": "A Guide to Building Your First NLP Application to Detect SPAM", "selftext": "nan", "upvote_ratio": 0.72, "id": "t3_puo2hu", "created_utc": 1632503020.0}
{"sub": "LanguageTechnology", "title": "Zero or Few Shot NER on Custom Entity", "selftext": "Hey ya'll, I'm try to get a baseline for how good a zero or few shot approach would be on recognizing a custom entity (in this case job titles in german). I've been skimming through a few papers and see that it's certainly possible to do this, but I haven't seen any out-of-box type code that I could use to get a baseline on how effective it'll be. Anyone have any thought or ideas on how to approach this?", "upvote_ratio": 1.0, "id": "t3_pumxao", "created_utc": 1632499661.0}
{"sub": "LanguageTechnology", "title": "FAISS and the Index Factory - an intro to composite indexes for similarity search", "selftext": "Hi all - I put together [an article and videos](https://www.pinecone.io/learn/composite-indexes/) covering the composite indexes for vector similarity search and how we can implement them in Faiss.\n\nI've done a lot of articles/videos on faiss + vector similarity search recently and I think this has to be the most useful for building good indexes imo!\n\nI hope some of you find it useful, and let me know what you think/if you have questions!", "upvote_ratio": 0.93, "id": "t3_pujbzg", "created_utc": 1632488682.0}
{"sub": "LanguageTechnology", "title": "UBIAI", "selftext": "Today, text annotation tools are one of the most prominent parts of machine learning. Research areas such as search engines, chatbots, sentiment analysis, and virtual assistants require text annotation tools for better training of machine learning models.\n\nThe machine learning industry and AI research require a large amount of annotated data. High-quality annotated data is like a goldmine for them. However, finding and creating this enormous amount of annotated data can be an arduous task, and most of the time, expensive.\n\nFortunately, text annotation tools can help annotate this enormous amount of data in a matter of time. These annotation tools help with named entity recognition annotation, entity extraction, sentiment analysis, relation annotation, document classification, and more.\n\nFind out more here: \n\n[https://ubiai.tools/](https://ubiai.tools/)", "upvote_ratio": 0.33, "id": "t3_pugio5", "created_utc": 1632477362.0}
{"sub": "LanguageTechnology", "title": "Fine-tuning BERT models, alternatives for the last layers?", "selftext": "I'm relatively new to the field of NLP, so excuse me if this is a trivial question.\n\nI'm fine-tuning a BERT model to do sentiment analysis, I have already succeeded. However, I find interesting that all tutorials and notebooks I found use the same layers after the BERT encoder, namely a dropout (sometimes) and a dense layer with the appropriate size for the task.\n\nIt is common to use different architectures for the layers after the encoder, for example, two (or more) dense layers, etc.\n\nThanks for any insight.", "upvote_ratio": 0.93, "id": "t3_pu66z3", "created_utc": 1632435749.0}
{"sub": "LanguageTechnology", "title": "Currently looking for a research internship for my masters thesis", "selftext": "I'm currently writing a letter to companies who will hopefully take me on and give me a project to work on.\n\nThe problem is that I have no idea what I'm interested in because I'm interested in most things to do with NLP / machine learning. I feel like I should just say \"something something transformers, algorithms\". \n\nI feel like it's hard to be specific when I'm asking them to give me a project? Does anyone else have this issue?", "upvote_ratio": 1.0, "id": "t3_pty2mo", "created_utc": 1632412025.0}
{"sub": "LanguageTechnology", "title": "Summarizing multiple documents into one summary", "selftext": "I have found lots of info on summarizing single documents. But what I am looking for is being able to take multiple documents on the same subject and generate one summary that encompasses several different source documents.  \n\nThe next level of this for me would be to highlight the outlier info in the different documents.\n\nHas this been done? Maybe I am searching using the wrong terms to find the info...\n\nAny help is appreciated", "upvote_ratio": 1.0, "id": "t3_ptv48z", "created_utc": 1632403196.0}
{"sub": "LanguageTechnology", "title": "Fine-tuning GPT-J: key takeaways", "selftext": "Hello all,\n\nWe've spent quite some time benchmarking the best fine-tuning techniques for GPT-J at [NLP Cloud](https://nlpcloud.io?utm_source=reddit&amp;utm_campaign=j431103c-ed8e-11eb-ba80-2242ac130007). Finding the best solution was not straightforward and we had to look at things like speed, server costs, ease of development, accuracy of the fine-tuned model... It took time but we ended up with a nice setup (and we are now officially proposing GPT-J fine-tuning + automatic deployment on our platform).\n\nHere are our key takeaways:\n\n* The best methodology seems to be the one from the Mesh Transformer Jax team: [https://github.com/kingoflolz/mesh-transformer-jax/blob/master/howto\\_finetune.md](https://github.com/kingoflolz/mesh-transformer-jax/blob/master/howto_finetune.md)\n* Fine-tuning on GPU is not ideal. Even several GPUs used in parallel with Deepspeed can be very slow. We used 4 GPUs Tesla T4 in parallel, and it took 1h30 to only compute our first checkpoint (+ 80GB of RAM used...), for a training dataset made up of 20k examples. Maybe a GPU A100 would be worth a try.\n* Fine-tuning on TPU is very efficient but it takes a TPU v3 because TPUs v2 are running out of memory. It takes around 15mns, for a training dataset made up of 20k examples, which is really awesome.\n* The overall process is not straightforward as it takes several kind of conversions (converting the datasets to the right format, making a slim version of the model, converting the weights to Transformers...)\n\nIn the end this is worth the effort, because combining fine-tuning and few-shot learning makes GPT-J very impressive and suited for all sorts of use cases. \n\nIf you guys have different feedbacks about GPT-J fine-tuning, please don't hesitate to comment, I would love to have your opinion.\n\nHope you found the above useful!", "upvote_ratio": 0.97, "id": "t3_pttzvk", "created_utc": 1632399661.0}
{"sub": "LanguageTechnology", "title": "Concatenate to LSTM models", "selftext": "\nI'm fairly new to NLP and building a model that takes two sub-models and concatenates them. The dataset has two text input columns and the predictor variable has 3 classes. Below is the code I wrote:\n\nmodel1 = Sequential() model1.add(Embedding(MAX_NB_WORDS,EMBEDDING_DIM,input_length=X1.shape[1])) model1.add(SpatialDropout1D(0.2)) model1.add(LSTM(100,dropout=0.2,recurrent_dropout=0.2))\n\n# Shape &lt;KerasTensor: shape=(None, 100) dtype=float32 (created by layer 'lstm_3')&gt;\n\nmodel2 = Sequential() model2.add(Embedding(MAX_NB_WORDS,EMBEDDING_DIM,input_length=X2.shape[1])) model2.add(SpatialDropout1D(0.2)) model2.add(LSTM(100,dropout=0.2,recurrent_dropout=0.2))\n\n# Shape &lt;KerasTensor: shape=(None, 100) dtype=float32 (created by layer 'lstm_4')&gt;\n\nconcat_layer = Concatenate()([model1.output, model2.output]) dense_layer = Dense(10, activation='relu')(concat_layer) output = Dense(3, activation='softmax')(dense_layer)\n\ninput_1 = Input(shape=(MAX_LEN,)) input_2 = Input(shape=(MAX_LEN,))\n\n# I have set Max_LEN=250 # Both input_1 and input_2 are of shape TensorShape([None, 250])\n\nmodel = Model(inputs=[input_1, input_2], outputs=output)\n\n# When I run the model I get the below error:\nValueError: Graph disconnected: cannot obtain value for tensor KerasTensor(type_spec=TensorSpec(shape=(None, 250), dtype=tf.float32, name='embedding_3_input'), name='embedding_3_input', description=\"created by layer 'embedding_3_input'\") at layer \"embedding_3\". The following previous layers were accessed without issue: []\n\nWhat mistake am I making?", "upvote_ratio": 0.69, "id": "t3_ptgfvf", "created_utc": 1632345128.0}
{"sub": "LanguageTechnology", "title": "Asking for Some Help Regarding a System to Help Facilitate Communication between a Deaf/Hard of Hearing Professor and Students in a Classroom Environment", "selftext": "nan", "upvote_ratio": 0.67, "id": "t3_ptbqlk", "created_utc": 1632329593.0}
{"sub": "LanguageTechnology", "title": "Interpret 3d/2d shape from its text description", "selftext": "I want to make a model that takes a text input such  as \"Make a round ball and a pyramid for me please\" and gives an output \"sphere and cone\" since they are the 3d shapes that are refereed to in the sentence. Any idea I can achieve something like this? Any links that can help me with this task?", "upvote_ratio": 0.81, "id": "t3_ptc0g4", "created_utc": 1632330357.0}
{"sub": "LanguageTechnology", "title": "Pre processing text", "selftext": "I am trying to clean some text from html tags however I cannot manage to remove new lines and slashes. What am I missing?\n\nraw text:\n\n'Is there an easy way to get a list of my blogs that require re-tagging?**\\[\\\\'&lt;div class=\"dm-section-hero--question\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\_\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\_body\"&gt;\\\\\\\\n                                &lt;p&gt;**Most of my blogs have migrated without a primary tag. I can work through them using the list from my profile page, but the further through the list I get the harder it is to keep track of those  I\\*\\*\\\\\\\\\\\\**'ve done and those I haven**\\\\\\\\\\\\\\*\\*'t . Is there an easy way to get a list of my blogs that need re-tagging? That would make the job a whole lot easier...**&lt;/p&gt;&lt;p&gt;**Steve.**&lt;/p&gt;\\\\\\\\n                            &lt;/div&gt;\\\\'\\]**'\n\nwhat I do:\n\n    soup = BeautifulSoup(raw_text)\n    text = soup.get_text()\n    text = re.sub(r'[\\ \\n]{2,}', ' ', text)\n    text = re.sub(r'[\\t\\r\\n]', '', text)\n    text = re.sub(r'\\n', ' ', text)\n    text.replace(\"\\\\n\", \"\")\n\nWhat I get:\n\n\"Is there an easy way to get a list of my blogs that require re-tagging?**\\['\\\\\\\\n** Most of my blogs have migrated without a primary tag. I can work through them using the list from my profile page, but the further through the list I get the harder it is to keep track of those  **I\\\\\\\\\\\\'ve** done and those I **haven\\\\\\\\\\\\'t** . Is there an easy way to get a list of my blogs that need re-tagging? That would make the job a whole lot easier...Steve.**\\\\\\\\n '\\]**\"\n\nWhat I want:\n\n\"Is there an easy way to get a list of my blogs that require re-tagging? Most of my blogs have migrated without a primary tag. I can work through them using the list from my profile page, but the further through the list I get the harder it is to keep track of those I 've done and those I haven't . Is there an easy way to get a list of my blogs that need re-tagging? That would make the job a whole lot easier...Steve.\"", "upvote_ratio": 1.0, "id": "t3_pt6fnm", "created_utc": 1632313508.0}
{"sub": "LanguageTechnology", "title": "Is there any white paper or research paper explaining the architecture of any NLP engine like Dialogflow or LUIS?", "selftext": "I tried to find on Google but couldn't find any research paper related to i design implementation of any NLP engine like Dialogflow,  LUIS etc.\nI would be really thankful if someone could provide.\nBasically I need to complete a POC for designing an NLP engine from scratch.", "upvote_ratio": 0.84, "id": "t3_pt2on7", "created_utc": 1632297274.0}
{"sub": "LanguageTechnology", "title": "Recognition of Resume and onvoice documents", "selftext": "Hello, i need help\n\nI am asked in my internship to detect only invoice and resume documents from large amount of documents that contains numerous types.\n\nI am asked to build a model with NLP, so i should extract text from image or PDF than i begin the process of detection/classification\n\nTo be honest, i don't know from where i can start, i find it difficult task\n\nCan any one help me and put me in the road", "upvote_ratio": 0.99, "id": "t3_psvb0k", "created_utc": 1632268331.0}
{"sub": "LanguageTechnology", "title": "Natural language processing course - Looking for feedback", "selftext": "I\u2019m Sourabh, I lead one of the core Tensorflow teams at Google Brain and worked on data products at Coursera with Andrew Ng. Kaushik Rangadurai, ML Engineer at Facebook and I are leading a live, cohort based course on NLP starting November 1st. [https://corise.com/course/natural-language-processing](https://corise.com/course/natural-language-processing).\n\nWe wanted to share what we\u2019ve learned in machine learning over the years. You can join the first run of the course (capped at about 30 students) below. If you\u2019re open to giving feedback on the class on how we can do better, happy to give a discount.", "upvote_ratio": 0.82, "id": "t3_pssqci", "created_utc": 1632260151.0}
{"sub": "LanguageTechnology", "title": "Catogorize the Data- Topic Modelling algorithm", "selftext": "Team,\n\nI am new to NLP , there is a requirement asked for me to categorize the data, Data which i have is just one column data in excel and these are values are user daily search criteria on google browser.\n\nsimply the search text done on google browser.\n\n&amp;#x200B;\n\nI need to run a LDA (topic mapping algorithm ) on this data , so that the algorithm will classify them into some meaningful categories.\n\nThanks,", "upvote_ratio": 0.4, "id": "t3_psm07i", "created_utc": 1632240839.0}
